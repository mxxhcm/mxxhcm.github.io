---
title: 'Entropy,Cross Entropy, Kullback-Leibler divergence'
date: 2018-12-23 10:54:31
tags:
  - Machine Learning
  - Entropy
  - Cross Entropy
  - K-L散度 
categories: 机器学习
mathjax: true
---

## 乡农熵(Shannon entropy)

### 介绍
这里的熵指的是信息论中的熵，也叫乡农熵(shannon entropy)。信息熵是随机数据源产生信息的平均速率。
熵的度量方式在wekipeida中提到了两个。一个是与每个变量可能的取值相关的信息熵，另一个是只取概率分布的对数。
$$S = - \sum_i P_i lnP_i$$
当事件发生的概率较低时，该事件比高概率事件携带更多“信息”。这种方式定义的每个事件传达的信息量是一个随机变量，其期望值是信息熵。通常，熵是指无序或不确定性。信息熵通常以比特(或者称为shannons),自然单位(nats)或十进制数字(dits，bans或hartleys)来测量。具体的单位取决于用于定义熵的对数的基。
另一种熵的度量方式是只取概率分布的对数，因为它对于相互独立的事件是可加的。例如，投掷公平硬币的熵是1比特，投掷m个硬币的熵是m比特。以比特为单位的时候，如果n是2的指数次方，则需要$log_2n$位来表示一个具有n个取值的变量。如果该变量的n个取值发生的可能性是相等的，则熵等于$log_2n$。如果其中一个值发生的可能性比其他值发生的可能性更高，观察到该值发生的信息量少于观测到一些罕见的结果，即更罕见的事件在观察时提供更多的信息。由于小概率事件发生的可能性更低，因此最终的结果是从非均匀分布的数据接收的熵总是小于或等于$log_2n$。当一个结果肯定发生时，熵为零。熵量化考虑已知源数据的概率分布，而观察事件本身的意义（消息的含义）在这种度量方式的定义中无关紧要。这种度量方式只考虑特定事件发生的概率，因此它封装的信息是有关可能概率分布的信息，而不是事件本身的含义。

### 定义
乡农定义了entropy, 定义离散型随机变量X，其可能取值为${x_1,\cdots,x_n}$，它对应的概率质量函数(probability mass function) P(X)，则熵$H$为：
$$H(X) = E[I(X)] = E[-log(P(X))]$$
其中E是求期望，I是随机变量X的信息上下文, I(X)本身是一个随机变量。
它可以显示写成：
$$H(X) = \sum_{i=1}^nP(x_i)I(x_i) = -\sum_{i=1}^nP(x_i)log_bP(x_i)$$
其中b是自然对数的底，b常取的值为2，e，10，对应的熵的单位是bits, nats，bans。
当$P(x_i)=0$的时候，对应的PlogP的值为$0log_b(0)$, 和极限(limit)是一致的：
$$lim_{p\rightarrow 0_+}plog(p) = 0.$$
#### 连续型随机变量的熵
将概率质量函数替换为概率密度函数，即可得到连续性随机变量的熵：
$$h[f] = E[-ln(f(x))] = - \int_X f(x)ln(f(x))dx.$$

### 示例
抛一枚硬币，已知其正反两面出现的概率是不相等的，求其正面朝上的概率，该问题可以看做一个伯努利分布问题。
如果硬币是公平的，此时得到结果的熵是最大的。这是因为此时抛一次抛硬币的结果具有最大的不确定性。每一个抛硬币的结果会占满一整个bit位。因为
\begin{align\*}
H(X) &= - \sum_{i=1}^n P(x_i)log_bP(x_i)\\
&= - \sum_{i=1}^2\frac{1}{2}log_2\frac{1}{2}\\
&= - \sum_{i=1}^2\frac{1}{2}\cdot(-1)\\
&= 1
\end{align\*}
如果硬币是不公平的，正面向上的概率是p，反面向上的概率是q，$p \ne q$, 则结果的不确定性更小。因为每次抛硬币，出现其中一面的可能性要比另一面要大，减小的不确定性就得到了更小的熵：每一次抛硬币得到的信息都会小于1bit，比如，$p=0.7$时：
\begin{align\*}
H(X) &= -plog_2p - qlog_2q\\
&= -0.7log_20.7 - 0.3log_20.3\\
&= -0.7\cdot(-0.515) - 0.3\cdot(-1.737)\\
&= 0.8816\\
&\le 1
\end{align\*}
极不确定性跟变量的取值个数有关，也跟概率有关。端情况下是正反面一样，那么熵就是0，没有不确定性。

### 解释(rationale)
为了理解$-\sum p_i log(p_i)$的意义，首先定义一个信息函数I代表一个事件i和它的概率$p_i$。
- $I(p)$是单调下降的。
- $I(p) \ge 0$ 信息是非负的
- $I(1) = 0$ 一定发生的事件没有给出信息 
- $I(p_1p_2) = I(p_1) + I(p_2)$ 独立事件包含的信息是可加的 

最后一条属性很关键，它指出了两个独立信息源的联合分布包含的信息和两个事件分开的信息是一样多的。具体的,比如A事件有m个等可能性的结果，B事件有n个等可能性的结果，AB有mn个等可能性的结果。A事件需要$log_2(m)$bits去编码，B事件需要用$log_2(n)$bits去编码，AB需要$log_2(mn) = log_2(m) + log_2(n)$bits编码。乡农发现了log函数能够保留可加性，即：
$I(p) = log(\frac{1}{p}) = - log(p)$
事实上，这个函数I是唯一的(可以证明),让I当做信息函数，并且是二阶连续可微的。(这里给出了很多公式，需要的话可以看Wikipedia）。
如果一个分布中事件i发生的概率是$p_i$,那么采样N次，事件i发生的次数为$n_i = N p_i$, 所有$n_i$次的信息为$$\sum_in_iI(p_i) = - \sum_iNp_ilog(p_i).$$
每个事件的平均信息就是：
$$-\sum_ip_ilog(p_i)$$

## K-L散度(Kullback-Leibler divergence)
### 介绍
K-L散度也叫相对熵(relative entropy)，是用来衡量一个概率密度函数和另一个参考的概率密度函数的不同。组要应用包括信息论中的相对熵，时间序列的随机性，比较推理模型的信息增益。最简单的情况下，K-L散度为0，代表着两个分布完全一样。

### 定义
#### 离散型随机变量
P和Q定义在相同的概率空间，它们的K-L散度定义为：
\begin{align\*}
D_{KL}(P||Q) &= -\sum_iP(i)log(\frac{Q(i)}{Q(i)})\\
D_{KL}(P||Q) &= -\sum_iP(i)log(\frac{P(i)}{Q(i)})
\end{align\*}
它是概率分布P和Q对数差相对于概率分布P的期望。
#### 连续型随机变量
对于连续性随机变量的分布P和Q，K-L散度被定义为积分：
$$D_{KL}(P||Q) = \int_{-\infty}^{infty}p(x)log(\frac{p(x)}{q(x)})dx,$$
其中p和q代表分布P和分布Q的概率密度函数。
更一般的，P和Q表示是同一个集合X的概率分布，P相对于Q是绝对连续的，从Q到P的K-L散度定义为：
$$D_{KL}(P||Q) = \int_X log(\frac{dP}{dQ})dP$$
上式可以被写成：
$$D_{KL}(P||Q) = \int_X log(\frac{dP}{dQ})\frac{dP}{dQ}dP$$
### 示例

## 交叉熵(cross entropy)


## 参考文献(references)
1.https://en.wikipedia.org/wiki/Entropy_(information_theory)
2.https://en.wikipedia.org/wiki/Cross_entropy
3.https://en.wikipedia.org/wiki/Kullback-Leibler_divergence

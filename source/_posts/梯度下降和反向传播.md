---
title: 梯度下降和反向传播
date: 2019-03-18 15:19:07
tags:
 - 梯度下降
 - 反向传播
 - 机器学习
categories: 机器学习
---

梯度下降和反向传播，他们两个之间的关系？
## 导数，偏导数，梯度，方向倒数
### 导数
定义：
$$f^{'}(x_0) = {\lim_{\Delta x \to 0}}\frac{\Delta y}{\Delta x} = \lim_{\Delta x \to 0}\frac{f(x_0+\Delta x)-f(x_0)}{\Delta x}$$
反映的是函数y=f(x)在某一点处沿x轴正方向的变化率。也能表示在x点处的斜率
### 偏导数
定义：
$$\frac{\partial }{\partial x}f(x,y,z) = \lim_{\Delta x \to 0}\frac{f(x + \Delta x,y,z) - f(x,y,z)}{\Delta x}$$
导数与偏导数本质都是一样的，当自变量的变化量趋于0时，函数值的变化量与自变量变化量比值的极限，偏导数就是函数在某一点上沿坐标轴正方向上的变化率。比如函数f(x,y,z)，f(x,y,z)在某一点处可以分别求对于x，y，z轴正方向的偏导数。
### 方向导数
方向导数是某一点在某一趋近方向上的导数值，是函数在这个方向上的变化率。
定义：三元函数u=f(x,y,z)在点P(x,y,z)沿着l方向(方向角为$\alpha,\beta,\gamma$)的方向导数定义为
$$\frac{\partial f}{\partial l} = \lim_{\rho \to 0}\frac{f(x+\Delta x,y+\Delta y,z+\Delta z)-f(x,y,z)}{\rho}$$
### 梯度
梯度是方向导数中最大的那个向量，这个向量我们就称他为梯度，因为梯度是向量，所以才有梯度上升和下降的说法。梯度方向是函数增长最快的方向，梯度反方向是函数下降最快的方向。

## 梯度下降
神经网络的训练一般是通过定义一个loss函数，然后通过优化这个loss函数，实现神经网络的训练，一般的loss函数主要是定义了训练样本的预测结果和真实结果之间的差异，比如说定义交叉熵等。
至于优化loss函数的方法，就是通过梯度下降法来实现，该算法从任一点开始，沿该点梯度的反方向运动一段距离，再沿新位置的梯度反方向运行一段距离 ...... 如此迭代。解一直朝下坡最陡的方向运动，希望能运动到函数的全局最小点，梯度下降法是寻找函数局部最优解的有效方法（这里说的是局部最优解，而不是全局最优解，但是一般我们遇到的问题都是凸问题，局部最优解就是全局最优解），至于我们为什么不直接进行求解呢，因为计算量太大，如果有几百个参数的话，是不可行的（感觉这里说的不清楚，应该更具体的描述一下）。

## 反向传播算法
使用梯度下降算法的时候，我们需要计算函数的梯度，反向传播算法解释计算神经网络中误差函数梯度的一种方法。


## 参考文献
1.https://zhuanlan.zhihu.com/p/25355758
2.https://www.zhihu.com/question/36301367/answer/142096153
3.http://neuralnetworksanddeeplearning.com/


---
title: 花书第二章
date: 2019-08-22 16:44:26
tags:
 - Deep Learning
 - 线性代数
categories: Deep Learning
---

## 线性代数
这一章介绍一些线性代数的知识

## 标量，向量，矩阵和张量
标量，单个的数，用不加粗小写斜体字母表示。
向量，有序的一维数组，用加粗小写斜体字母表示，向量的每一个元素加下标访问。一般一个$n$维的向量表示为：
$$\mathbf{x} = \begin{bmatrix}x_1\\\\ \cdots x_n\end{bmatrix} = (x_1,\cdots, x_n)^T$$
矩阵：二维数组，用加粗大写斜体字母表示，向量的每个元素需要使用两个索引才能定位。
张量：多维数组，用加粗大写非斜体字母表示，访问一个$k$维的张量，需要使用$k$个索引才能进行定位。

## 向量和矩阵乘法
### 矩阵乘法
矩阵乘法要求两个矩阵的shape满足第一个矩阵的第二维和第二个矩阵的第一维相等。Shape为$m\times n$的矩阵$A$和shape维$n\times p$的矩阵$B$，使用矩阵相乘得到一个$m\times p$的矩阵$C$，计算公式如下：
$$C_{i,j} = \sum_{k=1}^n A_{i,k}B_{k,j}$$

### Hadamard乘法(element-wise乘法)
Hadamrad乘法要求两个矩阵的shape必须相同。Shape为$m\times n$的矩阵$A$和shape维$m\times n$的矩阵$B$，使用矩阵相乘得到一个$m\times n$的矩阵$C$，计算公式如下：
$$C_{i,j} = A_{i,j}B_{i,j}$$

### 向量点乘
向量点乘要求两个向量$x$和$y$的维度相同，点乘的结果可以看成$1\times n$的矩阵和$n\times 1$的矩阵进行矩阵乘法的结果。

### 矩阵乘向量
矩阵乘向量需要满足矩阵的第二维和向量的维度一致，得到新向量的维度和原来的向量维度一样：
$$Ax= b$$

### 其他定律
矩阵乘法有分配率，结合律，但是没有交换律。
转置的一个公式
$$(AB)^T = B^T A^T $$

## 单位矩阵和可逆矩阵
除了对角线上为$1$所有其他位置都为$0$的矩阵被称为单位矩阵，用$I_n$表示。矩阵$A$的可逆矩阵被表示为$A^{-1}$，定位为：
$$A^{-1}A= I_n$$
如果矩阵$A$的逆存在，那么$Ax=b$的解就是$x = A^{-1}b$。

## 线性独立和线性生成子空间（span）
如果$Ax=b$有且只有一个解，那么$A^{-1}$一定存在。但是有时候会没有解或者无穷多个解。不可能存在大于一个但是小于无穷多个解的情况，如果$x,y$是$Ax=b$的解，那么
$\alpha x + (1 - \alpha) y$也一定是$Ax=b$的解。
**线性组合**：对于向量集中的每个向量乘上一个系数再相加得到的结果就是一个线性组合。最简单的形式：$ax+by$就是一个线性组合，它的结果就是一个span（线性生成子空间）。$Ax$也是一个线性组合，它有一个特殊的名字，叫做$A$的column space，如果$b$在$A$的colum中，那么这个方程组有且只有一个解。


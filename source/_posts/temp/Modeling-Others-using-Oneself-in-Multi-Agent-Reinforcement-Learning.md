---
title: Modeling Others using Oneself in Multi-Agent Reinforcement Learning
date: 2019-01-29 13:19:33
tags:
 - 强化学习
 - 深度学习
categories: 强化学习
mathjax: true
---

## 摘要
我们考虑使用不完全信息的多智能体强化学习问题，每个智能体的目标是最大化自身的效用。奖励函数取决于两个智能体的隐藏状态（或者目标），每一个智能体必须从它观察到的行为中推断出其他玩家的隐藏目标从而完成任务。我们提出了一种新的方法在这些领域中进行学习：自我其他建模（SOM），智能体使用自己的策略来预测其他智能体的动作并实时更新其他智能体隐藏状态的置信度。我们在三个不同的任务上对该方法进行了评估，结果表明智能体无论在合作还是对抗环境中都能使用他们对其他玩家隐藏状态的估计来学习到更好的策略。

## 引言
在多智能体系统中推理其他智能体的意图并预测它们的行为是很重要的，这些智能体可能有不同的甚至是竞争的目标集。由于多智能体系统的不稳定性，这仍然是一个非常具有挑战性的问题。
在本文中，我们介绍了一种从其他智能体的行为中估计对应的未知的目标和并利用这些估计的目标选择动作的新方法。我们证明了在本文提到的任务中，在游戏中显式的对其他玩家进行建模比将其他智能体看做环境的一部分会有更好的性能。我们将问题定义为双人随机游戏，也叫双人马尔可夫游戏，其中环境对于智能体是完全可见的，但是没有关于其他智能体目标的明确知识而且没有沟通信道。每个智能体在回合结束时收到的奖励取决于两个智能体的目标，因此是每个智能体最优的策略都必须考虑到所有智能体的目标。
认知科学研究表明，人类维持与他们联系的其他人的模型，这些模型用来捕捉那些人的目标，信仰或偏好。在某些情况下，人类利用自己的心理过程来模拟他人的行为。这使他们能够理解其他人的意图或动机，并能在社交场合采取相应的行动。受这些研究的启发，关键想法是要理解游戏中其他玩家正在做什么，智能体应该问自己“如果我扮演另一个玩家的角色，我的目标是什么？”。我们通过使用一个多层循环神经网络参数化智能体的动作和值函数来实现这个想法，该神经网络将状态和目标作为输入。当智能体玩游戏时，它通过直接使用自己的动作函数优化目标来最大化对方行动的可能性，从而推断出其他智能体的未知目标。

## 方法
**背景** 两个智能体的马尔可夫游戏由描述所有智能体的可能配置的一组状态集合$S$，两组动作集合$A_1$，$A_2$和两个智能体的观察$O_1$，$O_2$以及转换函数$\Tau$：$S\times A_1 \times A_2 \rightarrow S$作为当前状态和动作的函数给出下一个状态的概率分布。每个智能体$i$通过从随机策略$\pi_{\theta_i}:S\times A_i\rightarrow [0,1]$中采样选择动作。每个智能体都有一个奖励函数，它取决于智能体的状态和动作：$r_i：S\times A_i\rightarrow R$。每个智能体$i$试图最大化自己的总预期收益$R_i = \sum_{t =0}^T\gamma^tr_i^t$，其中$\gamma$是折扣因子，$T$是时间范围。在本文中，我们考虑了合作以及竞争环境。
接下来介绍自我其他模型（SOM），这是一种在一个回合内以实时方式推断其他智能体的目标并使用这些估计来选择动作的新方法。为了决定一个动作并估计一个状态的值，我们使用一个神经网络$f$将它自己的目标$z_{self}$，另一个玩家的估计目标$\hat{z}_{self}$，并且他自己的角度的观察状态$s_{self}$作为输入，输出动作$\pi$的一个概率分布和值估计$V$，即对于每个玩游戏的智能体，有： 
$$\begin{bmatrix}\pi^i\\V^i\end{bmatrix}=f^i(s_{self}^i,z_{self}^i,\hat{z}_{other}^i;\theta^i)$$
其中$\theta_i$是智能体$i$的神经网络$f$的参数，包括一个softmax层输出策略，一个线性层输出值函数，所有非输出层是共享的。动作是从策略$\pi$中采样得到的。观察状态$s_{self}^i$包含$f^i$智能体的位置，以及其他智能体的位置。每个智能体都有两个网络（为了简洁，省略了智能体上标$i$），一个计算它自己的动作和值函数，一个计算其他智能体的估计值，如下：
\begin{equation}
f_{self}(s_{self},z_{self},\hat{z}_{other};\theta_{self})
\end{equation}
\begin{equation}
f_{other}(s_{other},\hat{z}_{other},z_{self};\theta_{self})
\end{equation}
这两个网络使用的方式不同：$f_{self}$用于计算智能体自己的行为和价值，并以前馈方式运行。给出其他智能体观察到的动作，智能体使用$f_{other}$通过优化$\hat{z}_{other}$推断其他智能体的目标。
我们建议每个智能体使用自己的策略模拟其他玩家的行为，这样$f_{other}$的参数与$f_{self}$的参数是相同的。但请注意，两个网络的输入$z_{self}$和$\hat{z}_{other}$的相对位置不同。另外，由于环境是完全可观测的，两个智能体的观察状态的不同仅通过地图上智能体的身份体现出来（即，每个智能体将能够区分其自己的位置和另一个智能体的位置）。因此，在acting模式下，$f_{self}$网络将$s_{self}$作为输入；在推理模式下，$f_{other}$网络将$s_{other}$作为输入。在游戏的每一步，智能体需要推理$\hat{z}_{other}$将其作为(1)的输入并选择其动作。为了实现这个目的，在每一步中，智能体观察另一个智能体采取的行动，并且在下一步中，智能体使用先前观察到的另一个智能体的动作作为监督信号，使用式子(2)反向传播并优化其$\hat{z}_{other}$，如图1所示。
推理过程优化器中采取的步数是一个可根据游戏的不同而变化的超参数。因此，在游戏的每一步中其他智能体的目标估计$\hat{z}_{other}$会被更新多次。参数$\theta_{self}$在每个回合结束时使用和带有智能体获得的奖励信号的Asynchronous Advantage Actor-Critic（A3C）进行更新。
算法1给出了一个回合内训练SOM智能体的伪代码。这里考虑的所有任务的目标都是离散的，智能体的目标$\hat{z}_{self}$被
表示独热向量，维度是智能体目标所有可能的情况数。另一个玩家的目标嵌入$\hat{z}_{other}$有相同的维度。为了估计经过离散而不可微的变量$\hat{z}_{other}$的梯度，我们用Gumbel-Softmax分布上的一个可微样本$\hat{z}_{other}^G$代替它。这种重新参数化技巧被证明可以有效地产生低方差偏置的梯度。使用该方法在每一步优化过$\hat{z}_{other}$之后，$\hat{z}_{other}$通常偏离独热向量。在下一步中，$f_{self}$将对应于先前更新的$z_{other}$ argmax的一个独热向量量$\hat{z}_{other}^OH$作为输入。
智能体的策略由长短期记忆（LSTM）单元参数化，以及两个全连接的线性层和指数线性单元（ELU）激活函数。神经网络的权重用半正交矩阵初始化。
由于$f_{other}$的循环性，当推理步数$\gt 1$时必须特别小心。在这种情况下，在游戏的每一步中，我们在推理模式中的第一次前向传播之前保存$f_{other}$的循环状态，并且在每个推理步骤将循环状态初始化为此值。这个过程可以确保在动作和推理模式下$f_{other}$可以展开相同数量的步骤。

## 相关工作
不完全信息的游戏中对手建模一直在被广泛研究。但是，大多数以前的方法都侧重于研究特定领域内的概率先验或参数化策略的模型。相比之下，本文的工作为对手建模提出了一个更通用的框架。给定比赛历史，Davidson使用MLP预测对手的动作，但是智能体无法实时适应对手的行为。Lockett等人设计了一种神经网络结构，通过在给定的一组主要对手上学习权重的值来识别对手类型。然而，游戏并没有在强化学习框架内展开。
大量多智能体深度强化学习的研究中侧重于部分可见的，完全合作和紧急通信等环境。本文不允许智能体之间进行任何沟通，因此玩家必须利用他们观察到的行为间接推理他们对手的意图。作为对比，Leibo等考虑半合作多智能体环境，智能体根据任务类型和奖励结构制定合作和竞争策略。类似地，Lowe等人提出了一种集中AC框架，用于在具有混合策略的环境中进行高效的训练。 Lerer和Peysakhovich通过将针锋相对的著名游戏理论策略推广到多智能体马尔可夫游戏，设计了能够在复杂社会困境中保持合作的强化学习智能体。最近认知科学方面的工作试图通过使用分层的社会智能体模型来理解人类的决策，它能推断出其他人类智能体的意图，从而决定是否采取合作或竞争策略。然而，这些论文都没有设计出能够显式模拟环境中其他人工智能体或者估计他们意图的算法来改善智能体的决策。
逆强化学习领域也与本文考虑的问题有关。逆强化学习的目的是通过观察智能体的行为来推断智能体的奖励函数。相反，我们的方法使用观察到其他玩家的行为以在线方式直接推断他们的目标，然后在环境的acting模式中由智能体使用。这避免为了估计奖励函数收集其他智能体状态-动作对离线样本的需要，然后使用它来学习最大化该效用的单独策略。最近Hadfield-Menell等的论文也关注推理他人意图的问题，但他们关注的是人机交互和价值调整。在类似目标的推动下，Chandrasekaran等人考虑建立人工智能理论的问题，以改善人工智能交互和人工智能系统的可解释性。为了这个目标，他们展示了可以使用少量示例训练人们预测视觉问答模型的响应。
Foerster等人和He等人的工作与我们的工作最接近。Foerster等人设计强化学习智能体在更新自己的策略时同时考虑到环境中其他智能体的学习。这使得智能体能够发现自私而又协作的策略，例如在迭代囚徒困境中的针锋相对策略。虽然我们的工作没有明确地试图塑造其他智能体的学习，但它的优点是智能体可以在一个回合中更新他们的信念并以在线方式更新策略以获得更多奖励。我们的设置也有所不同，它认为每个智能体都有一些其他玩家所需的隐藏信息，以便最大化其回报。
我们的工作非常符合He等人的工作，作者构建了一个用于在强化学习环境中构建其他智能体的一般框架。He等人提出了一个模型，通过将对手的观察使用DQN进行编码，共同学习一个策略和对手的行为对手。他们的混合专家架构能够在两个纯对抗性任务中发现不同对手的策略模式。我们的工作与He等人的工作之间的一个区别在于，我们的目标不是推断其他智能体的策略，而是专注于显式估计他们在环境中的目标。此外，在这项工作中，智能体不是使用其他智能体动作的人工设计特征，而是根据自己的模型端到端的学习其他智能体模型。另一个区别是，在这项工作中，智能体使用优化推断其他智能体的隐藏状态，而不是通过前馈网络推断其他智能体的隐藏状态。在下面的实验中，我们表明SOM优于He等人的方法。

## 实验
在本节中，我们在三个任务上评估SOM模型：
- 硬币游戏，这是一个完全合作的任务，智能体的角色是对称的。
- 配方游戏，它是对抗的，但具有对称角色。
- 门禁游戏，它是完全合作的，但是两个玩家拥有不对称的角色。

我们将SOM与其他三个baselines以及一个可以访问其他智能体目标的ground truth的模型进行比较。所有任务都是在Mazebase gridworld环境中创建的。

### Baselines
TRUE-OTHER-GOAL（TOG）：我们提供了一个给出的模型性能上限的策略网络，该网络将其他智能体的真正目标$z_{other}$，以及状态特征$s_{self}$和自己的目标$z_{self}$作为输入。因为这个模型可以直接访问其他智能体真正的目标，因此不需要单独的网络来模拟其他智能体的行为。 TOG的结构与SOM的一个策略网络$f_{self}$相同。 NO-OTHER-MODEL（NOM）：我们使用的第一个baseline仅使用观察状态$s_{self}$和自身目标$z_{self}$作为输入。NOM与SOM的一个策略网络$f_{self}$有相同的架构。该baseline没有对其他智能体的显式建模或估计它们的目标。
集成-策略-预测器（IPP）：从NOM的体系结构和输入开始，我们构建了一个更强的baseline IPP，它有一个额外的最终线性层输出另一个智能体下一个动作的概率分布。除了用于训练该网络策略的A3C损失函数，我们还添加交叉熵损失项训练其他智能体的行为的预测。
分离-策略-预测器（SPP）：He等人提出了一个基于DQN的对手建模框架。在他们的方法中，给定对手特有的人工提取的状态信息，训练一个神经网络预测对手的动作。该网络的中间隐藏表示用作Q网络的输入。
我们修改了He等人的模型应用到本文的场景中。特别的，我们使用A3C而不是DQN，我们不使用特定领域的特征表示对手的隐藏状态。
最后产生的SPP模型由两个独立的网络组成，一个策略网络用于决定智能体的动作，一个对手网络用于预测其他智能体的动作。对手网络将世界状态$s$和自己的目标$z_{self}$作为输入，并输出其他智能体在下一步采取动作的概率分布，以及其隐藏状态（由网络的循环给出）。与IPP一样，我们使用其他智能体的真实动作训练对手策略预测器的交叉熵损失。在每一步中，该网络输出的隐藏状态以及智能体观察状态和智能体自身的目标被作为智能体的策略网络的输入。策略网络和对手策略预测器都是与SOM结构相同的LSTM网络。
与SOM作对比，SPP没有显式推断出其他智能体的目标。相反，它通过预测智能体在每个时间步的动作来隐式的构建对手模型。在SOM中，一个参考的目标作为策略网络的附加输入。而在SPP，类似的参考目标是从对手策略预测器得到的隐藏表示，把它作为策略网络的附加输入。
**训练细节**。在我们的所有实验中，我们使用系数为$0.01$的熵，价值损失系数为$0.5$，折扣系数为$0.99$的A3C训练智能体的策略。使用Adam优化智能体商策略的参数，其中$\beta_1= 0.9,\beta_2= 0.999,\epsilonn =1\times 10^{-8}$，权重衰减为$0$。学习率为$0.1$的SGD用于推断另一个智能体的目标，$\hat{z}_{other}$。
硬币和食谱游戏中策略网络的隐藏层维度为$64$，门游戏中为$128$。所有游戏和模型的学习率都是$1\times 10^{-4}$。
观测状态$s$用一些独热向量表示，包括环境中所有物体的位置，以及智能体和另一个智能体的位置。这个输入状态的维度是$1\times n$特征，其中Coin，Recipe和Door游戏的特征数分别为$384$,$192$和$900$。对于每个实验，我们使用5个不同的随机种子训练模型。除非特殊说明，否则论文中展示的所有游戏结果都是每步进行的10次优化更新的结果。

<!--
### 硬币游戏。
首先，我们在一个完全合作的任务上评估模型，在这个任务中，当智能体使用他们两个的目标而不仅仅是他们自己的目标时，他们可以获得更多的奖励。因此，估计其他玩家的目标并在采取行动时使用该信息符合每个智能体人的最佳利益。如图4的左图所示，游戏在8×8网格上进行，该网格包含12个3种不同颜色的硬币（每种颜色4个硬币）。在每集开始时，智能体被随机分配三种颜色中的一种。动作空间包括：上，下，左，右或通过。一旦智能体人踩到硬币，那个硬币就会从网格中消失。游戏在20个步骤后结束（即每个智能体需要10个步骤）。两名特工在比赛结束时收到的奖励由下面的公式给出：
2），其他n其他Cself是自我目标颜色的硬币数量，由其他智能体人收集，而n self Cneither是与自己收集的智能体人目标相对应的硬币数量。对于图4中的示例，智能体1具有Cself =橙色和Cother =青色，而智能体2的Cself是青色而Cother是橙色。对于两种药剂，两者都是红色的。
收集不符合任何智能体人目标的硬币的惩罚的作用是避免收敛到暴力政策，在这种政策中，智能体人可以通过收集其附近的所有硬币而获得不可忽视的奖励金额，而不是关于他们的颜色。为了最大化其回报，每个智能体人需要收集自己的硬币或其合作者的颜色，而不是剩余颜色的硬币。因此，当两个智能体人能够在游戏中尽可能早地高精度地推断其合作者的目标时.
-->
## 讨论
在本文中，我们介绍了一种新方法，用于从其他智能体的行为中推断他们的隐藏状态，并使用这些估计来选择动作。我们证明了智能体能够在合作和竞争环境中估计其他参与者的隐藏目标，这使他们能够收敛到更好的政策并获得更高的回报。在本文提出的任务中，对其他智能体的显式建模比仅仅考虑其他代理成为环境的一部分更好的性能。 	SOM的一个限制是它比其他baseline需要更长的训练时间，因为我们在每一步都进行了反向传播。但是，它的online更新方式对于适应环境中其他智能体的动作变化至关重要。SOM的一些主要优点是简单性和灵活性，它不需要任何额外参数来模拟环境中的其他代理，可以使用任何强化学习算法进行训练，并且可以轻松地与任何策略参数化或网络结构集成。SOM可以适应具有两个以上智能体的环境，因为智能体可以使用自己的策略来模拟任意数量的智能体的动作并推断其目标。而且，它可以很容易地推广到许多不同的环境和任务。
我们计划通过评估更复杂环境中的模型来扩展这项工作，包括两个以上的参与者，混合策略，更多样化的智能体类型（例如具有不同动作空间的智能体，奖励函数，角色或策略），以及假设其他玩家和自己一样的模型偏差。
未来研究的其他重要途径是设计能够适应环境中其他智能体非平稳策略的模型，处理具有分层目标的任务，并在测试时遇到新智能体时表现良好。
最后，许多研究领域可以从拥有其他智能体的模型中受益，这些智能体能够推理其他智能体的意图并预测他们的动作。这些模型可能对人机或师生互动，以及价值对齐问题有恒大帮助。此外，这些方法可用于多智能体任务中基于模型的强化学习，因为前向模型的准确性很大程度上取决于预测其他智能体动作的能力。

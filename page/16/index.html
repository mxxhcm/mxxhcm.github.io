<!DOCTYPE html>












  


<html class="theme-next pisces use-motion" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
































<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=6.6.0" rel="stylesheet" type="text/css">



  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.jpg?v=6.6.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.jpg?v=6.6.0">










<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '6.6.0',
    sidebar: {"position":"left","display":"always","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="记录硕士三年自己的积累">
<meta property="og:type" content="website">
<meta property="og:title" content="mxxhcm&#39;s blog">
<meta property="og:url" content="http://mxxhcm.github.io/page/16/index.html">
<meta property="og:site_name" content="mxxhcm&#39;s blog">
<meta property="og:description" content="记录硕士三年自己的积累">
<meta property="og:locale" content="zh-CN">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="mxxhcm&#39;s blog">
<meta name="twitter:description" content="记录硕士三年自己的积累">



  <link rel="alternate" href="/atom.xml" title="mxxhcm's blog" type="application/atom+xml">




  <link rel="canonical" href="http://mxxhcm.github.io/page/16/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>mxxhcm's blog</title>
  












  <noscript>
  <style>
    .use-motion .motion-element,
    .use-motion .brand,
    .use-motion .menu-item,
    .sidebar-inner,
    .use-motion .post-block,
    .use-motion .pagination,
    .use-motion .comments,
    .use-motion .post-header,
    .use-motion .post-body,
    .use-motion .collection-title { opacity: initial; }

    .use-motion .logo,
    .use-motion .site-title,
    .use-motion .site-subtitle {
      opacity: initial;
      top: initial;
    }

    .use-motion .logo-line-before i { left: initial; }
    .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">mxxhcm's blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
    
      
    

    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>标签</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">

    
    
    
      
    

    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>分类</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>归档</a>

  </li>

      
      
    </ul>
  

  

  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
            

          
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://mxxhcm.github.io/2019/03/02/dqn/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="马晓鑫爱马荟荟">
      <meta itemprop="description" content="记录硕士三年自己的积累">
      <meta itemprop="image" content="/images/favicon.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="mxxhcm's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/03/02/dqn/" class="post-title-link" itemprop="http://mxxhcm.github.io/page/16/index.html">DQN</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-03-02 19:29:35" itemprop="dateCreated datePublished" datetime="2019-03-02T19:29:35+08:00">2019-03-02</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-10-18 17:34:03" itemprop="dateModified" datetime="2019-10-18T17:34:03+08:00">2019-10-18</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/强化学习/" itemprop="url" rel="index"><span itemprop="name">强化学习</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="背景">背景</h2>
<ol>
<li>Atari 2600是一个RL benchmark，有2600个游戏，每个agent会得到一个图像输入(60Hz的210 x 160 RGB视频)。本文的目标是设计一个NN架构尽可能学会更多游戏，网络的输入只有视频信息，reward和terminal信号以及可能采取的action，和人类玩游戏时得到的信息是一样的。</li>
<li>Agent与Atari模拟器不断交互，agent不能观测到模拟器的内部状态，只能得到当前屏幕信息的一个图片。这个task可以认为是部分可观测的，因为仅仅从当前的屏幕图像$x_t$上是不能完全理解整个游戏状况的。所有的序列都认为在有限步骤内是会结束的。</li>
<li>注意agent当前的得分取决于整个sequence的action和observation。一个action的feedback可能等到好几千个timesteps之后才能得到。</li>
<li>agent的目标最大化累计reward。定义$t$时刻的回报return为$R_t = \sum^T_{t’=t} \gamma^{t’-t}r_{t’}$，其中$\gamma$是折扣因子，$T$是游戏终止的时间步。</li>
<li>定义最优的动作值函数$Q^{*}(s,a)$是遵循最优策略在状态$s$处采取动作$a$能获得的最大的期望回报，$Q^{*}(s,a) = \max_{\pi}E[R_t|s_t=s,a_t=a,\pi]$。</li>
<li>最优的动作值函数遵循Bellman optimal equation。如果在下个时间步的状态$s’$处，对于所有可能的$a’$，$Q^{*}(s’,a’)$的最优值是已知的（这里就是对于每一个$a’$，都会有一个最优的$Q(s’,a’)$，最优的策略就是选择最大化$r+Q^{*}(s’,a’)$的动作$a’$：<br>
$$Q^{*}(s,a) = E_{s\sim E}[r+ \gamma \max_{a’} Q^{*}(s’,a’)|s,a], \tag{1}$$<br>
强化学习的一个思路就是使用Bellman optimal equation更新动作值函数，$Q_{i+1}(s,a) = E[r + \gamma Q_i(s’,a’)|s,a]$，当$i\rightarrow \infty$时，$Q_i \rightarrow Q^{*}$。</li>
<li>上述例子是state-action pair很少的情况，当有无穷多个的时候，是无法精确计算的。这时候可以采用函数来估计动作值函数，$Q(s,a;\theta) \approx Q^{*}(s,a)$。一般来说，通常采用线性函数进行估计，当然可以采用非线性的函数，如神经网络等等。这里采用的是神经网络，用$\theta$表示网络的参数，这个网络叫做Q网络，Q网络通过最小化下列loss进行训练：<br>
$$L_i(\theta_i) = E_{s,a\sim \rho(\cdot)}\left[(y_i - Q(s,a;\theta_i))^2\right]\tag{2}$$<br>
其中$y_i = E_{s’\sim E}[r+\gamma \max_{a’}Q(s’,a’;\theta_{i-1})]$是第$i$次迭代的target值，其中$\rho(s,a)$是$(s,a)$服从的概率分布。</li>
<li>注意在优化$L_i(\theta_i)$时，上一次迭代的$\theta_{i-1}$是不变的，target取决于网络参数，和监督学习作对比，监督学习的target和网络参数无关。</li>
<li>对Loss函数进行求导，得到下列的gradient信息：<br>
$$\nabla_{\theta_i}L_i(\theta_i) = E_{s,a\sim \rho(\cdot),s’\sim E}\left[(r+\gamma \max_{a’}Q(s’,a’;\theta_{i-1})-Q(s,a;\theta_i))\nabla_{\theta_i}Q(s,a;\theta_i)\right]\tag{3}$$<br>
通过SGD优化loss函数。如果权重是每隔几个timestep进行更新，并且用从分布$\rho$和环境$E$中采样得到的样本取代期望，就可以得到熟悉的Q-learning算法[2]。(这个具体为什么是这样，我也不清楚，可以看参考文献2)</li>
<li>什么是on-polciy算法：</li>
</ol>
<blockquote>
<p>On-policy methods attempt to evaluate or improve the policy that is used to make decisions, whereas  off-policy methods evaluate or improve a policy different from that used to generate the data.</p>
</blockquote>
<p>Sarsa和Q-learning的区别在于更新Q值时的target policy和behaviour policy是否相同。我觉的是是policy evaluation和value iteration的区别，policy evaluation使用动态规划算法更新$V(s)$，但是并没有改变行为策略，更新迭代用的数据都是利用之前的行为策略生成的。而值迭代是policy evaluation+policy improvement，每一步都用贪心策略选择出最大的$a$更新$V(s)$，target policy（greedy）和behaviour policy（$\varepsilon$-greedy）是不同的。</p>
<h2 id="强化学习需要解决的问题">强化学习需要解决的问题</h2>
<ol>
<li>大量有标记的训练数据。</li>
<li>delayed-reward。这个delay存在于action和reward之间，可以达到几千个timesteps那么远，和supervised learnign中输入和输入之间直接的关系相比要复杂的多。</li>
<li>大多数深度学习算法假设样本之间都是独立的，然而强化学习的一个sequence(序列)通常是高度相关的。</li>
<li>强化学习算法学习到的policy变化时，数据服从的分布通常会改变，然而深度学习通常假设数据服从一个固定的分布。</li>
</ol>
<h2 id="dqn">DQN</h2>
<p>论文名称<a href="https://arxiv.org/pdf/1312.5602.pdf" target="_blank" rel="noopener">Playing Atari with Deep Reinforcement Learning</a></p>
<h3 id="概述">概述</h3>
<p>DQN算法使用卷积神经网络代替Q-learning中tabular的值函数，并提出了几个trick促进收敛。DQN agnet的输入是原始的图片，输出是图片表示的state可能采取的action的$Q$值。</p>
<ol>
<li>dqn是Model-Free的，它直接从环境$E$中采样，并没有显式的对环境进行建模。</li>
<li>dqn是一个online的方法，即训练数据不断增加；offline是训练数据固定。</li>
<li>dqn是一个off-policy算法，target policy 是greedy policy，behaviour policy是$\varepsilon$-greedy policy，target policy和greedy policy策略不同。</li>
<li>DQN是不收敛的。</li>
</ol>
<h3 id="解决方案">解决方案</h3>
<h4 id="experience-replay">Experience replay</h4>
<ol>
<li>DQN使用了experience replay，将多个episodes中的经验存储到一个大小为$N$的replay buffer中。在更新$Q$值的时候，从replay buffer中进行采样更新。behaviour policy是$\varepsilon$-greedy策略，保持探索。target policy是$\varepsilon$ greedy 算法，因为replay buffer中存放的都是behaviour policy生成的experience，所以是off-policy算法。<br>
采用experience replay的DQN和Q-learning算法相比有三个好处，第一个是每一个experience可以多次用来更新参数，提高了数据训练效率；第二个是直接从连续的样本中进行学习是低效的，因为样本之间存在强关联性。第三个是on-policy的学习中，当前的参数决定下一次采样的样本，就可能使学习出来的结果发生偏移。</li>
<li>replay buffer中只存储最近N个experience。</li>
</ol>
<h4 id="data-preprocess">Data preprocess</h4>
<ol>
<li>原始图像是$210\times 160$的RGB图像，预处理首先将它变为灰度图，并进行下采样得到一个$110\times 84$的图像，然后从这个图像中截取一个$84\times 84$的图像。</li>
<li>作者使用预处理函数$\phi$处理连续四张的图像而不是一张，然后将这个预处理后的结果输入$Q$函数。</li>
<li>预处理函数$\phi$是一个卷积神经网络，输入是$84\times 84\times 4$的图像矩阵，经过$16$个stride为$4$的$8\times 8$filter，经过relu激活函数，再经过$32$个stride为$2$的$4\times 4$filter，经过relu激活函数，最后接一个256个单元的全连接层。输出层的大小根据不同游戏的动作个数决定。</li>
<li>$Q$网络的输入是预处理后的图像state，输出是所有当前state可能采取的action的$Q$值。</li>
</ol>
<h3 id="网络结构">网络结构</h3>
<p>输入：[batch_size, 84, 84, 4]<br>
第一个隐藏层：16个步长为$4$的$8\times 8$的filters<br>
第二个隐藏层：32个步长为$2$的$4\times 4$的filters<br>
全连接层：256个units<br>
输出层：softmax</p>
<h3 id="算法">算法</h3>
<p>算法 1 Deep Q-learning with Experience Replay<br>
Initialize replay memory D to capacity N<br>
Initialize action-value function Q with random weights<br>
for episode = $1, M$ do<br>
$\ \ \ \ \ \ \ \ $Initialize sequence $s_1 = {x_1}$ and preprocessed sequenced $\phi_1 = \phi(s_1)$<br>
$\ \ \ \ \ \ \ \ $for $t = 1,T$ do<br>
$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $With probability $\varepsilon$ select a random action $a_t$<br>
$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $otherwise select $a_t = \max_a Q^{∗}(\phi(s_t), a; θ)$<br>
$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $Execute action $a_t$ in emulator and observe reward $r_t$ and image $x_{t+1}$<br>
$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $Set $s_{t+1} = s_t, a_t, x_{t+1}$ and preprocess $\phi_{t+1} = \phi(s_{t+1})$<br>
$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $Store transition $(\phi_t, a_t, r_t, \phi_{t+1})$ in D<br>
$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $Sample random minibatch of transitions $(\phi_j, a_j, r_j, \phi_{j+1})$ from D<br>
$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $Set $y_j = \begin{cases}r_j&amp;\ \ \ \ for\ terminal\ \phi_{j+1}\\r_j+\gamma \max_{a’}Q(\phi_{j+1},a’|\theta)&amp;\ \ \ \ for\ non-terminal\ \phi_{j+1}\end{cases}$<br>
$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $Perform a gradient descent step on $(y_j − Q(\phi_j, a_j|θ))^2$<br>
$\ \ \ \ \ \ \ \ $end for<br>
end for</p>
<h3 id="experiments">Experiments</h3>
<h4 id="datasets">Datasets</h4>
<p>七个Atari 2600 games: B.Rider, Breakout, Enduro, Pong, Q bert, Seaquest, S.Invaders。<br>
在六个游戏上DQN是SOTA，在三个游戏上DQN的表现超过了人类。</p>
<h4 id="settings">Settings</h4>
<ol>
<li>不同游戏的reward变化很大，这里把正的reward全部设置为$1$，把负的reward全部设置为$-1$，reward为$0$的保持不变。这样子在不同游戏中也可以统一学习率。</li>
<li>采用RMSProp优化算法，batch size为$32$，behaviour policy采用的是$\varepsilon$-greedy，在前$100$万步内，$\varepsilon$从$1$变到$0.1$，接下来保持不变。</li>
<li>使用了fram-skip技术，每隔$k$步，agent才选择一个action，在中间的$k-1$步中，保持原来的action不变。这里选择了$k=4$，有的游戏设置的为$k=3$。</li>
<li>超参数设置没有说</li>
</ol>
<h4 id="metrics">Metrics</h4>
<p>每个agent训练$10$ millions帧，replay buffer size是$1$ million。每个epoch进行$50000$个minibatch weight updates或者大约$30$分钟的训练（这里有些不理解）。然后使用$\epsilon$-greedy($\epsilon=0.05$) evaluation $10000$个steps。</p>
<h5 id="average-total-reward">average total reward</h5>
<p>第一个metric是在一个episode或者一次游戏内total reward的平均值。这个metric带有很大噪音，因为policy权值一个很小的改变可能就会对policy访问states的分布造成很大的影响。</p>
<h5 id="action-value-function">action value function</h5>
<p>第二个metric是估计的action-value function，这里作者的做法是在训练开始前使用random policy收集一个固定的states set，然后track这个set中states最大预测$Q$值的平均。尽管缺乏理论收敛保证，DQN看起来还不错。</p>
<h4 id="baselines">Baselines</h4>
<ol>
<li>Sarsa</li>
<li>Contingency</li>
<li>DQN</li>
<li>Human</li>
</ol>
<h3 id="代码">代码</h3>
<p><a href="https://github.com/devsisters/DQN-tensorflow" target="_blank" rel="noopener">https://github.com/devsisters/DQN-tensorflow</a></p>
<h2 id="nature-dqn">Nature DQN</h2>
<h3 id="非线性拟合函数不收敛的原因">非线性拟合函数不收敛的原因</h3>
<ol>
<li>序列中状态的高度相关性。</li>
<li>$Q$值的一点更新就会对policy改变造成很大的影响，从而改变数据的分布。</li>
<li>待优化的$Q$值和target value(目标Q值)之间的关系，每次优化时的目标Q值都是固定上次的参数得来的，优化目标随着优化过程一直在变。<br>
前两个问题是通过DQN中提出的replay buffer解决的，第三个问题是Natura DQN中解决的，在一定时间步内，固定target network参数，更新待network的参数，然后每隔固定步数将network的参数拷贝给target network。</li>
</ol>
<blockquote>
<p>This instability has several causes: the correlations present in the sequence of observations, the fact that small updates to Q may significantly change the policy and therefore change the data distribution, and the correlations between the action-values (Q) and the target values $r+\gamma \max_{a’}Q(s’,a’)$.<br>
We address these instabilities with a novel variant of Q-learning, which uses two key ideas. First, we used a biologically inspired mechanism termed experience replay that randomizes over the data, thereby removing correlations in the observation sequence and smoothing over changes in the data distribution. Second, we used an iterative update that adjusts the action-values (Q) towards target values that are only periodically updated, thereby reducing correlations with the target.</p>
</blockquote>
<h3 id="解决方案-v2">解决方案</h3>
<ol>
<li>预处理的结构变了,CNN的层数增加了一层，</li>
<li>加了target network，</li>
<li>将error限制在$[-1,1]$之间。</li>
</ol>
<blockquote>
<p>clip the error term from the update $r + \gamma \max_{a’} Q(s’,a’;\theta_i^{-} - Q(s,a;\theta_i)$ to be between $-1$ and $1$. Because the absolute value loss function $|x|$ has a derivative of $-1$ for all negative values of $x$ and a derivative of $1$ for all positive values of $x$, clipping the squared error to be between $-1$ and $1$ corresponds to using an absolute value loss function for errors outside of the $(-1,1)$ interval.</p>
</blockquote>
<h3 id="框架和网络结构">框架和网络结构</h3>
<h4 id="框架">框架</h4>
<p>Nature-DNQ的框架如下所示<br>
<img src="/2019/03/02/dqn/nature-dqn.png" alt="ndqn"></p>
<h4 id="网络结构-v2">网络结构</h4>
<p>输入:[batch_size, 84, 84, 4]<br>
三个卷积层，两个全连接层（包含输出层）<br>
第一个隐藏层：$32$个步长为$4$的$8\times 8$filters，以及一个relu<br>
第二个隐藏层：$64$个步长为$2$的$8\times 8$filters，以及一个relu<br>
第三个隐藏层：$64$个步长为$1$的$8\times 8$filters，以及一个relu<br>
第四个隐藏层：$512$个units<br>
输出层：softmax，输出每个action对应的$Q$值</p>
<h3 id="算法-v2">算法</h3>
<p>算法 2 deep Q-learning with experience replay, target network<br>
Initialize replay memory D to capacity N<br>
Initialize action-value function Q with random weights $\theta$<br>
Initialize target action-value function $\hat{Q}$ with weights $\theta^{-}=\theta$<br>
for episode = $1, M$ do<br>
$\ \ \ \ \ \ \ \ $Initialize sequence $s_1 = {x_1}$ and preprocessed sequenced $\phi_1 = \phi(s_1)$<br>
$\ \ \ \ \ \ \ \ $for $t = 1,T$ do<br>
$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $With probability $\varepsilon$ select a random action $a_t$<br>
$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $otherwise select $a_t = \max_a Q^{∗}(\phi(s_t), a; θ)$<br>
$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $Execute action $a_t$ in emulator and observe reward $r_t$ and image $x_{t+1}$<br>
$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $Set $s_{t+1} = s_t, a_t, x_{t+1}$ and preprocess $\phi_{t+1} = \phi(s_{t+1})$<br>
$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $Store transition $(\phi_t, a_t, r_t, \phi_{t+1})$ in D<br>
$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $Sample random minibatch of transitions $(\phi_j, a_j, r_j, \phi_{j+1})$ from D<br>
$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $Set $y_j = \begin{cases}r_j&amp;\ \ \ \ for\ terminal\ \phi_{j+1}\\r_j+\gamma \max_{a’}Q(\phi_{j+1},a’|\theta^{-})&amp;\ \ \ \ for\ non-terminal\ \phi_{j+1}\end{cases}$<br>
$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $Perform a gradient descent step on $(y_j − Q(\phi_j, a_j|θ))^2$ with respect to the network parameters $\theta$<br>
$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $Every $C$ steps reset $\hat{Q} = Q$<br>
$\ \ \ \ \ \ \ \ $end for<br>
end for</p>
<h3 id="experiments-v2">Experiments</h3>
<h4 id="settings-v2">Settings</h4>
<ul>
<li>batch-size: 32</li>
<li>replacy memory size: 1000000 frames</li>
<li>history length: 4</li>
<li>target network update frequency: 10000</li>
<li>discount factor: 0.99</li>
<li>action repeat: 4</li>
<li>paramteter update frequency: 4</li>
<li>learning rate: 0.00025</li>
<li>gradient momentum: 0.95</li>
<li>squared gradient momentum: 0.95</li>
<li>min squared gradient: 0.01</li>
<li>initial exploration: 0.1</li>
<li>final exploration 0.1</li>
<li>final exploration frame: 1000000</li>
<li>replay start size: 50000</li>
<li>no-op max: 30</li>
<li>reward: clipped to [-1, 1]</li>
<li>total train frames: 50 millons frame（实际上是200 millions emulated frames，因为有设置为$4$ frame skip）。</li>
<li>选择random作为一个baseline，因为人类的极限是$10$hz，为了公平起见，random baseline以$10$hz的频率随机选择一个action，atari视频的频率是$60$hz，所以每隔$6$帧，随机选择一个action，在选择action中间的帧中保持这一个action。</li>
</ul>
<h4 id="experiments-v3">Experiments</h4>
<ul>
<li>Average score和average action value<br>
每个training epoch之后进行一次evaluation，记录evaluation过程中average episode reward。总共train $50$million frames，大概有$200$个epoch，也就是一个epoch是$25$万frames，在每个epoch后使用$\epsilon$-greedy($\epsilon =0.05$)策略evaluate $520k$个frames。</li>
<li>Main-Evaluation: Compartion between DQN and other baselines<br>
Baselines有Random play, best linear learner，SARSA，Huamn等。<br>
DQN总共训练了$50$ million frames，replay buffer存放最近的$1$ million framems。在完成训练后，至多执行$30$次no-op，产生随机初始状态，使用$\epsilon$-greedy($\epsilon=0.05$)玩$5$分钟，对多次结果取平均。<br>
Human的数据是在玩家首先进行了$2$个小时训练后，然后玩大约20 episodes，每个episode最长5 min的 average reward。<br>
表格中最后一列还给出了一个百分比，$100\times \frac{\text{DQN score} - \text{random play score}}{\text{human score} - \text{random play score}}$。</li>
<li>Replay buffer和target network的abalation实验<br>
在三个不同的learning rate下使用standard hyperparameters训练DQN $10$ million frames。每隔$250,000$ training frames对每个agent进行$135,000$ frames的validation，记录最高的average episode score。 这些valuation episodes并没有在$5$ min的时候截断，这个实验中Enduro得到的score要比main evaluation中高。这个实验中training的帧数($10$ million frames)要比baseline中training的frames($50$ million frames)少。</li>
<li>DQN和linear function approximator比较<br>
除了function approximator由CNN变成linear的，其他都没有变。<br>
在$5$个validation games上，每个agent在三个不同的learning rates使用标准的参数训练了$10$ million frames。每隔$250000$个training frames对agent进行$135,000$ frames的validation，reported 最高的average episode score。 这些valuation episodes并没有在$5$ min的时候截断，这个实验中Enduro得到的score要比main evaluation中高。这个实验中training的帧数($10$ million frames)要比baseline中training的frames($50$ million frames)少。</li>
</ul>
<h2 id="gorila-dqn">Gorila DQN</h2>
<p>论文名称：<br>
Massively Parallel Methods for Deep Reinforcement Learning<br>
下载地址：<br>
<a href="https://arxiv.org/pdf/1507.04296.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1507.04296.pdf</a></p>
<h3 id="experiments-v4">Experiments</h3>
<h4 id="settings-v3">Settings</h4>
<ul>
<li>网络结构和nature DQN一样。</li>
<li>使用了frame-skip，设置为$4$。</li>
<li>Replay memory是$1$M&gt;</li>
</ul>
<h4 id="evaluation">Evaluation</h4>
<p>Evaluation有两种：</p>
<h5 id="null-op-starts">null op starts</h5>
<p>每个agent在它训练的游戏上evaluated $30$个episodes，每个episode随机的至多执行$30$次no-op之后，评估$5$min的emulator时间($18000$ frames)。然后取这$100$次的平均值。</p>
<h5 id="human-starts">human starts</h5>
<p>Human starts是用来衡量它对于agent可能没有遇到过的state的泛化能力。对于每一个游戏，从一个人类玩家的gameplay中随机取$100$个开始点，使用$\epsilon$-greedy policy玩三十分钟emulator时间（即$108000$frames)。</p>
<h2 id="double-dqn">Double DQN</h2>
<h3 id="dqn中的overestimate问题">DQN中的overestimate问题</h3>
<p>解决overestimate问题，Q-learning中在estimated values上进行了max操作，可能会导致某些更偏爱overestimated value而不是underestimated values。<br>
本文将Double Q-learning的想法推广到了dqn上形成了double-dqn。实验结果表明了overestimated value对于policy有影响，double 会产生好的action value，同时在一些游戏上会得到更高的scores。</p>
<h3 id="contributions">Contributions</h3>
<ol>
<li>解释了在large scale 问题上，Q-learning被overoptimistic的原因是学习固有的estimation errors。</li>
<li>overestimation在实践中是很常见，也很严重的。</li>
<li>Double Q-learning可以减少overoptimism</li>
<li>提出了double-dqn。</li>
<li>double-dqn在某些游戏上可以找到更好的policy。</li>
</ol>
<h3 id="double-q-learning">Double Q-learning</h3>
<p>Q-learning算法计算target value $y$的公式如下：<br>
$$y = r + \gamma \max_a’ Q(s’, a’|\theta_t)\tag{4}$$<br>
在计算target value的时候，使用同一个网络选择和评估action $a’$，这可能会让网络选择一个overestimated value，最后得到一个overoptimistic value estimate。所有就有了double Q-learning，计算公式如下：<br>
$$y = r + \gamma Q(s’, \arg\max_a’ Q(s’,a;\theta_t);\theta’_t)\tag{5}$$<br>
target policy还是greedy policy，通过使用$\theta$对应的网络选择action，然后在计算target value的时候使用$\theta’$对应的网络。<br>
原有的公式可以写成下式，<br>
$$y = r + \gamma Q(s’, \arg\max_a’ Q(s’,a;\theta_t);\theta_t)\tag{6}$$<br>
即选择action和计算target value都是使用的同一个网络。</p>
<h3 id="double-dqn-v2">Double DQN</h3>
<p><img src="/2019/03/02/dqn/double-dqn.png" alt="double-dqn"><br>
Double Q-learnign的做法是分解target action中的max opearation为选择和evaluation。而在Nature-dqn中，提出了target network，所以分别使用network和target network去选择和evaluation action是一个很好的做法，这样子公式就变成了<br>
$$y = r + \gamma Q(s’, \arg\max_a’ Q(s’,a;\theta_t);\theta^{-}_t)\tag{7}$$<br>
和Q-learnign相比，将$\theta’$换成了$\theta^{-}$ evaluate action，target network的更新和nature-dqn一样，过一段时间复制network的参数。</p>
<h3 id="double-q-learning-vs-q-learning">Double Q learning vs Q-learning</h3>
<p>可以在数学上证明，Q-learning是overestimation的，但是double q leraing是无偏的。。。证明留待以后再说。<br>
<todo></todo></p>
<h3 id="网络结构-v3">网络结构</h3>
<p>网络结构和nature DQN一样。</p>
<h3 id="算法-v3">算法</h3>
<p>算法 3: Double DQN Algorithm.<br>
输入: replay buffer $D$, 初始network参数$\theta$,target network参数$\theta^{-}$<br>
输入 : replay buffer的大小$N_r$, batch size $N_b$, target network更新频率$N^{-}$<br>
<strong>for</strong> episode $e \in {1, 2,\cdots, M}$ do<br>
$\qquad$初始化frame sequence $\mathbf{x} \leftarrow ()$<br>
$\qquad$<strong>for</strong> $t \in {0, 1, \cdots}$ do<br>
$\qquad\qquad$设置state $s \leftarrow \mathbf{x}$, 采样 action $a \sim\pi_B$<br>
$\qquad\qquad$给定$(s, a)$，从环境$E$中采样接下来的frame $x_t$,接收reward $r$,在序列$\mathbf{x}$上拼接$x$<br>
$\qquad\qquad$<strong>if</strong> $|\mathbf{x}| \gt N_f$<br>
$\qquad\qquad$<strong>then</strong><br>
$\qquad\qquad\qquad$从$\mathbf{x}$中删除最老的frame $x_{t_min}$<br>
$\qquad\qquad$设置$s’ \leftarrow \mathbf{x}$,添加transition tuple (s, a, r, s 0 ) 到buffer D中，如果$|D| \ge N_r$替换最老的tuple<br>
$\qquad\qquad$采样$N_b$个tuples $(s, a, r, s’) \sim Unif(D)$<br>
$\qquad\qquad$计算target values, one for each of $N_b$ tuples:<br>
$\qquad\qquad$定义$a^{\max}(s’; \theta) = \arg \max_{a’} Q(s’, a’;\theta)$<br>
$\qquad\qquad y_j = \begin{cases}r&amp;\qquad if\ \ s’\ \ is\ \ terminal\\ r+\gamma Q(s’, a^{\max}(s’;\theta);\theta^{-}, &amp;\qquad otherwise\end{cases}$<br>
$\qquad\qquad$利用loss $||y_j − Q(s, a; \theta)||^2$的梯度更新<br>
$\qquad\qquad$每隔$N^{-}$个步骤更新一下target network 参数$\theta^{-}$<br>
$\qquad$<strong>end</strong><br>
<strong>end</strong></p>
<h3 id="experiments-v5">Experiments</h3>
<h4 id="settings-v4">Settings</h4>
<p>Tunned Double DQN，update frequency从$10000$改成了$30000$，训练时$\epsilon$在$1$ millon内从$0.1$退火到$0.01$。Evaluation时是$0.001$。</p>
<h4 id="evaluation-v2">Evaluation</h4>
<p>和Gorila DQN一样，用了两种：no-op和human starts。</p>
<h4 id="training">Training</h4>
<p>在每个游戏上，网络都训练了$200$M frames，也就是$50$M steps。每隔$1$M step进行一次evaluation，从evaluations中选出最好的policy作为输出。</p>
<h4 id="metric">Metric</h4>
<p>提出了一个指标，normalized score，计算公式如下：<br>
$$score_{normalized} = \frac{score_{agent}- score_{random}}{score_{human}-score_{random}}\tag{8}$$<br>
分母是human和random之差，对应$100%$。</p>
<h2 id="prioritized-dqn-per">Prioritized DQN(PER)</h2>
<h3 id="contributions-v2">contributions</h3>
<p>本文提出一种了proritizing experience的框架，在训练过程中多次使用重要的transtions replay进行更新，让训练变得的更有效率。<br>
使用TD-errors作为prioritization mechanism，给出了两种protitization计算方式，提出了一种stochastic prioritization以及importance sampling方法。</p>
<h3 id="prioritized-replay">Prioritized replay</h3>
<p>可以从两个维度上考虑replay memeory的改进，一个是存哪些experiences，一个是使用哪些experiences进行回放。本文是从第二个方向上进行的考虑。</p>
<p>从buffer中随机抽样的方法中，update steps和memory size是线性关系，作者想找一个update steps和memory size是log关系的oracle，但是很遗憾，这是不现实的，所以作者想要找一种比uniform random replay好尽量接近oracle的方法。</p>
<h4 id="prioritizion-with-td-error">Prioritizion with TD-error</h4>
<p>prioritized replay最重要的部分是如何评价每一个transition的重要程度。一个理想的criterion是agent在当前的state可以从某个transition中学到多少。这个measure metric是不确定的，一个替代方案是使用TD error $\delta$，表示how ‘suprising’ 或者upexpected the transition：就是当前的value离next-step bootstrap得到的value相差多少，booststrap就是基于其他估计值进行计算。。这中方法对于incremental,online RL方法，例如SARSA以及Q-learning来说都是很合适的，因为他们会计算TD-error，然后给TD-error一个比例系数用来更新参数。然后当reward是noisy的时候，TD-error效果可能很差。<br>
作者在一个人工设计的环境中使用了greedy TD-error prioritization算法，算法在每次存transition到replay buffer的时候，同时还会存一下该transition最新的TD-error，然后在更新的时候从memory中选择TD-error最大的transition。最新的transition TD-error没有算出来，就给它一个最大的priority，保证所有的experience都至少被看到过一次。<br>
采用二叉堆用实现优先队列，查找复杂度是$O(1)$，更新priorities的复杂度是$O(logN)$。</p>
<h4 id="stochastic-prioritization">Stochastic prioritization</h4>
<p>上述方法有很多问题。第一，每次都sweep整个replay memory的计算量很大，所以只有被replayed的experiences的TD-errors才会被更新。开始时一个TD error很小的transition可能很长一段事件不会被replayed，这就导致了replay buffer的sliding window不起作用了。第二，TD-error对于noise spike很敏感，还会被bootstrap加剧，估计误差可能会是另一个noise。第三，greedy prioritization集中在experiences的一个subset：errors减小的很慢，尤其是使用function appriximation时，这就意味着初始的高error的transitions会被replayed的很频繁，然后会over-fitting因为缺乏diversity。<br>
为了解决这些问题，引入了一个介于pure greedy prioritizaiton以及uniform random sampling之间的stochastic采样方法，priority高的transition有更大的概率被采样，而lowest-priority的transition也有概率被选中，具体的定义transition $i$的概率如下：<br>
$$P(i) = \frac{p_i^{\alpha}}{\sum_kp_k^{\alpha}}\tag{9}$$<br>
$\alpha$确定prioritizaiton的比重，如果$\alpha=0$就是unifrom。</p>
<p>有两种$p_i$的计算方法，一种是直接的proportional prioritization，$p_i = |\delta_i| + \varepsilon$，其中$\varepsilon$是一个小的正整数，确定当$p_i=0$时，该transition仍能被replay；第二种是间接的，$p_i = \frac{1}{rank(i)}$，其中$rank(i)$是所有replay memory中的experiences根据$|\delta_i|$排序后的rank。第二种方法的鲁棒性更好。<br>
在实现上，两种方法都有相应的trick，让复杂度不依赖于memory 大小$N$。Proportional prioritization采用了’sum-tree’数据结构，每一个节点都是它的子节点的children，priorities是leaf nodes。而rank-based方法，使用线性函数估计累计密度函数，具体怎么实现没有细看。</p>
<h4 id="annealing-the-bias">annealing the bias</h4>
<p>因为random sample方法，samples之间没有一点联系，选择每一个sample的概率都是相等的，但是如果加上了priority，就有一个bias toward高priority的samples。IS和prioritized replay的组合在non-learn FA中有一个用处，large steps可能会产生不好的影响，因为梯度信息可能是局部reliable，所以需要使用一个小点的step-size。<br>
在本文中，high-error的样本可能会观测到很多次，使用IS减小gradient的大小，对应于高priority的samples的weight被微调了一下，而对应于低priority的样本基本不变。<br>
weigth的计算公式如下：<br>
$$w_i = (\frac{1}{N}\cdot \frac{1}{P(i)})^{\beta}\tag{10}$$<br>
OK,这里IS的作用有些不明白。。。。<todo></todo></p>
<h3 id="算法-v4">算法</h3>
<p>算法 4<br>
输入: minibatch $k$, 学习率（步长）$\eta$, replay period $K$ and size $N$ , exponents $\alpha$ and $\beta$, budget $T$.<br>
初始化replay memory $H = \emptyset, \Delta = 0, p_1 = 1$<br>
根据$S_0$选择 $A_0 \sim \pi_{\theta}|(S_0)$<br>
<strong>for</strong> $t = 1,\cdots, T$ do<br>
$\qquad$观测$S_t, R_t, \gamma_t$<br>
$\qquad$存储transition $(S_{t−1}, A_{t−1}, R_t , \gamma_t, S_t)$ 到replay memory，以及$p_t$的最大priority $p_t = \max {i\lt t} p_i$<br>
$\qquad$<strong>if</strong> $t ≡ 0$ mod $K$ then<br>
$\qquad\qquad$<strong>for</strong> j = 1 to k do<br>
$\qquad\qquad\qquad$Sample transition $j \sim P(j) = \frac{p_j^{\alpha}}{\sum_i p_i^{\alpha}}$<br>
$\qquad\qquad\qquad$计算importance-sampling weight $w_j = \frac{(N \cdot P(j))^{\beta}}{\max_i w_i}$<br>
$\qquad\qquad\qquad$计算TD-error $\delta_j = R_j + \gamma_j Q_{target} (S_j$, $arg \max_a Q(S_j, a)) − Q(S_{j−1} , A\ {j−1})$<br>
$\qquad\qquad\qquad$更新transition的priority $p_j \leftarrow |\delta_j|$<br>
$\qquad\qquad\qquad$累计weight-change $\Delta \leftarrow \Delta + w_j \cdot \delta_j \cdot \nabla_{\theta} Q(S_{j−1}, A_{j−1})$<br>
$\qquad\qquad$<strong>end for</strong><br>
$\qquad\qquad$更新weights $\theta\leftarrow \theta+ \eta\cdot\Delta$, 重置$\Delta = 0$<br>
$\qquad\qquad$每隔一段时间更新target network $\theta_{target} \leftarrow \theta$<br>
$\qquad$<strong>end if</strong><br>
$\qquad$选择action $A_t \sim \pi_{\theta}(S_t)$<br>
<strong>end for</strong></p>
<h3 id="experiments-v6">Experiments</h3>
<p>两组实验，<br>
一组是DQN和proportional prioritization作比较。<br>
一组是tuned Double DQN和rank-based以及proportional prioritizaiton。</p>
<h4 id="metrics-v2">Metrics</h4>
<p>用的是double dqn提出来的nomalized score，这里在分母上加了绝对值。<br>
主要用的median scores和mean scores。</p>
<h2 id="dueling-dqn">Dueling DQN</h2>
<h3 id="介绍">介绍</h3>
<p>本文作者提出来将dueling网络框架应用在model-free算法上。The dueling architecture能用一个deep model同时表示$V(s)$和优势函数$A(s,a)$，网络的输出将$V$和$A$结合产生$Q(s,a)$。和advantage不一样的是，这种方式在构建时就将他们进行了解耦，因此，dueling architecture可以应用在各种各样的model free RL算法上。<br>
本文的架构是对算法创新的补充，它可以对之前已有的各种DQN算法进行结合。</p>
<h3 id="dueling-network-architecture">dueling network architecture</h3>
<p>这个新的architecture的核心想法是，没有必要估计所有states的action value。在一些states，需要action value去确定执行哪个action，但是在许多其他states，action values并没有什么用。当然，对于bootstrap算法来说，每一个state的value estimation都很重要。<br>
<img src="/2019/03/02/dqn/deuling-dqn.png" alt="dueling-dqn"><br>
作者给出了一个single Q-network的architecture，如图所示。<br>
网络结构和nature-dqn一样，但是这里加了两个fully connected layers，一个用于输出$V$，一个用于输出$A$。然后$A$和$V$结合在一起，产生$Q$，网络的输出和nature dqn一样，对应于某个state的一系列action value。<br>
从$Q$函数的定义$Q^{\pi}(s,a) = V^{\pi}(s)+A^{\pi}(s,a)$以及$Q$和$V$之间的关系$V^{\pi}(s) = \mathbb{E}_{a\sim\pi(s)}\left[Q^{\pi}(s,a)\right] = \pi(a|s)Q^{\pi}(s,a)$，所以有$\mathbb{E}_{a\sim\pi(s)}\left[A^{\pi}(s,a)\right]=0$。此外，对于deterministic policy，$a^{*} = \arg \max_{a’\in A}Q(s,a’)$，有$V(s) = Q(s,a^{*})$，即$A(s,a^{*}) = 0$。<br>
如图所示的network中，一个网络输出scalar $V(s;\theta, \beta)$，一个网络输出一个$|A|$维的vector $A(s,a;\theta, \alpha)$，其中$\theta$是网络参数，$\alpha$和$\beta$是两个全连接层的参数。<br>
根据advantage的定义，可以直接将他们加起来，即：<br>
$$Q(s,a;\theta, \alpha, \beta) = V(s;\theta, \beta) + A(s,a;\theta, \alpha) \tag{11}$$<br>
但是，我们需要知道的一点是，$Q(s, a;\theta, \alpha, \beta)$仅仅是$Q$的一个参数化估计。它由两部分组成，一部分是$V$，一部分是$A$，但是需要注意的是，这里的$V$和$Q$只是我们叫它$V$和$A$，它的实际意义并不是$V$和$A$。给了$Q$，我们可以得到任意的$Q(s, a) = V(s) + A(s,a)$，而$V$和$Q$并不代表value function和advantage functino。<br>
为了解决这个问题，作者提出了选择让advantage为$0$的action，即：<br>
$$Q(s, a; \theta,\alpha, \beta) = V(s; \theta, \beta) + \left(A(s,a;\theta,\alpha) - \max_{a’\in |A|}A(s, a’; \theta, \alpha)\right)\tag{12}$$<br>
选择$a^{*} = \arg \max_{a’\in A} Q(s, a’; \theta, \alpha, \beta) = \arg \max_{a’\in A}A(s, a’;\theta, \alpha)$，我们得到$Q(s,a^{*}; \theta, \alpha,\beta) = V(s;\theta, \beta)$。这个时候，输出$V$的网络给出的真的是state value的估计$V(s;\theta, \beta)$，另一个网络真的给出的是advantage的估计。<br>
另一种方法是用mean取代max操作：<br>
$$Q(s, a; \theta,\alpha, \beta) = V(s; \theta, \beta) + \left(A(s,a;\theta,\alpha)- \frac{1}{|A|}\sum_{a’}A(s, a’; \theta, \alpha)\right)\tag{13}$$<br>
一方面这种方法失去了$V$和$A$的原始语义，因为它们有一个常数的off-target，但是另一方面它增加了优化的稳定性，因为上式中advantage的改变只需要和mean保持一致即可，不需要optimal action’s advantange一有变化就要改变。</p>
<h3 id="算法-v5">算法</h3>
<h2 id="distributed-dqn">Distributed DQN</h2>
<h2 id="noisy-dqn">Noisy DQN</h2>
<h3 id="介绍-v2">介绍</h3>
<p>已有方法的exploration都是通过agent policy的random perturbations，比如常见的$\varepsilon$-greedy等方法。这些方法不能找出环境中efficient exploration的behavioural patterns。常见的方法有以下几种:<br>
第一种方法是optimism in the face of uncertainty，理论上证明可行，但是通常应用在state-action spaces很小的情况下或者linear FA，很难处理non-linearn FA，而且non-linear情况下收敛性没有保证。<br>
另一种方法是添加额外的intrinsic motivation term，该方法的问题是将算法的generalisation mechanism和exploration分割开，即有instrinsic reward和environment reward，它们的比例如何去设置，需要认为指定。如果不仔细调整，optimal policy可能会受intrinsic reward影响很大。此外为了增加exploration的鲁邦性，扰动项仍然是需要的。这些算法很具体也能应用在参数化policy上，但是很低效，而且需要很多次policy evaluation。<br>
本文提出NoisyNet学习网络参数的perturbations，主要想法是参数的一点改变可能会导致policy在很多个timsteps上的consistent，complex, state-dependent的变化，而如$\varepsilon$-greedy的dithering算法中，每一步添加到policy上的noise都是不相关的。pertubations从一个noise分布中进行采样，它的variance可以看成noise的energy，variance的参数和网络参数都是通过loss的梯度进行更新。网络参数中仅仅加入了噪音，没有distribution，可以自动学习。<br>
在高维度上，本文的算法是一个randomised value function，这个函数是neural network，网络的参数并没有加倍，linear 的参数加倍，而参数是noise的一个简单变换。<br>
还有人添加constant Gaussian niose到网络参数，而文本的算法添加的noise并不是限制在Gaussion noise distributions。添加noise辅助训练在监督学习等任务中一直都有，但是这些噪音都是不能训练的，而NoisyNet中的噪音是可以梯度下降更新的。</p>
<h3 id="noisynets">NoisyNets</h3>
<p><img src="/2019/03/02/dqn/noisy_linear_layer.png" alt="noisy_linear_layer"><br>
用$\theta$表示noisy net的参数，输入是$x$，输出是$y$，即$y=f_{\theta}(x)$。$\theta$定义为$\theta=\mu+\Sigma\odot\varepsilon$，其中$\zeta=(\mu,\Sigma)$表示可以学习的参数，$\varepsilon$表示服从固定分布的均值为$0$的噪音,$\varepsilon$是random variable。$\odot$表示element-wise乘法。最后的loss函数是关于$\varepsilon$的期望：$\bar{L}(\zeta)=\mathbb{E}\left[L(\theta)\right]$，然后优化相应的$\zeta$，$\varepsilon$不能被优化，因为它是random variable。<br>
一个有$p$个输入单元，$q$个输出单元的fully-connected layer表示如下：<br>
$$y=wx+b \tag{14}$$<br>
其中$w\in \mathbb{R}^{q\times p}$，$x\in \mathbb{R}^{p}$,$b\in \mathbb{R}^{q}$，对应的noisy linear layer定义如下：<br>
$$y=(\mu^w+\sigma^w\odot\varepsilon^w)x + \mu^b+\sigma^b\odot\varepsilon^b \tag{15}$$<br>
就是用$\mu^w+\sigma^w\odot\varepsilon^w$取代$w$，用$\mu^b+\sigma^b\odot\varepsilon^b$取代$b$。其中$\mu^w,\sigma^w\in \mathbb{R}^{q\times p} $，而$\mu^b,\sigma^b\in\mathbb{R}^{q}$是可以学习的参数，而$\varepsilon^w\in \mathbb{R}^{p\times q},\varepsilon^b \in \mathbb{R}^{q}$是random variable。<br>
作者提出了两种添加noise的方式，一种是Independent Gaussian noise，一种是Factorised Gaussion noise。使用Factorised的原因是减少随机变量的计算时间，这些时间对于单线程的任务来说还是很多的。</p>
<h4 id="independent-gaussian-noise">Independent Gaussian noise</h4>
<p>应用到每一个weight和bias的noise都是independent的，对于$\varepsilon^w$的每一项$\varepsilon_{i,j}^w$来说，它们的值都是从一个unit Gaussion distribution中采样得到的；$varepsilon^b$同理。所以对于一个$p$个输入,$q$个输出的noisy linear layer总共有$pq+q$个noise 变量。</p>
<h4 id="factorised-gaussian-noise">Factorised Gaussian noise</h4>
<p>通过对$\varepsilon_{i,j}^w$来说，可以将其分解成$p$个$\varepsilon_i$用于$p$个输入和$q$个$\varepsilon_j$用于$q$个输出，总共有$p+q$个noiss变量。每一个$\varepsilon_{i,j}^w$和$\varepsilon_{j}^b$可以写成：<br>
$$\varepsilon_{i,j}^w = f(\varepsilon_i)f(\varepsilon_j) \tag{16}$$<br>
$$\varepsilon_{j}^b = f(\varepsilon_j)\tag{17}$$<br>
其中$f$是一个实函数，在第一个式子中$f(x) = sng(x)\sqrt{|x|}$，在第二个式子中可以取$f(x)=x$，这里选择了和第一个式子中一致。<br>
因为noisy network的loss函数是$\bar{L}(\zeta)=\mathbb{E}\left[L(\theta)\right]$，是关于noise的一个期望，梯度如下：<br>
$$\nabla\bar{L}(\zeta)=\nabla\mathbb{E}\left[L(\theta)\right]=\mathbb{E}\left[\nabla_{\mu,\Sigma}L(\mu+\Sigma\odot\varepsilon)\right] \tag{18}$$<br>
使用Monte Carlo估计上述梯度，在每一个step采样一个sample进行optimization:<br>
$$\nabla\bar{L}(\zeta)\approx\nabla_{\mu,\Sigma}L(\mu+\Sigma\odot\varepsilon) \tag{19}$$</p>
<h3 id="noisy-dqn-and-dueling">Noisy DQN and dueling</h3>
<p>相对于DQN和dueling DQN来说，noisy DQN and dueling主要做了两方面的改进：</p>
<ol>
<li>不再使用$\varepsilon$-greedy behaviour policy了，而是使用greedy behaviour policy采样优化randomised action-value function。</li>
<li>网络中的fully connected layers全都换成了参数化的noisy network，noisy network的参数在每一次replay之后从noise服从的distribution中进行采样。这里使用的nose是factorised Gaussian noise。</li>
</ol>
<p>在replay 整个batch的过程中，noisy network parameter sample保持不变。因为DQN和Dueling每执行一个action step之后都会执行一次optimization，每次采样action之前都要重新采样noisy network parameters。</p>
<h4 id="loss">Loss</h4>
<p>$Q(s,a,\epsilon;\zeta)$可以看成$\zeta$的一个random variable，NoisyNet-DQN loss如下：<br>
$$\bar{L}(\zeta) = \mathbb{E}\left[\mathbb{E}_{(x,a,r,y)}\sim D\left[r + \gamma \max_{b\in A}Q(y, b, \varepsilon’;\zeta^{-}) - Q(x,a,\varepsilon;\zeta)\right]^2\right]\tag{20}$$<br>
其中外层的期望是$\varepsilon$相对于noisy value function $Q(x,a, \varepsilon;\zeta)$和$\varepsilon’$相对于noisy target value function $Q(x,a, \varepsilon’;\zeta^{-}$。对于buffer中的每一个transition，计算loss的无偏估计，只需要计算target value和true value即可，为了让target value和true之间没有关联，target network和online network采用independent noises。<br>
就double dqn中的action选择来说，采样一个新的independent sample $\varepsilon^{’’}$计算action value，然后使用greedy操作，NoisyNet-Dueling的loss如下：<br>
$$\bar{L}(\zeta) = \mathbb{E}\left[\mathbb{E}_{(x,a,r,y)}\sim D\left[r + \gamma Q(y, b^{*}(y), \varepsilon’;\zeta^{-} - Q(x,a,\varepsilon;\zeta)\right]^2\right]\tag{21}$$<br>
$$b^{*}(y) = \arg \max_{b\in A} Q(y, b(y), \varepsilon^{’’};\zeta)\tag{22}$$</p>
<h3 id="noisy-a3c">Noisy-A3C</h3>
<p>Noisy-A3C相对于A3C有以下的改进：</p>
<ol>
<li>entropy项被去掉了;</li>
<li>fully-connected layer被替换成了noisy network。</li>
</ol>
<p>A3C算法中没有像$\epsilon$-greedy这样进行action exploration，选中的action通常是从current policy中选的，加入entropy是为了鼓励exploration，而不是选择一个deterministic policy。当添加了noisy weights时，对参数进行采样就表示选择不同的current policy，就已经代表了exploration。NoisyNet相当于直接在policy space中进行exploration，而entropy项就可以去掉了。</p>
<h3 id="noisy-networks的初始化">Noisy Networks的初始化</h3>
<p>在unfactorised noisy networks中，每个$\mu_{i,j}$从独立的均匀分布$U\left[-\sqrt{\frac{3}{p}}, \sqrt{\frac{3}{p}}\right]$中采样初始化，其中$p$是对应linear layer的输入个数，$\sigma_{i,j}$设置为一个常数$0.0017$，这是从监督学习的任务中借鉴的。<br>
在factorised noisy netowrks中，每个$\mu_{i,j}$从独立的均匀分布$U\left[-\sqrt{\frac{1}{p}}, \sqrt{\frac{1}{p}}\right]$中进行采样，$\sigma_{i,j}$设置为$\frac{\sigma_0}{p}$，超参数$\sigma_0$设置为$0.5$。</p>
<h3 id="算法-v6">算法</h3>
<p>算法5 NoisyNet-DQN / NoisyNet-Dueling<br>
输入: Env Environment; $\varepsilon$ random variables of the network的集合<br>
输入: DUELING Boolean; &quot;true&quot;代表NoisyNet-Dueling and &quot;false&quot;代表 NoisyNet-DQN<br>
输入: $B$空replay buffer; $\zeta$初始的network parameters; $\zeta^{-}$初始的target network parameters<br>
输入: replay buffer大小$N_B$; batch size $N_T$; target network更新频率$N^{-}$<br>
输出: $Q(\cdot, \varepsilon; \zeta)$ action-value function<br>
<strong>for</strong> episode $e\in  {1,\cdots , M}$ do<br>
$\qquad$初始化state sequence $x_0 \sim Env$<br>
$\qquad$<strong>for</strong> $t \in {1,\cdots }$ do<br>
$\qquad\qquad$设置$x \leftarrow x_0$<br>
$\qquad\qquad$采样 a noisy network  $\xi\sim \varepsilon$<br>
$\qquad\qquad$选择an action $a \leftarrow \arg \max_{b\in A} Q(x, b, \xi; \zeta)$<br>
$\qquad\qquad$采样 next state $y \sim  P (\cdot|x, a)$, 接收 reward $r \leftarrow R(x, a) $以及$x_0 \leftarrow y$<br>
$\qquad\qquad$将transition (x, a, r, y)添加到replay buffer<br>
$\qquad\qquad$<strong>if</strong> $|B| \gt N_B$ then<br>
$\qquad\qquad\qquad$删掉最老的transition<br>
$\qquad\qquad$<strong>end if</strong><br>
$\qquad$采样一个大小为$N_T$的batch, transitions $((x_j, a_j, r_j, y_j) \sim D)_{j=1}^{N_T}$<br>
$\qquad\qquad$采样noisy variables用于online network $\xi \sim\varepsilon$<br>
$\qquad\qquad$采样noisy variables用于target network $\xi’\sim\varepsilon$<br>
$\qquad\qquad\qquad$<strong>if</strong> DUELING then<br>
$\qquad\qquad\qquad$采样noisy variables用于选择action的network $\xi\sim\varepsilon$<br>
$\qquad\qquad$<strong>end if</strong><br>
$\qquad\qquad$<strong>for</strong> $j \in {1,\cdots, N_T}$ do<br>
$\qquad\qquad\qquad$<strong>if</strong> $y_j$ is a terminal state then<br>
$\qquad\qquad\qquad\qquad$$\hat{Q}\leftarrow r_j$<br>
$\qquad\qquad\qquad$<strong>end if</strong><br>
$\qquad\qquad\qquad$<strong>if</strong> DUELING then<br>
$\qquad\qquad\qquad\qquad b^{*}(y_j) = \arg \max_{b\in A} Q(y_j, b, \xi^{’’}; \zeta)$<br>
$\qquad\qquad\qquad\qquad\qquad \hat{Q}\leftarrow r_j + \gamma Q(y_j, b^{*}(y_j), \xi’;\zeta^{-})$<br>
$\qquad\qquad\qquad$<strong>else</strong><br>
$\qquad\qquad\qquad\qquad$$\hat{Q}\leftarrow r_j + \gamma \max_{b\in A} Q(y_j, b, \xi’;\zeta^{-})$<br>
$\qquad\qquad$<strong>end if</strong><br>
$\qquad\qquad\qquad$利用loss $(\hat{Q}-Q(x_j,a_j, \xi;\zeta))^2$的梯度更新$\zeta$<br>
$\qquad\qquad$<strong>end for</strong><br>
$\qquad\qquad$每隔$N^{-}$步更新target network:$ \zeta^{−}\leftarrow \zeta$<br>
$\qquad$<strong>end for</strong><br>
<strong>end for</strong></p>
<p>算法6 NoisyNet-A3C for each actor-learner thread<br>
输入: Environment Env, 全局共享参数$(\zeta_{\pi},\zeta_{V})$ , 全局共享counter $T$和maximal time $T_{max}$<br>
输入: 每个线程的参数 $(\zeta’_{\pi},\zeta’_{V})$, random variables $\varepsilon$的集合, 每个线程的counter $t$和TD-$\gamma$的长度$t_{max}$<br>
输出: policy $\pi(\cdot; \zeta_{\pi}, \varepsilon)$和value $V(\cdot; \zeta_{V}, \varepsilon)$<br>
初始化线程counter $t \leftarrow 1$<br>
<strong>repeat</strong><br>
$\qquad$重置acumulative gradients: $d\zeta_{\pi}\leftarrow 0$和$d\zeta_V \leftarrow 0$<br>
$\qquad$Synchronise每个线程的parameters: $\zeta’_{\pi}\leftarrow \zeta_{\pi}$和$\zeta_V\leftarrow \zeta_V$<br>
$\qquad$counter $\leftarrow 0$<br>
$\qquad$从Env中得到state $x_t$<br>
$\qquad$采样noise: $\xi\sim\varepsilon$<br>
$\qquad r \leftarrow []$<br>
$\qquad a \leftarrow []$<br>
$\qquad x \leftarrow []$和$x[0] \leftarrow x_t$<br>
$\qquad$<strong>repeat</strong><br>
$\qquad\qquad$采样action: $a_t \sim\pi(\cdot|x_t;\zeta’_{\pi};\xi)$<br>
$\qquad\qquad$$a[−1]\leftarrow a_t$<br>
$\qquad\qquad$接收reward $r_t$和next state $x_{t+1}$<br>
$\qquad\qquad$$r[−1]\leftarrow r_t$和$x[−1]\leftarrow x_t+1$<br>
$\qquad\qquad$$t\leftarrow t + 1$和 $T\leftarrow T + 1$<br>
$\qquad\qquad$$counter = counter + 1$<br>
$\qquad\qquad$<strong>until</strong> $x_t\ \ terminal\ \ or\ \ counter == t_{max} + 1$<br>
$\qquad$<strong>if</strong> $x_t$ is a terminal state then<br>
$\qquad\qquad$$Q = 0$<br>
$\qquad$<strong>else</strong><br>
$\qquad\qquad$$Q = V(x_t; \zeta’_{V}, \xi)$<br>
$\qquad$<strong>end if</strong><br>
$\qquad$<strong>for</strong> $i \in {counter − 1, \cdots, 0}$ do<br>
$\qquad\qquad$更新Q: $Q\leftarrow r[i] + \gamma Q$<br>
$\qquad\qquad$累积policy-gradient: $d\zeta_{\pi} \leftarrow d\zeta_{\pi} + \nabla \zeta’_{\pi}log(\pi(a[i]|x[i]; \zeta’_{\pi}, \xi))[Q − V(x[i]; \zeta’_{\pi}V, \xi)]$<br>
$\qquad\qquad$累积 value-gradient: $d\zeta_V \leftarrow ← d\zeta_V+ \nabla \zeta’_{V}[Q − V(x[i]; \zeta’_{V}, \xi)]^2$<br>
$\qquad$<strong>end for</strong><br>
$\qquad$执行$\zeta_{\pi}$的asynchronous update: $\zeta_{\pi}\leftarrow \zeta_{\pi} + \alpha_{\pi}d\zeta_{\pi}$<br>
$\qquad$执行$\zeta_{V}$的asynchronous update: $\zeta_{V}\leftarrow \zeta_{V} − \alpha_VdV\zeta_{V}$<br>
<strong>until</strong> $T \gt T_{max}$</p>
<h2 id="rainbow">Rainbow</h2>
<h2 id="参考文献">参考文献</h2>
<p>1.<a href="https://blog.csdn.net/yangshaokangrushi/article/details/79774031" target="_blank" rel="noopener">https://blog.csdn.net/yangshaokangrushi/article/details/79774031</a><br>
2.<a href="https://link.springer.com/article/10.1007%2FBF00992698" target="_blank" rel="noopener">https://link.springer.com/article/10.1007%2FBF00992698</a><br>
3.<a href="https://www.jianshu.com/p/b92dac7a4225" target="_blank" rel="noopener">https://www.jianshu.com/p/b92dac7a4225</a><br>
4.<a href="https://datascience.stackexchange.com/questions/20535/what-is-experience-replay-and-what-are-its-benefits/20542#20542" target="_blank" rel="noopener">https://datascience.stackexchange.com/questions/20535/what-is-experience-replay-and-what-are-its-benefits/20542#20542</a><br>
5.<a href="https://stats.stackexchange.com/questions/897/online-vs-offline-learning" target="_blank" rel="noopener">https://stats.stackexchange.com/questions/897/online-vs-offline-learning</a><br>
6.<a href="https://www.freecodecamp.org/news/improvements-in-deep-q-learning-dueling-double-dqn-prioritized-experience-replay-and-fixed-58b130cc5682/" target="_blank" rel="noopener">https://www.freecodecamp.org/news/improvements-in-deep-q-learning-dueling-double-dqn-prioritized-experience-replay-and-fixed-58b130cc5682/</a><br>
7.<a href="https://jaromiru.com/2016/11/07/lets-make-a-dqn-double-learning-and-prioritized-experience-replay/" target="_blank" rel="noopener">https://jaromiru.com/2016/11/07/lets-make-a-dqn-double-learning-and-prioritized-experience-replay/</a><br>
8.<a href="https://datascience.stackexchange.com/questions/32873/prioritized-replay-what-does-importance-sampling-really-do" target="_blank" rel="noopener">https://datascience.stackexchange.com/questions/32873/prioritized-replay-what-does-importance-sampling-really-do</a><br>
9.<a href="https://papers.nips.cc/paper/5249-weighted-importance-sampling-for-off-policy-learning-with-linear-function-approximation.pdf" target="_blank" rel="noopener">https://papers.nips.cc/paper/5249-weighted-importance-sampling-for-off-policy-learning-with-linear-function-approximation.pdf</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://mxxhcm.github.io/2019/01/06/bayesian-networks/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="马晓鑫爱马荟荟">
      <meta itemprop="description" content="记录硕士三年自己的积累">
      <meta itemprop="image" content="/images/favicon.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="mxxhcm's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/01/06/bayesian-networks/" class="post-title-link" itemprop="http://mxxhcm.github.io/page/16/index.html">Bayesian Networks</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-01-06 14:32:55" itemprop="dateCreated datePublished" datetime="2019-01-06T14:32:55+08:00">2019-01-06</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-08-30 14:31:44" itemprop="dateModified" datetime="2019-08-30T14:31:44+08:00">2019-08-30</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/机器学习/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="介绍">介绍</h2>
<p>贝叶斯网络是一个有向无环图(directed acyclic graphs)，它用节点代表随机变量，用边代表变量之间的依赖关系。</p>
<h2 id="意义">意义</h2>
<p>贝叶斯网络可以用来表示任意的联合分布。</p>
<h2 id="推理">推理</h2>
<p>贝叶斯网络的一个基本任务就是求后验概率。<br>
在AI这本书中，贝叶斯网络中的变量被分为了证据变量(evidence variable)，隐变量(hidden variable)和查询变量(query variable)。<br>
而在PRML这本书中，贝叶斯网络中的变量被分为了观测变量(observed variable)和隐变量(latent variable,hidden variable)。</p>
<p>具体的可以看另外两篇笔记有详细的记录。</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://mxxhcm.github.io/2019/01/06/AI-chapter-14-Probabilistic-reasoning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="马晓鑫爱马荟荟">
      <meta itemprop="description" content="记录硕士三年自己的积累">
      <meta itemprop="image" content="/images/favicon.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="mxxhcm's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/01/06/AI-chapter-14-Probabilistic-reasoning/" class="post-title-link" itemprop="http://mxxhcm.github.io/page/16/index.html">AI chapter 14 Probabilistic reasoning</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-01-06 14:32:16" itemprop="dateCreated datePublished" datetime="2019-01-06T14:32:16+08:00">2019-01-06</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-08-30 13:42:18" itemprop="dateModified" datetime="2019-08-30T13:42:18+08:00">2019-08-30</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/机器学习/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>在这里加一些自己的总结，这一章主要讲的是贝叶斯网络，首先介绍了贝叶斯网络的定义，是一个有向无环图，节点代表随机变量，边代表因果关系。这里给出了贝叶斯公式的两个意义，一个是数值意义，用贝叶斯网络表示全概率分布，另一个是拓扑意义，给定某个节点的父节点，这个节点条件独立于所有它的非后裔节点，或者给定某个节点的马尔科夫毯，这个节点条件独立于所有其他节点。接下来讲了条件独立的高效表示，噪音或模型表示离散型父节点和离散型子节点之间的关系，用参数化模型表示连续型父节点和连续型子节点之间的关系，用probit模型或者logit模型表示连续型父节点和离散型子节点之间的关系。接下来就介绍了贝叶斯精确推理计算后验分布的集中方法，一种是枚举推理，一种是消元法。因为精确推理的复杂度太高了，没有实际应用价值，所以就给出了一些估计推理的方法，直接采样，拒绝采样，以及可能性加权，还有另一类采样方法，蒙特卡洛算法，主要介绍了吉布森采样，大概就是这些。后面的两个小节没有看。</p>
<p>第$13$章讲的是概率论的基础知识并且强调了在概率表示中独立(independence)和条件独立(conditional independence)之间的关系。本章引入了一个系统的方式–贝叶斯网络去表现独立和条件独立之间的关系。概括的来说，本章的内容可以分为以下五部分：</p>
<ol>
<li>首先定义了贝叶斯网络的语法(syntax)和语义(semantics)，并且展示了如何用贝叶斯网络表示不确定知识。</li>
<li>接下来介绍了概率推理在最坏的情况下是很难计算的(computionally intratable)，但是在很多情况下可以高效的完成。</li>
<li>介绍了一系列在精确推理(exact inference)不可行时可以采用的估计推理算法(approximate inference algorithms)。</li>
<li>介绍了一些在概率论中可以被应用到带对象和关系的世界的方法，即与命题，表示相对的一阶模型。</li>
<li>最后，介绍了一些其它不确定性推理的方法。</li>
</ol>
<h2 id="不确定域的知识表示-representing-knowledge-in-an-uncertain-domain">不确定域的知识表示(Representing knowledge in an uncertain domain)</h2>
<p>我们可以根据联合概率分布(full joint probability distribution)算出任何想要的概率值，但是随着随机变量个数的增加，联合概率分布可能会变得特别大。此外，一个一个的指定可能世界中的概率是不可行的。</p>
<h3 id="贝叶斯网络的定义">贝叶斯网络的定义</h3>
<p>如果在联合概率中引入独立和条件独立，将会显著的减少定义联合概率分布所需要的概率。所以这节就介绍了贝叶斯网络来表示变量之间的依赖关系。本质上贝叶斯网络可以表示任何联合概率分布，而且在很多情况下是非常精确地表示。一个贝叶斯网络是一个有向图，图中的节点包含量化后的概率信息。具体的说明如下：</p>
<ol>
<li>每一个节点对应一个随机变量，这个随机变量可以是离散的也可以是连续的。</li>
<li>有向边或者箭头连接一对节点。如果箭头是从节点$X$到节点$Y$，那么节点$X$称为节点$Y$的父节点。图中不能有环，因此贝叶斯网络是一个有向无环图(directed acyclic graph,DAG)。</li>
<li>每一个节点$X_i$有一个条件概率分布$P(x_i|Parents(X_i))$量化(quantifiy)父节点对其影响。</li>
</ol>
<p>网络的拓扑，即节点和边的集合，指定了条件概率分布之间的关系。箭头的直观意义是节点$X$对节点$Y$有直接的影响，$Y$发生的原因是其父节点的影响。通常对于一个领域(domain)的专家来说，指出该域受哪些因素的直接影响要比直接给出它的概率值简单的多。一旦贝叶斯网络的拓扑结构定了，给出一个变量的父节点，我们仅仅需要给出每个节点的条件概率分布。我们能看出，拓扑和条件概率的组合能计算出所有变量的联合概率分布。</p>
<h3 id="贝叶斯网络的示例">贝叶斯网络的示例</h3>
<h4 id="牙疼和天气">牙疼和天气</h4>
<p>给定一组随机变量牙疼(Toothache)，蛀牙(Cavity)，拔牙(Catch)和天气(Weather)。Weather是独立于另外三个随机变量的，此外，给定Cavity，Catch和Toothache是条件独立的，即给定Cavity，Catch和Toothache是相互不受影响的，如下图所示。正式的：给定Cavity，Toochache和Catch是条件独立的，图中Toothache和Catch之间缺失的边体现出了条件独立。直观上，网络表现出Cavity是Toothache和Catch发生的直接原因，然而在Toothache和Catch之间没有直接的因果关系。<br>
<img src="/2019/01/06/AI-chapter-14-Probabilistic-reasoning/" alt="figure 14.1"></p>
<h4 id="警报和打电话">警报和打电话</h4>
<p>我家里有一个新安装的防盗警报(burglar alarm)，这个警报对于小偷的检测是相当可靠的，但是也会对偶然发生的微小的地震响应。我有两个邻居(Mary和John)，他们听到警报后会打电话给我。John有时会把电话铃和警报弄混了，也会打电话。Mary听音乐很大声，经常会错过警报。现在给出John或者Mary谁是否打电话，估计警报响了的概率。<br>
<img src="/2019/01/06/AI-chapter-14-Probabilistic-reasoning/" alt="figure 14.2"><br>
该例子的贝叶斯网络如上图所示。该网络体现了小偷和地震两个因素会直接影响警报响的概率，但是John和Mary会不会打电话只取决于警报有没有响。贝叶斯网络展示出了我们的假设，即John和Mary不直接观察小偷有没有来，也不直接观察小的地震是否，也不受之前是否打过电话的影响。上图中的条件概率分布以一个条件概率分布表(conditional probability table,CPT)的形式展现了出来。这个表适合离散型的随机变量，但是不适合连续性随机变量。没有父节点的节点只有一行，用来表示随机变量的可能取值的先验概率(prior probabilities)。<br>
注意到这个网络中没有节点对应Mary听音乐很大时，也没有节点对应John把电话铃声当成了警报。事实上这些因素都被包含在和边Alarm到JohnCalls和MaryCall相关的不确定性中了，概率包含了无数种情况可能让警报失灵（停电，老鼠咬坏了，等等）或者John和Mary没有打电话的原因（吃饭去了，午睡了，休假了等等），这些不确定性都包含在了概率中了。</p>
<h2 id="贝叶斯网络的意义-the-semantics-of-bayesian-networks">贝叶斯网络的意义(the semantics of bayesian networks)</h2>
<p>上一节主要讲的是什么是贝叶斯网络，但是没有讲它的意义。本节主要给出两种方式可以理解贝叶斯网络的意义。第一个是一种数值化的意义，即&quot;numerical semantics&quot;，把它当成联合概率分布的一种表示形式。第二个是一种拓扑的意义，即&quot;topological semantics&quot;，将它看成条件独立的一种编码方式。事实上，这两种方式是等价的，但是第一种方式更有助于理解如何构建贝叶斯网络，第二种方式更有助于设计推理过程。</p>
<h3 id="贝叶斯网络表示联合分布-representing-the-full-joint-distribution">贝叶斯网络表示联合分布(Representing the full joint distribution)</h3>
<h4 id="定义">定义</h4>
<p>一个贝叶斯网络是一个有向无环图，并且每个节点都有一个数值参数。数值方式给出这个网络的意义是，它代表了所有变量的联合概率分布。之前说过节点上的值代表的是条件概率分布$P(X_i|Parents(X_i)$，这是对的，但是当赋予整个网络意义以后，这里我们认为它们只是一些数字$\theta(X_i|Parents(X_i)$。<br>
联合概率中的一个具体项(entry)表示的是每一个随机变量取某个值的联合概率，如$P(X_1=x_1 \wedge \cdots\wedge X_n = x_n)$，缩写为$P(x_1,\cdots,x_n)$。这个项的值可以通过以下公式进行计算：<br>
$$P(x_1,\cdots,x_n) = \prod_{i=1}^n \theta(x_i|parents(X_i)),$$<br>
其中$parents(X_i)$表示节点$X_i$在$x_1,\cdots,x_n$中的父节点。因此，联合概率分布中的每一项都可以用贝叶斯网络中某些条件概率的乘积表示。从定义中可以看出，很容易证明$\theta(x_i|parents(X_i))$就是条件概率$P(x_i|parents(X_i))$，因此，我们可以把上式写成：<br>
$$P(x_1,\cdots,x_n) = \prod_{i=1}^n P(x_i|parents(X_i)),$$<br>
换句话说：根据上上个式子定义的贝叶斯网络的意义，我们之前叫的条件概率表真的是条件概率表。（这句话。。。）</p>
<h4 id="示例">示例</h4>
<p>我们可以计算出警报响了，但是没有小偷或者地震发生，John和Mary都打电话了的概率。即计算联合分布$P(j,m,a,\neg b, \neg e)$（使用小写字母表示变量的值）：<br>
\begin{align*}<br>
P(j,m,a,\neg b, \neg e) &amp;=P(j|a)P(m|a)P(a|\neg b \wedge \neg e)P(\neg b)P(\neg e)\<br>
&amp;=0.90\times 0.70\times 0.001 \times 0.999 \times 0.998\<br>
&amp;=0.000628<br>
\end{align*}</p>
<h4 id="构建贝叶斯网络-constructing-bayesian-networks">构建贝叶斯网络(Constructing Bayesian networks)</h4>
<p>上面给出了贝叶斯网络的一种意义，接下来给出如何根据这种意义去构建一个贝叶斯网络。确定的条件独立可以用来指导网络拓扑的构建。首先，我们把联合概率的项用乘法公式写成条件概率表示：<br>
$$P(x_1,\cdots,x_n) = P(x_n|x_{n-1},\cdots,x_1)P(x_{n-1},\cdots,x_1)$$<br>
接下来重复这个过程，将联合概率(conjunctive probability)分解成一个条件概率和一个更小的联合概率。最后得到下式：<br>
\begin{align*}<br>
P(x_1,\cdots,x_n) &amp;= P(x_n|x_{n-1},\cdots,x_1)P(x_{n-1}|,x_{n-2}\cdots,x_1)\cdots P(x_2|x_1)P(x_1)\<br>
&amp;= \prod_{i=1}^nP(x_i|x_{i-1},\cdots,x_1)<br>
\end{align*}<br>
这个公式被称为链式法则，它对于任意的随机变量集都成立。对于贝叶斯网络中的每一个变量$X_i$，如果给定$Parents(X_i) \subset {X_{i-1},\cdots,X_1}$（每一个节点的序号应该和图结构的偏序结构一致），那么有：<br>
$$P(x_1,\cdots,x_n) = \prod_{i=1}^n P(x_i|parents(X_i)),$$<br>
将它和上式对比，得出：<br>
$$P(X_i|X_{i-1},\cdots,X_1) = P(X_i|Parents(X_i).$$<br>
这个公式成立的条件是给定每个节点的父节点，它条件独立于所有它的非父前置节点。这里给出一个生成贝叶斯网络的方式：</p>
<ol>
<li>节点：首先，确定需要对领域建模所需要的随机变量集合。对它们进行排序：${X_1,\cdots,X_n}$，任意顺序都行，但是如果随机变量的因(causes)在果(effects)之前，最终的结果会更加紧凑。</li>
<li>边：从$i = 1$到$n$，</li>
</ol>
<ul>
<li>从$X_1,\cdots,X_{i-1}$中选出$X_i$的最小父节点集合。</li>
<li>对于每一个父节点，插入一条从父节点到$X_i$的边。</li>
<li>写下条件概率表，$P(X_i| Parents(X_i))$。</li>
</ul>
<p>直观上，$X_i$的父节点应该包含$X_1,\cdots,X_{i-1}$中所有直接影响$X_i$的节点。因为每一个节点都只和它前面的节点相连，这就保证了每个网络都是无环的(acyclic)。此外，贝叶斯网络还不包含冗余的概率值，如果有冗余值，就会产生不一致：不可能生成一个违反概率论公理的贝叶斯网络。</p>
<h4 id="紧凑性和节点顺序-compactness-and-node-ordering">紧凑性和节点顺序(Compactness and node ordering)</h4>
<h5 id="紧凑性-compactness">紧凑性(compactness)</h5>
<p>因为不包含冗余信息，贝叶斯网络会比联合概率分布更加紧凑，这让它能够处理拥有很多变量的任务。贝叶斯网络的紧凑性是稀疏(sparse)系统或者局部结构化(local structured)系统普遍拥有的稀疏性的一个例子。在一个局部结构化系统中，每一个子部件仅仅和有限数量的其他部件进行交互，而不用管整个系统。局部结构化的复杂度通常是线性增加的而不是指数增加的。在贝叶斯网络中，一个随机变量往往最多受$k$个其他随机变量直接影响，这里的$k$是一个常数。为了简化问题，我们假设有$n$个布尔变量，指定一个条件概率表所需要的数字最多是$2<sup>k$个，整个网络则需要$n2</sup>k$个值；作为对比，联合概率分布需要$2^n$个值。举个例子，如果我们有$n=30$个节点，每一个节点至多有五个父节点(k=5)，那么贝叶斯网络只需要$960$个值，而联合概率分布需要超过十亿个值。<br>
但是在某些领域，可能每一个节点都会被所有其他节点直接影响，这时候网络就成了全连接的网络(fully connected)，它和联合概率分布需要同样多的信息。有时候，增加一条边，也就是一个依赖关系，可能会对结果产生影响，但是如果这个依赖很弱(tenuous)，添加这条边的花费比获得的收益还要大，那么就没有必要加这条边了。比如，警报的那个例子，如果John和Mary感受到了地震，他们认为警报是地震引起的，所以就不打电话了。是否添加Earthquake到JohnCalls和MaryCalls这两条边取决于额外的花费和得到更高的警报率之间的关系。</p>
<h5 id="节点顺序-node-ordering">节点顺序(node ordering)</h5>
<p>即使在一个局部结构化的领域，只有当我们选择好的节点顺序的时候，我们才能得到一个紧凑的贝叶斯网络。考虑警报的例子，我们给出下图：<br>
<img src="/2019/01/06/AI-chapter-14-Probabilistic-reasoning/" alt="figure 14.3"><br>
Figure 14.2和Figure 14.3两张图中的三个贝叶斯网络表达的都是同一个联合分布，但是Figure 14.3中的两张图没有表现出来条件独立，尤其是Figure 14.3(b)中的贝叶斯网络，它需要用和联合分布差不多相同个数的值才能表现出来。可以看出来，节点的顺序会影响紧凑性。</p>
<h3 id="贝叶斯网络中的条件独立-conditional-independence-relations-in-bayesion-networks">贝叶斯网络中的条件独立(Conditional independence relations in Bayesion networks)</h3>
<p>贝叶斯网络的一个数值意义(“numerical” semantics)是用来表示联合概率分布。根据这个意义，给定每个节点的父节点，使得每一个节点条件独立于它的父节点之外的节点，我们能构建一个贝叶斯网络。此外我们也可以从用图结构编码整个条件独立关系的拓扑意义出发，然后推导出贝叶斯网络的数值意义。拓扑语义说的是给定每个节点的父节点，则该节点条件独立于所有它的非后裔(non-descendants)节点。举例来说，Figure 14.2的警报例子中，给定alarm后，JohnCalls独立于Burglary,Eqrthquake和MaryCalls。如图Figuree 14.4(a)中所示。从条件独立断言(assertions)和网络参数$\theta(x_i|parents(X_i))$就是条件概率$P(x_i|parents(X_i))$的解释中，联合概率可以计算出来。在这种情况下，数值意义和拓扑语义是相同的。<br>
另一个拓扑意义的重要属性是：给定某个节点的马尔科夫毯(Markov blanket)，即节点的父节点，子节点，子节点的父节点，这个节点条件独立于所有其他的节点。如图Figure 14.4(b)所示。</p>
<h2 id="条件分布的高效表示-efficient-representation-of-conditional-distributions">条件分布的高效表示(Efficient representation of conditional distributions)</h2>
<p>即使每个节点有$k$个父节点，一个节点的CPT还需要$O(2^k)$，最坏的情况下父节点和子节点是任意连接的。一般情况下，这种关系可以用符合一些标准模式(standard pattern)的规范分布(canonical distribution)表示，这样子就可以仅仅提供分布的一些参数就能生成整个CPT。<br>
最简单的例子是确定性节点(deterministic node)。一个确定性节点的值被它的父节点的值精确确定。这个确定性关系可以是逻辑关系：父节点是加拿大，美国和墨西哥，子节点是北美洲，它们之间的关系是子节点是父节点所在的洲。这个关系也可以是数值型的，一条河的流量是流入它的流量减去流出它的流量。<br>
不确定关系通常称为噪音逻辑关系(noisy logical relationships)。一个例子是噪音或(noisy-OR)，它是逻辑或的推广。在命题逻辑中，当且仅当感冒(Cold)，流感(Flu)或者疟疾(Malaria)是真的时候，发烧(Fever)才是真的。噪音或模型允许不确定性，即每一个父节点都有可能让子节点为真，可能父节点和子节点之间的关系被抑制了(inhibited)，可能一个人感冒了，但是没有表现出发烧。这个模型做了两个假设。第一个，它假设所有的原因都被列了出来，有时候会加一个节点(leak node)包含所有的其他原因(miscellaneous causes)。第二个，抑制每一个父节点和子节点之间的原因是独立的，比如抑制疟疾产生发烧和抑制感冒产生发烧的原因是独立的。所以，当且仅当所有的父节点都是假的时候，发烧才一定不会发生。给出以下的假设：<br>
$q_{cold} = P(\neg fever| cold,\neg flu, \neg malaria) = 0.6$<br>
$q_{flu} = P(\neg fever|\neg cold, flu, \neg malaria) = 0.2$<br>
$q_{malaria} = P(\neg fever|\neg cold,\neg flu, malaria) = 0.1$<br>
根据这些信息，以及噪音或的假设，整个CPT可以被创建。一般的规则是：<br>
$P(x_i|parents(X_i)) = 1 - \prod_{j:X_j=ture} q_j.$<br>
最后生成如下的表：</p>
<table>
<thead>
<tr>
<th style="text-align:center">Cold</th>
<th style="text-align:center">Flu</th>
<th style="text-align:center">Malaria</th>
<th style="text-align:center">P(Fever)</th>
<th style="text-align:center">P($\neg$Fever)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">F</td>
<td style="text-align:center">F</td>
<td style="text-align:center">F</td>
<td style="text-align:center">$0.0$</td>
<td style="text-align:center">$1.0$</td>
</tr>
<tr>
<td style="text-align:center">F</td>
<td style="text-align:center">F</td>
<td style="text-align:center">T</td>
<td style="text-align:center">$0.9$</td>
<td style="text-align:center">$0.1$</td>
</tr>
<tr>
<td style="text-align:center">F</td>
<td style="text-align:center">T</td>
<td style="text-align:center">F</td>
<td style="text-align:center">$0.8$</td>
<td style="text-align:center">$0.2$</td>
</tr>
<tr>
<td style="text-align:center">F</td>
<td style="text-align:center">T</td>
<td style="text-align:center">T</td>
<td style="text-align:center">$0.98$</td>
<td style="text-align:center">$0.1\times 0.2=0.02$</td>
</tr>
<tr>
<td style="text-align:center">T</td>
<td style="text-align:center">F</td>
<td style="text-align:center">F</td>
<td style="text-align:center">$0.4$</td>
<td style="text-align:center">$0.6$</td>
</tr>
<tr>
<td style="text-align:center">T</td>
<td style="text-align:center">F</td>
<td style="text-align:center">T</td>
<td style="text-align:center">$0.94$</td>
<td style="text-align:center">$0.6\times 0.1 = 0.06 $</td>
</tr>
<tr>
<td style="text-align:center">T</td>
<td style="text-align:center">T</td>
<td style="text-align:center">F</td>
<td style="text-align:center">$0.88$</td>
<td style="text-align:center">$0.5\times 0.2 = 0.12 $</td>
</tr>
<tr>
<td style="text-align:center">T</td>
<td style="text-align:center">T</td>
<td style="text-align:center">T</td>
<td style="text-align:center">$0.988$</td>
<td style="text-align:center">$0.6\times 0.2\times 0.1 = 0.012$</td>
</tr>
</tbody>
</table>
<p>对于这个表，感觉自己一直有点转不过来圈。就是有症状不一定发烧，也可能不发烧，没有症状一定不发烧。什么时候不发烧呢，只有某个症状表现出来不发烧，如果多个症状的话，直接把有症状表现但不发烧的概率相乘。<br>
一般情况下，噪声逻辑模型中，有$k$个父节点的变量可以用$O(k)$个参数表示而不是$O(2^k)$去表示整个CPT。这让访问(assessment)和学习(learning)更容易了。</p>
<h3 id="连续性随机变量的贝叶斯网络-bayesian-nets-with-continuous-variables">连续性随机变量的贝叶斯网络(Bayesian nets with continuous variables)</h3>
<h4 id="常用方法">常用方法</h4>
<p>现实中很多问题都是连续型的随机变量，它们有无数可能的取值，所以显式的指定每一个条件概率行不通。常用的总共有三种方法，第一个可能的方法是离散(discretization)连续型随机变量，将随机变量的可能取值划分成固定的区间。比如，温度可以分成，小于$0$度的，$0$度到$100$度之间的，大于$100$度的。离散有时候是可行的，但是通常会造成精度的缺失和非常大的CPT。第二个方法也是最常用的方法是通过指定标准概率密度函数的参数，比如指定高斯分布的均值和方差。第三种方法是非参数化(nonparametric)表示，用隐式的距离去定义条件分布。</p>
<h4 id="示例-v2">示例</h4>
<p>一个同时拥有离散型和随机性变量的网络被称为混合贝叶斯网络(hybrid Bayesian network)。为了创建这样一个网络，我们需要两种新的分布。一种是给定离散或者连续的父节点，子节点是连续型随机变量的条件概率，另一种是给定连续的父节点，子节点是离散型随机变量的条件概率。</p>
<h5 id="连续型子节点">连续型子节点</h5>
<p><img src="/2019/01/06/AI-chapter-14-Probabilistic-reasoning/" alt="figure 14.5"><br>
考虑Figure 14.5的例子，一个顾客买了一些水果，买水果的量取决取水果的价格(Cost)，水果的价格取决于收成(Harvest)和政府是否有补助(Subsidy)。其中，Cost是连续型随机变量，他有连续的父节点Harvest和离散的父节点Subsidy，Buys是离散的，有一个连续型的父节点Cost。<br>
对于变量Cost，我们需要指定条件概率$P(Cost|Subsidy,Harvest)$。离散的父节点通过枚举(enumeration)来表示，指定$P(Cost|subsidy,Harvest)$和$P(Cost|\neg subsidy,Harvest)$。为了表示Harvest，可以指定一个分布来表示变量Cost的值$c$取决于连续性随机变量Harvest的值$h$。换句话说，将$c$看做一个$h$的函数，然后给出这个函数的参数即可，最常用的是线性高斯分布。比如这里，我们可以用两个不同参数的高斯分布来表示有补贴和没补贴时Harvest对Cost的影响：<br>
$$P(c|h, subsidy) = N(a_th+b_t,\sigma_t^2)© = \frac{1}{\sigma_t \sqrt{2\pi} }e^{- \frac{1}{2}(\frac{c-(a_th+b_t)}{\sigma_t})^2}$$<br>
$$P(c|h,\neg subsidy) = N(a_fh+b_t,\sigma_f^2)© = \frac{1}{\sigma_f \sqrt{2\pi} }e^{- \frac{1}{2}(\frac{c-(a_fh+b_f)}{\sigma_f})^2}$$<br>
所以，只需要给出$a_t,b_t,\sigma_t,a_f,b_f,\sigma_f$这几个参数就行了，Figure 14.6(a)和(b)就是一个示例图。注意到坡度(slope)是负的，因为随着供应的增加，cost在下降，当然，这个线性模型只有在harvest在很小的一个区间内才成立，而且cost有可能为负。假设有补贴和没补贴的两种可能性相等，是$0.5$，那么就有了Figure 14.6©的图$P(c|h)$。</p>
<h5 id="连续型父节点">连续型父节点</h5>
<p>当离散型随机变量有连续型父节点时，如Figure 14.5中的Buys节点。我们有一个合理的假设是：当cost高的时候，不买，cost底的时候，买，在中间区域买不买是一个变化很平滑的概率。我们可以把条件分布当成一个软阈值函数(soft-threshold)，一种方式是用标准正态分布的积分(intergral)。<br>
$$\Phi(x) = \int_{-\infty}^{x} N(0,1)(x)dx$$<br>
给定Cost买的概率可能是:<br>
$$P(buys|Cost = c) = \Phi((-x+\nu)/ \sigma))$$<br>
其中cost的阈值在$\nu$附近，阈值的区域和正比于$\sigma$，当价格升高的时候，买的概率会下降。这个probit distribution模型如Figure 14.7(a)所示。<br>
另一个可选择的模型是logit distribution，使用logistic function $1/(1+e^{-x})$来生成一个软阈值：<br>
$$P(buys|Cost = c) = \frac{1}{1+exp(-2\frac{-c+u}{\sigma})}.$$<br>
如Figure 14.7(b)所示，这两个分布很像，但是logit有更长的尾巴。probit更符合实际情况，但是logit数学上更好算。它们都可以通过对父节点进行线性组合推广到多个连续性父节点的情况。</p>
<h2 id="贝叶斯网络的精确推理-exact-inference-in-bayesian-networks">贝叶斯网络的精确推理(Exact inference in bayesian networks)</h2>
<p>概率推理系统的基本任务就是给出一些观察到的事件，即给证据变量(evidence variable)赋值，然后计算一系列查询变量(query variable)的后验概率。我们用$X$表示查询变量，用$\mathbf{E}$表示证据变量$E_1,\cdots,E_m$的集合，$\mathbf{e}$是一个特定的观测事件，$\mathbf{Y}$表示既不是证据变量，也不是查询变量的变量$Y_1,\cdots,Y_l$的集合（隐变量,hidden variables)。变量的所有集合是$\mathbf{X}={X}\cup \mathbf{E}\cup \mathbf{Y}$。一个典型的查询是求后验概率$P(X|\mathbf{e})$。<br>
在这一节中主要讨论的是计算后验概率的精确算法以及这些算法的复杂度。事实上，在一般情况下精确推理的复杂度都是很高的，为了降低复杂度，就只能进行估计推理(approximate inference)了，这个会在下一节中介绍到。</p>
<h3 id="枚举实现精确推理-inference-by-enumeration">枚举实现精确推理(Inference by enumeration)</h3>
<p>任何条件概率都可以用联合概率分布的项相加得到，即：<br>
$$P(X|\mathbf{e}) = \alpha P(X,\mathbf{e}) = \alpha \sum_{\mathbf{y}}P(X,\mathbf{e},\mathbf{y})$$<br>
贝叶斯网络给出了所有的联合概率分布，任何项$P(x,\mathbf{e},\mathbf{y})$都可以用贝叶斯网络中的条件概率的乘积表示出来。比如警报例子中的查询$P(Burglary|JohnCalls=true,MaryCalls=true)$。隐变量是Earthquake和Alarm，我们可以算出：<br>
$$P(B|j,m) = \alpha P(B,j,m) = \alpha \sum_{e}\sum_{a}P(B,j,m,e,a).$$<br>
贝叶斯网络已经给出了所有CPT项的表达式，比如当Burglary = true时：<br>
$$P(b|j,m) = \alpha \sum_e\sum_aP(b,j,m,e,a) = \alpha \sum_e\sum_aP(b)P(e)P(a|b,e)P(j|a)P(m|a).$$<br>
为了计算这个表达式，我们得计算一个四项的加法，分别是e为true和false,a为true和false对应的$P(b,j,m)$的值，每一项都是五个数的乘法。最坏的情况下，所有的变量都用到了，那么拥有$n$个布尔变量的贝叶斯网络的时间复杂度是$O(n2^n)$。我们可以做一些简化，将一些重复的计算保存下来，比如将上面的式子变成：<br>
$$P(b|j,m) = \alpha \sum_e\sum_aP(b,j,m,e,a) = \alpha P(b) \sum_eP(e)\sum_aP(a|b,e)P(j|a)P(m|a).$$<br>
这样子可以按照顺序进行计算，具体的计算过程如Figure 14.8所示。这种算法叫做ENUMERATION-ASK，它的空间复杂度是线性的，但是它的事件复杂度是$O(2<sup>n)$比$O(n2</sup>n)$要好，却仍然是实际上不可行的。（这里我理解的是$O(2<sup>n)$而不是$O(n2</sup>n)$的原因是，总共有$n$个布尔变量，所以总共有$2^n$个可能的取值，每次算一个，存一个，而原来的是算完之后不存。）<br>
事实上，Figure 14.8中的计算过程还有很多重复计算，比如$P(j|a)P(m|a)$和$P(j|\neg a)P(m|\neg a)$这两项被计算了两次。我原来在想这里是不是和上面一段说的冲突了，事实上是没有的，这$2^n$个值，其中可能会有$P(b,j,m,e,a)$和$P(b,j,m,e,\neg a)$，这两个概率中都用到了$P(j|a)P(m|a)$，但是这里就会计算两次，事实上有很多值都会被重复计算很多次。下面就介绍一个避免这种运算的方法。</p>
<h3 id="消元法-the-variable-elimination-algorithm">消元法(The variable elimination algorithm)</h3>
<p>上面问题的解决思路就是保存已经计算过的值，实际上这是一种动态规划。还有很多其他方法可以解决这个问题，这里介绍了最简单的消元算法。消元法对表达式进行从右至左的计算，而枚举法是自底向上的。所有的中间值被报存起来，最对和每个变量有关的表达式进行求和。例如对于下列表达式：<br>
$$P(B|j,m) = \alpha \underbrace{P(B)}<em>{f_1(B)} \sum_e\underbrace{P(e)}</em>{f_2(E)} \sum_a\underbrace{P(a|B,e)}<em>{f_3(A,B,E)} \underbrace{P(j|a)}</em>{f_4(A)} \underbrace{P(m|a)}_{f_5(A)}.$$<br>
表达式的每一部分都是一个新的因子，每一个因子都是由它的参数变量(argument variables)决定的矩阵，参数变量指定的取值是没有固定的变量。比如因子$f_4(A)$和$f_5(A)$对应$P(j|a)$和$P(m|a)$的表达式只取决于$A$的值因为$J$和$M$在这个查询中都是固定的。它们都是两个元素的向量：<br>
$$f_4(A) = \begin{pmatrix}P(j|a)\P(j|\neg a)\end{pmatrix} = \begin{pmatrix}0.90\0.05\end{pmatrix}$$<br>
$$f_5(A) = \begin{pmatrix}P(m|a)\P(m|\neg a)\end{pmatrix} = \begin{pmatrix}0.70\0.01\end{pmatrix}$$<br>
$f_3(A,B,E)$是一个$2\times 2\times 2$的矩阵。用因子表达的话，查询的表达式变成了：<br>
$$P(B|j,m) = \alpha f_1(B)\times \sum_ef_2(E)\times \sum_af_3(A,B,E)\times f_4(A)\times f_5(A)$$<br>
其中$\times$不是普通的矩阵乘法，而是对应元素相乘(pointwise product)。整个表达式的计算过程可以看成从右到左变量相加的过程，将现有的因子消去产生新的因子，最后只剩下一个因子的过程。具体的步骤如下：<br>
首先先利用$f_3,f_4,f_5$把变量$A$消掉，产生一个新的$2\times 2$的只含有变量$B$和$E$的新因子$f_6(B,E)$：<br>
\begin{align*}<br>
f_6(B,E) &amp;= \sum_af_3(A,B,E)\times f_4(A) \times f_5(A)\<br>
&amp;= (f_3(a,B,E)\times f_4(a) \times f_5(a)) + (f_3(\neg a,B,E)\times f_4(\neg a)\times f_5(\neg a)<br>
\end{align*}<br>
这样目标变成了：<br>
$$P(B|j,m) = \alpha f_1(B)\times \sum_ef_2(E)\times \sum_af_6(B,E)$$<br>
利用$f_2,f_6$消去$E$：<br>
\begin{align*}<br>
f_7(B) &amp;= \sum_ef_2(E)\times \sum_af_6(B,E)\<br>
&amp; = f_2(e)\times f_6(B,e) + f_2(\neg e)\times f_6(B,\neg e)<br>
\end{align*}<br>
将表达式化成：<br>
$$P(B|j,m) = \alpha f_1(B)\times f_7(B)$$<br>
显然，根据这个表达式就可以计算出我们想要的结果了。上面的过程可以总结成两步，第一步是point-wise的因子乘法，第二步是利用因子的乘法进行消元。</p>
<h4 id="因子运算-operations-on-factors">因子运算(Operations on factors)</h4>
<p>两个因子$f_1$和$f_2$进行point-wise乘法运算产生新的因子(factor)$f$的变量是$f_1$和$f_2$变量的并，新的因子中的元素的值是$f_1$和$f_2$中对应项的积。假设两个因子有公共变量$Y_1,\cdots,Y_k$，那么就有：<br>
$$f(X_1,\cdots,X_j,Y_1,\cdots,Y_k,Z_1,\cdots,Z_l)=f_1(X_1,\cdots,X_j,Y_1,\cdots,Y_k)f_2(Y_1,\cdots,Y_k,Z_1,\cdots,Z_l).$$<br>
如果所有的变量都是二值化的，那么$f_1$和$f_2$各有$2<sup>{j+l}$和$2</sup>{l+k}$项，$f$有$2^{j+l+k}$项。比如，$f_1(A,B),f_2(B,C)$，那么point-wise乘法产生的$f_3(A,B,C)=f_1\times f_2$有$8$项，如Figure 14.10所示。<br>
<img src="/2019/01/06/AI-chapter-14-Probabilistic-reasoning/" alt="figure 14.10"><br>
根据图中给出的值，消去$f_3(A,B,C)$中的$A$：<br>
\begin{align*}<br>
f(B,C) &amp;= \sum_af_3(A,B,C)\<br>
&amp;= f_3(a,B,C) + f_3(\neg a,B,C)\<br>
&amp;= \begin{pmatrix} 0.06&amp;0.24\0.42&amp;0.28\end{pmatrix} + \begin{pmatrix}0.18&amp;0.72\0.06&amp;0.04\end{pmatrix}\<br>
&amp;= \begin{pmatrix}0.24&amp;0.96\048&amp;0.32\end{pmatrix}<br>
\end{align*}<br>
产生新的因子用的是pointwise乘法，消元用的是累乘。给定pointwise乘法和消元函数，消元算法就变得很简单，一个消元算法如Figure 14.11所示。<br>
<img src="/2019/01/06/AI-chapter-14-Probabilistic-reasoning/" alt="figure 14.11"></p>
<h4 id="变量顺序和变量相关性-variable-ordering-and-variable-relevance">变量顺序和变量相关性(Variable ordering and variable relevance)</h4>
<p>Figure 14.11中的算法包含一个没有给出具体实现的排序函数Order()对要消去的变量进行排序，每一种排序选择都会产生一组有效的算法，但是不同的消元顺序会产生不同的中间因子。一般情况下，消元法的时间和空间复杂度是由算法产生的最大因子决定的，这个最大因子是由消元的顺序和贝叶斯网络的结构决定的，选取最优的消元顺序是很困难的，但是有一些小的技巧：总是消去让新产生的因子最小的变量。<br>
另一个属性是：每一个不是查询变量或者证据变量的祖先变量都和这次查询无关，在实现消元算法的时候可以把这些变量都去掉。（具体的示例可以看第十四章，在$528$页）。</p>
<h3 id="精确推理的复杂度-the-complexity-of-exact-inference">精确推理的复杂度(The complexity of exact inference)</h3>
<p>贝叶斯网络的精确推理跟网络的结构有很大的关系。<br>
Figure 14.2中警报贝叶斯网络中的复杂度是线性的。该网络中任意两个节点只有一条路径，这种网络称为单连接的(singly-connected)或者多树(polytrees)，这种结构有一个很好的属性就是：多树结构中精确推理的时间，空间复杂度对于网络大小来说都是线性关系，这里网络大小指的是CPT项的个数。如果每一个节点的父节点都是一个有界的常数，那么复杂度和节点数之间也是线性关系。<br>
对于多连接(multiply connected)的网络，如Figure 14.12(a)所示，最坏情况下，即使每一个节点的父节点个数都是有界常数，消元法的时间和空间复杂度也都是指数级别的。因为贝叶斯网络的推理也是NP难问题。<br>
<img src="/2019/01/06/AI-chapter-14-Probabilistic-reasoning/" alt="figure 14.12"></p>
<h3 id="聚类算法-clustering-algorithms">聚类算法(clustering algorithms)</h3>
<p>用消元法来计算单个的后验概率是简单而高效的，但是如果要计算网络中所有变量的后验概率是很低效的。例如：在单连接的网络中，每一个查询都是$O(n)$，总共有$O(n)$个查询，所以总共的代价是$O(n^2)$。使用聚类算法(clustering algorithms)，代价可以降到$O(n)$，因此贝叶斯网络中的聚类算法已经被广泛商用。（这里不明白为什么？）。<br>
聚类算法的基本思想是将网络中的一些节点连接成聚点(cluster nodes)，最后形成一个多树(polytree)结构。例如Figure 14.12(a)中的多连接网络可以转换成Figure 14.12(b)所示的多树，Sprinkler和Rain节点形成了SPrinkler+Rain聚点，这两个布尔变量被一个大节点(meganode)取代，这个大节点有四个可能的取值：$tt,tf,ft,ff$。一旦一个多树形式的网络生成了以后，就需要特殊的推理算法进行推理了，因为普通的推理算法不能处理共享变量的大节点，有了这样一个特殊的算法，后验概率的时间复杂度就是线性于聚类网络的大小。但是，NP问题并没有消失，如果消元需要指数级别的时间和空间复杂度，聚类网络中的CPT也是指数级别大小。</p>
<h2 id="贝叶斯网络的估计推理-approximate-inference-in-bayesian-networks">贝叶斯网络的估计推理(Approximate inference in bayesian networks)</h2>
<p>因为多连接网络中的推理是不可行的，所以用估计推理取代精确推理是很有用的。这一节会介绍随机采样算法，也叫蒙特卡洛算法(Monte Carlo)，它的精确度取决于生成的样本数量。我们的目的是采样用于计算后验概率。这里给出了两类算法，直接采样(direct sampling)和马尔科夫链采样(Markov chain sampling)。变分法(variational methods)和循环传播(loopy propagation)将会在本章的最后进行介绍。</p>
<h3 id="直接采样-direct-sampling-methods">直接采样(Direct sampling methods)</h3>
<p>任何采样算法都是通过一个已知的先验概率分布生成样本。比如一个公平的硬币，服从一个先验分布$P(coin) = &lt;0.5,0.5 &gt; $，从这个分布中采样就像抛硬币。<br>
一个最简单的从贝叶斯网络中进行随机采样的方法就是：从没有证据和它相关的网络中生成事件，即按照拓扑顺序对每一个变量进行采样。如Figure 14.13所示的算法，每一个变量的采样都取决于前之前已经采样过了的父节点变量的值。按照Figure 14.13中的算法对Figure 14.12(a)中的网络进行采样，假设一个采样顺序是[Cloudy,Sprinkler,Rain,WetGrass]：</p>
<ol>
<li>从$P(Cloudy)=&lt;0.5,0.5&gt;$中采样，采样值是true；</li>
<li>从$P(Sprinkler|Cloudy=true) = &lt;0.1,0.9&gt;$中采样，采样值是false；</li>
<li>从$P(Rain|Cloudy=true)=&lt;0.8,0.2&gt;$中采样，采样值是true；</li>
<li>从$P(WetGrass|Sprinkler=false,Rain=true)=&lt;0.9,0.1&gt;$中采样，采样值是true；</li>
</ol>
<p>这个例子中，PRIOR-SAMPLE算法返回事件[true,false,true,true]。可以看出来，PRIOR-SAMPLE算法根据贝叶斯网络指定的先验联合分布生成样本。假设$S_{PS}(x_1,\cdot,x_n)$是PRIOR-SAMPLE算法生成的一个样本事件，从采样过程中我们可以得出：<br>
$$S_{PS}(x_1,\cdots,x_n) = \prod_{i=1}^nP(x_i|parents(X_i))$$<br>
即每一步采样都只取决于父节点的值。这个式子和贝叶斯网络的联合概率分布是一样的，所以，我们可以得到：<br>
$$S_{PS} = P(x_1,\cdots,x_n).$$<br>
通过采样让这个联合分布的求解很简单。<br>
<img src="/2019/01/06/AI-chapter-14-Probabilistic-reasoning/" alt="figure 14.13"><br>
事实上在任何采样算法中，结果都是通过对产生的样本进行计数得到的。假设生成了$N$个样本，$N_{PS}(x_1,\cdots,x_n)$是样本集中的一个具体事件$(x_1,\cdots,x_n)$发生的次数。我们希望这个值比上样本总数取极限和采样概率$S_{PS}$是一样的，即：<br>
$$ lim_{N\rightarrow \infty}\frac{N_{PS}(x_1,\cdots,x_n)}{N} = S_{PS}(x_1,\cdots,x_n) = P(x_1,\cdots,x_n).$$<br>
例如之前利用PRIOR-SAMPLE算法产生的事件[true,false,true,true]，这个事件的采样概率是：<br>
$$S_{PS}(true,false,true,true) = 0.5 \times 0.9 \times 0.8 \times 0.9 = 0.324.$$<br>
即当$N$取极限时，我们希望有$32.4%$的样本都是这个事件。(这里为什么要用采样进行计算呢，我的想法是因为实际情况中，采样概率$S_{PS}$是很难计算的，就通过不断的采样，计算出某个样本出现的概率。)<br>
我们用$\approx$表示估计概率(estimated probability)在样本数量$N$取极限时和真实概率一样的估计，这叫一致(consistent)估计。比如，对于任意的含有隐变量的事件(partially spefified event)，$x_1,\cdots,x_m,m\le n$，会产生一个一致估计：<br>
$$P(x_1,\cdots,x_m)\approx N_{PS}(x_1,\cdots,x_m)/N.$$<br>
这个事件的概率可以看成所有满足观测变量条件的样本事件（隐变量所有值都可以取）比上所有样本事件的比值。比如在Spinkler网络中，生成$1000$个样本，其中有$511$个样本的Rain=true，那么rain的估计概率就是$\hat{P}(Rain=true) = 0.511.$</p>
<h4 id="贝叶斯网络的拒绝采样-rejection-sampling-in-bayesian-networks">贝叶斯网络的拒绝采样(Rejection sampling in Bayesian networks)</h4>
<h5 id="算法">算法</h5>
<p><img src="/2019/01/06/AI-chapter-14-Probabilistic-reasoning/" alt="figure 14.14"><br>
拒绝采样(rejection sampling)利用容易采样的分布来生成难采样分布的样本，计算后验概率$P(X|\mathbf{e})$，算法流程如Figure 14.14所示，首先根据贝叶斯网络的先验分布生成样本，接下来拒绝(reject)那些和证据变量不匹配的结果，最后在剩下的样本中统计每个$X=x$出现的概率，估计$\hat{P}(X|\mathbf{e}).$<br>
用$\hat{P}(X|\mathbf{e})$表示估计概率分布，利用拒绝采样算法的定义计算：<br>
$$\hat{P}(X|\mathbf{e}) = \alpha N_{PS}(X,\mathbf{e}) = \frac{N_{PS}(X,\mathbf{e})}{N_{PS}(\mathbf{e})}.$$<br>
而根据$P(x_1,\cdots,x_m)\approx N_{PS}(x_1,\cdots,x_m)/N$，就有：<br>
$$\hat{P}(X|\mathbf{e}) = \alpha N_{PS}(X,\mathbf{e}) = \frac{N_{PS}(X,\mathbf{e})}{N_{PS}(\mathbf{e})} =  \frac {P(X,\mathbf{e})}{P(\mathbf{e})} = P(X|\mathbf{e}).$$<br>
所以，拒绝采样产生了真实概率的一个一致估计(consistent estimate)，但是这个一致估计和无偏估计还不一样。</p>
<h5 id="示例-v3">示例</h5>
<p>举一个例子来说明，假设我们要估计概率$P(Rain|Sprinkler=true)$，生成了$100$个样本，其中$73$个是$Sprinkler=false$，$27$是$Sprinkler=true$，这$27$个中有$8$个$Rain=true$，有$19$个$Rain=false$，因此：<br>
$$P(Rain|Sprinkler=true)\approx NORMALIZE \lt\lt 8,19&gt;&gt; = &lt;0.296,0.704&gt;.$$<br>
正确答案是$&lt;0.3,0.7&gt;$，可以看出来，估计值和真实值差的不多。生成的样本越多，估计值就会和正确值越接近，概率的估计误差和$1/\sqrt{n}$成比例，$n$是用来估计概率的样本数量。</p>
<h5 id="不足">不足</h5>
<p>拒绝采样最大的问题是它拒绝了很多样本，随着证据变量的增加，和证据$\mathbf{e}$一致的样本指数速度减少，所以这个方法对于复杂的问题是不可行的。拒绝假设和现实生活中条件概率是很像的，比如估计观测到晚上天空是红的，第二天下雨的概率$P(Rain|RedSkyAtNight=ture)$，这个条件概率的估计就是根据日常生活的观察实现的。但是如果天空很少是红的，就需要很长时间才能估计它的值，这就是拒绝假设的缺点。</p>
<h4 id="可能性加权-likelihood-weighting">可能性加权(Likelihood weighting)</h4>
<h5 id="算法-v2">算法</h5>
<p>可能性加权(Likelihood weighting)只产生和证据$\mathbf{e}$一致的事件，因此避免了拒绝采样的低效。它是统计学中重要性采样的一个例子，专门为贝叶斯推理设计的。<br>
如Figure 14.15所示，加权似然固定证据变量$\mathbf{E}$的值，只对非证据变量进行采样，这就保证了每一个事件都是和证据一致的。但是，不是所有的事件权重都是一样的。给定每一个证据变量的父节点，它的可能性(likelihood)是证据变量的条件概率的乘积，每一个事件都根据证据的可能性进行加权。</p>
<h5 id="示例-v4">示例</h5>
<p>对于Figure 14.12(a)中的例子，计算后验概率$P(Rain|Cloudy=true,WetGrass=true)$，采样顺序是Cloudy,Sprinkler,Rain,WetGrass。过程如下，首先，权重$w$设为$1$，一个事件生成过程如下：</p>
<ol>
<li>Cloudy是一个证据变量，它的值是true,因此，令：<br>
$$w\leftarrow w\times P(cloudy=true) = 0.5.$$</li>
<li>Sprinkler是隐变量，所以从$P(Sprinkler|Cloudy=true)=&lt;0.1,0.9&gt;$中采样，假设采样结果是false；</li>
<li>Rain是隐变量，从$P(Rain|Cloudy=true)=&lt;0.8,0.2&gt;$中采样，假设采样结果是true；</li>
<li>WetGrass是证据变量，值是true,令：<br>
$$w\leftarrow w\times P(WetGrass=true|Sprinkler=false,Rain=true) = 0.45.$$</li>
</ol>
<p>所以WEIGHTED-SAMPLE算法生成事件[true,false,true,true]，相应的权重是$0.45$。</p>
<h5 id="原理">原理</h5>
<p>用$S_{WS}$表示WEIGHTED-SAMPLE算法中事件的采样概率，证据变量$\mathbf{E}$的取值$\mathbf{e}$是固定的，用$\mathbf{Z}$表示非证据变量，包括隐变量$\mathbf{Y}$和查询变量$\mathbf{X}$。给定变量$\mathbf{Z}$的父节点，算法对变量$\mathbf{Z}$进行采样：<br>
$$S_{WS}(\mathbf{z},\mathbf{e}) = \prod_{i=1}^lP(z_i|parents(Z_i)).$$<br>
其中$Parents(Z_i)$可能同时包含证据变量和非证据变量。<br>
和先验分布$P(\mathbf{z})$不同的是，每一个变量$Z_i$的取值会受到$Z_i$的祖先(ancestor)变量的影响。比如，对Sprinkler进行采样的时候，算法会受到它的父节点中的证据变量Cloudy=true的影响，而先验分布不会。另一方面，$S_{WS}$比后验分布$P(\mathbf{z}|\mathbf{e})$受证据的影响更小，因为对$Z_i$的采样忽略了$Z_i$的非祖先(non-ancestor)变量中的证据。比如，对Sprinkler和Rain进行采样的时候，算法忽略了子节点中的证据变量WetGrass=true，事实上这个证据已经排除了(rule out)Sprinkler=false和Rain=false的情况，但是WEIGHTED-SAMPLE还会产生很多这样的样本事件。<br>
理想情况下，我们想要一个采样分布和真实的后验概率$P(\mathbf{z}|\mathbf{e})$相等，不幸的是不存在这样的多项式时间的算法。如果有这样的算法的话，我们可以用多项式数量的样本以任意精度逼近想要求的概率值。<br>
可能性权重$w$弥补了实际的分布和我们想要的分布之间的差距。一个由$\mathbf{z}$和$\mathbf{e}$组成的样本$\mathbf{x}$的权重是给定了父节点的证据变量的可能性乘积：<br>
$$w(\mathbf{z},\mathbf{e}) = \prod_{i=1}^mP(e_i|parents(E_i)).$$<br>
将上面的两个式子乘起来，可以得到一个样本的加权概率(weighted probability)是：<br>
$$S_{WS}(\mathbf{z},\mathbf{e})w(\mathbf{z},\mathbf{e}) = \prod_{i=1}<sup>lP(z_i|parents(Z_i))\prod_{i=1}</sup>mP(e_i|parents(E_i)) = P(\mathbf{z},\mathbf{e}).$$<br>
可能性加权估计是一致估计。对于任意的$x$，估计的后验概率按下式计算：<br>
\begin{align*}<br>
\hat{P}(x|\mathbf{e}) &amp;= \alpha \sum_{\mathbf{y}} N_{WS}(x,\mathbf{y},\mathbf{e})w(x,\mathbf{y},\mathbf{e})\<br>
&amp;\approx \alpha’\sum_{\mathbf{y}}S_{WS}(x,\mathbf{y},\mathbf{e})w(x,\mathbf{y},\mathbf{e})\<br>
&amp;=\alpha’\sum_{\mathbf{y}}P(x,\mathbf{y},\mathbf{e})\<br>
&amp;=\alpha’\sum_{\mathbf{y}}P(x,\mathbf{y},\mathbf{e})\<br>
&amp;=P(x|\mathbf{e})<br>
\end{align*}<br>
算法中真实实现的是第一行，即统计出用WEIGHTED-SAMPLE产生的样本$(x,\mathbf{y},\mathbf{e})$数量$N_{WS}$，以及对应的权重$w(x,\mathbf{y},\mathbf{e})$，后面的都是理论推导，当$N$取极限的时候$lim_{N\rightarrow \infty}\frac{N_{WS}(x_1,\cdots,x_n)}{N} = S_{WS}(x_1,\cdots,x_n)$，后面的都是为了证明算法是一致估计。</p>
<h5 id="不足-v2">不足</h5>
<p>可能性加权算法使用了所有生成的样本，它比拒绝假设算法更高效。然而，随着证据变量的增加，算法性能会退化(degradation)，这是因为很多样本的权重都会很小，因此加权估计可能会受一小部分权重很大的样本的影响(dominated)。如果证据变量在非证据变量的后边，这个问题会加剧，因为它们的父节点或者祖先节点没有证据变量来指导样本的生成。这就意味着生成的样本和证据变量支撑的真实情况可能差距很大(bear little resemblance)。</p>
<h3 id="马尔科夫链仿真推理-inference-by-markov-chain-simulation">马尔科夫链仿真推理(Inference by Markov chain simulation)</h3>
<p><a href="https://mxxhcm.github.io/2019/08/01/Monte-Carlo-Markov-Chain/">马尔科夫链蒙特卡洛(Markov chain Monte Carlo,MCMC)</a>算法和拒绝采样以及可能性加权很不一样。那两个方法每次都从头开始生成样本，而MCMC算法在之前的样本上做一些随机的变化。可以将MCMC算法看成指定了每一个变量值的特殊当前状态(current state)，通过对当前状态(current state)做任意的改变生成下一个状态(next state)。这一节要介绍的一种MCMC算法是吉布森采样(Gibbs sampling)。</p>
<h4 id="贝叶斯网络中的吉布森采样-gibbs-sampling-in-bayesian-networks">贝叶斯网络中的吉布森采样(Gibbs sampling in Bayesian networks)</h4>
<h5 id="算法-v3">算法</h5>
<p>贝叶斯网络中的吉布森采样从任意一个状态开始，其中证据变量的取值固定为观测值，通过随机选取非证据变量$X_i$的值生成下一个状态。变量$X_i$的采样取决于变量$X_i$的马尔科夫毯的当前值。算法在状态空间（所有非证据变量的全部可能取值空间）中随机采样，每次采样都保持证据变量不变，一次改变一个非证据变量的值。完整的算法如Figure 14.16所示。<br>
<img src="/2019/01/06/AI-chapter-14-Probabilistic-reasoning/" alt="figure 14.16"></p>
<h5 id="示例-v5">示例</h5>
<p>Figure 14.12(a)中的查询(query)$P(Rain|Sprinkler=true, WetGrass=true)$，证据变量Spinkler和WetGrass取它们的观测值不变，非证据变量Cloudy和Rain随机初始化，假设取的是true和false。那么初始状态就是[true,true,false,true]，接下来对非证据变量进行重复的随机采样。<br>
比如第一次对Cloudy采样（也可以对Rain采样），给定它的马尔科夫毯变量，然后从$P(Cloudy|Sprinkler=true,Rain=false)$中进行采样，假设采样结果是false，新的状态就是[false,true,false,true]。接下来随机可以对Rain采样（也可以对Cloudy采样），给定Rain的马尔科夫毯变量的取值，从$P(Rain|Cloudy=false,Sprinkler=true,WetGrass=true)$中进行采样，假设采样值是true,那么新的状态是[true,true,false,false]。接下来可以一直进行采样。。最终利用生成的样本计算出相应的概率。</p>
<h4 id="为什么吉布森采样有用-why-gibbs-sampling-works">为什么吉布森采样有用(Why Gibbs sampling works)</h4>
<p>接下来给出为什么吉布森采样计算后验概率是一致估计。基本的解释是直截了当的：采样过程建立了一个动态平衡，每个状态花费的时间长期来说和它的后验概率是成比例的。<br>
具体的，不想看了。。。就随缘吧</p>
<h2 id="关系和一阶概率模型-relational-and-first-order-probability-models">关系和一阶概率模型(Relational and first-order probability models)</h2>
<h2 id="其他不确定性推理的方法-other-approaches-to-uncertain-reasoning">其他不确定性推理的方法(Other approaches to uncertain reasoning)</h2>
<h2 id="参考文献">参考文献</h2>
<p>1.<a href="http://aima.cs.berkeley.edu/" target="_blank" rel="noopener">Artificial Intelligence A Modern Approach Third Edition,Stuart Russell,Peter Norvig.</a><br>
2.<a href="https://en.wikipedia.org/wiki/Chain_rule_(probability)" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Chain_rule_(probability)</a><br>
3.<a href="https://en.wikipedia.org/wiki/Consistent_estimator" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Consistent_estimator</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://mxxhcm.github.io/2019/01/03/linear-algebra-singular-value-decomposition/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="马晓鑫爱马荟荟">
      <meta itemprop="description" content="记录硕士三年自己的积累">
      <meta itemprop="image" content="/images/favicon.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="mxxhcm's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/01/03/linear-algebra-singular-value-decomposition/" class="post-title-link" itemprop="http://mxxhcm.github.io/page/16/index.html">singular value decomposition（奇异值分解）</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-01-03 15:19:54" itemprop="dateCreated datePublished" datetime="2019-01-03T15:19:54+08:00">2019-01-03</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-09-09 16:53:25" itemprop="dateModified" datetime="2019-09-09T16:53:25+08:00">2019-09-09</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/线性代数/" itemprop="url" rel="index"><span itemprop="name">线性代数</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="特征值分解-eigen-value-decomposition">特征值分解(eigen value decomposition)</h2>
<p>要谈奇异值分解，首先要从特征值分解(eigen value decomposition, EVD)谈起。<br>
矩阵的作用有三个：一个是旋转，一个是拉伸，一个是平移，都是线性操作。如果一个$n\times n$方阵$A$对某个向量$x$只产生拉伸变换，而不产生旋转和平移变换，那么这个向量就称为方阵$A$的特征向量(eigenvector)，对应的伸缩比例叫做特征值(eigenvalue)，即满足等式$Ax = \lambda x$。其中$A$是方阵，$x$是方阵$A$的一个特征向量，$\lambda$是方阵$A$对应特征向量$x$的特征值。<br>
假设$S$是由方阵$A$的$n$个线性无关的特征向量构成的方阵，$\Lambda$是方阵$A$的$n$个特征值构成的对角矩阵，则$A=S\Lambda S^{-1}$，这个过程叫做对角化过程。<br>
证明：<br>
因为$Ax_1 = \lambda_1 x_1,\cdots,Ax_n = \lambda_n x_n$,<br>
所以<br>
\begin{align*}AS &amp;= A\begin{bmatrix}x_1&amp; \cdots&amp;x_n\end{bmatrix}\\<br>
&amp;=\begin{bmatrix} \lambda_1x_1&amp;\cdots&amp;\lambda x_n\end{bmatrix}\\<br>
&amp;= \begin{bmatrix}x_1&amp; \cdots&amp;x_n\end{bmatrix} \begin{bmatrix}\lambda_1&amp; &amp; &amp;\\&amp;\lambda_2&amp;&amp;\\&amp;&amp;\cdots&amp;\\&amp;&amp;&amp;\lambda_n\end{bmatrix}\<br>
&amp;= S\Lambda<br>
\end{align*}<br>
所以$AS=S\Lambda, A=S\Lambda S^{-1}, S^{-1}AS=\Lambda$。<br>
若方阵$A$为对称矩阵，矩阵$A$的特征向量是正交的，将其单位化为$Q$，则$A=Q\Lambda Q^T$，这个过程就叫做特征值分解。</p>
<h2 id="奇异值分解-singular-value-decomposition">奇异值分解(singular value decomposition)</h2>
<p>特征值分解是一个非常好的分解，因为它能把一个方阵分解称两类非常好的矩阵，一个是正交阵，一个是对角阵，这些矩阵都便于进行各种计算，但是它对于原始矩阵的要求太严格了，必须要求矩阵是对称正定矩阵，这是一个很苛刻的条件。所以就产生了奇异值分解，奇异值分解可以看作特征值分解在$m\times n$维矩阵上的推广。对于对称正定矩阵来说，有特征值，对于其他一般矩阵，有奇异值。</p>
<p>奇异值分解可以看作将一组正交基映射到另一组正交基的变换。普通矩阵$A$不是对称正定矩阵，但是$AA^T $和$A^TA $一定是对称矩阵，且至少是半正定的。从对$A^TA $进行特征值分解开始，$A^T A=V\Sigma_1V^T $，$V$是一组正交的单位化特征向量${v_1,\cdots,v_n}$，则$Av_1,\cdots,Av_n$也是正交的。<br>
证明：<br>
\begin{align*}Av_1\cdot Av_2 &amp;=(Av_1)^T Av_2\\<br>
&amp;=v_1^T A^T Av_2\\<br>
&amp;=v_1^T \lambda v_2\\<br>
&amp;=\lambda v_1^T v_2\\<br>
&amp;=0<br>
\end{align*}<br>
所以$Av_1,Av_2$是正交的，同理可得$Av_1,\cdots,Av_n$都是正交的。<br>
而：<br>
\begin{align*}<br>
Av_i\cdot Av_i &amp;= v_i^T A^T Av_i\\<br>
&amp;=v_i \lambda v_i\\<br>
&amp;=\lambda v_i^2\\<br>
&amp;=\lambda<br>
\end{align*}<br>
将$Av_i$单位化为$u_i$，得$u_i = \frac{Av_i}{|Av_i|} = \frac{Av_i}{\sqrt{\lambda_i}}$，所以$Av_i = \sqrt{\lambda_i}u_i$。<br>
将向量组${v_1,\cdots,v_r}$扩充到$R^n $中的标准正交基${v_1,\cdots,v_n}$，将向量组${u_1,\cdots,u_r}$扩充到$R^n $中的标准正交基${u_1,\cdots,u_n}$，则$AV = U\Sigma$，$A=U\sigma V^T $。</p>
<p>事实上，奇异值分解可以看作将行空间的一组正交基加上零空间的一组基映射到列空间的一组正交基加上左零空间的一组基的变换。对一矩阵$A,A\in \mathbb{R}^{m\times n} $，若$r(A)=r$，取行空间的一组特殊正交基${v_1,\cdots,v_r}$，当矩阵$A$作用到这组基上，会得到另一组正交基${u_1,\cdots,u_r}$，即$Av_i = \sigma_iu_i$。<br>
矩阵表示是：<br>
\begin{align*}<br>
AV &amp;= A\begin{bmatrix}v_1&amp;\cdots&amp;v_r\end{bmatrix}\\<br>
&amp;= \begin{bmatrix}\sigma_1u_1 &amp; \cdots &amp; \sigma_ru_r\end{bmatrix}\\<br>
&amp;= \begin{bmatrix}u_1&amp;u_2&amp;\cdots&amp;u_r\end{bmatrix}\begin{bmatrix}\sigma_1&amp;&amp;&amp;\\&amp;\sigma_2&amp;&amp;\\&amp;&amp;\cdots&amp;\\&amp;&amp;&amp;\sigma_n\end{bmatrix}\\<br>
&amp;=U\Sigma<br>
\end{align*}<br>
其中$A\in \mathbb{R}^{m\times n}, V\in \mathbb{R}^{n\times r},U\in \mathbb{R}^{m\times r}, \Sigma \in \mathbb{R}^{r\times n}$。<br>
当有零空间的时候，行空间的一组基是$r$维，加上零空间的$n-r$维，构成$R^n $空间中的一组标准正交基。列空间的一组基也是$r$维的，加上左零空间的$m-r$维，构成$R^m $空间的一组标准正交基。零空间中的向量在对角矩阵$\Sigma$中体现为$0$，<br>
则$A=U\Sigma V^{-1} $，$V$是正交的，所以$A=U\Sigma V^T $，其中$V\in \mathbb{R}^{n\times n}, U\in \mathbb{R}^{m\times m}, \Sigma \in \mathbb{R}^{m\times n}$。</p>
<p>$A=U\Sigma V^T $,<br>
$A^T = V\Sigma^T U^T $,<br>
$AA^T = U\Sigma V^T V\Sigma^T U^T $,<br>
$A^T A = V\Sigma^T U^T U\Sigma V^T $<br>
对$A A^T $和$A^T A$作特征值分解，则$A A^T = U\Sigma_1U^T $,$A^T A=V\Sigma_2V^T $，所以对$AA^T $作特征值分解求出来的$U$和对$A^T A$作特征值分解求出来的$V$就是对$A$作奇异值分解求出来的$U$和$V$，$AA^T $和$A^T A$作特征值分解求出来的$\Sigma$的非零值是相等的，都是对$A$作奇异值分解的$\Sigma$的平方。</p>
<h3 id="a-t-a-和-aa-t-的非零特征值是相等的">$A^T A$和$AA^T $的非零特征值是相等的</h3>
<p>证明：对于任意的$m\times n$矩阵$A$，$A^T A$和$AA^T $的非零特征值相同的。 设$A^T A$的特征值为$\lambda_i$，对应的特征向量为$v_i$，即$A^T Av_i = \lambda_i v_i$。<br>
则$AA^T Av_i = A\lambda_iv_i = \lambda_i Av_i$。<br>
所以$AA^T $的特征值为$\lambda_i$，对应的特征向量为$Av_i$。<br>
因此$A^T A$和$AA^T $的非零特征值相等。</p>
<h3 id="几何意义">几何意义</h3>
<p>对于任意一个矩阵，找到其行空间(加上零空间)的一组正交向量，使得该矩阵作用在该向量序列上得到的新的向量序列保持两两正交。奇异值的几何意义就是这组变化后的新的向量序列的长度。</p>
<h3 id="物理意义">物理意义</h3>
<p>奇异值往往对应着矩阵隐含的重要信息，且重要性和奇异值大小正相关。每个矩阵都可以表示为一系列秩为$1$的“小矩阵”的和，而奇异值则衡量了这些秩一矩阵对$A$的权重。<br>
奇异值分解的物理意义可以通过图像压缩表现出来。给定一张$m\times n$像素的照片$A$，用奇异值分解将矩阵分解为若干个秩一矩阵之和，即：<br>
\begin{align*}<br>
A&amp;=\sigma_1 u_1v_1^T +\sigma_2 u_2v_2^T +\cdots+\sigma_r u_rv_r^T\\<br>
&amp;= \begin{bmatrix}u_1&amp;u_2&amp;\cdots&amp;u_r\end{bmatrix}\begin{bmatrix}\sigma_1&amp;&amp;&amp;\&amp;\sigma_2&amp;&amp;\&amp;&amp;\cdots&amp;\&amp;&amp;&amp;\sigma_n\end{bmatrix}\begin{bmatrix}v_1<sup>T\v_2</sup>T\ \vdots\v_r^T\end{bmatrix}\\<br>
&amp;=U\Sigma V^T<br>
\end{align*}</p>
<p>这个也叫部分奇异值分解。其中$V\in R^{r\times n}, U\in R^{m\times r}, \Sigma \in R^{r\times r}$。因为不含有零空间和左零空间的基，如果加上零空间的$n-r$维和左零空间的$m-r$维，就是奇异值分解。<br>
较大的奇异值保存了图片的主要信息，特别小的奇异值有时可能是噪声，或者对于图片的整体信息不是特别重要。做图像压缩的时候，可以只取一部分较大的奇异值，比如取前八个奇异值作为压缩后的图片：<br>
$$A = \sigma_1 u_1v_1^T +\sigma_2 u_2v_2^T + \cdots + \sigma_8 u_8v_8^T$$<br>
现实中常用的做法有两个：</p>
<ol>
<li>保留矩阵中$90%$的信息：将奇异值平方和累加到总值的%90%为止。</li>
<li>当矩阵有上万个奇异值的时候，取前面的$2000$或者$3000$个奇异值。。</li>
</ol>
<h2 id="参考文献-references">参考文献(references)</h2>
<p>1.Gilbert Strang, MIT Open course：Linear Algebra<br>
2.<a href="https://www.cnblogs.com/pinard/p/6251584.html" target="_blank" rel="noopener">https://www.cnblogs.com/pinard/p/6251584.html</a><br>
3.<a href="http://www.ams.org/publicoutreach/feature-column/fcarc-svd" target="_blank" rel="noopener">http://www.ams.org/publicoutreach/feature-column/fcarc-svd</a><br>
4.<a href="https://www.zhihu.com/question/22237507/answer/53804902" target="_blank" rel="noopener">https://www.zhihu.com/question/22237507/answer/53804902</a><br>
5.<a href="http://charleshm.github.io/2016/03/Singularly-Valuable-Decomposition/" target="_blank" rel="noopener">http://charleshm.github.io/2016/03/Singularly-Valuable-Decomposition/</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://mxxhcm.github.io/2018/12/24/convex-optimization-chapter-2-Convex-sets/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="马晓鑫爱马荟荟">
      <meta itemprop="description" content="记录硕士三年自己的积累">
      <meta itemprop="image" content="/images/favicon.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="mxxhcm's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2018/12/24/convex-optimization-chapter-2-Convex-sets/" class="post-title-link" itemprop="http://mxxhcm.github.io/page/16/index.html">convex optimization chapter 2 Convex sets</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-12-24 16:28:45" itemprop="dateCreated datePublished" datetime="2018-12-24T16:28:45+08:00">2018-12-24</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-09-09 16:25:28" itemprop="dateModified" datetime="2019-09-09T16:25:28+08:00">2019-09-09</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/凸优化/" itemprop="url" rel="index"><span itemprop="name">凸优化</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="仿射集-affine-sets-和凸集-convex-sets">仿射集(affine sets)和凸集(convex sets)</h2>
<h3 id="直线-line-和线段-line-segmens">直线(line)和线段(line segmens)</h3>
<p>假设$x_1,x_2 \in \mathbb{R}^n $是n维空间中不重合$(x_1 \ne x_2)$的两点，给定：<br>
$$y = \theta x_1 + (1 - \theta)x_2,$$<br>
当$\theta\in R$时，$y$是经过点$x_1$和点$x_2$的直线。当$\theta=1$时，$y=x_1$,当$\theta=0$时，$y=x_2$。当$\theta\in[0,1]$时，$y$是$x_1$和$x_2$之间的线段(line segment)。 把$y$改写成如下形式： $$y = x_2 + \theta(x_1 - x_2)$$，可以给出另一种解释，$y$是点$x_2$和方向$x_1 - x_2$(从$x_2$到$x_1$的方向)乘上一个缩放因子$\theta$的和。<br>
如下图所示，可以将y看成$\theta$的函数。<br>
<img src="https://ws1.sinaimg.cn/large/006wtfMEly1fyhy7m4llij30mz0alwep.jpg" alt="line_line-segment"></p>
<h3 id="仿射集-affine-sets">仿射集(affine sets)</h3>
<h4 id="仿射集的定义">仿射集的定义</h4>
<p>给定一个集合$C\subset \mathbb{R}^n $,如果经过$C$中任意两个不同点的直线仍然在$C$中，那么$C$就是一个仿射集。即，对于任意$x_1,x_2\in C$和$\theta\in R$，都有$\theta x_1 + (1 - \theta)x_2 \in C$。换句话说，给定线性组合的系数和为$1$，$C$中任意两点的线性组合仍然在$C$中，我们就称这样的集合是仿射的(affine)。</p>
<h4 id="仿射组合-affine-combination">仿射组合(affine combination)</h4>
<p>我们可以把两个点的线性组合推广到多个点的线性组合，这里称它为仿射组合。<br>
仿射组合的定义：给定$\theta_1+\cdots+\theta_k = 1$,则$\theta_1 x_1 + \cdots + \theta_k x_k$是点$x_1,\cdots,x_k$的仿射组合(affine combination)。<br>
根据仿射集的定义，一个仿射集(affine set)包含集合中任意两个点的仿射（线性）组合，那么可以推导出仿射集包含集合中任意点（大于等于两个）的仿射组合，即：如果$C$是一个仿射集，$x_1,\cdots,x_k\in C$,且$\theta_1 x_1 + \cdots + \theta_k x_k = 1$,那么点$\theta_1 x_1 + \cdots + \theta_k x_k$仍然属于$C$。</p>
<h4 id="仿射集的子空间-subspce">仿射集的子空间(subspce)</h4>
<p>如果$C$是一个仿射集，$x_0 \in C$,那么集合<br>
$$V = C - x_0 = {x - x_0\big|x \in C}$$<br>
是一个子空间(subspace),因为$V$是加法封闭和数乘封闭的。<br>
证明：<br>
假设$v_1, v_2 \in V$，并且$\alpha,\beta \in R$。<br>
要证明V是一个子空间，那么只需要证明$\alpha v_1 + \beta v_2 \in V$即可。<br>
因为$v_1, v_2 \in V$，则$v_1+x_0, v_2+x_0 \in C$。<br>
而$x_0 \in C$，所以有<br>
$$\alpha(v_1+x_0) + \beta(v_2+x_0) + (1 - \alpha - \beta)x_0 \in C$$<br>
即：<br>
\begin{align*}<br>
\alpha v_1 + \beta v_2 + (\alpha + \beta + 1 - \alpha - \beta)x_0 &amp;\in C\\<br>
\alpha v_1 + \beta v_2 + x_0 &amp;\in C<br>
\end{align*}<br>
所以$\alpha v_1 + \beta v_2 \in V$。<br>
所以，仿射集$C$可以写成：<br>
$$C = V + x_0 = { v + x_0\big| v \in V},$$<br>
即，一个子空间加上一个偏移(offset)。而与仿射集$C$相关的子空间$V$与$x_0$的选择无关，即$x_0$可以为$C$中任意一点。</p>
<h4 id="示例">示例</h4>
<p>线性方程组的解。一个线性方程组的解可以表示为一个仿射集:$C={x\big|Ax = b}$,其中 $A\in \mathbb{R}^{m \times n}, b \in \mathbb{R}^m $。<br>
证明：<br>
设$x_1, x_2 \in C$,即$Ax_1 = b, Ax_2 = b$。对于任意$\theta \in R$,有:<br>
\begin{align*}<br>
A(\theta x_1 + (1-\theta x_2) &amp;= \theta Ax_1 + (1-\theta)Ax_2\\<br>
&amp;= \theta b + (1 - \theta) b\\<br>
&amp;= b \end{align*}<br>
所以线性方程组的解是一个仿射组合：$\theta x_1 + (1 - \theta) x_2$，这个仿射组合在集合$C$中，所以线性方程组的解集$C$是一个仿射集。<br>
和该仿射集$C$相关的子空间$V$是$A$的零空间(nullspace)。因为仿射集$C$中的任意点都是方程$Ax = b$的解，而$V = C - x_0 = {x - x_0\big|x \in C}$，有$Ax = b, Ax_0 = b$，则$Ax - Ax_0 = A(x - x_0) = b - b = 0$，所以$V$是$A$的零空间。</p>
<h4 id="仿射包-affine-hull">仿射包(affine hull)</h4>
<p>给定集合$C\subset \mathbb{R}^n $，集合中点的仿射组合称为集合$C$的仿射包(affine hull),表示为$aff C$:<br>
$aff C = {\theta_1 x_1 + \cdots + \theta_k x_k\big| x_1,\cdots,x_k \in C, \theta_1 + \cdots + \theta_k = 1}$<br>
集合$C$可以是任意集合。仿射包是包含集合$C$的最小仿射集（一个集合的仿射包只有一个，是不变的）。即如果$S$是任意仿射集，满足$C\subset S$，那么有$aff C \subset S$。或者说仿射包是所有包含集合$C$的仿射集的交集。</p>
<h3 id="仿射纬度-affine-dimension-和相对内部-relative-interior">仿射纬度(affine dimension)和相对内部(relative interior)</h3>
<h4 id="拓扑-topology">拓扑(topology)</h4>
<p>拓扑(topology)，开集(open sets),闭集(close sets),内部(interior),边界(boundary),闭包(closure),邻域(neighbood),相对内部(relative interior)<br>
同一个集合可以有很多个不同的拓扑。</p>
<h5 id="定义">定义</h5>
<p>给定一个集合$X$,$\tau$是$X$的一系列子集，如果$\tau$满足以下条件：</p>
<ol>
<li>空集(empty set)和全集X都是$\tau$的元素;</li>
<li>$\tau$中任意元素的并集(union)仍然是$\tau$的元素;</li>
<li>$\tau$中任意有限多个元素的交集(intersection)仍然是$\tau$中的元素。</li>
</ol>
<p>则称$\tau$是集合$X$上的一个拓扑。<br>
如果$\tau$是$X$上的一个拓扑，那么$(X,\tau)$对称为一个拓扑空间(topological space)。<br>
如果$X$的一个子集在$\tau$中，这个子集被称为开集(open set)。<br>
如果$X$的一个子集的补集是在$\tau$中，那么这个子集是闭集(closed set)。<br>
$X$的子集可能是开集，闭集，或者都是，都不是。<br>
空集和全集是开集，也是闭集（定义）。</p>
<h5 id="示例-v2">示例</h5>
<ol>
<li>给定集合$X={1,2,3,4}$, 集合$\tau = { {},{1,2,3,4} }$就是$X$上的一个拓扑。</li>
<li>给定集合$X={1,2,3,4}$, 集合$\tau = { {},{1}, {3,4},{1,3,4},{1,2,3,4} }$就是$X$上的另一个拓扑。</li>
<li>给定集合$X={1,2,3,4}$, $X$的幂集(power set)也是$X$上的另一个拓扑。</li>
</ol>
<p><strong>通常如果不说的话，默认是在欧式空间(1维，2维,…,n维欧式空间)的拓扑，即欧式拓扑。以下讲的一些概念是在欧式空间的拓扑（通常拓扑）上的定义和一般拓扑直观上可能不太一样，但实际上意义是相同的。</strong></p>
<h4 id="epsilon-disc-或-epsilon-邻域">$\epsilon-disc$或$\epsilon$邻域</h4>
<h5 id="定义-v2">定义</h5>
<p>给定$x\in \mathbb{R}^n $以及$\epsilon\gt 0$，集合<br>
$$D(x,\epsilon) = {y\in \mathbb{R}^n \big|d(x,y) \lt \epsilon}$$<br>
称为关于$x$的$\epsilon-disc$或者$\epsilon$邻域(neighbood)或者$\epsilon$球(ball)。即所有离点$x$距离小于$\epsilon$的点$y$的集合。</p>
<h4 id="开集-open-sets">开集(open sets)</h4>
<h5 id="定义-v3">定义</h5>
<p><strong>给定集合$A\subset \mathbb{R}^n $，对于$A$中的所有元素，即$\forall x\in A$，都存在$\epsilon \gt 0$使得$D(x,\epsilon)\subset A$，那么就称该集合是开的。</strong><br>
即集合$A$中所有元素的$spsilon$邻域都还在集合$A$中（定理$1$）。<br>
<strong>注意：必须满足$\epsilon \gt 0$</strong></p>
<h5 id="定理">定理</h5>
<h6 id="定理-1-epsilon-邻域是开集">定理$1$ $epsilon$邻域是开集</h6>
<ul>
<li>在$\mathbb{R}^n $中，对于一个$\epsilon \gt 0, x\in \mathbb{R}^n $,那么集合$x$的$\epsilon$邻域$D(x,\epsilon)$是开的，给定一个$\epsilon$，能找到一个更小的$epsilon$邻域。</li>
</ul>
<h6 id="定理-2">定理$2$</h6>
<ul>
<li>$\mathbb{R}^n $中有限个开子集的交集是$\mathbb{R}^n $的开子集。</li>
<li>$\mathbb{R}^n $中任意个开子集的并集是$\mathbb{R}^n $的开子集。</li>
</ul>
<p><strong>注意：任意开集的交可能不是开集，一个点不是开集，但是它是所有包含它的开集的交。</strong></p>
<h5 id="示例-v3">示例</h5>
<p><img src="/2018/12/24/convex-optimization-chapter-2-Convex-sets/unit_circle.png" alt="unit_circle"></p>
<ol>
<li>$\mathbb{R}^2 $中的不包含边界的球是开的，如图。</li>
<li>考虑一个$\mathbb{R}^1 $中的开区间，如$(0,1)$，它是一个开集，但是如果把它放在二维欧式空间中(是x轴上的一个线段)，它不是开的，不满足定义，所以开集是必须针对于某一个给定的集合$X$。</li>
<li>$\mathbb{R}^2 $上的包含边界的单位圆$X = {x\in \mathbb{R}^2 \big||x|\le 1}$不是开的。因为边界上的点$x$不满足$\epsilon \gt 0, D(x,\epsilon) \subset X$。</li>
<li>集合$S={(x,y) \in \mathbb{R}^2 \big|0 \lt x \lt 1}$是开集。对于每个点$(x,y)\in S$,我们可以画出半径$r = min{x,1-x}$的邻域并且其全部含于$S$，所以$S$是开集。</li>
<li>集合$S={(x,y) \in \mathbb{R}^2 \big|0 \lt x \le 1}$不是开集。因为点$(1,0) \in S$的邻域包含点$(x,0)$,其中$x\gt 1$。</li>
</ol>
<h4 id="内部-interior">内部(interior)</h4>
<h5 id="定义-v4">定义</h5>
<p><strong>给定集合$A\subset \mathbb{R}^n $,点$x \in A$，如果有一个开集$U$使得$x \in U\subset A$,那么该点就称为$A$的一个内点。或者说对于$x\in A$，有一个$\epsilon \gt 0$使得$D(x,\epsilon)\subset A$。$A$的所有内点组成的集合叫做$A$的内部(interior)，记做$int(A)$。</strong></p>
<h5 id="属性">属性</h5>
<ol>
<li>集合内部可能是空的，单点的内部就是空的。</li>
<li>单位圆的内部是不包含边界的单位圆。</li>
<li>事实上$A$的内部是$A$所有开子集的并，由开集的定理得$A$的内部是开的，且$A$的内部是$A$的最大的开集。</li>
<li>当且仅当$A$的内部等于$A$的时候，$A$是开集（$A$可能是闭集）。</li>
<li>只需要寻找集合内$\epsilon$邻域还在这个集合内的点即可。</li>
</ol>
<h5 id="示例-v4">示例</h5>
<ol>
<li>给定集合$S={(x,y)\in \mathbb{R}^2 \big| 0 \lt x \le 1}$，$int(S) = {(x,y)\big|0 \lt x \lt 1}$。因为区间$(0,1)$中的点都满足它们的$\epsilon$邻域在$S$中。</li>
<li>$int(A) \cup int(B) \ne int(A\cup B)$。在实数轴上，$A=[0,1],B=[1,2]$，那么$int(A) = (0,1),int(B) = (1,2)$，所以$int(A) \cup int(B) = (0,1)\cup (1,2) = (0,2)\backslash {1}$，而$int(A\cup B) = int[0,2] = (0,2)$。</li>
</ol>
<h4 id="闭集-closed-set">闭集(closed set)</h4>
<h5 id="定义-v5">定义</h5>
<p><strong>对于$\mathbb{R}^n $中的集合$B$，如果它在$\mathbb{R}^n $的补（即集合$\mathbb{R}^n \backslash B$）是开集，那么它是闭集。</strong><br>
单点是闭集。含有边界的单位圆组成的集合是闭集，因为它的补集不包含边界。一个集合可能既不是开集也不是闭集。例如，在一维欧几里得空间，半开半闭区间（如$(0,1]$）既不是开集也不是闭集。</p>
<h5 id="定理-v2">定理</h5>
<ol>
<li>$\mathbb{R}^n $中有限个闭子集的并是闭集。</li>
<li>$\mathbb{R}^n $中任意个闭子集的交是闭集。</li>
</ol>
<p>这个定理是从开集的定理中得出的，在对开集取补变成闭集时候，并与交相互变换即可。</p>
<h5 id="示例-v5">示例</h5>
<ol>
<li>给定集合$S={(x,y) \in \mathbb{R}^2 \big| 0 \lt x \le 1, 0 \lt y \lt 1}$，$S$不是闭集。因为目标区域的下边界不在S中。</li>
<li>给定集合$S={(x,y) \in \mathbb{R}^2 \big| x^2 +y^2 \le 1}$，$S$是闭集，因为它的闭集是$\mathbb{R}^2 $中的开集。</li>
<li>$\mathbb{R}^n $中任何有限集是闭集。因为单点是闭集，有限集可以看成很多个单点的并，由定理$1$可以得出。</li>
</ol>
<h4 id="聚点-accumulation-point">聚点(accumulation point)</h4>
<h5 id="定义-v6">定义</h5>
<p>对于点$x\in \mathbb{R}^n $，如果包含$x$的每个开集$U$包含不同于$x$但依然属于集合$A$中的点，那么就称$x$是$A$的一个聚点(accumulation points)，也叫聚类点(cluster points)。**注意这里是包含集合$A$中的点，而不是全部是集合$A$中的点，所以集合的聚点不一定必须在集合中。**如，在一维欧式空间中，单点集合没有聚点，开区间$(0,1)$的聚点是$[0,1]$，${0,1}$不在区间内，但是是聚点。<br>
此外，$x$是聚类点等价于：对于每个$\epsilon \gt 0$，$D(x,\epsilon)$包含$A$中的某点$y$且$y\ne x$。</p>
<h5 id="定理-v3">定理</h5>
<p>当且仅当集合$S$的所有聚点属于$S$时，$S\subset \mathbb{R}^n $是闭集。</p>
<h5 id="示例-v6">示例</h5>
<ol>
<li>给定集合$S={x\in R\big|x\in [0,1]且x是有理数}$，$S$的聚点为$[0,1]$中所有点。任何不属于$[0,1]$的点都不是聚点，因为这类点有一个包含它的$\epsilon$邻域与$[0,1]$不相交。</li>
<li>给定集合$S={(x,y)\in \mathbb{R}^2 \big| 0 \le  x\le 1\ or\ \ x = 2}$, 它的聚点是它本身，因为它是闭集。</li>
<li>给定集合$S={(x,y)\in \mathbb{R}^2 \big|y \lt x^2 + 1}$，S的聚点为集合${(x,y)\in \mathbb{R}^2 \big|y \le x^2 + 1}$，</li>
</ol>
<h4 id="闭包-closure">闭包(closure)</h4>
<h5 id="定义-v7">定义</h5>
<p>给定集合$A\subset \mathbb{R}^n $,集合$A$的闭包$cl(A)$定义成所有包含$A$的闭集的交，所以$cl(A)$是一个闭集。定价的定义是给定集合$A$，包含$A$的最小闭集叫做这个集合$X$的闭包(closure)，用$cl(A)$或者${\overline{A}}$表示。</p>
<h5 id="定理-v4">定理</h5>
<p>给定$A\subset \mathbb{R}^n $，那么$cl(A)$由$A$和$A$的所有聚点组成。</p>
<h5 id="示例-v7">示例</h5>
<ol>
<li>$R$中$S=[0,1)\cup {2}$的闭包是$[0,1]$和${2}$。$S$的聚点是$[0,1]$，再并上$S$得到$S$的闭包是$[0,1]\cup{2}$。</li>
<li>对于任意$S\subset \mathbb{R}^n $，$\mathbb{R}^n \backslash cl(S)$是开集。因为$cl(S)$是闭集，所以它的补集是开集。</li>
<li>$cl(A\cap B) \ne cl(A)\cap cl(B)$。比如$A=(0,1),B(1,2),cl(A)=[0,1],cl(B)=[1,2]$,$A\cap B = \varnothing$,$cl(A\cap B) = \varnothing$,而$cl(A)\cap cl(B) = {1}$。</li>
</ol>
<h4 id="边界-boundary">边界(boundary)</h4>
<h5 id="定义-v8">定义</h5>
<p>对于$\mathbb{R}^n $中的集合$A$，边界定义为集合：<br>
$bd(A) = cl(A)\cap cl(\mathbb{R}^n \backslash A)$<br>
即集合$A$的补集的闭包和$A$的闭包的交集，所以$bd(A)$是闭集。$bd(A)$是$A$与$\mathbb{R}^n \backslash A$之间的边界。</p>
<h5 id="定理-v5">定理</h5>
<p>给定$A\subset \mathbb{R}^n $，当且仅当对于每个$\epsilon \gt 0$，$D(x,\epsilon)$包含$A$与$\mathbb{R}^n \backslash A$的点，$x\in bd(A)$。</p>
<h5 id="示例-v8">示例</h5>
<ol>
<li>给定集合$S={x\in R\big|x\in [0,1],x是有理数}$，$bd(S) = [0,1]$。因为对于任意$\epsilon \gt 0, x\in [0,1],D(x,\epsilon) = (x-\epsilon, x+\epsilon)$包含有理数和无理数，即x是有理数和无理数之间的边界。</li>
<li>给定$x\in bd(S)$，$x$不一定是聚点。给定集合$S = {0} \subset R$，$bd(S) = {0}$，但是单点没有聚点。</li>
<li>给定集合$S={(x,y)\in \mathbb{R}^2 \big| x^2 -y^2 \gt 1 }$，$bd(S)={(x,y)\big|x^2 - y^2 = 1}$。</li>
</ol>
<h4 id="仿射维度-affine-dimension">仿射维度(affine dimension)</h4>
<h5 id="定义-v9">定义</h5>
<p>给定一个仿射集$C$，仿射维度是它的仿射包的维度。<br>
仿射维度和其他维度的定义不总是相同的，具体可以看以下的示例。</p>
<h5 id="示例-v9">示例</h5>
<p>给定一个二维欧几里得空间的单位圆，${x\in C\big|x_1^2 +x_2^2 =1}$。它的仿射包是整个$\mathbb{R}^2$，所以二维平面的单位圆仿射维度是$2$。但是在很多定义中，二维平面的单位圆的维度是$1$。</p>
<h4 id="相对内部-relative-interior">相对内部(relative interior)</h4>
<p>给定一个集合$C\subset \mathbb{R}^n $，它的仿射维度可能小于$n$，这个时候仿射集$aff\ C \ne \mathbb{R}^n $。</p>
<h5 id="定义-v10">定义</h5>
<p>给定集合$C$，相对内部的定义如下：<br>
$relint\ C = {x\in C\big|(B(x,r)\cup aff\ C) \subset C, \exists \ r \gt 0}.$<br>
就是集合$C$内所有$\epsilon$球在$C$的仿射集内的点的集合。<br>
其中$B(x,r)={y \big|\Vert y- x\Vert \le r}$，是以$x$为中心，以$r$为半径的圆。这里的范数可以是任何范数，它们定义的相对内部是相同的。</p>
<h5 id="示例-v10">示例</h5>
<p>给定一个$\mathbb{R}^3 $空间中$(x_1,x_2)$平面上的正方形，$C={x\in \mathbb{R}^3 \big|-1 \le x_1 \le 1, -1\le x_2 \le 1, x_3 = 0}$。它的仿射包是$(x_1,x_2)$平面，$aff\ C = {x\in \mathbb{R}^3 \big|x_3=0}$。$C$的内部是空的，但是相对内部是：<br>
$relint\ C = {x \in \mathbb{R}^3 \big|-1 \le x_1 \le 1, -1\le x_2 \le 1,x_3=0}$。</p>
<h4 id="相对边界-relative-boundary">相对边界(relative boundary)</h4>
<h5 id="定义-v11">定义</h5>
<p>给定集合$C$，相对边界(relative boundary)定义为$cl\ C \backslash relint\ C$，其中$cl\ C$是集合$C$的闭包(closure)。</p>
<h5 id="示例-v11">示例</h5>
<p>对于上例（相对内部的示例）来说，它的边界(boundary)是它本身。它的相对内部是边框，${x\in \mathbb{R}^3 \big|max{|x_1|,|x_2|}=1,x_3=0}$。</p>
<h3 id="凸集-convex-sets">凸集(convex sets)</h3>
<h4 id="凸集定义">凸集定义</h4>
<p>给定一个集合$C$，如果集合$C$中经过任意两点的线段仍然在$C$中，这个集合就是一个凸集。<br>
给定$\forall x_1,x_2 \in C, 0 \le \theta \le 1$，那么我们有$\theta x_1 + (1-\theta)x_2 \in C$。<br>
每一个仿射集都是凸的，因为它包含经过任意两个不同点的直线，所以肯定就包含过那两个点的线段。</p>
<h4 id="凸组合-convex-combination">凸组合(convex combination)</h4>
<p>给定$k$个点$x_1,x_2,\cdots,x_k$，如果具有$\theta_1 x_1 + \cdots, \theta_k x_k$形式且满足$\theta_1 + \cdots + \theta_k=1, \theta_i \ge 0,i=1,\cdots,k$,那么就称这是$x_1,\cdots,x_k$的一个凸组合。<br>
当且仅当一个集合包含其中所有点的凸组合，这个集合是一个凸集。点的一个凸组合可以看成点的混合或者加权，$\theta_i$是第$i$个点$x_i$的权重。<br>
凸组合可以推广到无限维求和，积分，概率分布等等。假设$\theta_1,\theta_2,\cdots$满足：<br>
$$\theta_i \le 0, i = 1,2,\cdots, \sum_{i=1}^{\infty}\theta_i = 1$$<br>
并且$x_1,x_2,\cdots \in C$，$C\subset \mathbb{R}^n $是凸的，如果(series)$\sum_{i=1}^{\infty} \theta_i x_i$收敛，那么$\sum_{i=1}^{\infty} \theta_i x_i \in C$。<br>
更一般的，假设概率分布$p$，$\mathbb{R}^n \rightarrow R$满足$p(x)\le 0 for\ all\ x\in C, \int_{C}p(x)dx = 1$,其中$C\subset \mathbb{R}^n $是凸的，如果$\int_{C}p(x)xdx$存在的话，那么$\int_{C}p(x)xdx\in C$。</p>
<h4 id="凸包-convex-hull">凸包(convex hull)</h4>
<p>给定一个集合$C$，凸包的定义为包含集合$C$中所有点的凸组合的结合，记为$conv\ C$，公式如下：<br>
$conv\ C = {\theta_1 x_1 + \cdots + \theta_k x_k\big|x_i \in C, \theta_i \ge 0, i = 1,\cdots,k,\theta_1 +\cdots + \theta_k = 1}$<br>
任意集合都是有凸包的。一个集合的凸包总是凸的。集合$C$的凸包是包含集合$C$的最小凸集。如果集合$B$是任意包含$C$的凸集，那么$conv\ C \subset B$。</p>
<h4 id="示例-v12">示例</h4>
<p><img src="/2018/12/24/convex-optimization-chapter-2-Convex-sets/figure2_2.png" alt="figure 2.2"><br>
<img src="/2018/12/24/convex-optimization-chapter-2-Convex-sets/figure2_3.png" alt="figure 2.3"></p>
<h3 id="锥-cones">锥(cones)</h3>
<h4 id="锥-cones-和凸锥-convex-cones-的定义">锥(cones)和凸锥(convex cones)的定义</h4>
<p>给定集合$C$，如果$\forall x \in C, \theta \ge 0$，都有$\theta x\in C$，这样的集合就称为一个锥(cone)，或者非负同质(nonnegative homogeneour)。<br>
一个集合$C$如果既是锥又是凸的，那这个集合是一个凸锥(convex cone)，即：$\forall x_1,x_2 \in C, \theta_1,\theta_2 \ge 0$,那么有$\theta_1 x_1+\theta_2 x_2 \in C$。几何上可以看成经过顶点为原点，两条边分别经过点$x_1$和$x_2$的$2$维扇形。</p>
<h4 id="锥组合-conic-combination">锥组合(conic combination)</h4>
<p>给定$k$个点$x_1,x_2,\cdots,x_k$，如果具有$\theta_1 x_1 + \cdots, \theta_k x_k$形式且满足$\theta_i \ge 0,i=1,\cdots,k$,那么就称这是$x_1,\cdots,x_k$的一个锥组合(conic combination)或者非负线性组合(nonnegative combination)。<br>
给定集合$C$是凸锥，那么集合$C$中任意点$x_i$的锥组合仍然在集合$C$中。反过来，当且仅当集合$C$包含它的任意元素的凸组合时，这个集合是一个凸锥(convex cone)。</p>
<h4 id="锥包-conic-hull">锥包(conic hull)</h4>
<p>给定集合$C$，它的锥包(conic hull)是集合$C$中所有点的锥组合。即：<br>
$conic\ C = {\theta_1 x_1 + \cdots + \theta_k x_k\big|x_i \in C, \theta_i \ge 0, i = 1,\cdots,k}$<br>
集合$C$的锥包是包含集合$C$的最小凸锥。</p>
<h4 id="示例-v13">示例</h4>
<p><img src="/2018/12/24/convex-optimization-chapter-2-Convex-sets/figure2_4.png" alt="figure 2.4"><br>
<img src="/2018/12/24/convex-optimization-chapter-2-Convex-sets/figure2_5.png" alt="figure 2.5"></p>
<h3 id="一些想法">一些想法</h3>
<p>在我自己看来，在几何上<br>
仿射集可以看成是集合中任意两个点的直线的集合。<br>
凸集可以看成是集合中任意两个点的线段的集合，因为直线一定包含线段，所以仿射集一定是凸集。<br>
锥集可以看成是集合中任意一个点和原点构成的射线的集合，锥集不一定是连续的（两条射线也是锥集），所以锥集不一定是凸集。<br>
而凸锥既是凸集又是锥集。<br>
我在stackexchange看到这样一句话觉得说的挺好的。</p>
<blockquote>
<p>What basically distinguishes the definitions of convex, affine and cone, is the domain of the coefficients and the constraints that relate them.</p>
</blockquote>
<p>区别凸集，仿射和锥的是系数的取值范围和一些其他限制。仿射集要求$\theta_1+\cdots+\theta_k = 1$，凸集要求$\theta_1 +\cdots +\theta_k = 1, 0\le \theta \le 1$，锥的要求是$\theta \ge 0$，凸锥的要求是$\theta_i \ge 0,i=1,\cdots,k$。<br>
仿射集不是凸集的子集，凸集也不是仿射集的子集。所有仿射集的集合是所有凸集的集合的子集，一个仿射集是一个凸集。</p>
<h2 id="示例-v14">示例</h2>
<ul>
<li>$\emptyset$，单点(single point)${x_0}$，整个$\mathbb{R}^n $空间都是$\mathbb{R}^n $中的仿射子集，所以也是凸集，点不一定是凸锥（在原点熵是凸锥），空集是凸锥，$\mathbb{R}^n $维空间也是凸锥。<strong>根据定义证明。</strong></li>
<li>任意一条直线都是仿射的，所以是凸集。如果经过原点，它是一个子空间，也就是一个凸锥，否则不是。</li>
<li>任意一条线段都是凸集，不是仿射集，当它退化成一点的时候，它是仿射的，线段不是凸锥。</li>
<li>一条射线${x_0 + \theta v\big| \theta \ge 0}$是凸的，但是不是仿射的，当$x_0=0$时，它是凸锥。</li>
<li>任意子空间都是仿射的，也是凸锥，所以是凸的。</li>
</ul>
<p>补充最后一条，任意子空间都是仿射的，也是凸锥。<br>
如果$V$是一个子空间，那么$V$中任意两个向量的线性组合还在$V$中。即如果$x_1,x_2\in V$，对于$\theta_1,\theta_2 \in R$，都有$\theta_1 x_1 + \theta_2 x_2 \in V$。正如前面说的，子空间是加法和数乘封闭的。<br>
而根据仿射集的定义，如果$x_1,x_2$在一个仿射集$C$中，那么对于$\theta_1+\theta_2 = 1$，都有$\theta_1 x_1 + \theta_2 x_2 \in C$。我们可以看出来，如果取子空间中线性组合的系数和为$1$，那么就成了仿射集。如果取子空间中的系数$\theta_1,\theta_2 \in R_+$,那么就成了锥，如果同时满足$\theta_1+\theta_2 = 1$，那么就成凸锥。那么如果加上这些限制条件，即取子空间中线性组合的系数和为$1$，或者取子空间中的系数$\theta_1,\theta_2 \in R_+$,同时满足$\theta_1+\theta_2 = 1$。<br>
事实上，子空间要求的条件比仿射集和凸锥的条件要更严格。仿射集和凸锥只要求在系数$\theta_i$满足相应的条件时,有$\theta_1 x_1 + \theta_2 x_2 \in \mathbb{R}^n $；而子空间要求的是在系数$\theta_i$取任意值的时候，都有$\theta_1 x_1 + \theta_2 x_2 \in \mathbb{R}^n $，所以子空间一定是仿射集，也一定是凸锥。（拿二维的举个例子，给定$x_1$和$x_2$，仿射集可以看成是$\theta_1$的函数，因为$\theta_2=1-\theta_1$，而子空间可以看成$\theta_1$和$\theta_2$的函数，一个是一元函数，一个是二元函数）</p>
<h3 id="超平面-hyperplane-和半空间-halfspace">超平面(hyperplane)和半空间(halfspace)</h3>
<p>超平面是一个仿射集，也是凸集，但不一定是锥集(过原点才是锥集，也是一个子空间)。<br>
闭的半空间是一个凸集，不是仿射集。</p>
<h4 id="超平面-hyperplane">超平面(hyperplane)</h4>
<p>超平面通常具有以下形式：<br>
$${x\big|a^T x=b},$$<br>
其中$a\in \mathbb{R}^n ,a\ne 0,b\in R$，它其实是一个平凡(nontrivial)线性方程组的解，因此也是一个仿射集。几何上，超平面可以解释为和一个给定向量$a$具有相同内积(inner product)的点集，或者说是法向量为$a$的一个超平面。常数$b$是超平面和原点之间的距离(offset)。<br>
几何意义可以被表示成如下形式：<br>
$${x\big|a^T (x-x_0) = 0},$$<br>
其中$x_0$是超平面上的一点，即满足$a^T x_0=0$。可以被表示成如下形式：<br>
$${x\big|a^T (x-x_0)=0} = x_0+a^{\perp},$$<br>
其中$a^{\perp} $是$a$的正交补，即所有与$a$正交的向量的集合，满足$a^{\perp} ={v\big|a^T v=0}$。所以，超平面的几何解释可以看做一个偏移(原点到这个超平面的距离)加上所有垂直于一个特定向量$a$(正交向量)的向量，即这些垂直于$a$的向量构成了一个过原点的超平面，再加上这个偏移量就是我们要的超平面。几何表示如下图所示。<br>
<img src="/2018/12/24/convex-optimization-chapter-2-Convex-sets/figure2_6.png" alt="figure 2.6"></p>
<h4 id="半空间-halfspace">半空间(halfspace)</h4>
<p>一个超平面将$\mathbb{R}^n $划分为两个半空间(halfspaces)，一个是闭(closed)半空间，一个是开半空间。闭的半空间可以表示成${x\big|a^T x\le b}$，其中$a\ne 0$，半空间是凸的，但不是仿射的。下图便是一个闭的半空间。<br>
<img src="/2018/12/24/convex-optimization-chapter-2-Convex-sets/figure2_7.png" alt="figure 2.7"><br>
这个半空间也可以写成：<br>
$${x\big|a^T (x-x_0)\le 0},$$<br>
其中$x_0$是划分两个半空间的超平面上的一点，即满足$a^T x_0=b$。一个几何解释是：半空间由一个偏移$x_0$加上所有和一个特定向量$a$(超平面的外(outward)法向量)成钝角(obtuse)或者直角(right)的所有向量组成。<br>
这个半空间的边界是超平面${x\big|a^T x=b}$。这个半空间${x\big|a^T x\le b}$的内部是${x\big|a^T x\lt b}$，也被称为一个开半平面。</p>
<h3 id="欧几里得球-euclidean-ball-和椭球-ellipsoid">欧几里得球(Euclidean ball)和椭球(ellipsoid)</h3>
<h4 id="欧几里得球">欧几里得球</h4>
<p>$\mathbb{R}^n $空间中的欧几里得球或者叫球，有如下的形式：<br>
$$B(x_r,r = {x\big|\Vert x-x_c\Vert_2\le r}={x \big|(x-x_c)^T (x-x_c)\le \mathbb{R}^2 },$$<br>
其中$r\gt 0$,$\Vert \cdot\Vert_2$是欧几里得范数(第二范数)，即$\Vert u\Vert_2=(u^T u)^{\frac{1}{2}} $。向量$x_c$是球心，标量$r$是半径。$B(x_c,r)$包含所有和圆心$x_c$距离小于$r$的球。<br>
欧几里得球的另一种表示形式是：<br>
$$B(x_c,r)={x_c + ru\big| \Vert u \Vert_2 \le 1},$$<br>
一个欧几里得球是凸集，如果$\Vert x_1-x_c\Vert_2 \le r,\Vert x_2-x_c\Vert_2\le r, 0\le\theta\le1$，那么：<br>
\begin{align*}<br>
\Vert\theta x_1 + (1-\theta)x_2 - x_c\Vert_2 &amp;= \Vert\theta(x_1-x_c)+(1-\theta)(x_2-x_c)\Vert_2\\<br>
&amp;\le\theta\Vert x_1-x_c\Vert_2 + (1-\theta)\Vert x_2 - x_c \Vert_2\\<br>
&amp;\le r<br>
\end{align*}<br>
用其次性和三角不等式可证明</p>
<h4 id="椭球">椭球</h4>
<p>另一类凸集是椭球，它们有如下的形式：<br>
$$\varepsilon ={x\big|(x-x_c)^T P^{-1} (x-x_c) \le 1},$$<br>
其中$P=P^T \succ 0$即$P$是对称和正定的。向量$x_c\in \mathbb{R}^n $是椭球的中心。矩阵$P$决定了椭球从$x_c$向各个方向扩展的距离。椭球$\varepsilon$的半轴由矩阵$P$的特征值$\lambda_i$算出，$\sqrt{\lambda_i}$，球是$P=\mathbb{R}^2 I$的椭球。<br>
<strong>这里这种表示形式为什么要用$P^{-1} $？</strong><br>
椭球的另一种表示是：<br>
$$\varepsilon = {x_c + Au\big| \Vert u \Vert_2 \le 1},$$<br>
其中$A$是一个非奇异方阵。假设$A$是对称正定的，取$A=P^{\frac{1}{2}} $，这种表示就和上面的表示是一样的。第一次看到这种表示的时候，我在想，椭球的边界上有无数个点，一个方阵$A$是怎么实现对这无数个操作的，后来和球做了对比，发现自己一直都想错了，这无数个点是通过范数实现的而不是通过矩阵$A$实现的，到球心距离为$\Vert u\Vert_2\le 1$的点有无数个，$A$对这无数个点的坐标都做了仿射变换，将一个球变换成了椭球，特殊情况下就是球。当矩阵$A$是对称半正定但是是奇异的时候，这个情况下称为退化椭球(degenerate ellipsoid)，它的仿射维度和矩阵$A$的秩(rank)是相同的。退化椭球也是凸的。</p>
<h3 id="范数球-norm-ball-和范数锥-norm-cone">范数球(norm ball)和范数锥(norm cone)</h3>
<h4 id="范数球-norm-ball">范数球(norm ball)</h4>
<h5 id="定义-v12">定义</h5>
<p>$\Vert \cdot\Vert$是$\mathbb{R}^n $上的范数。一个范数球(norm ball)可以看成一个以$x_c$为中心，以$r$为半径的集合，但是这个$r$可以是任何范数，即${x\big|\Vert x-x_c \Vert \le r}$，它是凸的。</p>
<h5 id="示例-v15">示例</h5>
<p>我们常见的球是二范数（欧几里得范数）对应的范数球。</p>
<h4 id="范数锥">范数锥</h4>
<h5 id="定义-v13">定义</h5>
<p>和范数相关的范数锥是集合：$C = {(x,t)\big|\Vert x\Vert \le t} \subset \mathbb{R}^{n+1} $，它也是凸锥。</p>
<h5 id="示例-v16">示例</h5>
<p><img src="/2018/12/24/convex-optimization-chapter-2-Convex-sets/figure2_10.png" alt="figure 2.10"><br>
二阶锥(second-order cone)是欧几里得范数对应的范数锥，如图所示，其表达式为：<br>
\begin{align*}<br>
C &amp;={(x,t)\in \mathbb{R}^{n+1} \big| \Vert x\Vert_2 \le t}\\<br>
&amp;= \left{ \begin{bmatrix}x\\t\end{bmatrix} \big| \begin{bmatrix}x\\t\end{bmatrix}^T \begin{bmatrix}I&amp;0\\ 0&amp;-1\end{bmatrix} \begin{bmatrix}x\\t\end{bmatrix}\le 0, t \gt 0 \right}<br>
\end{align*}<br>
这个二阶锥也被称为二次锥(quadratic cone)，因为它是通过一个二次不等式定义的，也被叫做Lorentz cone或者冰激凌锥(ice-cream cone)。</p>
<h4 id="范数锥和范数球的区别">范数锥和范数球的区别</h4>
<p>范数球是所有点到圆心$x_c$的范数小于一个距离$r$。<br>
范数锥是很多直线组成的锥。</p>
<h3 id="多面体-polyhedra">多面体(polyhedra)</h3>
<h4 id="定义-v14">定义</h4>
<p>多面体(polyhedron)是有限个线性不等式或者线性方程组的解集的集合：<br>
$P = {x\big|a_j^T x\le b_j, j=1,\cdots,m,c_j^T x=d_j,j=1,\cdots,p}$<br>
多面体因此也是有限个半空间或者超平面的交集。仿射集(如，子空间，超平面，直线)，射线，线段，半空间等等都是多面体，多面体也是凸集。有界的polyhedron有时也被称为polytope，一些作者会把它们两个反过来叫。<br>
上式的紧凑(compact)表示是：<br>
$$P={x\big|Ax\preceq b, Cx=d}$$<br>
其中$A=\begin{bmatrix}a_1^T \\ \vdots\\ a_m^T \end{bmatrix},C=\begin{bmatrix}c_1^T \\ \vdots\\c_p^T \end{bmatrix}$，$\preceq$表示$\mathbb{R}^m $空间中的向量不等式(vector ineuqalitied)或者分量大小的不等式，$u\preceq v$代表着$u_i\le v_i, i=1,\cdots,m$。</p>
<h5 id="simplexes">simplexes</h5>
<p>simplexes是另一类很重要的多面体。假设$\mathbb{R}^n $空间中的$k+1$个点是仿射独立(affinely independent)，意味着$v_1-v_0, \cdots,v_k-v_0$是线性独立的。由$k+1$个仿射独立的点确定的simplex是：<br>
$$C = conv{v_0,\cdots,v_k} = {\theta_0v_0+\cdots+\theta_kv_k\big| \theta \succeq 0, \mathcal{1}\theta=1 },$$<br>
其中$\mathcal{1}$是全为$1$的列向量。这个simplex的仿射维度是$k$，所以它也叫$\mathbb{R}^n $空间中的$k$维simplex。为什么仿射维度是$k$，我的理解是simplex是凸集，而凸集不是子空间，凸集去掉其中任意一个元素才是子空间，所以就是$k$维而不是$k+1$维。<br>
为了将simplex表达成一个紧凑形式的多面体。定义$y=(\theta_1,\cdots,\theta_k)$和$B=[v_1-v_0\ \cdots\ v_k-v_0]\in \mathbb{R}^{n\times k} $，当且仅当存在$y\succeq 0, \mathcal{1}^T y\le 1$，$x=v_0+By$有$x\in C$，<strong>疑问，这里为什么变成了$\mathcal{1}^T y\le 1$，难道是因为少了个$v_0$吗</strong>。点$v_0,\cdots,v_k$表明矩阵$B$的秩为$k$。因此存在一个非奇异矩阵$A=(A_1,A_2)\in \mathbb{R}^{n\times n} $使得：<br>
$$AB = \begin{bmatrix}A_1\\A_2\end{bmatrix}B= \begin{bmatrix}I\\0\end{bmatrix}.$$<br>
对$x = v_0+By$同时左乘$A$，得到：<br>
$$A_1x = A_1v_0+y, A_2x=A_xv_0.$$<br>
从中我们可以看出如果$A_2x=A_2v_0$，且向量$y=A_1x-A_1v_0$满足$y\succeq 0, \mathcal{1}^T y\le1$时，$x\in C$。换句话说，当且仅当$x$满足以下等式和不等式时：<br>
$$A_2x = A_2v_0,A_1x\succeq A_1v_0, \mathcal{1}A_1x\le1+\mathcal{1}^T A_1v_0,$$<br>
有$x\in C$。</p>
<h5 id="多面体的凸包描述">多面体的凸包描述</h5>
<p>一个有限集合${v_1,\cdots,v_k}$的凸包是：<br>
$$conv{v_1,\cdots,v_k} = {\theta_1 v_1 +\cdots +\theta_k v_k\big| \theta \succeq 0, \mathcal{1}^T \theta = 1}.$$<br>
这个集合是一个多面体，并且有界。但是它（除了simplex）不容易化成多面体的紧凑表示，即不等式和等式的集合。<br>
一个一般化的凸包描述是：<br>
$${\theta_1 v_1 +\cdots +\theta_k v_k\big| \theta_1+\cdots + \theta_m = 1,\theta_i \ge 0,i=1,\cdots,k}.$$<br>
其中$m\le k$，它可以看做是点$v_1,\cdots,v_m$的凸包加上点$v_{m+1},\cdots,v_{k}$的锥包。这个集合定义了一个多面体，反过来，任意一个多面体可以看做凸包加上锥包。<br>
一个多面体如何表示是很有技巧的。比如一个$\mathbb{R}^n $空间上的无穷范数单位球$C$：<br>
$$C={x\big|\ |x_i|\le 1,i = 1,\cdots,n}.$$<br>
集合$C$可以被表示成$2n$个线性不等式$\pm e_i^T x\le 1$，其中$e_i$是第$i$个单位向量。然而用凸包形式描述这个集合需要用至少$2^n $个点：<br>
$$C = conv{v_{1},\cdots,v_{2^n }},$$<br>
其中$v_{1},\cdots,v_{2n}$是$2^n $个向量，每个向量的元素都是$1$或$-1$。因此凸包描述和不等式描述有很大差异，尤其是$n$很大的时候。<br>
这里为什么是$2^n $个点呢？因为是无穷范数构成的单位圆，在数轴上是区间$[-1,1]$，在$\mathbb{R}^2 $是正方形${(x,)\big|-1 \le x\le 1,-1\le y\le 1}$，对应的四个点是${(1,1),(1,-1),(-1,1),(-1,-1)}$，而在$\mathbb{R}^3 $是立方体${(x,y,z)\big|-1 \le x\le 1,-1\le y\le 1, -1\le z\le 1}$，对应的是立方体的八个顶点${(1,1,1),(1,1,-1),(1,-1,1),(1,-1,-1),(-1,1,1),(-1,1,-1),(-1,-1,1),(-1,-1,-1)}$。</p>
<h4 id="示例-v17">示例</h4>
<ol>
<li>如图所示，是五个半平面的交集定义的多面体。<br>
<img src="/2018/12/24/convex-optimization-chapter-2-Convex-sets/figure2_11.png" alt="figure 2.11"></li>
<li>非负象限(nonnegative orthant)是非负点的集合，即：<br>
$$R_{+}^n = {x\in \mathbb{R}^n \big| x_i\ge 0, i = 1,\cdots,n} = {x\in \mathbb{R}^n \big| x\succeq 0}.$$<br>
非负象限是一个多面体，也是一个锥，所以也叫多面体锥(polyhedral cone)，也叫非负象限锥。</li>
<li>一个1维的simplex是一条线段。一个二维的simplex是一个三角形（包含它的内部）。一个三维的simple是一个四面体(tetrahedron)。</li>
<li>由$\mathbb{R}^n $中的零向量和单位向量确定的simplex是$n$维unit simplex。它是向量集合：<br>
$$x\succeq 0, \mathcal{1}^T x \le 1.$$</li>
<li>由$\mathbb{R}^n $中的单位向量确定的simplex是$n-1$维probability simplex。它是向量集合：<br>
$$x\succeq 0, \mathcal{1}^T x = 1.$$<br>
Probability simplex是中的向量可以看成具有$n$个元素的集合的概率分布，$x_i$解释为第$i$个元素的概率。</li>
</ol>
<h3 id="半正定锥-positive-sefidefinite-cone">半正定锥(positive sefidefinite cone)</h3>
<h4 id="定义-v15">定义</h4>
<p>用$S^n $表示$n\times n$的对称矩阵：$S^n ={X\in \mathbb{R}^{n\times n} \big| X = X^T }$，$S^n $是一个$n(n+1)/2$维基的向量空间。比如，三维空间中对称矩阵的一组基是：<br>
$$\begin{bmatrix}1&amp;0&amp;0\\0&amp;0&amp;0\\0&amp;0&amp;0\end{bmatrix}\begin{bmatrix}0&amp;0&amp;0\\1&amp;0&amp;0\\0&amp;0&amp;0\end{bmatrix}\begin{bmatrix}0&amp;0&amp;0\\0&amp;1&amp;0\\0&amp;0&amp;0\end{bmatrix}\begin{bmatrix}0&amp;0&amp;0\\0&amp;0&amp;0\\1&amp;0&amp;0\end{bmatrix}\begin{bmatrix}0&amp;0&amp;0\\0&amp;0&amp;0\\0&amp;1&amp;0\end{bmatrix}\begin{bmatrix}0&amp;0&amp;0\\0&amp;0&amp;0\\0&amp;0&amp;1\end{bmatrix}.$$<br>
用$S_{+}^n $表示半正定的对称矩阵集合：<br>
$$S_{+}^n = {X\in S^n \big| X\succeq 0},$$<br>
用$S_{++}^n $表示正定的对称矩阵集合：<br>
$$S_{+}^n = {X\in S^n \big| X\succ 0},$$<br>
集合$S_{+}^n $是凸锥：如果$\theta_1,\theta_2 \ge 0$且$A,B\in S_{+}^n $，那么$\theta_1 A+\theta_{2} B\in S_{+}^n $。这个可以直接从依靠半正定的定义来证明，如果$A,B\in S_{+}^n ,\theta_1,\theta_2\ge 0$，(<strong>这里原书中用的是$A,B\succeq 0$,我觉得应该是写错了吧</strong>)，对任意$\forall x \in \mathbb{R}^n $，都有：<br>
$$x^T (\theta_1A+\theta_2B)x = \theta_1x^T Ax + \theta_2x^T Bx.$$</p>
<h4 id="示例-v18">示例</h4>
<p>对于$S^2 $空间中的半正定锥，有<br>
$$X=\begin{bmatrix}x&amp;y\\y&amp;z\end{bmatrix}\in S_{+}^2 \Leftrightarrow x\ge 0,z\ge 0, xz\ge y^2 $$<br>
这个锥的边界如下图所示。<br>
<img src="/2018/12/24/convex-optimization-chapter-2-Convex-sets/figure2_12.png" alt="figure 2.12"></p>
<h3 id="常见的几种锥">常见的几种锥</h3>
<p>范数锥，非负象限锥，半正定锥，它们都过原点。<br>
想想对应的图像是什么样的。<br>
范数锥和非负象限锥图像还好理解一些，非负象限锥是$\mathbb{R}^n $空间所有非负半轴围成的锥，范数锥的边界像一个沙漏，但是是无限延伸的。半正定锥怎么理解，还没有太好的类比。</p>
<h2 id="保凸运算-operations-that-preserve-convexity">保凸运算(operations that preserve convexity)</h2>
<p>这一小节介绍的是一些保留集合凸性，或者从一些集合中构造凸集的运算。这些运算和simplex形成了凸集的积分去确定或者建立集合的凸性。</p>
<h3 id="集合交-intersection">集合交(intersection)</h3>
<p>凸集求交集可以保留凸性：如果$S_1$和$S_2$是凸集，那么$S_1\cup S_2$是凸集。扩展到无限个集合就是：如果$\forall \alpha \in A,S_{\alpha}$都是凸的，那么$\cup_{\alpha\in A S_{\alpha}}$是凸的</p>
<h3 id="仿射函数-affine-functions">仿射函数(affine functions)</h3>
<h3 id="线性分式-linear-fractional-和视角函数-perspective-functions">线性分式(linear-fractional)和视角函数(perspective functions)</h3>
<h4 id="线性分式-linear-fractional">线性分式(linear-fractional)</h4>
<h4 id="视角函数-perspective-functions">视角函数(perspective functions)</h4>
<h2 id="广义不等式-generalized-inequalities">广义不等式（Generalized inequalities)</h2>
<h3 id="真锥-proper-cones-和广义不等式-generalized-inequalities">真锥(Proper cones)和广义不等式（Generalized inequalities)</h3>
<h3 id="最小-minimum-和最小元素-minimal-elemetns">最小(Minimum)和最小元素(minimal elemetns)</h3>
<h2 id="separating和supporting-hyperplanes">Separating和supporting hyperplanes</h2>
<h3 id="separating-hyperplane-theorem">Separating hyperplane theorem</h3>
<h3 id="supporting-hyperplanes">Supporting Hyperplanes</h3>
<h2 id="对偶锥-dual-cones-和广义不等式-generalized-inequalities">对偶锥(dual cones)和广义不等式(generalized inequalities)</h2>
<h3 id="none"></h3>
<h2 id="符号定义">符号定义</h2>
<p>$\preceq$表示$\mathbb{R}^m $空间中的向量不等式(vector ineuqalitied)或者element-wise的不等式，$u\preceq v$代表着$u_i\le v_i, i=1,\cdots,m$。</p>
<h2 id="参考文献">参考文献</h2>
<p>1.stephen boyd. Convex optimization<br>
2.<a href="https://en.wikipedia.org/wiki/Topology" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Topology</a><br>
3.<a href="https://en.wikipedia.org/wiki/Topological_space" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Topological_space</a><br>
4.<a href="https://en.wikipedia.org/wiki/Power_set" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Power_set</a><br>
5.<a href="https://en.wikipedia.org/wiki/Open_set" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Open_set</a><br>
6.<a href="https://en.wikipedia.org/wiki/Closed_set" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Closed_set</a><br>
7.<a href="https://en.wikipedia.org/wiki/Clopen_set" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Clopen_set</a><br>
8.<a href="https://en.wikipedia.org/wiki/Interior_(topology)" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Interior_(topology)</a><br>
9.<a href="https://en.wikipedia.org/wiki/Closure_(topology)" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Closure_(topology)</a><br>
10.<a href="https://en.wikipedia.org/wiki/Boundary_(topology)" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Boundary_(topology)</a><br>
11.<a href="https://en.wikipedia.org/wiki/Ball_(mathematics)" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Ball_(mathematics)</a><br>
12.<a href="https://blog.csdn.net/u010182633/article/details/53792588" target="_blank" rel="noopener">https://blog.csdn.net/u010182633/article/details/53792588</a><br>
13.<a href="https://blog.csdn.net/u010182633/article/details/53819910" target="_blank" rel="noopener">https://blog.csdn.net/u010182633/article/details/53819910</a><br>
14.<a href="https://blog.csdn.net/u010182633/article/details/53983642" target="_blank" rel="noopener">https://blog.csdn.net/u010182633/article/details/53983642</a><br>
15.<a href="https://blog.csdn.net/u010182633/article/details/53997843" target="_blank" rel="noopener">https://blog.csdn.net/u010182633/article/details/53997843</a><br>
16.<a href="https://blog.csdn.net/u010182633/article/details/54093987" target="_blank" rel="noopener">https://blog.csdn.net/u010182633/article/details/54093987</a><br>
17.<a href="https://blog.csdn.net/u010182633/article/details/54139896" target="_blank" rel="noopener">https://blog.csdn.net/u010182633/article/details/54139896</a><br>
18.<a href="https://math.stackexchange.com/questions/1168898/why-is-any-subspace-a-convex-cone" target="_blank" rel="noopener">https://math.stackexchange.com/questions/1168898/why-is-any-subspace-a-convex-cone</a><br>
19.<a href="https://www.zhihu.com/question/22799760/answer/139753685" target="_blank" rel="noopener">https://www.zhihu.com/question/22799760/answer/139753685</a><br>
20.<a href="https://www.zhihu.com/question/22799760/answer/34282205" target="_blank" rel="noopener">https://www.zhihu.com/question/22799760/answer/34282205</a><br>
21.<a href="https://www.zhihu.com/question/22799760/answer/137768096" target="_blank" rel="noopener">https://www.zhihu.com/question/22799760/answer/137768096</a><br>
22.<a href="https://en.wikipedia.org/wiki/Positive-definite_matrix" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Positive-definite_matrix</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://mxxhcm.github.io/2018/12/22/convex-optimization-chapter-1-Introduction/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="马晓鑫爱马荟荟">
      <meta itemprop="description" content="记录硕士三年自己的积累">
      <meta itemprop="image" content="/images/favicon.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="mxxhcm's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2018/12/22/convex-optimization-chapter-1-Introduction/" class="post-title-link" itemprop="http://mxxhcm.github.io/page/16/index.html">convex optimization chapter 1 Introduction</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-12-22 13:44:11" itemprop="dateCreated datePublished" datetime="2018-12-22T13:44:11+08:00">2018-12-22</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-09-04 20:51:00" itemprop="dateModified" datetime="2019-09-04T20:51:00+08:00">2019-09-04</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/凸优化/" itemprop="url" rel="index"><span itemprop="name">凸优化</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="数学优化-mathematical-optimization">数学优化(mathematical optimization)</h2>
<h3 id="定义">定义</h3>
<p>一个数学优化问题（或者称为优化问题）通常有如下的形式：<br>
\begin{align*}<br>
&amp;minimize \quad f_0(x)\\<br>
&amp;subject \ to \quad f_i(x) \le b_i, i = 1,\cdots,m.<br>
\end{align*}<br>
其中$x = (x_1, \cdots, x_m)$被称为优化变量(optimization variables), 或者决策变量(decision variables)。 $f_0(x):\mathbb{R}^n \rightarrow \mathbb{R}$是目标函数(object function), $f_i(x):\mathbb{R}^n \rightarrow \mathbb{R},i =1,\cdots,m$是约束函数(constraint functions)。 常量(constraints) $b_1,\cdots,b_m$是约束的限界(limits)或者边界(bounds), $b_i$可以为$0$，这个可以通过移项构造出新的$f_i(x)$实现。如果向量$x$使得目标函数取得最小的值，并且满足所有的约束条件，那么这个向量被称为最优解$x^{*} $。</p>
<h4 id="线性优化-linear-program">线性优化(linear program)</h4>
<p>目标函数和约束函数$f_0,\cdots,f_m$是线性的, 它们满足不等式：<br>
$$f_i(\alpha x+\beta y) = \alpha f_i(x) + \beta f_i(y)$$<br>
对于所有的$x,y \in \mathbb{R}^n $和所有的$\alpha, \beta \in\mathbb{R}$。<br>
线性优化是凸优化的一个特殊形式, 它的目标函数和约束函数都是线性的等式。</p>
<h4 id="非线性问题-non-linear-problem">非线性问题(non-linear problem)</h4>
<p>如果优化问题不是线性的，就是非线性问题。只要目标函数或者约束函数至少有一个不是线性的，那么这个优化问题就是非线性优化问题。</p>
<h4 id="凸问题-convex-problem">凸问题(convex problem)</h4>
<p>凸问题是目标函数和约束函数都是凸的的优化问题，它们满足：<br>
$$f_i(\alpha x + \beta y) \le \alpha f_i(x) + \beta f_i(y)$$<br>
对于所有的$x,y \in \mathbb{r}^n $和所有的$\alpha, \beta \in \mathbb{r}$且$\alpha + \beta = 1, \alpha \ge 0, \beta \ge 0$。<br>
凸性比线性的范围更广，不等式取代了更加严格的等式，不等式只有在$\alpha$和$\beta$取一些特定值时才成立。凸优化和线性问题以及非线性问题都有交集，它是线性问题的超集(superset)，是非线性问题的子集(subset)。技术上来说，nonlinear problem包括convex optimization(除了linear programming), 可以用来描述不确定是非凸的问题。<br>
Nonlinear program &gt; convex problem &gt; linear problem</p>
<h3 id="示例">示例</h3>
<h4 id="组合优化-portfolio-optimization">组合优化(portfolio optimization)</h4>
<p>变量：不同资产的投资数量<br>
约束：预算，每个资产最大/最小投资数量，至少要得到的回报<br>
目标：所有的风险，获利的变化</p>
<h4 id="电子设备的元件大小-device-sizing-in-electronic-circuits">电子设备的元件大小(device sizing in electronic circuits)</h4>
<p>变量：元件的宽度和长度<br>
约束：生产工艺的炼制，时间要求，面积等<br>
目标：节约能耗</p>
<h4 id="数据拟合-data-fitting">数据拟合(data fitting)</h4>
<p>变量：模型参数<br>
约束：先验知识，参数约束条件<br>
目标：错误率</p>
<h3 id="优化问题求解-solving-optimization-problems">优化问题求解(solving optimization problems)</h3>
<p>所有的问题都是优化问题。<br>
绝大部分优化问题我们解不出来。</p>
<h4 id="一般的优化问题-general-optimization-problem">一般的优化问题(general optimization problem)</h4>
<ul>
<li>很难解出来。</li>
<li>做一些compromise，比如要很长时间才能解出来，或者并不总能找到解。</li>
</ul>
<h4 id="一些例外-some-exceptions">一些例外(some exceptions)</h4>
<ul>
<li>最小二乘问题(least-squares problems)</li>
<li>线性规划问题(linear programming problems)</li>
<li>凸优化问题(convex optimization problems)</li>
</ul>
<h2 id="最小二乘-least-squares-和线性规划-linear-programming">最小二乘(least-squares)和线性规划(linear programming)</h2>
<p>least-squares和linear programming是凸优化问题中最有名的两个子问题。</p>
<h3 id="最小二乘问题-least-squares-problems">最小二乘问题(least-squares problems)</h3>
<p>最小二乘问题是一个无约束的优化问题，它的目标函数是项$a_i^T x-b_i$的平方和。<br>
\begin{align*}<br>
minimize \quad f_0(x) &amp;= {||Ax-b||}^2_2 \\<br>
&amp;=\sum_{i=1}^k (a_i^T x-b_i)^2<br>
\end{align*}</p>
<h4 id="求解-solving-least-squares-problems">求解(solving least-squares problems)</h4>
<ul>
<li>最小二乘问题的解可以转换为求线性方程组$(A^T A)x = A^T b$的解。线性代数上我们学过该方程组的解析解为$x=(A^T A)^{-1} A^T b$。</li>
<li>时间复杂度是$n^2 k = n*k*n+n*k+n*n*n, (k &gt; n)$(转置，求逆，矩阵乘法)。</li>
<li>该问题具有可靠且高效的求解算法。</li>
<li>是一个很成熟的算法</li>
</ul>
<h4 id="应用-using-least-squares">应用(using least-squares)</h4>
<p>很容易就可以看出来一个问题是最小二乘问题，我们只需要验证目标函数是不是二次函数，以及对应的二次型是不是正定的即可。</p>
<h5 id="加权最小二乘-weighted-least-squares">加权最小二乘(weighted least-squares)</h5>
<p>加权最小二乘形式如下:<br>
$$\sum_{i=1}^k \omega_i(a_i^T x-b_i)^2 ,$$<br>
其中$\omega_1,\cdots,\omega_k$是正的，被最小化。 这里选出权重$\omega$来体现不同项$a_i^T x-b_i$的比重, 或者仅仅用来影响结果。</p>
<h5 id="正则化-regularization">正则化(regularization)</h5>
<p>目标函数中被加入了额外项, 形式如下：<br>
$$\sum_{i=1}^k (a_i^T x-b_i)^2 + \rho \sum_{i=1}^n x_i^2 ,$$<br>
正则项是用来惩罚大的$x$, 求出一个仅仅最小化第一个求和项的不出来的好结果。合理的选择参数$\rho$在原始的目标函数和正则化项之间做一个trade-off, 使得$\sum_{k=1}^i (a_i^T - b_i)^2 $和$\rho \sum_{k=1}^n  x_i^2 $都很小。<br>
正则化项和加权最小二乘会在第六章中讲到，它们的统计解释在第七章给出。</p>
<h3 id="线性规划-linear-programming">线性规划(linear programming)</h3>
<p>线性规划问题装目标函数和约束函数都是线性的：<br>
\begin{align*}<br>
&amp;minimize \quad c^T x\\<br>
&amp;subject \ to \quad a_i^T \le b_i, i = 1, \cdots, m.<br>
\end{align*}<br>
其中向量$c,a_1,\cdots,a_m \in \mathbb{R}^n $, 和标量$b_1,\cdots, b_m \in \mathbb{R}$是指定目标函数和约束函数条件的参数。</p>
<h4 id="求解线性规划-solving-linear-programs">求解线性规划(solving linear programs)</h4>
<ul>
<li>除了一个特例，没有解析解公式(和least-squares不同)；</li>
<li>有可靠且高效的算法实现；</li>
<li>时间复杂度是$O(n^2 m)$, m是约束条件的个数, m是维度$；</li>
<li>是一个成熟的方法。</li>
</ul>
<h4 id="应用-using-linear-programs">应用(using linear programs)</h4>
<p>一些应用直接使用线性规划的标准形式,或者其中一个标准形式。在很多时候，原始的优化问题没有一个标准的线性规划形式，但是可以被转化为等价的线性规划形式。比如切米雪夫近似问题(Chebyshev approximation problem)。它的形式如下：<br>
$$minimize \quad max_{i=1,\cdots,k}|a_i^T x-b_i|$$<br>
其中$x\in \mathbb{R}^n $是变量，$a_1,\cdots,a_k \in \mathbb{R}^n , b_1,\cdots,b_k \in \mathbb{R}$是实例化的问题参数,和least-squares相似的是，它们的目标函数都是项$a^T_i x-b_i$。不同之处在于，least-squares用的是该项的平方和作为目标函数，而Chebyshev approximation中用的是绝对值的最大值。Chebyshev approximation problem的目标函数是不可导的(max operation), least-squares problem的目标函数是二次的(quadratic), 因此可导的(differentiable)。</p>
<h2 id="凸优化-convex-optimization">凸优化(Convex optimization)</h2>
<p>凸优化问题是优化问题的一种,它的目标函数和优化函数都是凸的。<br>
具有以下形式的问题是一种凸优化问题：<br>
\begin{align*}<br>
&amp;minimize \quad f_0(x)\\<br>
&amp;subject \ to \quad f_i(x) \le b_i, i = 1,\cdots,m.<br>
\end{align*}<br>
其中函数$f_0,\cdots,f_m:\mathbb{R}^n \rightarrow \mathbb{R}$是凸的(convex), 如满足<br>
$$f_i(\alpha x+ \beta y) \le \alpha f_i(x) + \beta f_i(y)$$<br>
对于所有的$x,y \in \mathbb{R}^n $和所有的$\alpha, \beta \in \mathbb{R}$且$\alpha + \beta = 1, \alpha \ge 0, \beta \ge 0$。<br>
或者：<br>
$$f_i(\theta x+ (1-\theta) y) \le \theta f_i(x) + (1 - \theta) f_i(y)$$<br>
其中$\theta \in [0,1]$。<br>
课上有人问这里为$\theta$是0和1, 有没有什么物理意义，Stephen Boyd回答说这是定义，就是这么定义的。<br>
The least-squares和linear programming problem都是convex optimization problem的特殊形式。线性函数(linear functions)也是convex，它们正处在边界上，它们的曲率(curvature)为0。一种方式是用正曲率去描述凸性。</p>
<h3 id="凸优化求解-solving-convex-optimization-problems">凸优化求解(solving convex optimization problems)</h3>
<ul>
<li>没有解析解；</li>
<li>有可靠且有效的算法；</li>
<li>时间复杂度正比于$max{n^3 , n^2 m,F},$F$是评估$f$和计算一阶导数和二阶导数的时间；</li>
<li>有成熟的方法，如interior-point methods。</li>
</ul>
<h3 id="凸优化的应用-using-convex-optimization">凸优化的应用(using convex optimization)</h3>
<p>将实际问题形式化称凸优化问题。</p>
<h2 id="非线性优化-nonlinear-optimization">非线性优化(Nonlinear optimization)</h2>
<h3 id="非线性优化">非线性优化</h3>
<p>非线性优化用来描述目标函数和约束函数都是非线性函数(但不是凸的)优化问题。因为凸优化问题包括least-squares和linear programming, 它们是线性的。刚开始给出的优化问题就是非线性优化问题，目前没有有效的方法解该问题。目前有一些方法来解决一般的非线性问题，但是都做了一些compromise。</p>
<h4 id="局部优化-local-optimization">局部优化(local optimization)</h4>
<p>局部优化是非线性优化的一种解法，compromise是寻找局部最优点，而不是全局最优点，在可行解附近最小化目标函数，不保证能得到一个最小的目标值。<br>
局部优化需要随机初始化一个初值，这个初值很关键，很大程度的影响了局部解得到的目标值, 也就是说是一个初值敏感的算法。关于初始值和全局最优值距离有多远并没有很多有用的信息。局部优化对于算法的参数值很敏感，需要根据具体问题去具体调整。<br>
使用局部优化的方法比解least-squares problems, linear program, convex optimization problem更有技巧性，因为它牵扯到算法的选择，算法参数的选择，以及初值的选取。</p>
<h4 id="全局优化-global-optimization">全局优化(global optimization)</h4>
<p>全局优化也是非线性优化的一种解法, 在全局优化中，优化目标的全局最优解被找到， compromise是效率。</p>
<h4 id="凸优化问题在非凸优化问题中的应用-role-of-convex-optimization-in-nonconvex-problems">凸优化问题在非凸优化问题中的应用(role of convex optimization in nonconvex problems)</h4>
<h5 id="初始化局部优化-initialization-for-local-optimization">初始化局部优化(initialization for local optimization)</h5>
<h5 id="用于非凸优化的凸的启发式搜索-convex-heuristics-for-nonconvex-optimization">用于非凸优化的凸的启发式搜索(convex heuristics for nonconvex optimization)</h5>
<h5 id="全局最优的边界-bounds-for-global-optimization">全局最优的边界(bounds for global optimization)</h5>
<h2 id="大纲-outline">大纲(outline)</h2>
<h3 id="理论-part-one-theory">理论(part one: Theory)</h3>
<p>第一部分是理论，给出一些概念和定义，第一章是Introduction, 第二章和第三章分别介绍凸集(convex set)和凸函数(convex function), 第四章介绍凸优化问题， 第五章引入拉格朗日对偶性。</p>
<h3 id="应用-part-two-applications">应用(part two: Applications)</h3>
<p>第二部分主要给出凸优化在一些领域的应用，如概率论与数理统计，经济学，计算几何以及数据拟合等领域。<br>
凸优化如何应用在实践中。</p>
<h3 id="算法-part-three-algorithms">算法(part three: Algorithms)</h3>
<p>第三部分给出了凸优化的数值解法，如牛顿法(Newton’s algorithm)和内点法(interior-point)。<br>
第三部分有三章，分别包含了无约束优化，等式约束优化和不等式约束优化。章节之间是递进的，解一个问题被分解为解一系列简单问题。二次优化问题(包括，如least-squares)是最底层的基石，它可以通过线性方程组精确求解。牛顿法，在第十章和第十一章介绍到，是下个层次，无约束问题或者等式约束问题被转化成一系列二次优化问题的求解。第十一章介绍了内点法，是最顶层, 这些方法将不等式约束问题转化为一系列无约束或者等式约束的问题。</p>
<h2 id="参考文献">参考文献</h2>
<p>1.stephen boyd. Convex optimization</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://mxxhcm.github.io/2018/12/21/reinforcement-learning-an-introduction-第3章笔记/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="马晓鑫爱马荟荟">
      <meta itemprop="description" content="记录硕士三年自己的积累">
      <meta itemprop="image" content="/images/favicon.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="mxxhcm's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2018/12/21/reinforcement-learning-an-introduction-第3章笔记/" class="post-title-link" itemprop="http://mxxhcm.github.io/page/16/index.html">reinforcement learning an introduction 第3章笔记</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-12-21 15:13:38" itemprop="dateCreated datePublished" datetime="2018-12-21T15:13:38+08:00">2018-12-21</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-10-04 12:03:17" itemprop="dateModified" datetime="2019-10-04T12:03:17+08:00">2019-10-04</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/强化学习/" itemprop="url" rel="index"><span itemprop="name">强化学习</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="马尔科夫过程-markov-process-马尔科夫链-markov-chain">马尔科夫过程(markov process)、马尔科夫链(markov chain)</h2>
<p>马尔科夫过程或者马尔科夫链(markov chain)是一个tuple $\lt S,P\gt$,其中S是一个有限(或者无限)的状态集合,P是状态转移矩阵(transition probability matrix)或马尔科夫矩阵(markov matrix),$P_{ss’}= P[S_{t+1} = s’|S_t = s]$.</p>
<h2 id="马尔科夫奖励过程-markov-reward-process">马尔科夫奖励过程(markov reward process)</h2>
<p>马尔科夫奖励过程是一个tuple $\lt S,P,R,\gamma\gt$,和马尔科夫过程相比，它多了一个奖励R，R和某个具体的状态相关，MRP中的reward只和state有关,和action无关。<br>
S是一个(有限)状态的集合。<br>
P是一个状态转移概率矩阵。<br>
R是一个奖励函数$R = \mathbb{E}[R_{t+1}|S_t = s]$, <strong>这里为什么是t+1时刻的reward?这仅仅是一个约定，为了描述RL问题中涉及到的observation，action，reward比较方便。这里可以理解为离开这个状态才能获得奖励而不是进入这个状态即获得奖励。如果改成$R_t$也是可以的，这时可以理解为进入这个状态获得的奖励。</strong><br>
$\gamma$称为折扣因子(discount factor), $\gamma \epsilon [0,1]$.<strong>为什么引入$\gamma$，David Silver的公开课中提到了四个原因:(1)数学上便于计算回报(return)；(2)避免陷入无限循环；(3)长远利益具有一定的不确定性；(4)符合人类对眼前利益的追求。</strong></p>
<h3 id="奖励-reward">奖励(reward)</h3>
<p>每个状态s在一个时刻t立即可得到一个reward,reward的值需要由环境给出,这个值可正可负。目前的强化学习算法中reward都是人为设置的。</p>
<h3 id="回报-return">回报(return)</h3>
<p>回报是累积的未来的reward,其计算公式如下:<br>
$$G_t = R_{t+1} + R_{t+2} + … = \sum_{k=0}^{\infty} {\gamma^k R_{t+k+1}} \tag{1}$$<br>
它是一个马尔科夫链上从t时刻开始往后所有奖励的有衰减(带折扣因子)的总和。</p>
<h3 id="值函数-value-function">值函数(value function)</h3>
<p>值函数是回报(return)的期望(expected return), 一个MRP过程中某一状态的value function为从该状态开始的markov charin return的期望，即$v(s) = \mathbb{E}[G_t|S_t=s]$.<br>
MRP的value function和MDP的value function是不同的, MRP的value function是对于state而言的，而MDP的value function是针对tuple $\lt$state, action$\gt$的。<br>
这里为什么要取期望,因为policy是stotastic的情况时，在每个state时，采取每个action都是可能的，都有一定的概率，next state也是不确定的了，所以value funciton是一个随机变量，因此就引入期望来刻画随机变量的性质。<br>
为什么在当前state就知道下一时刻的state了?对于有界的RL问题来说，return是在一个回合结束时候计算的；对于无界的RL问题来说，由于有衰减系数，只要reward有界，return就可以计算出来。</p>
<h3 id="马尔科夫奖励过程的贝尔曼方程-bellman-equation-for-mrp">马尔科夫奖励过程的贝尔曼方程(bellman equation for MRP)</h3>
<p>\begin{align*}<br>
v(s) &amp;= \mathbb{E}[G_t|S_t = s]\\<br>
&amp;= \mathbb{E}[R_{t+1} + \gamma R_{t+2} + … | S_t = s]\\<br>
&amp;= \mathbb{E}[R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3} + …|S_t = s]\\<br>
&amp;= \mathbb{E}[R_{t+1} + \gamma G_{t+1} |S_t = s]\\<br>
&amp;= \mathbb{E}[R_{t+1} + \gamma v(S_{t+1})|S_t = s]\\<br>
v(s) &amp;= \mathbb{E}[R_{t+1} + \gamma v(S_{t+1})|S_t = s]<br>
\end{align*}<br>
v(s)由两部分组成，一部分是immediate reward的期望(expectation)，$\mathbb{E}[R_{t+1}]$, 只与当前时刻state有关；另一部分是下一时刻state的value function的expectation。如果用s’表示s状态下一时刻的state，那么bellman equation可以写成：<br>
$$v(s) = R_s + \gamma \sum_{s’ \epsilon S} P_{ss’}v(s’)$$<br>
我们最终的目的是通过迭代使得t轮迭代时的v(s)和第t+1轮迭代时的v(s)相等。将其写成矩阵形式为：<br>
$$v_t = R + \gamma P v_{t+1}$$<br>
$$(v_1,v_2,…,v_n)^T = (R_1,R_2,…,R_n)^T + \gamma \begin{bmatrix}P_{11}&amp;P_{12}&amp;…&amp;P_{1n}\\P_{21}&amp;P_{22}&amp;…&amp;P_{2n}\\&amp;&amp;…&amp;\\P_{n1}&amp;P_{n2}&amp;…&amp;P_{nn}\end{bmatrix} (v_1,v_2,…,v_n)^T $$<br>
MRP的Bellman方程组是线性的，可以直接求解:<br>
\begin{align*}<br>
v &amp;= R + \gamma Pv\\<br>
(1-\gamma P) &amp;= R\\<br>
v &amp;= (1 - \gamma P)^{-1} R<br>
\end{align*}<br>
可以直接解方程，但是复杂度为$O(n^3)$，对于大的MRP方程组不适用，可以通过迭代法求解，常用的迭代法有动态规划,蒙特卡洛算法和时序差分算法等求解(动态规划是迭代法吗？）</p>
<h2 id="马尔科夫决策过程-markov-decision-process">马尔科夫决策过程(markov decision process)</h2>
<p>马尔科夫决策过程，比markov reward process多了一个A,它也是一个tuple $\lt S,A,P,R,\gamma\gt$, 在MRP中奖励R仅仅和状态S相关，在MDP中奖励R和概率P对应的是某个状态S和某个动作A的组合。<br>
\begin{align*}<br>
P_{ss’}^a &amp;= P[S_{t+1} = s’ | S_t = s, A_t = a]\\<br>
R_s^a &amp;= \mathbb{E}[R_{t+1} | S_t = s, A_t = a]<br>
\end{align*}<br>
这里的reward不仅仅与state相关，而是与tuple $\lt state，action\gt$相关。</p>
<h3 id="回报">回报</h3>
<p>MDP中的$G_t$和式子$(1)$的$G_t$是一样的，将$G_t$写成和后继时刻相关的形式如下：<br>
\begin{align*}<br>
G_t &amp;= R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \gamma^3 R_{t+4} + …\\<br>
&amp;= R_{t+1} + \gamma (R_{t+2} + \gamma^1 R_{t+3} + \gamma^2 R_{t+4} + …)\\<br>
&amp;= R_{t+1} + \gamma G_{t+1} \tag{2}<br>
\end{align*}<br>
这里引入$\gamma$之后，即使是在continuing情况下，只要$G_t$是非零常数，$G_t$也可以通过等比数列求和公式进行计算，即:<br>
$$G_t = \sum_{k=1}^{\infty} \gamma^k = \frac{1}{1-\gamma} \tag{3}$$</p>
<h3 id="策略-policy">策略(policy)</h3>
<p>策略$\pi$的定义:给定状态时采取各个动作的概率分布。<br>
$$\pi(a|s) = P[A_t = a | S_t = a] \tag{4}$$</p>
<h3 id="值函数-value-function-v2">值函数(value function)</h3>
<p>这里给出的是值函数的定义，就是这么定义的。<br>
MDP的值函数有两种，状态值函数(state value function)和动作值函数(action value function), 这两种值函数的含义其实是一样的，也可以相互转换。具体来说, 值函数定义为给定一个policy $\pi$，得到的回报的期望(expected return)。<br>
一个MDP的状态s对应的值函数(state value function) $v_{\pi}(s)$是从状态s开始采取策略$\pi$得到的回报的期望。<br>
\begin{align*}<br>
v_{\pi}(s) &amp;= \mathbb{E}_{\pi}[G_t|S_t = s]\\<br>
&amp;=\mathbb{E}_{\pi}[\sum_{k=0}^{\infty} \gamma^{k} R_{t+k+1}|S_t=s] \tag{5}<br>
\end{align*}<br>
这里的$G_t$是式子(2)中的回报。<br>
一个MDP过程中动作值函数(action value function) $q_{\pi}(s,a)$是从状态s开始,采取action a，采取策略$\pi$得到的回报的期望。<br>
&lt;action value function $q_{\pi}(s,a)$ is the expected return starting from states, taking action a, and then following policy \pi.&gt;<br>
\begin{align*}<br>
q_{\pi}(s,a) &amp;= \mathbb{E}_{\pi}\left[G_t | S_t = s, A_t = a\right]\\<br>
&amp;= \mathbb{E}_{\pi}\left[\sum_{k=0}^{\infty} \gamma^{k} R_{t+k+1}|S_t=s, A_t=a\right] \tag{6}<br>
\end{align*}</p>
<h4 id="状态值函数-state-value-function">状态值函数(state value function)</h4>
<p>\begin{align*}<br>
v_{\pi}(s) &amp;= \sum_{a \epsilon A} \pi(a|s) q_{\pi} (s,a) \tag{7}\\<br>
v_{\pi}(s) &amp;= \sum_a \pi(a|s)\sum_{s’,r}p(s’,r|s,a) \left[r + \gamma v_{\pi}(s’) \right] \tag{8}\\<br>
\end{align*}<br>
式子$(7)$是$v(s)$和$q(s,a)$的关系，式子$(8)$是$v(s)$和它的后继状态$v(s’)$的关系。<br>
式子$(8)$的推导如下：<br>
\begin{align*}<br>
v_{\pi}(s) &amp;= \mathbb{E}_{\pi}[G_t|S_t = s]\\<br>
&amp;= \mathbb{E}_{\pi}\left[R_{t+1}+\gamma G_{t+1}|S_t = s\right]\\<br>
&amp;= \sum_a \pi(a|s)\sum_{s’}\sum_rp(s’,r|s,a) \left[r + \gamma \mathbb{E}_{\pi}\left[G_{t+1}|S_{t+1}=s’\right]\right]\\<br>
&amp;= \sum_a \pi(a|s)\sum_{s’,r}p(s’,r|s,a) \left[r + \gamma v_{\pi}(s’) \right]\\<br>
\end{align*}</p>
<h4 id="动作值函数-action-value-function">动作值函数(action value function)</h4>
<p>\begin{align*}<br>
q_{\pi}(s,a) &amp;= \sum_{s’}\sum_r p(s’,r|s,a)(r + \gamma  v_{\pi}(s’)) \tag{9}\\<br>
q_{\pi}(s,a) &amp;= \sum_{s’}\sum_r p(s’,r|s,a)(r + \gamma  \sum_{a’}\pi(a’|s’)q(s’,a’)) \tag{10}\\<br>
\end{align*}<br>
式子$(9)$是$q(s,a)$和$v(s)$的关系，式子$(10)$是$q(s,a)$和它的后继状态$q(s’,a’)$的关系。<br>
以上都是针对MDP来说的，在MDP中，给定policy $\pi$下，状态s下选择a的action value function，$q_{\pi}(s,a)$类似MRP里面的v(s)，而MDP中的v(s)是要考虑在state s下采率各个action后的情况。</p>
<h3 id="贝尔曼期望方程-bellmam-expectation-equation">贝尔曼期望方程(Bellmam expectation equation)</h3>
<p>\begin{align*}<br>
v_{\pi}(s) &amp;= \mathbb{E}_{\pi}[R_{t+1} + \gamma v_{\pi}(S_{t+1})|S_t = s] \tag{11}\\<br>
v_{\pi}(s) &amp;= \mathbb{E}_{\pi}\left[q_{\pi}(S_t,A_t)|S_t=s,A_t=a\right]\tag{12}\\<br>
q_{\pi}(s,a)&amp;= \mathbb{E}_{\pi}\left[R_{t+1} + \gamma v_{\pi}(S_{t+1}) |S_t=s,A_t=a\right]\tag{13}\\<br>
q_{\pi}(s,a) &amp;= \mathbb{E}_{\pi}[R_{t+1} + \gamma q_{\pi}(S_{t+1},A_{t+1}) | S_t = s, A_t = a] \tag{14}<br>
\end{align*}</p>
<h4 id="矩阵形式">矩阵形式</h4>
<p>\begin{align*}<br>
v_{\pi} &amp;= R^{\pi} + \gamma P^{\pi} v_{\pi}\\<br>
v_{\pi} &amp;= (I-\gamma P^{\pi} )^{-1} R^{\pi}<br>
\end{align*}</p>
<h2 id="最优策程的求解-how-to-find-optimal-policy">最优策程的求解(how to find optimal policy)</h2>
<h3 id="最优价值函数-optimal-value-function">最优价值函数(optimal value function)</h3>
<p>$v_{*} = \max_{\pi}v_{\pi}(s)$,从所有策略产生的state value function中，选取使得state s的价值最大的函数<br>
$q_{*}(s,a) = \max_{\pi} q_{\pi}(s,a)$,从所有策略产生的action value function中，选取使$\lt s,a\gt$价值最大的函数<br>
当我们得到了optimal value function，也就知道了每个state的最优价值，便认为这个MDP被解决了</p>
<h3 id="最优策略-optimal-policy">最优策略(optimal policy)</h3>
<p>对于每一个state s，在policy $\pi$下的value 大于在policy $\pi’$的value， 就称策略$\pi$优于策略$\pi’$， $\pi \ge \pi’$ if $v_{\pi}(s) \ge v_{\pi’}(s)$, 对于任意s都成立<br>
对于任何MDP，都满足以下条件：</p>
<ol>
<li>都存在一个optimal policy，它比其他策略好或者至少相等；</li>
<li>所有的optimal policy的optimal value function是相同的；</li>
<li>所有的optimal policy 都有相同的 action value function.</li>
</ol>
<h3 id="寻找最优策略">寻找最优策略</h3>
<p>寻找optimal policy可以通过寻找optimal action value function来实现：<br>
$${\pi}_{*}(a|s) =<br>
\begin{cases}1, &amp;if\quad a = \arg\max\ q_{*}(s,a)\\0, &amp;otherwise\end{cases}$$</p>
<h3 id="贝尔曼最优方程-bellman-optimal-equation">贝尔曼最优方程(bellman optimal equation)</h3>
<p>*号表示最优的策略。</p>
<h4 id="最优状态值函数-state-value-function">最优状态值函数(state value function)</h4>
<p>\begin{align*}<br>
v_{*}(s) &amp;= \max_a q_{*}(s,a)\\<br>
&amp;= \max_a\mathbb{E}_{\pi_{*}}\left[G_t|S_t=s,A_t=a\right]\\<br>
&amp;= \max_a\mathbb{E}_{\pi_{*}}\left[R_{t+1}+\gamma G_t|S_t=s,A_t=a\right]\\<br>
&amp;= \max_a\mathbb{E}\left[R_{t+1} +\gamma v_{*}(S_{t+1})|S_t=s,A_t=a\right]\\<br>
&amp;= \max_a \left[\sum_{s’,r} p(s’,r|s,a)(r+\gamma v_{*}(s’) )\right] \tag{15}\\<br>
\end{align*}</p>
<h4 id="最优动作值函数-action-value-function">最优动作值函数(action value function)</h4>
<p>\begin{align*}<br>
q_{*}(s,a) &amp;= \sum_{s’,r} p(s’,r|s,a) (r + \gamma v_{*}(s’))\\<br>
&amp;= \sum_{s’,r} p(s’,r|s,a) (r + \gamma \max_{a’} q_{*}(s’,a’))\\<br>
&amp;=\mathbb{E}\left[R_{t+1}+\gamma \max_{a’}q_{*}(S_{t+1},a’)|S_t=s,A_t=a \right]\tag{16}\\<br>
\end{align*}</p>
<h3 id="贝尔曼最优方程的求解-solution-to-bellman-optimal-equation">贝尔曼最优方程的求解(solution to Bellman optimal equation)</h3>
<p>Bellman equation和Bellman optimal equation相比，一个是对于给定的策略，求其对应的value function,是对一个策略的估计，而bellman optimal equation是要寻找最优策略，通过对action value function进行贪心。<br>
Bellman最优方程是非线性的，没有固定的解决方案，只能通过迭代法来解决，如Policy iteration，value iteration，Q-learning，Sarsa等。</p>
<h2 id="参考文献">参考文献</h2>
<p>1.<a href="http://incompleteideas.net/book/the-book-2nd.html" target="_blank" rel="noopener">http://incompleteideas.net/book/the-book-2nd.html</a><br>
2.<a href="https://www.bilibili.com/video/av32149008/?p=2" target="_blank" rel="noopener">https://www.bilibili.com/video/av32149008/?p=2</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://mxxhcm.github.io/2018/12/20/linux-常见问题/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="马晓鑫爱马荟荟">
      <meta itemprop="description" content="记录硕士三年自己的积累">
      <meta itemprop="image" content="/images/favicon.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="mxxhcm's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2018/12/20/linux-常见问题/" class="post-title-link" itemprop="http://mxxhcm.github.io/page/16/index.html">linux 常见问题（不定期更新）</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-12-20 20:30:34" itemprop="dateCreated datePublished" datetime="2018-12-20T20:30:34+08:00">2018-12-20</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-06-04 10:37:22" itemprop="dateModified" datetime="2019-06-04T10:37:22+08:00">2019-06-04</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/linux/" itemprop="url" rel="index"><span itemprop="name">linux</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="问题1-undefined-reference-to-pthread-create-in-linux">问题1 Undefined reference to pthread_create in Linux</h2>
<p>在阅读自然语言处理的一篇论文时，读到了bype pair encoding(bpe)算法。在github找到了一个实现<a href="https://github.com/glample/fastBPE" target="_blank" rel="noopener">fastBPE</a>, 算法是用C++写的，在编译的过程中遇到了问题&quot;Undefined reference to pthread_create in Linux&quot;,</p>
<h3 id="terminal下解决方案">terminal下解决方案</h3>
<p>查阅资料了解到pthread不是Linux操作系统默认的库函数，所以需要在编译的时候将pthread链接该库函数，后来在看fastBPE的文档时发现文档中已经有说明:<br>
Compile with:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">g++ -std=c++11 -pthread -O3 fast.cc -o fast</span><br></pre></td></tr></table></figure>
<h3 id="codeblocks下解决方案">codeblocks下解决方案</h3>
<p>上面给出的方案是使用gcc在terminal进行编译时加入静态库，但是对于不习惯在命令行使用gdb进行调试的人来说没有用。<br>
在codeblocks中，如果要链接静态库,找到Settings --&gt; Compiler… --&gt; Linker settings，点击add，添加相应的库函数即可。</p>
<h2 id="参考文献">参考文献</h2>
<p>1:<a href="https://stackoverflow.com/questions/1662909/undefined-reference-to-pthread-create-in-linux" target="_blank" rel="noopener">https://stackoverflow.com/questions/1662909/undefined-reference-to-pthread-create-in-linux</a><br>
2:<a href="https://blog.csdn.net/zhaoyue007101/article/details/7705753" target="_blank" rel="noopener">https://blog.csdn.net/zhaoyue007101/article/details/7705753</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://mxxhcm.github.io/2018/10/21/classfication/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="马晓鑫爱马荟荟">
      <meta itemprop="description" content="记录硕士三年自己的积累">
      <meta itemprop="image" content="/images/favicon.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="mxxhcm's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2018/10/21/classfication/" class="post-title-link" itemprop="http://mxxhcm.github.io/page/16/index.html">classfication</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-10-21 18:47:44" itemprop="dateCreated datePublished" datetime="2018-10-21T18:47:44+08:00">2018-10-21</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-10-21 22:53:18" itemprop="dateModified" datetime="2019-10-21T22:53:18+08:00">2019-10-21</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/机器学习/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="classfication">Classfication</h2>
<h2 id="lda">LDA</h2>
<h2 id="logistic-regression">Logistic Regression</h2>
<h3 id="logistic-function">Logistic function</h3>
<p>$$ S(x) = \frac{1}{1+e^{x} }$$<br>
如下图所示：<br>
<img src="/2018/10/21/classfication/logistic_function.png" alt="logistic_func"><br>
它的取值在$[0,1]$之间。<br>
logistic regression的目标函数是：<br>
$$h(x) = \frac{1}{1+e<sup>{-\theta</sup>T x} 3}$$<br>
其中$x$是输入，$\theta$是要求的参数。</p>
<h3 id="思路">思路</h3>
<p>Logistic regression利用logistic function进行分类，给出一个输入，经过参数$\theta$的变换，输出一个$[0,1]$之间的值，如果大于$0.5$，把它分为一类，小于$0.5$，分为另一类。这个$0.5$只是一个例子，可以根据不同的需求选择不同的值。<br>
$\theta^T x$相当于给出了一个非线性的决策边界。</p>
<h3 id="cost-function">Cost function</h3>
<p>$$J(\theta) = -\log L(\theta) = -\sum_{i=1}^m (y(i)\log h(x^{(i)}) + (1-y<sup>{(i)})\log(1-h(x</sup>{(i)} )) )$$<br>
给出两种方式推导logistic regression的cost function</p>
<h4 id="maximum-likelyhood-estimation">Maximum likelyhood estimation</h4>
<p>通过极大似然估计推导得到的，当是两个类别的分类时，即$0$或者$1$，有：<br>
$$P(y=1|x,\theta) = h(x)$$<br>
$$P(y=0|x,\theta) = 1- h(x)$$<br>
服从二项分布，写成一个式子是：<br>
$$P(y|x,\theta) = h(x)^y (1-h(x))^{1-y}$$<br>
其中$y$取值只有$0$和$1$。<br>
有了$y$的表达式，我们就可以使用最大似然估计进行求解了：<br>
$$L(\theta) = \prod_{i=1}^m (h(x<sup>{(i)})</sup>{y(i)}(1-h(x^{(i)} ))<sup>{(1-y</sup>{(i)})}$$<br>
似然函数要求最大化，即求使得$m$个observation出现概率最大的$\theta$，<br>
损失函数是用来衡量损失的，令损失函数取负的对数似然，然后最小化loss也就是最大化似然函数了：<br>
$$J(\theta) = -\log L(\theta) = -\sum_{i=1}^m (y(i)\log h(x^{(i)}) + (1-y<sup>{(i)})\log(1-h(x</sup>{(i)} )) )$$</p>
<h4 id="cross-entropy">Cross-entropy</h4>
<p>对于$k$类问题，写出交叉熵公式如下所示：<br>
$$J(\theta) = -\frac{1}{n}\left[\sum_{i=1}^m \sum_k y_k^{(i)} \log h(x_k^{(i)} ) \right]$$<br>
当$k=2$时：<br>
$$J(\theta) = -\frac{1}{n}\left[\sum_{i=1}^m  y^{(i)} \log h(x^{(i)} ) + (1-y^{(i)}) \log (1-h(x^{(i)} ))\right]$$</p>
<h3 id="梯度下降">梯度下降</h3>
<p>$$J(\theta) = -\log L(\theta) = -\sum_{i=1}^m \left[y(i)\log h(x^{(i)}) + (1-y<sup>{(i)})\log(1-h(x</sup>{(i)} )) \right]$$</p>
<p>\begin{align*}<br>
\nabla J &amp; =  -\sum_{i=1}^m \left[ y(i)\frac{1}{h(x^{(i)})}\nabla h(x^{(i)}) - (1-y<sup>{(i)})\frac{1}{\log(1-h(x</sup>{(i)} ))}\nabla\log(1-h(x^{(i)} ))\right]<br>
&amp;=-\sum_{i=1}^m  (h(x^{(i)}) - y^{(i)}) x^{(i)}<br>
\end{align*}</p>
<h2 id="参考文献">参考文献</h2>
<p>1.<a href="https://blog.csdn.net/jk123vip/article/details/80591619" target="_blank" rel="noopener">https://blog.csdn.net/jk123vip/article/details/80591619</a><br>
2.<a href="https://zhuanlan.zhihu.com/p/28408516" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/28408516</a><br>
3.<a href="https://www.cnblogs.com/pinard/p/6029432.html" target="_blank" rel="noopener">https://www.cnblogs.com/pinard/p/6029432.html</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://mxxhcm.github.io/2018/10/21/regression/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="马晓鑫爱马荟荟">
      <meta itemprop="description" content="记录硕士三年自己的积累">
      <meta itemprop="image" content="/images/favicon.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="mxxhcm's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2018/10/21/regression/" class="post-title-link" itemprop="http://mxxhcm.github.io/page/16/index.html">regression</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-10-21 18:47:01" itemprop="dateCreated datePublished" datetime="2018-10-21T18:47:01+08:00">2018-10-21</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-10-21 18:50:24" itemprop="dateModified" datetime="2019-10-21T18:50:24+08:00">2019-10-21</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/机器学习/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="none"></h2>
<h2 id="参考文献">参考文献</h2>
<ol>
<li></li>
</ol>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/15/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/15/">15</a><span class="page-number current">16</span>
  </nav>



          </div>
          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <div class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/favicon.jpg" alt="马晓鑫爱马荟荟">
            
              <p class="site-author-name" itemprop="name">马晓鑫爱马荟荟</p>
              <p class="site-description motion-element" itemprop="description">记录硕士三年自己的积累</p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">160</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  <a href="/categories/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">16</span>
                    <span class="site-state-item-name">分类</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  <a href="/tags/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">193</span>
                    <span class="site-state-item-name">标签</span>
                  </a>
                </div>
              
            </nav>
          

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="https://github.com/mxxhcm" title="GitHub &rarr; https://github.com/mxxhcm" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="mailto:mxxhcm@gmail.com" title="E-Mail &rarr; mailto:mxxhcm@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                </span>
              
            </div>
          

          

          
          

          
            
          
          

        </div>
      </div>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">马晓鑫爱马荟荟</span>

  

  
</div>


  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v3.8.0</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> v6.6.0</div>




        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  
    <script src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  


  


  <script src="/js/src/utils.js?v=6.6.0"></script>

  <script src="/js/src/motion.js?v=6.6.0"></script>



  
  


  <script src="/js/src/affix.js?v=6.6.0"></script>

  <script src="/js/src/schemes/pisces.js?v=6.6.0"></script>



  

  


  <script src="/js/src/bootstrap.js?v=6.6.0"></script>



  



  






<!-- LOCAL: You can save these files to your site and update links -->
    
        
        <link rel="stylesheet" href="https://aimingoo.github.io/gitmint/style/default.css">
        <script src="https://aimingoo.github.io/gitmint/dist/gitmint.browser.js"></script>
    
<!-- END LOCAL -->

    

    







  





  

  

  

  

  
  

  
  
    
      
        
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
  

  
    
      <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      },
      TeX: {equationNumbers: { autoNumber: "AMS" }}
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<script src="//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<style>
.MathJax_Display {
    overflow: auto hidden;
}
</style>

    
  


  
  

  

  

  

  

  

  

</body>
</html>

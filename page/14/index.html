<!DOCTYPE html>












  


<html class="theme-next pisces use-motion" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
































<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=6.6.0" rel="stylesheet" type="text/css">



  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.jpg?v=6.6.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.jpg?v=6.6.0">










<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '6.6.0',
    sidebar: {"position":"left","display":"always","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="记录硕士三年自己的积累">
<meta property="og:type" content="website">
<meta property="og:title" content="mxxhcm&#39;s blog">
<meta property="og:url" content="http://mxxhcm.github.io/page/14/index.html">
<meta property="og:site_name" content="mxxhcm&#39;s blog">
<meta property="og:description" content="记录硕士三年自己的积累">
<meta property="og:locale" content="zh-CN">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="mxxhcm&#39;s blog">
<meta name="twitter:description" content="记录硕士三年自己的积累">



  <link rel="alternate" href="/atom.xml" title="mxxhcm's blog" type="application/atom+xml">




  <link rel="canonical" href="http://mxxhcm.github.io/page/14/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>mxxhcm's blog</title>
  












  <noscript>
  <style>
    .use-motion .motion-element,
    .use-motion .brand,
    .use-motion .menu-item,
    .sidebar-inner,
    .use-motion .post-block,
    .use-motion .pagination,
    .use-motion .comments,
    .use-motion .post-header,
    .use-motion .post-body,
    .use-motion .collection-title { opacity: initial; }

    .use-motion .logo,
    .use-motion .site-title,
    .use-motion .site-subtitle {
      opacity: initial;
      top: initial;
    }

    .use-motion .logo-line-before i { left: initial; }
    .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">mxxhcm's blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
    
      
    

    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>标签</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">

    
    
    
      
    

    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>分类</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>归档</a>

  </li>

      
      
    </ul>
  

  

  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
            

          
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://mxxhcm.github.io/2019/03/18/python-hdf5笔记/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="马晓鑫爱马荟荟">
      <meta itemprop="description" content="记录硕士三年自己的积累">
      <meta itemprop="image" content="/images/favicon.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="mxxhcm's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/03/18/python-hdf5笔记/" class="post-title-link" itemprop="http://mxxhcm.github.io/page/14/index.html">h5py笔记</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-03-18 15:12:03" itemprop="dateCreated datePublished" datetime="2019-03-18T15:12:03+08:00">2019-03-18</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-06-13 10:06:17" itemprop="dateModified" datetime="2019-06-13T10:06:17+08:00">2019-06-13</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/python/" itemprop="url" rel="index"><span itemprop="name">python</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="python包安装"><a href="#python包安装" class="headerlink" title="python包安装"></a>python包安装</h2><p>~$:pip install h5py</p>
<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><h3 id="创建和打开h5py文件"><a href="#创建和打开h5py文件" class="headerlink" title="创建和打开h5py文件"></a>创建和打开h5py文件</h3><p>f = h5py.File(“pathname”,”w”)<br>w     create file, truncate if exist<br>w- or x  create file,fail if exists<br>r         readonly, file must be exist r+        read/write,file must be exist<br>a        read/write if exists,create othrewise (default)</p>
<h3 id="删除一个dataset或者group"><a href="#删除一个dataset或者group" class="headerlink" title="删除一个dataset或者group"></a>删除一个dataset或者group</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">del</span> group[<span class="string">"dataset_name/group_name"</span>]</span><br></pre></td></tr></table></figure>
<h2 id="dataset"><a href="#dataset" class="headerlink" title="dataset"></a>dataset</h2><h3 id="什么是dataset"><a href="#什么是dataset" class="headerlink" title="什么是dataset"></a>什么是dataset</h3><p>datasets和numpy arrays挺像的</p>
<h3 id="创建一个dataset"><a href="#创建一个dataset" class="headerlink" title="创建一个dataset"></a>创建一个dataset</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">f = h5py.File(<span class="string">"pathname"</span>,<span class="string">"w"</span>)</span><br><span class="line">f.create_dataset(<span class="string">"dataset_name"</span>, (<span class="number">10</span>,), dtype=<span class="string">'i'</span>)</span><br><span class="line">f.create_dataset(<span class="string">"dataset_name"</span>, (<span class="number">10</span>,), dtype=<span class="string">'c'</span>)</span><br></pre></td></tr></table></figure>
<p>第一个参数是dataset的名字, 第二个参数是dataset的shape, dtype参数是dataset中元素的类型。</p>
<h3 id="如何访问一个dataset"><a href="#如何访问一个dataset" class="headerlink" title="如何访问一个dataset"></a>如何访问一个dataset</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dataset = f[<span class="string">"dataset_name"</span>]                           <span class="comment"># acess like a python dict</span></span><br><span class="line">dataset = f.create_dateset(<span class="string">"dataset_name"</span>)  <span class="comment"># or create a new dataset</span></span><br></pre></td></tr></table></figure>
<h3 id="dataset的属性"><a href="#dataset的属性" class="headerlink" title="dataset的属性"></a>dataset的属性</h3><p>dataset.name        #输出dataset的名字<br>dataset.tdype        #输出dataset中elements的type<br>dataset.shape        #输出dataset的shape<br>dataset.value<br>dataset doesn’t hava attrs like keys,values,items,etc..</p>
<h3 id="给h5py-dataset复制numpy-array"><a href="#给h5py-dataset复制numpy-array" class="headerlink" title="给h5py dataset复制numpy array"></a>给h5py dataset复制numpy array</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">array = np.zero((<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">h[<span class="string">'array'</span>] = array        <span class="comment"># in h5py file, you need't to explicit declare the shape of array, just assign it an object of numpy array</span></span><br></pre></td></tr></table></figure>
<h2 id="group"><a href="#group" class="headerlink" title="group"></a>group</h2><h3 id="什么是group"><a href="#什么是group" class="headerlink" title="什么是group"></a>什么是group</h3><p>group和字典挺像的</p>
<h3 id="创建一个group"><a href="#创建一个group" class="headerlink" title="创建一个group"></a>创建一个group</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">group = f.create_group(<span class="string">"group_name"</span>)    <span class="comment">#在f下创建一个group</span></span><br><span class="line">group.create_group(<span class="string">"group_name"</span>)        <span class="comment">#在group下创建一个group</span></span><br><span class="line">group.create_dataset(<span class="string">"dataset_name"</span>)    <span class="comment">#在group下创建一个dataset</span></span><br></pre></td></tr></table></figure>
<h3 id="访问一个group-the-same-as-dataset"><a href="#访问一个group-the-same-as-dataset" class="headerlink" title="访问一个group(the same as dataset)"></a>访问一个group(the same as dataset)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">group = f[<span class="string">"group_name"</span>]                           <span class="comment"># acess like a python dict</span></span><br><span class="line">group = f.create_dateset(<span class="string">"group_name"</span>)  <span class="comment"># or create a new group</span></span><br></pre></td></tr></table></figure>
<h3 id="group的属性和方法"><a href="#group的属性和方法" class="headerlink" title="group的属性和方法"></a>group的属性和方法</h3><p>group.name        #输出group的名字<br>以下内容分为python2和python3版本</p>
<h4 id="python-2-版本"><a href="#python-2-版本" class="headerlink" title="python 2 版本"></a>python 2 版本</h4><p>group.values()    #输出group的value<br>group.keys()        #输出gorup的keys<br>group.items()    #输出group中所有的item，包含group和dataste</p>
<h4 id="python-3-版本"><a href="#python-3-版本" class="headerlink" title="python 3 版本"></a>python 3 版本</h4><p>list(group.keys())<br>list(group.values())<br>list(group.items())</p>
<h2 id="属性"><a href="#属性" class="headerlink" title="属性"></a>属性</h2><h3 id="设置dataset属性"><a href="#设置dataset属性" class="headerlink" title="设置dataset属性"></a>设置dataset属性</h3><p>dataset.attrs[“attr_name”]=”attr_value”    #设置attr<br>print(dataset.attrs[“attr_name”])                #访问attr</p>
<h3 id="设置group属性"><a href="#设置group属性" class="headerlink" title="设置group属性"></a>设置group属性</h3><p>group.attrs[“attr_name”]=”attr_value”    #设置attr<br>print(group.attrs[“attr_name”])                #访问attr</p>
<h2 id="numpy-and-h5py"><a href="#numpy-and-h5py" class="headerlink" title="numpy and h5py"></a>numpy and h5py</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">f = h5py.File(pathname,<span class="string">"r"</span>)</span><br><span class="line"></span><br><span class="line">data = f[<span class="string">'data'</span>]    <span class="comment"># type 是dataset</span></span><br><span class="line">data = f[<span class="string">'data'</span>][:] <span class="comment">#type是numpy ndarray</span></span><br><span class="line">f.close()</span><br></pre></td></tr></table></figure>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>1.<a href="http://docs.h5py.org/en/latest/index.html" target="_blank" rel="noopener">http://docs.h5py.org/en/latest/index.html</a><br>2.<a href="https://stackoverflow.com/questions/31037088/discovering-keys-using-h5py-in-python3" target="_blank" rel="noopener">https://stackoverflow.com/questions/31037088/discovering-keys-using-h5py-in-python3</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://mxxhcm.github.io/2019/03/14/activation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="马晓鑫爱马荟荟">
      <meta itemprop="description" content="记录硕士三年自己的积累">
      <meta itemprop="image" content="/images/favicon.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="mxxhcm's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/03/14/activation/" class="post-title-link" itemprop="http://mxxhcm.github.io/page/14/index.html">神经网络-激活函数</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-03-14 11:45:46" itemprop="dateCreated datePublished" datetime="2019-03-14T11:45:46+08:00">2019-03-14</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-05-07 00:22:27" itemprop="dateModified" datetime="2019-05-07T00:22:27+08:00">2019-05-07</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/机器学习/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="激活函数的一些问题">激活函数的一些问题</h2>
<h3 id="为什么要使用non-linear激活函数不使用linear激活函数？">为什么要使用non-linear激活函数不使用linear激活函数？</h3>
<p><img src="/2019/03/14/activation/fnn.png" alt="fnn"><br>
给定一个如图所示的前馈神经网络。有一个输入层，一个隐藏层，一个输出层。输入是$2$维的，有$4$个隐藏单元，输出是$2$维的。<br>
则：$ \hat{f}(x) = \sigma(w_1x+b_1)w_2 + b_2$<br>
这里$\sigma$是一个线性的激活函数，不妨设$\sigma(x) = x$。<br>
那么就有：<br>
\begin{align*}<br>
\hat{f}(x) &amp;= \sigma(w_1x+b_1)w_2 + b_2\<br>
&amp;= (w_1x+b_1)w_2 + b_2\<br>
&amp;= w_1w_2x + w_2b1 + b_2\<br>
&amp;= (w_1w_2) x + (w_2b1 + b_2)\<br>
&amp;= w’ x + b’<br>
\end{align*}<br>
因此，当使用线性激活函数的时候，我们可以把一个多层感知机模型化简成一个线性模型。当使用线性激活函数时，增加网络的深度没有用，使用线性激活函数的十层感知机和一层感知机没有区别，并不能增加网络的表达能力。因为任意两个仿射函数的组合还是仿射函数。</p>
<h3 id="为什么relu激活函数是non-linear的？">为什么ReLU激活函数是non-linear的？</h3>
<p>ReLU的数学表达形式如下：<br>
$$g(x) = max(0, x)$$<br>
首先考虑一下什么是linear function,什么是non-linear function。在微积分上，平面内的任意一条直线是线性函数，否则就是非线性函数。<br>
考虑这样一个例子，输入数据的维度为$1$，输出数据的维度也为$1$，用$g(ax+b)$表示ReLU激活函数。如果我们使用两个隐藏单元，那么$h_1(x) = g(x)+g(-x)$可以用来表示$f(x)=|x|$，而函数$|x|$是一个非线性函数，函数图像如下所示。<br>
<img src="/2019/03/14/activation/absolute.png" alt="f(x)=|x|"><br>
我们还可以用ReLU逼近二次函数$f(x) = x^2$，如使用函数$h_2(x) = g(x) + g(-x) + g(2x-2) + g(2x+2)$逼近二次函数，对应的图像如下。<br>
<img src="/2019/03/14/activation/quadratic.png" alt="h_2(x)"><br>
使用的项越多，最后近似出来的图像也就和我们要逼近的二次函数越像。<br>
同理，可以使用ReLU激活函数去逼近任意非线性函数。</p>
<h3 id="为什么relu比sigmod还有tanh激活函数要好？">为什么ReLU比sigmod还有tanh激活函数要好？</h3>
<p>ReLU收敛的更快，因为梯度更大。<br>
当CNN的层数越来越深的时候，实验表明，使用ReLU的CNN要比使用sigmod或者tanh的CNN训练的更容易，更快收敛。<br>
为什么会这样，目前有两种理论，见参考文献[4]。<br>
第一个，$tanh(x)$有梯度消散问题(vanishing gradient)。当$x$趋向于$\pm\infty$时，$tanh(x)$的导数趋向于$0$。如下图所示。</p>
<blockquote>
<p>Vanishing gradients occur when lower layers of a DNN have gradients of nearly 0 because higher layer units are nearly saturated at -1 or 1, the asymptotes of the tanh function. Such vanishing gradients cause slow optimization convergence, and in some cases the final trained network converges to a poor local minimum.</p>
</blockquote>
<blockquote>
<p>One way ReLUs improve neural networks is by speeding up training. The gradient computation is very simple (either 0 or 1 depending on the sign of x). Also, the computational step of a ReLU is easy: any negative elements are set to 0.0 – no exponentials, no multiplication or division operations.</p>
</blockquote>
<p><img src="/2019/03/14/activation/tanh.png" alt="tanh(x)"><br>
ReLU是non-saturating nonlinearity的激活函数，sigmod和tanh是saturating nonlinearity激活函数，会将输出挤压到一个区间内。</p>
<blockquote>
<p>f是non-saturating 当且仅当$|lim_{z\rightarrow -\infty} f(z)| \rightarrow + \infty$或者$|lim_{z\rightarrow +\infty} f(z)| \rightarrow + \infty$</p>
</blockquote>
<p>tanh和sigmod将输入都挤压在某一个很小的区间内，比如(0,1)，输入发生很大的变化，经过激活函数以后变化很小，经过好几层之后，基本上就没有差别了。而当网络很深的时候，反向传播主要集中在后几层，而输入层附近的权值没办法好好学习。而对于ReLU来说，任意深度的神经网络，都不存在梯度消失。</p>
<p>第二种理论是说有一些定理能够证明，在某些假设条件下，局部最小就是全局最小。如果使用sigmod或者tanh激活函数的时候，这些假设不能成立，而使用ReLU的话，这些条件就会成立。</p>
<h3 id="为什么发生了梯度消失以后训练结构很差？">为什么发生了梯度消失以后训练结构很差？</h3>
<p>我的想法是，</p>
<h2 id="参考文献">参考文献</h2>
<p>1.<a href="https://stats.stackexchange.com/a/391971" target="_blank" rel="noopener">https://stats.stackexchange.com/a/391971</a><br>
2.<a href="https://stats.stackexchange.com/a/299933" target="_blank" rel="noopener">https://stats.stackexchange.com/a/299933</a><br>
3.<a href="https://stats.stackexchange.com/a/141978" target="_blank" rel="noopener">https://stats.stackexchange.com/a/141978</a><br>
4.<a href="https://stats.stackexchange.com/a/335972" target="_blank" rel="noopener">https://stats.stackexchange.com/a/335972</a><br>
5.<a href="https://stats.stackexchange.com/a/174438" target="_blank" rel="noopener">https://stats.stackexchange.com/a/174438</a><br>
6.<a href="https://stats.stackexchange.com/questions/391968/relu-vs-a-linear-activation-function" target="_blank" rel="noopener">https://stats.stackexchange.com/questions/391968/relu-vs-a-linear-activation-function</a><br>
7.<a href="https://stats.stackexchange.com/questions/141960/why-are-rectified-linear-units-considered-non-linear" target="_blank" rel="noopener">https://stats.stackexchange.com/questions/141960/why-are-rectified-linear-units-considered-non-linear</a><br>
8.<a href="https://stats.stackexchange.com/questions/299915/how-does-the-rectified-linear-unit-relu-activation-function-produce-non-linear" target="_blank" rel="noopener">https://stats.stackexchange.com/questions/299915/how-does-the-rectified-linear-unit-relu-activation-function-produce-non-linear</a><br>
9.<a href="https://stats.stackexchange.com/questions/226923/why-do-we-use-relu-in-neural-networks-and-how-do-we-use-it/226927#226927" target="_blank" rel="noopener">https://stats.stackexchange.com/questions/226923/why-do-we-use-relu-in-neural-networks-and-how-do-we-use-it/226927#226927</a><br>
10.<a href="https://www.zhihu.com/question/264163033" target="_blank" rel="noopener">https://www.zhihu.com/question/264163033</a><br>
11.<a href="http://ai.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf" target="_blank" rel="noopener">http://ai.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://mxxhcm.github.io/2019/03/13/cnn/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="马晓鑫爱马荟荟">
      <meta itemprop="description" content="记录硕士三年自己的积累">
      <meta itemprop="image" content="/images/favicon.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="mxxhcm's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/03/13/cnn/" class="post-title-link" itemprop="http://mxxhcm.github.io/page/14/index.html">CNN</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-03-13 15:21:27" itemprop="dateCreated datePublished" datetime="2019-03-13T15:21:27+08:00">2019-03-13</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-07-13 20:29:42" itemprop="dateModified" datetime="2019-07-13T20:29:42+08:00">2019-07-13</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/机器学习/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="cnn">CNN</h2>
<h3 id="图片的表示">图片的表示</h3>
<p>图像在计算机中是一堆按顺序排列的顺子，数值为0到255。0表示最暗，255表示最亮。我们可以把这堆数字用一个长长的一维数组来表示，但是这样会失去平面结构的信息，为保留该结构信息，我们通常会选择矩阵的表示方式，用一个nn的矩阵来表示一个图像。对于黑白颜色的灰度图来说，我们只需要一个nn的矩阵表示即可。对于一个彩色图像，我们会选择RGB颜色模型来表示。<br>
在彩色图像中，我们需要用三个矩阵去表示一张图，也可以理解为一个三维张量，每一个矩阵叫做这张图片的一个channel。这个三维张量可以表示为(width,length,depth),一张图片就可以用这样一个张量来表示。</p>
<h3 id="卷积神经网络-cnn">卷积神经网络(CNN)</h3>
<h4 id="作用">作用</h4>
<p>让权重在不同位置共享</p>
<h4 id="filter和stride">filter和stride</h4>
<p>filter又叫做kernel或者feature detector。filter会对输入的局部区域进行处理，filter处理的局部区域的范围叫做filter size。比如说一个filter的大小为(3,3),那么这个filter会一次处理width=3，length = 3的区域。卷积神经网络会用filter对整个输入进行扫描，一次移动的多少叫做stride。filter处理一次的输出为一个feature map。</p>
<h4 id="depth">depth</h4>
<p>对于filter来说，我们一般说它的大小为（3，3）只说了它在平面的大小，但是输入的图片一般是一个RGB的三维张量，对于deepth这一个维度，如果为1的话，那么filter是（3,3），但是如果deepth大于1的话，这个filter的deepth维度一般是和张量中的deepth维度一样的。<br>
deepth=1时，filter=（3,3），处理输入中33 个节点的值<br>
deepth=2时，filter=（3,3），会处理输入中332个节点的值<br>
deepth=n时，filter=（3,3），会处理输入中$33\times n$个节点的值</p>
<h4 id="zero-paddings">zero paddings</h4>
<p>因为经过filter处理后，输入的矩阵维度会变小，所以，如果经过很多层filter处理后，就会变得越来越少，因此，为了解决这个问题，提出了zero paddings，zero padding是在filter要处理的输入上，在输入的最外层有选择的加上一行（列）或多行（列）0，从而保持输入经过filter处理之后形状不变。</p>
<h4 id="feature-map">feature map</h4>
<p>一个filter的输出就是一个feature map，该feature map的width和height为：$(input_size + 2\times padding_size - filter_size)/stride + 1$<br>
一个filter可以提取一个feature，得到一个feature map，为了提取多个feature，需要使用多个filters，最后可以得到多个feature map。</p>
<p>所以说，feature map是一类值，因为它对应的是一个filter，给定不同的输入images，一个feature map可以有不同的取值。这个问题是我在看ZFNet中遇到的，因为它在原文中说<br>
“For a given feature map, we show the top 9 activations”。给定一个feature map，这里应该是在所有样本中选择最大的$9$个activations对应的images。<br>
“the strongest activation (across all training examples) within a given feature map”。给定一个feature map，在所有样本中选择一个最强的activation。</p>
<h4 id="activate-function">activate function</h4>
<p>一般使用非线性激活函数relu对feature map进行变化</p>
<h4 id="pooling">pooling</h4>
<h5 id="maxpooling">maxpooling</h5>
<p>它基本上采用一个filter和一个同样长度的stride通常是（2,2）和2，然后把它应用到输入中，输出filter卷积计算的每个区域中的最大数字，这个pooling是在各个维度上分别进行的。<br>
比如一个 22422464的input，经过一个（2,2）的maxpooling会输出一个11211232的张量</p>
<h5 id="averagepooling">averagepooling</h5>
<h4 id="fc-layers">fc layers</h4>
<h2 id="alexnet-2012">Alexnet(2012)</h2>
<p>论文名称：ImageNet Classification with Deep Convolutional Neural Networks<br>
论文地址：<a href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" target="_blank" rel="noopener">http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf</a></p>
<h3 id="概述">概述</h3>
<p>作者提出了一个卷积神经网络架构对Imagenet中$1000$类中的$120$万张图片进行分类。网络架构包含$5$个卷积层，$3$个全连接层，和一个$1000$-way的softmax层，整个网络共有$6000$万参数，$65000$个神经元。作者提出了一些方法提高性能和减少训练的时间，并且介绍了一些防止过拟合的技巧。最后在imagenet测试集上，跑出$37.5%$的top-1 error以及$17.0%$的top-5 error。<br>
本文主要的contribution：</p>
<ol>
<li>给出了一个benchmark－Imagenet</li>
<li>提出了一个CNN架构</li>
<li>ReLU激活函数</li>
<li>dropout的使用</li>
<li>数据增强，四个角落和中心的crop以及对应的horizontial 翻转。</li>
</ol>
<h3 id="问题">问题</h3>
<p>1.数据集太小，都是数以万计的，需要更大的数据集。</p>
<h3 id="创新">创新</h3>
<h4 id="relu非线性激活函数">ReLU非线性激活函数</h4>
<h5 id="作用-v2">作用</h5>
<p>作者说实验表明ReLU可以加速训练过程。</p>
<h5 id="saturating-nonlinearity">saturating nonlinearity</h5>
<p>一个饱和的激活函数会将输出挤压到一个区间内。</p>
<blockquote>
<p>A saturating activation function squeezes the input.</p>
</blockquote>
<p><strong>定义</strong><br>
f是non-saturating 当且仅当$|lim_{z\rightarrow -\infty} f(z)| \rightarrow + \infty$或者$|lim_{z\rightarrow +\infty} f(z)| \rightarrow + \infty$<br>
f是saturating 当且仅当f不是non-saturating<br>
<strong>例子</strong><br>
ReLU就是non-saturating nonlinearity的激活函数，因为$f(x) = max(0, x)$，如下图所示。<br>
<img src="/2019/03/13/cnn/relu.png" alt="relu"><br>
当$x$趋于无穷时，$f(x)$也趋于无穷。<br>
sigmod和tanh是saturating nonlinearity激活函数，如下图所示。<br>
<img src="/2019/03/13/cnn/sigmod.png" alt="sigmo"><br>
<img src="/2019/03/13/cnn/tanh.png" alt="tanh"></p>
<h4 id="多块gpu并行">多块GPU并行</h4>
<p>作者使用了两块GPU一块运行，每个GPU中的参数个数是一样的，在一些特定层中，两个GPU中的参数信息可以进行通信。</p>
<h4 id="overlapping-pooling">Overlapping Pooling</h4>
<p>就是Pooling kernel的size要比stride大。比如一个$12\times 12$的图片，用$5\times 5$的pooling kernel，步长为$3$，步长要比kernel核小，即$3$比$5$小。<br>
为什么这能减小过拟合？</p>
<ul>
<li>可能是减小了Pooling过程中信息的丢失。</li>
</ul>
<blockquote>
<p>If the pooling regions do not overlap, the pooling regions are disjointed and if that is the case, more information is lost in each pooling layer. If some overlap is allowed the pooling regions overlap with some degree and less spatial information is lost in each layer.[4]</p>
</blockquote>
<h4 id="数据增强">数据增强</h4>
<p>目的：防止过拟合</p>
<h5 id="裁剪和翻转">裁剪和翻转</h5>
<p>输入是$256\times 256 \times 3$的图像。<br>
训练：对每张图片都提取多个$224\times 224$大小的patch，这样子总共就多产生了$(256-224)\times (256-224) = 1024$个样本，然后对每个patch做一个水平翻转，就有$1024\times 2 = 2048$个样本。<br>
测试：通过对每张图片裁剪五个（四个角落加中间）$224\times 224$的patches，并且对它们做翻转，也就是有$10$个patches，网络对十个patch的softmax层输出做平均作为预测结果。</p>
<h5 id="在图片上调整rgb通道的密度">在图片上调整RGB通道的密度</h5>
<p>使用PCA对RGB值做主成分分析。对于每张训练图片，加上主成分，其大小正比于特征值乘上一个均值为$0$，方差为$0.1$的高斯分布产生的随机变量。对于一张图片$x,y$点处的像素值$I_{xy}=[I_{xy}^R, I_{xy}<sup>G,I_{xy}</sup>B]^T$，加上$[\bold{p_1},\bold{p_2},\bold{p_3}][\alpha_1\lambda_1,\alpha_2\lambda_2,\alpha_3\lambda_3]$，其中$[\bold{p_1},\bold{p_2},\bold{p_3}]$是特征向量，$\lambda_i$是特征值，$\alpha_i$就是前面说的随机变量。</p>
<h4 id="dropout">Dropout</h4>
<p>通过学习鲁棒的特征防止过拟合。<br>
在训练的时候，每个隐藏单元的输出有$p$的概率被设置为$0$，在该次训练中，如果这个神经元的输出被设置为$0$，它就对loss函数没有贡献，反向传播也不会被更新。对于一层有$N$个神经单元的全连接层，总共有$2^N$种神经元的组合结果，这就相当于训练了一系列共享参数的模型。<br>
在测试的时候，所有隐藏单元的输出都不丢弃，但是会乘上$p$的概率，相当于对一系列集成模型取平均。具体可见<a href="https://mxxhcm.github.io/2019/03/23/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-dropout/">dropout</a><br>
在该模型中，作者在三层全连接层的前两层输出上加了dropout。</p>
<h4 id="局部响应归一化-local-response-normalizaiton">局部响应归一化(Local Response Normalizaiton)</h4>
<p>事实上，后来发现这个东西没啥用。但是这里还是给出一个公式。</p>
<p>$$ b^i_{x,y} = \frac{a^i_{x,y}}{(k+\alpha \sum<sup>{min(N-1,\frac{i+n}{2})}_{j=max(0,\frac{i-n}{2})}(a</sup>j_{x,y})^2)^{\beta}}$$<br>
其中$a^i_{x,y}$是在点$(x,y)$处使用kernel $i$之后，在经过ReLU激活函数。$k,n,\alpha,\beta$是超参数。</p>
<blockquote>
<p>It seems that these kinds of layers have a minimal impact and are not used any more. Basically, their role have been outplayed by other regularization techniques (such as dropout and batch normalization), better initializations and training methods.</p>
</blockquote>
<h3 id="整体架构">整体架构</h3>
<h4 id="目标函数">目标函数</h4>
<p>多峰logistic回归。</p>
<h4 id="并行框架">并行框架</h4>
<p>下图是并行的架构，分为两层，上面一层用一个GPU，下面一层用一个GPU，它们只在第三个卷积层有交互。<br>
<img src="/2019/03/13/cnn/alexnet.png" alt="alexnet"></p>
<h4 id="简化框架">简化框架</h4>
<p>下图是简化版的结构，不需要使用两个GPU。<br>
<img src="/2019/03/13/cnn/alexnet_simple.png" alt="alexnet_simple"></p>
<h4 id="数据流-简化框架">数据流（简化框架）</h4>
<p>输入是$224\times 224 \times 3$的图片，第一层是$96$个stride为$4$的$11\times 11\times 3$卷积核构成的卷积层，输出经过max pooling(步长为2，kernel size为3)输入到第二层；第二层有$256$个$5\times 5\times 96$个卷积核，输出经过max pooling(步长为2，kernel size为3)输入到第三层；第三层到第四层，第四层到第五层之间没有经过pooling和normalization)，第三层有384个$3\times 3\times 256$个卷积核，第四层有$384$个$3\times 3\times 384$个卷积核，第五层有$256$个$3\times 3\times 384$个卷积核。然后接了两个$2048$个神经元的全连接层和一个$1000$个神经元的全连接层。</p>
<h3 id="实验">实验</h3>
<h4 id="datasets">Datasets</h4>
<p>ILSVRC-2010</p>
<h4 id="baselines">Baselines</h4>
<ul>
<li>Sparse coding</li>
<li>SIFT+FV</li>
<li>CNN</li>
</ul>
<h4 id="metric">Metric</h4>
<ul>
<li>top-1 error rate</li>
<li>top-5 error rate</li>
</ul>
<h3 id="代码">代码</h3>
<p>pytorch实现<br>
<a href="https://github.com/mxxhcm/myown_code/blob/master/CNN/alexnet.py" target="_blank" rel="noopener">https://github.com/mxxhcm/myown_code/blob/master/CNN/alexnet.py</a></p>
<h2 id="maxout-networks">Maxout networks</h2>
<p>论文名称：Maxout Networks<br>
下载地址：<a href="https://arxiv.org/pdf/1302.4389.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1302.4389.pdf</a></p>
<h2 id="nin">NIN</h2>
<p>论文名称：Network In Network<br>
论文地址：<a href="https://arxiv.org/pdf/1312.4400.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1312.4400.pdf</a></p>
<h3 id="摘要">摘要</h3>
<p>这篇文章作者使用更复杂的micro神经网络代替CNN，用一个mlp实例化micro nn。CNN中的filter用的是generalized linear model(GLM)。本文使用nonlinear的FA，作者用一个multi layers perceptron 取代GLM。通过和cnn类似的操作对input进行sliding得到feature maps，然后传入下一层，deep NIN通过堆叠多层类似的结构生成。同时作者使用average pooling取代最后的fullcy connected layer。<br>
本文的两个contribution是：</p>
<ol>
<li>使用MLP代替CNN中linear model，引入$1\times 1$的filter</li>
<li>使用average pooling代替fully connected layer。</li>
</ol>
<p>在传统的CNN中，一个concept的不同variation可能需要多个filters，这样子会让下一层的的计算量太大。高层CNN的filters对应input的区域更大，高层的concept是通过对底层的concepts进行组合得到的。这里作者在每一层都对local patch进行组合，而不是在高层才开始进行组合，在每一层中，micro network计算更加local patches更abstract的特征。</p>
<h3 id="network-in-network">Network in Network</h3>
<h4 id="mlp-convolution-layers">MLP convolution layers</h4>
<p>为什么使用MLP代替GLP？</p>
<ol>
<li>MLP和CNN的结构兼容，可以使用BP进行训练；</li>
<li>MLP本身就是一个deep model，满足feature复用的想法。</li>
</ol>
<p>如下图所示，是MLP CNN和GLP CNN的区别。<br>
<img src="/2019/03/13/cnn/mlp_vs_linear.png" alt="mvl_vs_glp"></p>
<p>MLP的公式如下。<br>
<img src="/2019/03/13/cnn/equ.png" alt="equ"><br>
从cross channel(feature maps)的pooling角度来看，上面的公式相当于在一个正常的conv layer上进行多次的parametric pooling，每一个pooling layer对输入的feature map进行线性加权，经过一个relu层之后在下一层继续进行pooling。Cross channel pooled的feature maps在接下来的层中多次进行cross channel pooling。这个cross channel pooling的结构的作用是学习复杂的cross channel信息。<br>
其实整个cross channel的paramteric pooling结构相当于一个普通的卷积加上了多个$1\times 1$的卷积，如下图所示：<br>
<img src="/2019/03/13/cnn/11filter.png" alt="11filter"></p>
<h4 id="global-average-pooling">Global average pooling</h4>
<p>FC layers证明是容易过拟合的，dropout被提出来正则化fc layers的参数。<br>
本文提出的global average pooling取代了CNN的fc layers，直接在最后一个mlpconv layer中对应于分类任务中的每个类别生成一个feature map。然后用在feature maps上的average pooling代替fc layers，然后把它送入softmax layer。原来的CNN是将feature map reshape成一个一维向量，现在是对每一个feature map进行一个average pooling，有多少个feature map就有多少个pooling，相当于一个feature map对应与一个类型。<br>
这样做有以下几个好处：</p>
<ol>
<li>在fc layers上的global average pooling让feature map和categories对应起来，feature map可以看成类别的置信度。</li>
<li>直接进行average pooling不用优化fc layer的参数，也就没有过拟合问题。</li>
<li>global average pooling对全局信息进行了加和，对于input的spatial信息更加鲁邦。</li>
</ol>
<h4 id="nin-v2">NIN</h4>
<p>如下图所示，是NIN的整体架构。<br>
<img src="/2019/03/13/cnn/nin.png" alt="nin"><br>
下图是一个具体参数化的示例<br>
<img src="/2019/03/13/cnn/instance.png" alt="instance"></p>
<h3 id="实验-v2">实验</h3>
<h2 id="overfeat-2013">OverFeat(2013)</h2>
<p>论文名称：OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks<br>
论文地址：<a href="https://arxiv.org/pdf/1312.6229.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1312.6229.pdf</a></p>
<h3 id="概述-v2">概述</h3>
<p>本文提出了一个可用于classification, localization和detection等任务的CNN框架。<br>
ImageNet数据集中大部分选择的是几乎填满了整个image中心的object，image中我们感兴趣的objects的大小和位置也可能变化很大。为了解决这个问题，作者提出了三个方法：</p>
<ol>
<li>用sliding window和multiple scales在image的多个位置apply ConvNet。即使这样，许多window中可能包含能够完美识别object类型的一部分，比如一个狗头。。。最后的结果是classfication很好，但是localization和detection结果很差。</li>
<li>训练一个网络不仅仅预测每一个window的category distribution，还预测包含object的bounding box相对于window的位置和大小。</li>
<li>在每个位置和大小累加每个category的evidence</li>
</ol>
<h3 id="vision任务">Vision任务</h3>
<p>classification，localization和detection。classification和localization通常只有一个很大的object，而detection需要找到很多很小的objects。<br>
classification任务中，每个image都有一个label对应image中主要的object的类型。为了找到正确的label，每个图片可以猜$5$次（图片中可能包含了没有label的数据）。localization任务中，不仅要给出label，还需要找到这个label对应的bouding box，bounding box和groundtruth至少要有$50$匹配，label和bounding box也需要匹配。detection和localization不同的是，detection任务中可以有任何数量的objects，false positive会使用mean average precison measure。localization任务可以看成classification到detection任务的一个中间步。</p>
<h3 id="fcn">FCN</h3>
<p>用卷积层代替全连接层。具体是什么意思呢。<br>
alexnet中，有5层卷积层，3层全连接层。假设第五层的输出是$5\times 5 \times 512$，$512$是output channels number，$5\times 5$是第五层的feature maps的大小。如果使用全连接的话，假设第六层的输出单元是$N$个，第六层权重总共是$(5\times 5\times 512) * (N)$，对于一个训练好的网络，图片的输入大小是固定的，因为第六层是一个全连接层，输入的大小是需要固定的。如果输入一个其他大小的图片，网络就会出错，所以就有了Fully Convolutional networks，它可以处理不同大小的输入图片。<br>
如下所示，使用某个大小的image训练的网络，在classifier处用卷积层替换全连接层，如果使用全连接层，首先将$(5, 5, out_channels)$的feature map进行flatten $5\times 5\times out_channels$，然后经过三层全连接，最后输出一个softmax的结果。而fcn使用卷积层代替全连接，使用$N$个$5\times 5$的卷积核，直接得到$1\tims 1 \times N$的结果，最后得到一个$1\times 1\times C$的输出，$C$代表图像类别，$N$代表全连接层中隐藏节点的数量。<br>
<img src="/2019/03/13/cnn/fcn.png" alt="fcn"><br>
事实上，FCN和全连接的本质上都是一样的，只不过一个进行了flatten，一个直接对feature map进行操作，直接对feature map操作可以处理不同大小的输入，而flatten不行。<br>
当输入图片大小发生变化时，输出大小也会改变，但是网络并不会出错，如下所示：<br>
<img src="/2019/03/13/cnn/fcn2.png" alt="fcn2"><br>
最后输出的结果是$2\times 2 \times C$的结果，可以直接对它们取平均，最后得到一个$1\times 1\times C$的分类结果。</p>
<h3 id="offset-max-pooling">offset Max pooling</h3>
<p>我们之前做max pooling的时候，设$kernel_size=3, stride_size=1$，如果feature map是$3$的倍数，那么只有一个pooling的结果，但是如果不是$3$的倍数，max pooling会很多个结果，比如有个$20\times 20$的feature map，在$x,y$上做max pooling分别有三种结果，分别从$x,y$的位置$0$开始，位置$1$开始，位置$2$开始，排列组合有$9$中情况，这九种情况的结果是不同的。<br>
如下图所示，在一维的长为$20$的pixels上做maxpooling，有三种情况。<br>
<img src="/2019/03/13/cnn/offset_maxpooling.png" alt="offset_maxpooling"></p>
<h3 id="overfeat">overfeat</h3>
<p>这两个方法中，fcn是在输入图片上进行的window sliding，而offset maxpooling是在feature map进行的window sliding，这两个方法结合起来就是overfeat，要比alexnet直接在输入图片上进行window sliding 要好。</p>
<h3 id="classification">Classification</h3>
<h4 id="training">training</h4>
<ul>
<li>datset<br>
Image 2012 trainign set（1.2million iamges，C=$1000$ classes)。</li>
<li>data argumented<br>
对每张图片进行下采样，所以每个图片最小的dimension需要是$256$。<br>
提取$5$个random crops以及horizaontal flips，总共$10$个$221\times 221$的图片</li>
<li>batchsize<br>
$128$</li>
<li>初始权重<br>
$(\mu, \sigma)= (0, 1\times 10^{-2})$</li>
<li>momentum<br>
0.6</li>
<li>l2 weigth decay<br>
$1\times 10^{-5}$</li>
<li>lr<br>
初始是$5\times 10^{-2}$，在$(30,50,60,70,80)$个epoches后，乘以$0.5$</li>
<li>non-spatial<br>
这个说的是什么呢，在test的时候，会输出多个output maps，对他们的结果做平均，而在training的时候，output maps是$1\times 1$。</li>
</ul>
<h4 id="model架构">model架构</h4>
<p>下图展示的是fast model，spatial input size在train和test时候是不同的，这里展示的是train时的spatial seize。layer 5是最上层的CNN，receptive filed最大。后续是FC layers，在test时候使用了sliding window。在spatial设置中，FC-layers替换成了$1\times 1$的卷积。<br>
<img src="/2019/03/13/cnn/overfeat_fast.png" alt="overfeat_fast"><br>
下图给出了accuracy model的结构，<br>
<img src="/2019/03/13/cnn/overfeat_accuracy.png" alt="overfeat_accuracy"><br>
总的来说，这两个模型都在alexnet上做了一些修改，但是整体架构没有大的创新。</p>
<h4 id="多scale-classification">多scale classification</h4>
<p>alexnet中，对一张照片的$10$个views（中间，四个角和horizontal flip)的结果做了平均，这种方式可能会忽略很多趋于，同时如果不同的views有重叠的话，计算很redundant。此外，alexnet中只使用了一个scale。<br>
作者对每个iamge的每一个location和多个scale都进行计算。<br>
如下图，对应了不同大小的输入图片，layer 5 post pool中$(m\times n)\time(3\times 3)$，前面$m\times n$是fcn得到的不同位置的feature map，后面$3\times 3$是$kernel_size=3$的offset max pooling得到的featrue map。乘起来是所有的预测结果。<br>
<img src="/2019/03/13/cnn/multi_scale.png" alt="multi_scale"></p>
<h3 id="localization">localization</h3>
<h3 id="detection">Detection</h3>
<h2 id="zfnet-2014">ZFNet(2014)</h2>
<p>论文名称：Visualizing and Understanding Convolutional Networks<br>
论文地址：<a href="https://cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf" target="_blank" rel="noopener">https://cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf</a><br>
为什么叫ZFNet，两个作者名字首字母的拼写。</p>
<p>首先我有一个问题？就是什么是一个activation。在原文的$2.1$节，有这样一个介绍：</p>
<blockquote>
<p>We present a novel way to map these activities back to the input pixel space, showing what input pattern originally caused a given activation in the feature maps.<br>
我的理解是一个activation就是feature map中的一个unit。事实上，feature map也叫activation map，因为它是image中不同parts的acttivation，而叫feature map是因为它是image中找到特定的feature。</p>
</blockquote>
<h3 id="概述-v3">概述</h3>
<p>这篇文章从可视化的角度给出中间特征的和classifier的特点，分析如何改进alexnet来提高imagenet classification的accuracy。<br>
为什么CNN结果这么好？</p>
<ol>
<li>training set越来越大</li>
<li>GPU的性能越来越好</li>
<li>Dropout等正则化技术</li>
</ol>
<p>但是CNN还是一个黑盒子，我们不知道它为什么表现这么好？这篇文章给出了一个可视化方法可视化任意层的feature。</p>
<p>那么本文的contribution是什么呢？使用deconvnet进行可视化，通过分析特征行为，对alexnet进行fine tune提升模型性能。</p>
<h3 id="使用deconvnet可视化">使用deconvnet可视化</h3>
<p>什么是deconvnet？可以看成和convnet拥有同样组成部分（pooling, filter)等，但是是反过来进行的。如下图所示，convnet是把pixels映射到feature，或者到底层features映射到高层features，而deconvnet是把高层features映射到底层features，或者把features映射到pixels。在测试convnet中给定feature maps的一个activation时，设置所有其他的activation为0，将这个feature map传入deconvnet网络中。<br>
<img src="/2019/03/13/cnn/fig1.png" alt="fig1"><br>
图片左上为deconv，右上为conv。conv的流程为filter-&gt;rectify-&gt;pooling；deconv的流程为unpool-&gt;rectify-&gt;filter。</p>
<h4 id="unpooling">Unpooling</h4>
<p>convnet中的max pooling是不可逆的，这里作者使用switch variables记录下max pooling后的元素在没有pooling时的位置，进行近似的恢复。</p>
<h4 id="rectification">Rectification</h4>
<p>convnet使用relu non-linearities。deconvnet还是使用relu，这里我有些不理解，为什么？为什么deconve还是使用relu</p>
<h4 id="filtering">Filtering</h4>
<p>deconvnet使用convnet中filters的transposed版本。</p>
<h3 id="training-v2">Training</h3>
<h4 id="整体架构-v2">整体架构</h4>
<p><img src="/2019/03/13/cnn/fig3.png" alt="fig3.png"></p>
<ul>
<li>training set<br>
1.3百万张图片，1000类</li>
<li>processed<br>
每个RGB图像resized成最小边维度为$256$，cropping中间的$256 \times 256$，减去所有像素的平均值。crops$10$个$224\times 224$（四个角落和中心以及horizontal flips)</li>
<li>优化方法<br>
带momentumSGD</li>
<li>batch size<br>
128</li>
<li>lr<br>
初始是$10^{-2}$,然后手动anneal</li>
<li>momentum<br>
0;9</li>
<li>Dropout<br>
layer 6和layer 7,0.5</li>
<li>weights和biases初始化<br>
weights设置为$10^{-2}$，biases设置为$0$</li>
<li>normalizaiton<br>
对第一层的filter，如果RMS超过了$10^{-1}$就设置为$10^{-1}$</li>
<li>训练次数<br>
70epochs</li>
</ul>
<h3 id="visualizaiton">Visualizaiton</h3>
<h4 id="feature-visualization">Feature visualization</h4>
<p>如下图所示，使用deconvnet可视化一些feacutre activation。给定一个feature map，选择其中最大的$9$个activations对应的样本，一个feature map是通过一个filter得到的，而一个filter提取的是一个特征，所以这$9$个activations都是一个filter提取的不同图片中的同一个特征。然后将它们输入deconvnet，得到pixel spaces，可以查看哪些不同的结构（哪些原始）产生了这个feature，展现这个filter对于输入deformation的invariance。在黑白图像的旁边有对应的图像原图，他们要比feature的variation更多，因为feature关注的是图像的invariance。比如layer 5的第一行第二列的九个图，这几个patch看起来差异很大，但是却在同一个feature map中，因为这个feature map关注的是背景中的草，并不是其他objects。更多的我们可以看出来，第二层对应corner和edge等，第三次对应更复杂的invariances，比如textures和text等。第四层更class-specific，第五层是object variation。<br>
<img src="/2019/03/13/cnn/fig2.png" alt="fig2"></p>
<h4 id="feature-evolution-durign-training">Feature evolution durign training</h4>
<p>下图随机选择了几个不同的feature，然后展示了他们在不同layer不同epochs（1, 2, 5, 10, 20, 30, 40, 64）的可视化结果。<br>
<img src="/2019/03/13/cnn/fig4.png" alt="fig4"></p>
<h4 id="架构选择">架构选择</h4>
<p>通过可视化alexnet的first layer和second layer，有了各种各样的问题。First layer中主要是high和low frequency的信息，而2nd layer有很多重复的，因为使用stride为$4$而不是$2$。作者做了两个改进：</p>
<ol>
<li>将first layer的filter size从$11\times 11$改成了$7\times 7$</li>
<li>卷积的步长从$4$改成了$2$</li>
</ol>
<p>如下图所示：<br>
<img src="/2019/03/13/cnn/fig5.png" alt="fig5"></p>
<h4 id="occlusion-sensitivity">Occlusion Sensitivity</h4>
<p>model是否真的识别了object在image中的位置，还是仅仅使用了上下文信息？下图中的例子证明了model真的locate了object,当遮挡住物体的部分增大时，给出正确分类的概率就减小了。移动遮挡方块的位置，给出一个和方块位置相关的分类概率函数，我们可以看出来，model really works。<br>
<img src="/2019/03/13/cnn/fig6.png" alt="fig6"></p>
<h3 id="实验-v3">实验</h3>
<p>第一个实验通过使用，证明了前面的特征提取层和fc layers都是有用的。<br>
第二个实验保留前面的特征提取层和fc layers，将最后的softmax替换。</p>
<h2 id="vggnet-2014">VGGNet(2014)</h2>
<p>论文名称：VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION<br>
论文地址：<a href="https://arxiv.org/pdf/1409.1556.pdf%20http://arxiv.org/abs/1409.1556.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1409.1556.pdf http://arxiv.org/abs/1409.1556.pdf</a><br>
VGG是Visual Geometry Group的缩写</p>
<h3 id="概述-v4">概述</h3>
<p>这篇文章主要研究了CNN深度对大规模图像识别问题精度的影响。本文的主要contribution就是使用多层的$3\times 3$ filters替换大的filter，增加网络深度，提高识别精度。</p>
<h3 id="方案">方案</h3>
<h4 id="架构">架构</h4>
<p><strong>训练</strong>，输入$224\times 224$大小的RGB图片。对每张图片减去训练集上所有图片RGB 像素的均值。预处理后的图片被输入多层CNN中，CNN的filter是$3\times 3$的，作何也试了$1\times 1$的filter，相当于对输入做了一个线性变换，紧跟着一个non-linear 激活函数，这里的$1\times 1$的filter没有用于dimention reduction。stride设为$1$，添加padding使得卷积后的输出大小不变。同时使用了$5$个max-pooling层（并不是每一层cnn后面都有max-pooling)，max-pooling的window是$2\times 2$，stride是$2$。<br>
在训练的时候CNN后面接的是三个FC layers，前两个是$4096$单元，最后一层是$1000$个单元的softmax。所有隐藏层都使用ReLu非线性激活函数。<br>
在测试的时候使用fcn而不是直接flatten。</p>
<h4 id="配置">配置</h4>
<p>这篇文章给出了五个网络架构，用$A-E$表示，它们只有在深度上有所不同：从$11$层($8$个conv layers和$3$个FC layers)到$19$层（$16$个conv layers和$3$个FC layers）。Conv layers的channels很小，从第一层的$64$，每过一个max pooling layers，变成原理啊的两倍，直到$512$。具体如下表所示。<br>
<img src="/2019/03/13/cnn/vgg_conf.png" alt="vgg_conf"><br>
网络的参数个数如下表所示。<br>
<img src="/2019/03/13/cnn/vgg_weights_num.png" alt="vgg_weights_num"><br>
网络$A$的参数计算：<br>
\begin{align*}<br>
64\times 3\times 3\times 3 + \\<br>
128\times 3\times 3\times 64 + \\<br>
256\times 3\times 3\times 128 + \\<br>
256\times 3\times 3\times 256 + \\<br>
512\times 3\times 3\times 256 + \\<br>
512\times 3\times 3\times 512 + \\<br>
2\times 512\times 3\times 3\times 512 + \\<br>
7\times 7\times 512\times 4096 + \\<br>
4096\times 4096 + \\<br>
4096\times 1000 = \\<br>
132851392<br>
\end{align*}<br>
网络$B$的参数计算：<br>
\begin{align*}<br>
64\times 3\times 3\times 3 + \\<br>
128\times 3\times 3\times 64 + \\<br>
128\times 3\times 3\times 128 + \\<br>
256\times 3\times 3\times 128 + \\<br>
256\times 3\times 3\times 256 + \\<br>
256\times 3\times 3\times 256 + \\<br>
512\times 3\times 3\times 256 + \\<br>
512\times 3\times 3\times 512 + \\<br>
2\times 512\times 3\times 3\times 512 + \\<br>
7\times 7\times 512\times 4096 + \\<br>
4096\times 4096 + \\<br>
4096\times 1000 = \\<br>
133588672<br>
\end{align*}<br>
其实主要的网络参数还是在全连接层，$7\times 7\times 512\times 4096=102760448<br>
$。</p>
<h4 id="卷积核作用">卷积核作用</h4>
<ol>
<li>为什么要用三个$3\times 3$的conv layers替换$7\times 7$个conv layers？</li>
</ol>
<ul>
<li>使用三个激活函数而不是一个，让整个决策更discriminative。</li>
<li>减少了网络参数，三个有$C$个通道的$3\times 3$conv layers,总的参数是$3\tims(3<sup>2C</sup>2)=27C^2$，而一个$C$通道的$7\times 7$ conv layers，总参数是$49C^2$。可以看成是一种正则化。</li>
</ul>
<ol start="2">
<li>$1\times 1$ conv layers用来增加非线性程度，本文中使用的$1\times 1$的conv layers可以看成加了非线性激活函数的投影。</li>
</ol>
<h3 id="分类框架">分类框架</h3>
<h4 id="training-v3">training</h4>
<ul>
<li>目标函数<br>
多峰logistic regression</li>
<li>训练方法<br>
mini-batch gradient descent with momentum</li>
<li>batch size<br>
256</li>
<li>momentum<br>
0.9</li>
<li>正则化<br>
$L_2$参数正则化(5\codt 10^{-4})<br>
0.5 dorpout 用于前两个FC layers</li>
<li>lr<br>
初始值为$10^{-2}$，当验证集的accuracy不再提升时，除以$10$。学习率总共降了$3$次，$370K$次迭代后停止。</li>
<li>图像预处理<br>
从rescaled中随机cropped $224\times 224$的RGB图像。<br>
使用alexnet中的随机horizontal flipping和随机RGB colour shift。</li>
<li>iamge rescale<br>
用$S$表示training image的小边的大小，$S$也叫作train sacle。网络的输入是从training image中cropped得到的$224\times 224$的图像。所以只要$S$取任何不小于$224$的值即可，如果$S=224$，那么crop在统计上会captuer整个图片，完全包含training image最小的那边；$S&gt;&gt;224$的时候，crop会产生很小一部分的图像。<br>
作者尝试了固定$S$和不固定的$S$。对于固定$S$，设置$S=256$和$S=384$，首先在$S=256$上训练，然后用$S=256$训练的参数初始化$S=384$的参数，使用更小的初始学习率$10^{-3}$。不固定$S$时，$S$从$[S_{min}, S_{max}](S_{max}=512,S_{min}=256)$任意采样，然后crop。</li>
<li>VGG vs alexnet<br>
VGG参数多，深度深，但是收敛快，原因：</li>
</ul>
<ol>
<li>更小的filter带来的implicit regularisation</li>
<li>某些层的预先初始化。<br>
这个解决的是网络深度过深，某些初值使得网络不稳定的问题。解决方法：先随机初始化不是很深的网络A，进行训练。在训练更深网络的时候，使用A网络的值初始化前$4$个卷基层和最后三个FC layers。随机初始化的网络参数，从均值为$0$，方差为$10^{-2}$的高斯分布中采样得到。</li>
</ol>
<h4 id="testing">testing</h4>
<ol>
<li>测试的时候先把input image的窄边缩放到$Q$，$Q$也叫test scale，$Q$和$S$不一定需要相等。</li>
<li>这里和overfeat模型一样，在卷积网络之后采用了fcn，而不是fc layers。</li>
</ol>
<h3 id="classfication">classfication</h3>
<p>ILSVRC-2012，training($1.3M$张图片)，validation($50K张$)，testing($100K$张)<br>
两个metrics：top-1和top-5 error。top-1 error是multi-class classification error，不正确分类图像占的比例；top-5 error是预测的top-5都不是ground-truth。</p>
<h4 id="single-scale-evaluation">single scale evaluation</h4>
<p>$S$固定时，设置test image size $Q=S=256$；<br>
$S$抖动时，设置test image size $Q=0.5(S_{min}+S_{max})=0.5(256+512)=384$，$S\in [S_{min},S_{max}]$。</p>
<h4 id="multi-scale-evaluation">multi scale evaluation</h4>
<p>用同一个模型对不同rescaled大小的图片多次test，即对于不同的$Q$。<br>
固定$S$时，在三个不同大小的test image size $Q={S-32,S,S+32}$评估。<br>
$S$抖动时，模型是在$S\in [S_{min},S_{max}]$上训练的，在$Q={S_{min}, 0.5(S_{min}+S_{max}), S_{max}}$上进行test。</p>
<h4 id="多个crop-evaluation">多个crop evaluation</h4>
<p>这个是为了和alexnet做对比，alexnet网络在testing时，对每一张图片都进行多次cropped，对testing的结果做平均。</p>
<h4 id="convnet-funsion">convnet funsion</h4>
<p>之前作者的evaluation都是在单个的网络上进行的，作者还试了将不同网络的softmax输出做了平均。</p>
<h2 id="inception-v1-googlelenet">Inception V1(GoogleLeNet)</h2>
<h3 id="摘要-v2">摘要</h3>
<p>提出一种方法能够在不增加太多计算代价的同时增加网络的深度和宽度。</p>
<h3 id="motivation">motivation</h3>
<p>直接增加网络的深度和宽度有两个缺点：</p>
<ol>
<li>参数更多，容易过拟合，尤其是训练集太小的情况下，高质量的训练集很难生成。</li>
<li>需要更多的计算资源。比如两层CNN，即使每一层中线性增加filters的个数也会造成计算代价指数级增加。如果增加的权重接近$0$的话，计算代价就浪费了。而现实中的计算资源是有限的。</li>
</ol>
<p>如何解决这个问题呢？使用sparsity layers取代fully connetcted layers。但是现在的计算资源在处理non-uniform 的sparse data时是非常低效的，即使数值操作减小$100$倍，查找的时间也是很多的。而针对CPU和GPU的dense matrix计算能够加快fc layer的学习。现在绝大部分的机器学习视觉模型在sparsity spatial domain都仅仅利用了CNN，而convolution是和前一层patches的dense connection。1998年的convnet为了打破网络对称性，改善学习结果，使用的是random和sparse连接，而在alexnet中为了并行优化计算，使用了全连接。当前cv的state-of-the-art架构使用的都是unifrom structure，为了高效的进行dense计算，filters和batch size的数量都是很大的。<br>
稀疏性可以解决过拟合和资源消耗过多的问题，而稠密连接可以提高计算效率。所以接下来要做的是一个折中，利用filter维度的稀疏结构，同时利用硬件在dense matrices上的计算进行加速。<br>
Inception架构就是使用一个dense组件去逼近sparse结构的例子。</p>
<h3 id="算法">算法</h3>
<p>Inception的idea是使用dense组件近似卷积的局部稀疏结构。本文的旋转不变型是利用convolutional building blocks完成的，找到optimal local construction，然后不断堆叠。文章[11]中建议layer-by-layer的构建，分析上一层之间的关系，并将具有高相关性的units进行分组。这些相关的units cluster构建成了下一层的units，并且和上一层的units相连接。假设之前层中的每一个unit都对应输入图片中的一些region，这些units分组构成filter banks。这就意味着在靠近输入的层中我们会得到很多关于local regions相关的units。通过在下一层中使用$1\times 1$的卷积，可以找到关注于同一个region的很多个clusters。（这里加一些我自己的理解，$1\times 1$的卷积层可以找到那些重复的feature map？？）当然，也有可能有更大的cluster可以通过在更大的patches上进行卷积得到，所以这里同时在一层中同时使用$1\times 1, 3\times 3, 5\times 5$的filters，使用这些大小的filter仅仅是因为方便，然后将他们的输出进行组合当做下一层的输入。当然可以加上pooling，如下图所示。<br>
<img src="/2019/03/13/cnn/naive_inception.png" alt="naive inception"><br>
但是，这样子计算量还是很大，大量$3 \times 3, 5\times 5$在卷积时的计算量，如果再加上输入shape和输出shape相等的max pooling操作，下一层的输入维度相当大，计算开销j就爆炸了。这就使用了本文的第二个idea：使用$1\times 1$的filter降维减少计算量。在$3\times 3, 5\times 5$大小filter之前添加$1\times 1$的卷积进行降维。<br>
<img src="/2019/03/13/cnn/dr_inception.png" alt="dimension reduction inception"></p>
<p>这个架构的好处：</p>
<ol>
<li>在每一层都可以增加units的数量而不用担心计算量暴增。首先将上一层大量filters的输出进行进行降维，然后输入到下一层。</li>
<li>visual信息用不同的scales进行处理，然后拼接起来，这样子在下一层可以同时从不同scales中提出features。</li>
</ol>
<h3 id="googlenet">GoogLeNet</h3>
<p>作者给出了Inception的一个示例，叫GoogLeNet。网络具体配置如下：<br>
<img src="/2019/03/13/cnn/GoogLeNet.png" alt="GoogLeNet"><br>
其中，&quot;#$3 \times 3$ reduce&quot;和&quot;#$5 \times 5$ reduce&quot;表示在$3\times 3, 5\times 5$卷积之前使用$1\times 1$的filters个数，pool proj这一列表示在max pooling之后的$1\times 1$的filters个数。<br>
作者在GoogLeNet中还使用了两个额外的分类层辅助训练。通过观察得知相对shallower的网络有很好的性能，那么在反向传播时，深层网络的中间特征应该是很有判别力的。<br>
通过在网络中间添加辅助的classfiers，作者想要让网络底层也有判别力。在训练的时候，在$4a$和$4d$模块后添加分类器，然后将所有的loss乘上一个权重加到总的loss上，在test时，这些辅助网络被扔掉。</p>
<h2 id="batch-normalization">Batch Normalization</h2>
<p>论文名称：Batch Normalization: Accelerating Deep Network Training b<br>
y Reducing Internal Covariate Shift<br>
论文地址：<a href="https://arxiv.org/pdf/1502.03167.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1502.03167.pdf</a></p>
<h3 id="概述-v5">概述</h3>
<p>在训练深度神经网络的时候，随着训练的不断进行，网络权重在不停的变，除了第一层之外的每层输入也在不停的变，所以就使得权重每次都要去适应新的输入distributions。这就导致训练速度很慢，学习率的要很小，很难使用saturaing nonlinearities激活函数训练。作者把这个问题叫做internal covariate shift，提出了batch normalization解决该问题，bn对于参数初始化的要求没那么高，允许使用更高的学习率。<br>
BN可以看成一种正则化手段。</p>
<h3 id="简介">简介</h3>
<p>SGD相对于单个样本的GD来说，使用mini-batch的梯度作为整个训练集的估计值，效果更好；同时并行计算提高了效率。之前的工作使用ReLU，更好的初始化以及小的学习率来解决梯度消失问题，而本文作者的想法是让非线性输入的分布尽可能稳定，从而解决梯度饱和等问题，加快训练。本文提出的batch normalization通过固定每一层输入的均值和方差减少internal covariate shift，同时减少了gradients对于初始参数的依赖性。在使用了BN的网络中，也可以使用如sigmod和tanh的saturating nonlirearities激活函数，并不是一定要用relu激活函数。</p>
<h3 id="mini-batch-normalization">Mini-Batch Normalization</h3>
<p>Whitening每一层的所有inputs需要很大的代价，而且并不是每个地方都是可导的。作者进行了两个简化。第一个是并不是对所有输入的features进行whiten，而是对每一个feautre单独的normalization，将他们转化成均值为0，方差为1的数据。对于一个d维的输入$x=(x^1,\cdots, x^d)，对每一维进行normalize：<br>
$$\hat{x}^k= \frac{x^k - \mathbb{E}\left[x<sup>k\right]}{\sqrt{Var\left[x</sup>k\right]}}$$<br>
其中的期望和方差是整个training set 的期望和方差。但是仅仅normalize每一层的输入可能改变这一层的表示。比如normalize sigmod的输入会将它们的输出限制在非线性的线性区域。为了解决这个问题，在网络中添加的这个transformation应该能够表示identity transform，作者对每个activation $x<sup>k$引入了一对参数，$\gamma</sup>k, \beta^k$，它们对normalized value进行scale和shift：<br>
$$y^k = \gamma^k \hat{x}^k + \beta^k$$<br>
这些参数和模型参数一块，都是学习出来的，如果学习到$\gamma<sup>k=\sqrt{Var\left[x</sup>k\right]},\beta^k = \mathbb{E}\left[x^k\right]$，就可以表示恒等变换了。。<br>
上面说的是使用整个training set的方差和期望进行normaliza，事实上，在sgd中这是不切合实际的。因此，就引入了第二个简化，使用每个mini-batch的方差和期望进行normalize，并且方差和期望是针对于每一个维度计算的。给出一个大小为$m$的batch $B$，normalization独立的应用于每一个维度。用$\hat{x}_{1,\cdots, m}$表示normalized values，以及它们的linear transformation：$y_{1,\cdots,m}$。这个transform表示为：$BN_{\gamma, \beta}:x_{1,\cdots, m} \rightarrow y_{1,\cdots,m}$，称为Batch Normalization Transform，完整的算法如下：<br>
算法1 Batch Normalizing Transform<br>
输入：　mini-batch：$B={x_{1,\cdots, m}}，要学习的参数$\gamma,\beta$<br>
输出：${y_i=BN_{\gamma,\beta}(x_i)}$<br>
$\mu\leftarrow \frac{1}{m}\sum_{i=1}^mx_i$  计算batch的mean<br>
$\sigma^2_B\leftarrow \sum_{i=1}<sup>m(x_i-\mu_B)</sup>2$  计算batch的variance<br>
$\hat{x}_i\leftarrow \frac{x_i-\mu_B}{\sqrt{\simga^2_B+\epsilon}}$ normalize<br>
$y_i\leftarrow \gamma \hat{x}_i+ \beta \equiv BN_{\gamma, \beta}(x_i)$ scale以及shift。<br>
整个过程的loss还可以通过backpropagate进行传播，即它是可导的。</p>
<h3 id="none"></h3>
<h3 id="batch-normalized-cnn">Batch-Normalized CNN</h3>
<p>原来的CNN是<br>
$$ z= g(Wu+b)$$<br>
现在在nonlinearity前加上BN transform。<br>
$$ z= g(BN(Wu+b))$$<br>
但是事实上，Wu+b和Wu的效果是一样的，因为normalized的时候会减去均值，所以最后就是：<br>
$$ z= g(BN(Wu))$$<br>
BN在Wu的每一个维度上单独使用BN，每一个维度有一对$\gamma<sup>k,\beta</sup>k$。</p>
<h3 id="bn能使用更大的学习率">BN能使用更大的学习率</h3>
<h3 id="bn正则化模型">BN正则化模型</h3>
<h2 id="residual-network-2015">Residual Network(2015)</h2>
<p>论文名称：Deep Residual Learning for Image Recognition<br>
论文地址：<a href="https://arxiv.org/pdf/1512.03385.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1512.03385.pdf</a></p>
<h3 id="概述-v6">概述</h3>
<p>作者提出了参差网络，容易优化，仅仅增加深度就能得到更高的accuracy。在Imagenet上使用比VGG深八倍的152层的residual网络，但是计算复杂度更低。<br>
网络是不是越深越好？并不是！事实上，随着网络的加深，会出现退化问题－即增加网络的深度，accuracy反而会下降。导致这个问题的原因并不是过拟合，至于是什么原因？<br>
在这篇文章中，作者提出了deep residual network。使用一些stacked non-linear layers你和一个residual mapping，而不是直接学习一个underlying mapping。用$H(x)$表示一个underlying mapping，我们的目标是学习一个residual mapping：$F(x) = H(x)-x$，underlying mapping可以写成$H(x) = F(x)+x$。在某种情况下，如果identity mapping是optimal，那么让$F(x)$接近于$0$可能比让stacked non linear layers拟合一个identity mapping要简单。。如下图所示，$H(x)$可以用下图表示，由一个feedforward nn加上shortcut connections（skip one or more layers的connection）组成：<br>
<img src="/2019/03/13/cnn/residual_block.png" alt="residual block"><br>
shortcut connection在这里就是一个identity mapping，不需要额外的参数和计算量，shorcut的输出和$F(x)$的输出再一块经过relu激活函数。</p>
<p>本文的contribution是什么？<br>
加了一个恒等映射让深度网络的训练变得更容易。具体原理是什么？可以从这样一个角度看，在每一层都可以把不同维度的feature进行重组。residual connection是skip的一种方式？？</p>
<h3 id="residual-learning">Residual Learning</h3>
<p>用$H(x)$表示stacked non linear layers拟合的一个underlying mapping，$x$为stacked layers的输入。原来我们用这些layers逼近一个复杂的函数，现在我们用它逼近residual function，即$F(x) = H(x) -x$（假设输入和输出的维度是一样的），原来想要拟合的函数变成了$F(x)+x$，它们的意义是一样的，但是对于learning的帮助却有很大差别。<br>
如网络degradation问题中，如果更深的网络中添加的新layers是identity mapping，那么这个更深的网络的training error至少也要和浅一些的网络一样，然而事实上并不是这样的。在degradation问题中，说明multip nonlinear layers在近似identity mappings时效果并不是很好。而在residual learnign中，如果identity mapping是optimal，那么可以让non linear layers的权重接近于0，最后得到一个indetity mappings。虽然在real cases中，identity mapping几乎不可能是optimal的，但是如果optimal function更接近identity mapping而不是zero ampping，residual learning的效果就要更好。<br>
<img src="/2019/03/13/cnn/residual_block.png" alt="residual block"></p>
<h3 id="identity-mapping-by-shortcuts">Identity Mapping by Shortcuts</h3>
<p>本文中采用的residual block如上上图所示，用公式表示为：<br>
$$y = F(x, {W_i}) + x$$<br>
其中$x,y$是输入和输出向量，函数$F(x, {W_i})$表示要学习的residual mapping，residual block块中有两层，$F=W_2\sigma(W_1x)$表示第一层和第二层，然后$F+x$表示shortcut connection以及element-wise addition。如果$x$和$F(x)$的维度不一样的话，可以进行一个linear projection：<br>
$$y=F(x,{W_i}) + W_sx$$<br>
$W_s$表示线性变换的矩阵。如果必要的话，$W_s$可以走一样线性变换，事实上，实验表明如果维度一样的话，identity mapping足够解决degradation问题，$W_s$就是用来进行dimension matting。<br>
$F$的形式是很灵活的，可以像本文一样使用linear layers，当然也可以使用更多layers，无所谓。</p>
<h3 id="网络架构">网络架构</h3>
<p>作者给出了三个网络架构，一个是VGG，一个是VGG修改得到的网络，另一个是这个修改的网络加上shortcut connection，如图所示。基于VGG的修改有以下两个原则：</p>
<ol>
<li>feature map的大小不变的话，filters的数量不变</li>
<li>feature map的大小减半的话，filters的数量变为原来的$2$倍，保证每一层的计算复杂度不变。</li>
</ol>
<p>网络最后接一个global average pooling layer和一个1000way的fc layer和softmax。</p>
<h3 id="其他细节">其他细节</h3>
<ol>
<li>image的短边被resize到$[256, 480]$之间。然后从中裁剪一个$224 \times 224$的样本或者它的horizontal filp。</li>
<li>使用标准的颜色增强。</li>
<li>使用BN</li>
<li>从头开始训练网络</li>
<li>使用batch size为$256$的SGD</li>
<li>学习率从$0.1$开始，每到error不再改变时，除以$10$，总共进行$60\times 10^4$次迭代。</li>
<li>权重decay为$0.0001$，mementum为$0.9$。</li>
<li>测试时，对十个crop取平均，使用fcn，对多个scales上的scores进行平均。</li>
</ol>
<h3 id="结论">结论</h3>
<p>14.<a href="https://www.quora.com/How-does-deep-residual-learning-work" target="_blank" rel="noopener">https://www.quora.com/How-does-deep-residual-learning-work</a><br>
15.<a href="https://kharshit.github.io/blog/2018/09/07/skip-connections-and-residual-blocks" target="_blank" rel="noopener">https://kharshit.github.io/blog/2018/09/07/skip-connections-and-residual-blocks</a><br>
16.<a href="https://stats.stackexchange.com/questions/56950/neural-network-with-skip-layer-connections" target="_blank" rel="noopener">https://stats.stackexchange.com/questions/56950/neural-network-with-skip-layer-connections</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://mxxhcm.github.io/2019/03/13/python-常见问题/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="马晓鑫爱马荟荟">
      <meta itemprop="description" content="记录硕士三年自己的积累">
      <meta itemprop="image" content="/images/favicon.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="mxxhcm's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/03/13/python-常见问题/" class="post-title-link" itemprop="http://mxxhcm.github.io/page/14/index.html">python 常见问题（不定期更新）</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-03-13 10:40:03" itemprop="dateCreated datePublished" datetime="2019-03-13T10:40:03+08:00">2019-03-13</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-10-11 13:30:51" itemprop="dateModified" datetime="2019-10-11T13:30:51+08:00">2019-10-11</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/python/" itemprop="url" rel="index"><span itemprop="name">python</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="问题1-‘dict-values’-object-does-not-support-indexing’"><a href="#问题1-‘dict-values’-object-does-not-support-indexing’" class="headerlink" title="问题1-‘dict_values’ object does not support indexing’"></a>问题1-‘dict_values’ object does not support indexing’</h2><p>参考文献[1,2,3]</p>
<h3 id="报错"><a href="#报错" class="headerlink" title="报错"></a>报错</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&apos;dict_values&apos; object does not support indexing&apos;</span><br></pre></td></tr></table></figure>
<h3 id="原因"><a href="#原因" class="headerlink" title="原因"></a>原因</h3><p>The objects returned by dict.keys(), dict.values() and dict.items() are view objects. They provide a dynamic view on the dictionary’s entries, which means that when the dictionary changes, the view reflects these changes.<br>python3 中调用字典对象的一些函数，返回值是view objects。如果要转换为list的话，需要使用list()强制转换。<br>而python2的返回值直接就是list。</p>
<h3 id="代码示例"><a href="#代码示例" class="headerlink" title="代码示例"></a>代码示例</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">m_dict = &#123;<span class="string">'a'</span>: <span class="number">10</span>, <span class="string">'b'</span>: <span class="number">20</span>&#125;</span><br><span class="line">values = m_dict.values()</span><br><span class="line">print(type(values))</span><br><span class="line">print(values)</span><br><span class="line">print(<span class="string">"\n"</span>)</span><br><span class="line"></span><br><span class="line">items = m_dict.items()</span><br><span class="line">print(type(items))</span><br><span class="line">print(items)</span><br><span class="line">print(<span class="string">"\n"</span>)</span><br><span class="line"></span><br><span class="line">keys = m_dict.keys()</span><br><span class="line">print(type(keys))</span><br><span class="line">print(keys)</span><br><span class="line">print(<span class="string">"\n"</span>)</span><br></pre></td></tr></table></figure>
<p>如果使用python3执行以上代码，输出结果如下所示：</p>
<blockquote>
<p>class ‘dict_values’<br>dict_values([10, 20])<br>class ‘dict_items’<br>dict_items([(‘a’, 10), (‘b’, 20)])<br>class ‘dict_keys’<br>dict_keys([‘a’, ‘b’])</p>
</blockquote>
<p>如果使用python2执行以上代码，输出结果如下所示：</p>
<blockquote>
<p>type ‘list’<br>[10, 20]<br>type ‘list’<br>[(‘a’, 10), (‘b’, 20)]<br>type ‘list’<br>[‘a’, ‘b’]</p>
</blockquote>
<h2 id="问题2-‘TimeLimit’-object-has-no-attribute-‘ale’"><a href="#问题2-‘TimeLimit’-object-has-no-attribute-‘ale’" class="headerlink" title="问题2-‘TimeLimit’ object has no attribute ‘ale’"></a>问题2-‘TimeLimit’ object has no attribute ‘ale’</h2><p>参考文献[4,5,6]</p>
<h3 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h3><p>运行github clone 下来的<a href="https://github.com/devsisters/DQN-tensorflow" target="_blank" rel="noopener">DQN-tensorflow</a>，报错:</p>
<blockquote>
<p>AttributeError: ‘TimeLimit’ object has no attribute ‘ale’.</p>
</blockquote>
<h3 id="原因-1"><a href="#原因-1" class="headerlink" title="原因"></a>原因</h3><p>是因为gym版本原因，在gym 0.7版本中，可以使用env.ale.lives()访问ale属性，但是0.8版本以及以上，就没有了该属性，可以在系列函数中添加如下修改：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, config)</span>:</span></span><br><span class="line">    self.step_info = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_step</span><span class="params">(self, action)</span>:</span></span><br><span class="line">    self._screen, self.reward, self.terminal, self.step_info = self.env.step(action)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lives</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> self.step_info <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> self.step_info[<span class="string">'ale.lives'</span>]</span><br></pre></td></tr></table></figure></p>
<h3 id="ale属性是什么"><a href="#ale属性是什么" class="headerlink" title="ale属性是什么"></a>ale属性是什么</h3><p>我看官方文档也没有看清楚，但是我觉得就是生命值是否没有了</p>
<blockquote>
<p>info (dict): diagnostic information useful for debugging. It can sometimes be useful for learning (for example, it might contain the raw probabilities behind the environment’s last state change). However, official evaluations of your agent are not allowed to use this for learning.</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> gym</span><br><span class="line">env = gym.make(<span class="string">'CartPole-v0'</span>)</span><br><span class="line"><span class="keyword">for</span> i_episode <span class="keyword">in</span> range(<span class="number">20</span>):</span><br><span class="line">    observation = env.reset()</span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">        env.render()</span><br><span class="line">        print(observation)</span><br><span class="line">        action = env.action_space.sample()</span><br><span class="line">        observation, reward, done, info = env.step(action)</span><br><span class="line">        <span class="keyword">if</span> done:</span><br><span class="line">            print(<span class="string">"Episode finished after &#123;&#125; timesteps"</span>.format(t+<span class="number">1</span>))</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">env.close()</span><br></pre></td></tr></table></figure>
<h2 id="问题3-cannot-import-name"><a href="#问题3-cannot-import-name" class="headerlink" title="问题3-cannot import name ***"></a>问题3-cannot import name ***</h2><p>参考文献[7]</p>
<h3 id="报错-1"><a href="#报错-1" class="headerlink" title="报错"></a>报错</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cannot import name tqdm</span><br></pre></td></tr></table></figure>
<h3 id="问题原因"><a href="#问题原因" class="headerlink" title="问题原因"></a>问题原因</h3><p>谷歌了半天，没有发现原因，然后百度了一下，发现了原因，看来还是自己太菜了。。<br>因为自己起的文件名就叫tqdm，然后就和库中的tqdm冲突了，这也太蠢了吧。。。</p>
<h2 id="问题4-linux下python执行shell脚本输出重定向"><a href="#问题4-linux下python执行shell脚本输出重定向" class="headerlink" title="问题4-linux下python执行shell脚本输出重定向"></a>问题4-linux下python执行shell脚本输出重定向</h2><p><a href="https://mxxhcm.github.io/2019/06/03/linux-python调用shell脚本并将输出重定向到文件/">详细介绍</a></p>
<h2 id="问题4-ImportError-No-module-named-conda-cli’"><a href="#问题4-ImportError-No-module-named-conda-cli’" class="headerlink" title="问题4-ImportError: No module named conda.cli’"></a>问题4-ImportError: No module named conda.cli’</h2><h3 id="问题描述-1"><a href="#问题描述-1" class="headerlink" title="问题描述"></a>问题描述</h3><p>anaconda的python版本是3.7，执行了conda install python=3.6之后，运行conda命令出错。报错如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from conda.cli import main </span><br><span class="line">ModuleNotFoundError: No module named &apos;conda&apos;</span><br></pre></td></tr></table></figure></p>
<h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><p>找到anaconda安装包，加一个-u参数，如下所示。重新安装anaconda自带的package，自己安装的包不会丢失。<br>~$:sh xxx.sh -u</p>
<h2 id="问题5-python-pip使用国内源"><a href="#问题5-python-pip使用国内源" class="headerlink" title="问题5-python-pip使用国内源"></a>问题5-python-pip使用国内源</h2><h3 id="暂时使用国内pip源"><a href="#暂时使用国内pip源" class="headerlink" title="暂时使用国内pip源"></a>暂时使用国内pip源</h3><p>使用清华源<br>~\$:pip install -i <a href="https://pypi.tuna.tsinghua.edu.cn/simple" target="_blank" rel="noopener">https://pypi.tuna.tsinghua.edu.cn/simple</a> package-name<br>使用阿里源<br>~\$:pip install -i <a href="https://mirrors.aliyun.com/pypi/simple" target="_blank" rel="noopener">https://mirrors.aliyun.com/pypi/simple</a> package-name</p>
<h3 id="将国内pip源设为默认"><a href="#将国内pip源设为默认" class="headerlink" title="将国内pip源设为默认"></a>将国内pip源设为默认</h3><p>~\$:pip install pip -U<br>~\$:pip config set global.index-url <a href="https://pypi.tuna.tsinghua.edu.cn/simple" target="_blank" rel="noopener">https://pypi.tuna.tsinghua.edu.cn/simple</a><br>~\$:pip config set global.timeout 60</p>
<blockquote>
<p>Writing to /home/username/.config/pip/pip.conf</p>
</blockquote>
<h4 id="查看pip配置文件"><a href="#查看pip配置文件" class="headerlink" title="查看pip配置文件"></a>查看pip配置文件</h4><p>~\$:find / -name pip.conf<br>我的是在/home/username/.config/pip/pip.conf</p>
<h2 id="问题6-ImportError-lib-x86-64-linux-gnu-libc-so-6-version-GLIBC-2-28-not-found"><a href="#问题6-ImportError-lib-x86-64-linux-gnu-libc-so-6-version-GLIBC-2-28-not-found" class="headerlink" title="问题6-ImportError: /lib/x86_64-linux-gnu/libc.so.6: version GLIBC_2.28 not found"></a>问题6-ImportError: /lib/x86_64-linux-gnu/libc.so.6: version GLIBC_2.28 not found</h2><h3 id="问题描述-2"><a href="#问题描述-2" class="headerlink" title="问题描述"></a>问题描述</h3><p>安装roboschool之后，出现ImportError。报错如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ImportError: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.28&apos; not found (required by /usr/local/lib/python3.6/dist-packages/roboschool/.libs/libQt5Core.so.5)</span><br></pre></td></tr></table></figure></p>
<h3 id="解决方案-1"><a href="#解决方案-1" class="headerlink" title="解决方案"></a>解决方案</h3><p>在roboschool上找到一个issue，说从1.0.49版本退回到1.0.48即可。我退回之后，又出现以下错误：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ImportError: libpcre16.so.3: cannot open shared object file: No such file or directory</span><br></pre></td></tr></table></figure></p>
<p>安装相应的库即可。完整的命令如下<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">~$:pip install roboschool==1.0.48</span><br><span class="line">~$:sudo apt install libpcre3-dev</span><br></pre></td></tr></table></figure></p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>1.<a href="https://www.cnblogs.com/timxgb/p/8905290.html" target="_blank" rel="noopener">https://www.cnblogs.com/timxgb/p/8905290.html</a><br>2.<a href="https://docs.python.org/3/library/stdtypes.html#dictionary-view-objects" target="_blank" rel="noopener">https://docs.python.org/3/library/stdtypes.html#dictionary-view-objects</a><br>3.<a href="https://stackoverflow.com/questions/43663206/typeerror-unsupported-operand-types-for-dict-values-and-int" target="_blank" rel="noopener">https://stackoverflow.com/questions/43663206/typeerror-unsupported-operand-types-for-dict-values-and-int</a><br>4.<a href="https://github.com/devsisters/DQN-tensorflow/issues/29" target="_blank" rel="noopener">https://github.com/devsisters/DQN-tensorflow/issues/29</a><br>5.<a href="https://gym.openai.com/docs" target="_blank" rel="noopener">https://gym.openai.com/docs</a><br>6.<a href="https://github.com/openai/baselines/issues/42" target="_blank" rel="noopener">https://github.com/openai/baselines/issues/42</a><br>7.<a href="https://blog.csdn.net/m0_37561765/article/details/78714603" target="_blank" rel="noopener">https://blog.csdn.net/m0_37561765/article/details/78714603</a><br>8.<a href="https://blog.csdn.net/u014432608/article/details/79066813" target="_blank" rel="noopener">https://blog.csdn.net/u014432608/article/details/79066813</a><br>9.<a href="https://mirrors.tuna.tsinghua.edu.cn/help/pypi/" target="_blank" rel="noopener">https://mirrors.tuna.tsinghua.edu.cn/help/pypi/</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://mxxhcm.github.io/2019/03/07/tensorflow-problems/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="马晓鑫爱马荟荟">
      <meta itemprop="description" content="记录硕士三年自己的积累">
      <meta itemprop="image" content="/images/favicon.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="mxxhcm's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/03/07/tensorflow-problems/" class="post-title-link" itemprop="http://mxxhcm.github.io/page/14/index.html">tensorflow 常见问题（不定期更新）</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-03-07 14:51:01" itemprop="dateCreated datePublished" datetime="2019-03-07T14:51:01+08:00">2019-03-07</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-07-18 20:27:16" itemprop="dateModified" datetime="2019-07-18T20:27:16+08:00">2019-07-18</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/tensorflow/" itemprop="url" rel="index"><span itemprop="name">tensorflow</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="问题1-the-value-of-a-feed-cannot-be-a-tf-tensor-object">问题1-The value of a feed cannot be a tf.Tensor object</h2>
<h3 id="报错">报错</h3>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">TypeError: The value of a feed cannot be a tf.Tensor object</span><br></pre></td></tr></table></figure>
<h3 id="问题原因">问题原因</h3>
<p>sess.run(op, feed_dict={})中的feed value不能是tf.Tensor类型。</p>
<h3 id="解决方法">解决方法</h3>
<p>sess.run(train, feed_dict={x:images, y:labels}的输入不能是tensor，可以使用sess.run(tensor)得到numpy.array形式的数据再喂给feed_dict。</p>
<blockquote>
<p>Once you have launched a sess, you can use your_tensor.eval(session=sess) or sess.run(your_tensor) to get you feed tensor into the format of numpy.array and then feed it to your placeholder.</p>
</blockquote>
<h2 id="问题2-could-not-create-cudnn-handle-cudnn-status-internal-error">问题2-Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR</h2>
<h3 id="配置">配置</h3>
<p>环境配置如下：</p>
<ul>
<li>Ubuntu 18.04</li>
<li>CUDA 10.0</li>
<li>CuDNN 7.4.2</li>
<li>Python3.7.3</li>
<li>Tensorflow 1.13.1</li>
<li>Nvidia Drivers 430.09</li>
<li>RTX2070</li>
</ul>
<h3 id="报错-v2">报错</h3>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">2019-05-12 14:45:59.355405: E tensorflow/stream_executor/cuda/cuda_dnn.cc:334] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR</span><br><span class="line">2019-05-12 14:45:59.357698: E tensorflow/stream_executor/cuda/cuda_dnn.cc:334] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<h3 id="问题原因-v2">问题原因</h3>
<p>GPU不够用了。</p>
<h3 id="解决方法-v2">解决方法</h3>
<p>在代码中添加下面几句：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">config = tf.ConfigProto()</span><br><span class="line">config.gpu_options.allow_growth = <span class="literal">True</span></span><br><span class="line">session = InteractiveSession(config=config)</span><br></pre></td></tr></table></figure>
<h2 id="问题3-libcublas-so-10-0-cannot-open-shared-object-file-no-such-file-or-directory">问题3-libcublas.so.10.0: cannot open shared object file: No such file or directory</h2>
<p>在命令行或者pycharm中import tensorflow报错</p>
<h3 id="报错-v3">报错</h3>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ImportError: libcublas.so.10.0: cannot open shared object file: No such file or directory</span><br><span class="line">Failed to load the native TensorFlow runtime.</span><br></pre></td></tr></table></figure>
<h3 id="问题原因-v3">问题原因</h3>
<p>没有配置CUDA环境变量</p>
<h3 id="解决方法-v3">解决方法</h3>
<h4 id="命令行中">命令行中</h4>
<p>在.bashrc文件中加入下列语句：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export PATH=/usr/local/cuda/bin$&#123;PATH:+:$&#123;PATH&#125;&#125;</span><br><span class="line">export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$&#123;LD_LIBRARY_PATH:+:$&#123;LD_LIBRARY_PATH&#125;&#125;</span><br><span class="line">export CUDA_HOME=/usr/local/cuda</span><br></pre></td></tr></table></figure>
<h4 id="pycharm中">pycharm中</h4>
<h5 id="方法1-这种方法我没有实验成功-不知道为什么">方法1（这种方法我没有实验成功，不知道为什么）</h5>
<p>在左上角选中<br>
File&gt;&gt;Settings&gt;&gt;Build.Execution,Deployment&gt;&gt;Console&gt;&gt;Python Console<br>
在Environment下的Environment variables中添加<br>
LD_LIBRARY_PATH=/usr/local/cuda/lib64:${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}即可。</p>
<h5 id="方法2">方法2</h5>
<p>修改完.bashrc文件后从终端中运行pycharm。</p>
<h2 id="问题4-dlerror-libcupti-so-10-0-cannot-open-shared-object-file-no-such-file-or-directory">问题4-dlerror: libcupti.so.10.0: cannot open shared object file: No such file or directory</h2>
<p>执行mnist_with_summary代码时报错</p>
<h3 id="报错-v4">报错</h3>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">I tensorflow/stream_executor/dso_loader.cc:142] Couldn&apos;t open CUDA library libcupti.so.10.0. LD_LIBRARY_PATH: /usr/local/cuda/lib64:</span><br><span class="line">2019-05-13 23:04:10.620149: F tensorflow/stream_executor/lib/statusor.cc:34] Attempting to fetch value instead of handling error Failed precondition: could not dlopen DSO: libcupti.so.10.0; dlerror: libcupti.so.10.0: cannot open shared object file: No such file or directory</span><br><span class="line">Aborted (core dumped)</span><br></pre></td></tr></table></figure>
<h3 id="问题问题问题问题问题问题问题问题问题原因">问题问题问题问题问题问题问题问题问题原因</h3>
<p>libcupti.so.10.0包没找到</p>
<h3 id="解决方法-v4">解决方法</h3>
<p>执行以下命令，找到相关的依赖包：<br>
~$:find /usr/local/cuda/ -name libcupti.so.10.0<br>
输出如下：</p>
<blockquote>
<p>/usr/local/cuda/extras/CUPTI/lib64/libcupti.so.10.0</p>
</blockquote>
<p>然后修改~/.bashrc文件中相应的环境变量:<br>
export LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64/😒{LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}<br>
重新运行即可。</p>
<h2 id="问题5-unhashable-type-list">问题5-unhashable type: ‘list’</h2>
<p>sess.run(op, feed_dict={})中feed的数据中包含有list的时候会报错。</p>
<h3 id="报错-v5">报错</h3>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">TypeError: unhashable type: &apos;list&apos;</span><br></pre></td></tr></table></figure>
<h3 id="问题原因-v4">问题原因</h3>
<p>feed_dict中不能的value不能是list。</p>
<h3 id="解决方法-v5">解决方法</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">feed_dict = &#123;</span><br><span class="line">               placeholder : value </span><br><span class="line">                  <span class="keyword">for</span> placeholder, value <span class="keyword">in</span> zip(placeholder_list, inputs_list))</span><br><span class="line">            &#125;</span><br></pre></td></tr></table></figure>
<h3 id="代码示例">代码示例</h3>
<p><a href="https://github.com/mxxhcm/code/blob/master/tf/ops/tf_placeholder_list.py" target="_blank" rel="noopener">代码地址</a></p>
<h2 id="问题6-attempting-to-use-uninitialized-value">问题6-Attempting to use uninitialized value</h2>
<p>tf.Session()和tf.InteractiveSession()混用问题。</p>
<h3 id="报错-v6">报错</h3>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensorflow.python.framework.errors_impl.FailedPreconditionError: Attempting to use uninitialized value prediction/l1/w</span><br><span class="line">	 [[&#123;&#123;node prediction/l1/w/read&#125;&#125;]]</span><br><span class="line">	 [[&#123;&#123;node prediction/LogSoftmax&#125;&#125;]]</span><br></pre></td></tr></table></figure>
<h3 id="问题原因-v5">问题原因</h3>
<p>声明了如下session:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sess = tf.Session()</span><br><span class="line">sess.run(tf.global_variables_initializer())</span><br></pre></td></tr></table></figure>
<p>在接下来的代码中，因为我声明的是tf.Session()，使用了op.eval()函数，这种用法是tf.InteractiveSession的用法，所以就相当于没有初始化。<br>
result = op.eval(feed_dict={})<br>
然后就报了未初始化的错误。<br>
把代码改成：<br>
result = sess.run([op], feeed_dct={})<br>
即可，即上下文使用的session应该一致。</p>
<h3 id="解决方案">解决方案</h3>
<p>使用统一的session类型</p>
<h2 id="问题7-setting-an-array-element-with-a-sequence">问题7-setting an array element with a sequence</h2>
<p>feed_dict键值对中中值必须是numpy.ndarray，不能是其他类型。</p>
<h3 id="报错-v7">报错</h3>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">value error setting an array element with a sequence,</span><br></pre></td></tr></table></figure>
<h3 id="问题原因-v6">问题原因</h3>
<p>feed_dict中key-value的value必须是numpy.ndarray，不能是其他类型，尤其不能是tf.Variable。</p>
<h3 id="解决方法-v6">解决方法</h3>
<p>检查sess.run(op, feed_dict={})中的feed_dict，确保他们的类型，不能是tf.Variable()类型的对象，需要是numpy.ndarray。</p>
<h2 id="问题8-访问tf-variable-的值">问题8-访问tf.Variable()的值</h2>
<p>如何获得tf.Variable()对象的值</p>
<h3 id="解决方法-v7">解决方法</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">x = tf.Varialbe([<span class="number">1.0</span>, <span class="number">2.0</span>])</span><br><span class="line">sess = tf.Session()</span><br><span class="line">sess.run(tf.global_variables_initializer())</span><br><span class="line">value = sess.run(x)</span><br></pre></td></tr></table></figure>
<p>或者</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">x = tf.Varialbe([1.0, 2.0])</span><br><span class="line">sess = tf.InteractiveSession()</span><br><span class="line">sess.run(tf.global_variables_initializer())</span><br><span class="line">x.eval()</span><br></pre></td></tr></table></figure>
<h2 id="问题9-can-not-convert-a-ndarray-into-a-tensor-or-operation">问题9-Can not convert a ndarray into a Tensor or Operation</h2>
<h3 id="报错-v8">报错</h3>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Can not convert a ndarray into a Tensor or Operation.</span><br></pre></td></tr></table></figure>
<h3 id="问题原因-v7">问题原因</h3>
<p>原因是sess.run()前后参数名重了，比如outputs = sess.run(outputs)，outputs本来是自己定义的一个op，但是sess.run(outputs)之后outputs就成了一个变量，就把定义的outputs op覆盖了。</p>
<h3 id="解决方法-v8">解决方法</h3>
<p>换个变量名字就行</p>
<h2 id="问题10-本地使用gpu-server的tensorboard">问题10-本地使用gpu server的tensorboard</h2>
<h3 id="问题描述">问题描述</h3>
<p>在gpu server跑的实验结果，然后summary的记录也在server上，但是又没办法可视化，只好在本地可视化。</p>
<h3 id="解决方法-v9">解决方法</h3>
<p>使用ssh进行映射好了。</p>
<h4 id="本机设置">本机设置</h4>
<p>~$:ssh -L 12345:10.1.114.50:6006 <a href="mailto:mxxmhh@127.0.0.1" target="_blank" rel="noopener">mxxmhh@127.0.0.1</a><br>
将本机的12345端口映射到10.1.114.50的6006端口，中间服务器使用的是本机。<br>
或者可以使用10.1.114.50作为中间服务器。<br>
~$:ssh -L 12345:10.1.114.50:6006 <a href="mailto:liuchi@10.1.114.50" target="_blank" rel="noopener">liuchi@10.1.114.50</a><br>
或者可以使用如下方法：<br>
~$:ssh -L 12345:127.0.0.1:6006 <a href="mailto:liuchi@10.1.114.50" target="_blank" rel="noopener">liuchi@10.1.114.50</a><br>
从这个方法中，可以看出127.0.0.1这个ip是中间服务器可以访问的ip。<br>
以上三种方法中，-L后的端口号12345可以随意设置，只要不冲突即可。</p>
<h4 id="服务端设置">服务端设置</h4>
<p>然后在服务端运行以下命令：<br>
~$:tensorboard --logdir logdir -port 6006<br>
这个端口号也是可以任意设置的，不冲突即可。</p>
<h4 id="运行">运行</h4>
<p>然后在本机访问<br>
<a href="https://127.0.0.1:12345" target="_blank" rel="noopener">https://127.0.0.1:12345</a>即可。</p>
<h2 id="问题11-每一步summary一个list的每一个元素">问题11-每一步summary一个list的每一个元素</h2>
<h3 id="问题原因-v8">问题原因</h3>
<p>有一个tf list的placeholder，但是每一步只能生成其中的一个元素，所以怎么样summary中其中的某一个？</p>
<h3 id="解决方法-v10">解决方法</h3>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">number = 3</span><br><span class="line">x_ph_list = []</span><br><span class="line">for i in range(number):</span><br><span class="line">    x_ph_list.append(tf.placeholder(tf.float32, shape=None))</span><br><span class="line"></span><br><span class="line">x_summary_list = []</span><br><span class="line">for i in range(number):</span><br><span class="line">    x_summary_list.append(tf.summary.scalar("x%s" % i, x_ph_list[i]))</span><br><span class="line"></span><br><span class="line">writer = tf.summary.FileWriter("./tf_summary/scalar_list_summary/sep")</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    scope = 10</span><br><span class="line">    inputs = np.arange(scope*number)</span><br><span class="line">    inputs = inputs.reshape(scope, number)</span><br><span class="line">    # inputs = np.random.randn(scope, number)</span><br><span class="line">    for i in range(scope):</span><br><span class="line">        for j in range(number):</span><br><span class="line">            out, xj_s = sess.run([x_ph_list[j], x_summary_list[j]], feed_dict=&#123;x_ph_list[j]: inputs[i][j]&#125;)</span><br><span class="line">            writer.add_summary(xj_s, global_step=i)</span><br></pre></td></tr></table></figure>
<h2 id="问题12-for-value-in-summary-value-attributeerror-list-object-has-no-attribute-value">问题12- for value in summary.value: AttributeError: ‘list’ object has no attribute ‘value’</h2>
<h3 id="问题描述-v2">问题描述</h3>
<p>writer.add_summary时报错</p>
<h3 id="报错-v9">报错</h3>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">File &quot;/home/mxxmhh/anaconda3/lib/python3.7/site-packages/tensorflow/python/summary/writer/writer.py&quot;, line 127, in add_summary</span><br><span class="line">    for value in summary.value:</span><br><span class="line">AttributeError: &apos;list&apos; object has no attribute &apos;value&apos;</span><br></pre></td></tr></table></figure>
<h3 id="问题原因-v9">问题原因</h3>
<p>执行以下代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">s_ = sess.run([loss_summary], feed_dict=&#123;p_losses_ph: inputs1, q_losses_ph: inputs2&#125;)</span><br><span class="line">writer.add_summary(s_, global_step=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>因为[loss_summary]加了方括号，就把它当成了一个list。。返回值也是list，就报错了</p>
<h3 id="解决方法-v11">解决方法</h3>
<ul>
<li>方法1，在等号左边加一个逗号，取出list中的值</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">s_, = sess.run([loss_summary], feed_dict=&#123;p_losses_ph: inputs1, q_losses_ph: inputs2&#125;)</span><br></pre></td></tr></table></figure>
<ul>
<li>方法2，去掉loss_summary外面的中括号。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">s_ = sess.run(loss_summary, feed_dict=&#123;p_losses_ph: inputs1, q_losses_ph: inputs2&#125;)</span><br></pre></td></tr></table></figure>
<h2 id="问题13-tf-get-default-session-always-returns-none-type">问题13- tf.get_default_session() always returns None type:</h2>
<h3 id="问题描述-v3">问题描述</h3>
<p>调用tf.get_default_session()时，返回的是None</p>
<h3 id="报错-v10">报错</h3>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">    tf.get_default_session().run(y)</span><br><span class="line">AttributeError: &apos;NoneType&apos; object has no attribute &apos;run&apos;</span><br></pre></td></tr></table></figure>
<h3 id="问题原因-v10">问题原因</h3>
<p>只有在设定default session之后，才能使用tf.get_default_session()获得当前的默认session，在我们写代码的时候，一般会按照下面的方式写：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    some operations</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure>
<p>这种情况下已经把tf.Session()生成的session当做了默认session，但是如果仅仅使用以下代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">sess =  tf.Session()</span><br><span class="line">sess.run(some operations)</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>是没有把tf.Session()当成默认session的，即只有在with block内，才会将这个session当做默认session。</p>
<h3 id="解决方案-v2">解决方案</h3>
<h2 id="参考文献">参考文献</h2>
<p>1.<a href="https://github.com/tensorflow/tensorflow/issues/4842" target="_blank" rel="noopener">https://github.com/tensorflow/tensorflow/issues/4842</a><br>
2.<a href="https://github.com/tensorflow/tensorflow/issues/24496" target="_blank" rel="noopener">https://github.com/tensorflow/tensorflow/issues/24496</a><br>
3.<a href="https://github.com/tensorflow/tensorflow/issues/9530" target="_blank" rel="noopener">https://github.com/tensorflow/tensorflow/issues/9530</a><br>
4.<a href="https://stackoverflow.com/questions/51128427/how-to-feed-list-of-values-to-a-placeholder-list-in-tensorflow" target="_blank" rel="noopener">https://stackoverflow.com/questions/51128427/how-to-feed-list-of-values-to-a-placeholder-list-in-tensorflow</a><br>
5.<a href="https://github.com/tensorflow/tensorflow/issues/11897" target="_blank" rel="noopener">https://github.com/tensorflow/tensorflow/issues/11897</a><br>
6.<a href="https://stackoverflow.com/questions/34156639/tensorflow-python-valueerror-setting-an-array-element-with-a-sequence-in-t" target="_blank" rel="noopener">https://stackoverflow.com/questions/34156639/tensorflow-python-valueerror-setting-an-array-element-with-a-sequence-in-t</a><br>
7.<a href="https://stackoverflow.com/questions/33679382/how-do-i-get-the-current-value-of-a-variable" target="_blank" rel="noopener">https://stackoverflow.com/questions/33679382/how-do-i-get-the-current-value-of-a-variable</a><br>
8.<a href="https://blog.csdn.net/michael__corleone/article/details/79007425" target="_blank" rel="noopener">https://blog.csdn.net/michael__corleone/article/details/79007425</a><br>
9.<a href="https://stackoverflow.com/questions/47721792/tensorflow-tf-get-default-session-after-sess-tf-session-is-none" target="_blank" rel="noopener">https://stackoverflow.com/questions/47721792/tensorflow-tf-get-default-session-after-sess-tf-session-is-none</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://mxxhcm.github.io/2019/03/04/linux-查看python-package的安装位置/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="马晓鑫爱马荟荟">
      <meta itemprop="description" content="记录硕士三年自己的积累">
      <meta itemprop="image" content="/images/favicon.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="mxxhcm's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/03/04/linux-查看python-package的安装位置/" class="post-title-link" itemprop="http://mxxhcm.github.io/page/14/index.html">linux-查看python package的安装位置</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-03-04 14:52:16" itemprop="dateCreated datePublished" datetime="2019-03-04T14:52:16+08:00">2019-03-04</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-05-12 12:03:26" itemprop="dateModified" datetime="2019-05-12T12:03:26+08:00">2019-05-12</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/linux/" itemprop="url" rel="index"><span itemprop="name">linux</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>使用pip install package-name之后，不知道该包存在了哪个路径下。<br>
可以再次使用pip install package-name，这时候就会给出该包存放在哪个路径下。</p>
<h2 id="参考文献">参考文献</h2>
<ol>
<li><a href="https://blog.csdn.net/weixin_41712059/article/details/82940516" target="_blank" rel="noopener">https://blog.csdn.net/weixin_41712059/article/details/82940516</a></li>
</ol>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://mxxhcm.github.io/2019/03/04/linux-shadowsocks服务端以及客户端配置/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="马晓鑫爱马荟荟">
      <meta itemprop="description" content="记录硕士三年自己的积累">
      <meta itemprop="image" content="/images/favicon.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="mxxhcm's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/03/04/linux-shadowsocks服务端以及客户端配置/" class="post-title-link" itemprop="http://mxxhcm.github.io/page/14/index.html">shadowsocks服务端以及客户端配置</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-03-04 13:03:57" itemprop="dateCreated datePublished" datetime="2019-03-04T13:03:57+08:00">2019-03-04</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-06-19 11:39:56" itemprop="dateModified" datetime="2019-06-19T11:39:56+08:00">2019-06-19</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/linux/" itemprop="url" rel="index"><span itemprop="name">linux</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="服务器端配置">服务器端配置</h2>
<p>首先需要有一个VPS账号，vultr,digitalocean,搬瓦工等等都行。<br>
首先到下面两个网站检测22端口是否开启，如果关闭的话，vps换个ip把。。<br>
<a href="http://tool.chinaz.com/port" target="_blank" rel="noopener">http://tool.chinaz.com/port</a><br>
<a href="https://www.yougetsignal.com/tools/open-ports/" target="_blank" rel="noopener">https://www.yougetsignal.com/tools/open-ports/</a></p>
<h3 id="启用bbr加速">启用BBR加速</h3>
<p>~#:apt update<br>
~#:apt upgrade<br>
~#:echo “net.core.default_qdisc=fq” &gt;&gt; /etc/sysctl.conf<br>
~#:echo “net.ipv4.tcp_congestion_control=bbr” &gt;&gt; /etc/sysctl.conf<br>
~#:sysctl -p<br>
上述命令就完成了BBR加速，执行以下命令验证：<br>
~#:lsmod |grep bbr<br>
看到输出包含tcp_bbr就说明已经成功了。</p>
<h3 id="搭建shadowsocks-server">搭建shadowsocks server</h3>
<h4 id="安装shadowsocks-server">安装shadowsocks server</h4>
<p>~#:apt install python-pip<br>
~#:pip install shadowsocks<br>
需要说一下的是，shadowsocks目前还不支持python3.5及以上版本，上次我把/usr/bin/python指向了python3.6，就是系统默认的python指向了python3.6，然后就gg了。一定要使用Python 2.6,2.7,3.3,3.4中的一个版本才能使用。。</p>
<h4 id="创建shadowsocks配置文件">创建shadowsocks配置文件</h4>
<p>如果你的VPS支持ipv6的话，那么可以开多进程分别运行ipv4和ipv6的shadowsocks server。本地只有ipv4的话，可以用本地ipv4访问ipv6，从而访问byr等网站，但是六维空间对此做了屏蔽。如果本地有ipv6的话，还可以用本地的ipv6访问ipv6实现校园网不走ipv4流量。</p>
<h5 id="ipv4配置">ipv4配置</h5>
<p>~#:vim /etc/shadowsocks_v4.json<br>
配置文件如下</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line"><span class="attr">"server"</span>:<span class="string">"0.0.0.0"</span>,</span><br><span class="line"><span class="attr">"server_port"</span>:<span class="string">"你的端口号"</span>,</span><br><span class="line"><span class="attr">"local_address"</span>:<span class="string">"127.0.0.1"</span>,</span><br><span class="line"><span class="attr">"local_port"</span>:<span class="number">1080</span>,</span><br><span class="line"><span class="attr">"password"</span>:<span class="string">"你的密码"</span>,</span><br><span class="line"><span class="attr">"timeout"</span>:<span class="number">600</span>,</span><br><span class="line"><span class="attr">"method"</span>:<span class="string">"aes-256-cfb"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="ipv6配置">ipv6配置</h5>
<p>~#:vim /etc/shadowsocks_v6.json<br>
配置文件如下</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line"><span class="attr">"server"</span>:<span class="string">"::"</span>,</span><br><span class="line"><span class="attr">"server_port"</span>:<span class="string">"你的端口号"</span>,</span><br><span class="line"><span class="attr">"local_address"</span>:<span class="string">"127.0.0.1"</span>,</span><br><span class="line"><span class="attr">"local_port"</span>:<span class="number">1080</span>,</span><br><span class="line"><span class="attr">"password"</span>:<span class="string">"你的密码"</span>,</span><br><span class="line"><span class="attr">"timeout"</span>:<span class="number">600</span>,</span><br><span class="line"><span class="attr">"method"</span>:<span class="string">"aes-256-cfb"</span></span><br><span class="line">&#125;</span><br><span class="line">``` </span><br><span class="line">注意这两个文件的server_port一定要不同，以及双引号必须是英文引号。</span><br><span class="line">##### 1.2.2.3.手动运行shadowsocks server</span><br><span class="line">~#:ssserver -c /etc/shadowsock_v4.json -d start --pid-file ss1.pid</span><br><span class="line">~#:ssserver -c /etc/shadowsock_v6.json -d start --pid-file ss2.pid</span><br><span class="line">注意这里要给两条命令分配不同的进程号。</span><br><span class="line"></span><br><span class="line">### 设置shadowsocks server开机自启</span><br><span class="line">如果重启服务器的话，就需要重新手动执行上述命令，这里我们可以把它写成开机自启脚本。</span><br><span class="line">~#:vim /etc/init.d/shadowsocks_v4</span><br><span class="line">内容如下：</span><br><span class="line">``` shell</span><br><span class="line">#!/bin/sh</span><br><span class="line">### BEGIN INIT INFO</span><br><span class="line"># Provides:          apache2</span><br><span class="line"># Required-Start:    $local_fs $remote_fs $network $syslog</span><br><span class="line"># Required-Stop:     $local_fs $remote_fs $network $syslog</span><br><span class="line"># Default-Start:     2 3 4 5</span><br><span class="line"># Default-Stop:      0 1 6</span><br><span class="line"># Short-Description: apache2 service</span><br><span class="line"># Description:       apache2 service daemon</span><br><span class="line">### END INIT INFO</span><br><span class="line">start()&#123;</span><br><span class="line">  ssserver -c /etc/shadowsocks_v4.json -d start --pid-file ss2.pid</span><br><span class="line">&#125;</span><br><span class="line">stop()&#123;</span><br><span class="line">  ssserver -c /etc/shadowsocks_v4.json -d stop --pid-file ss2.pid</span><br><span class="line">&#125;</span><br><span class="line">case "$1" in</span><br><span class="line">start)</span><br><span class="line">  start</span><br><span class="line">  ;;</span><br><span class="line">stop)</span><br><span class="line">  stop</span><br><span class="line">  ;;</span><br><span class="line">restart)</span><br><span class="line">  stop</span><br><span class="line">  start</span><br><span class="line">  ;;</span><br><span class="line">*)</span><br><span class="line">  echo "Uasage: $0 &#123;start|reload|stop&#125;$"</span><br><span class="line">  exit 1</span><br><span class="line">  ;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure>
<p>~#:vim /etc/init.d/shadowsocks_v6<br>
内容如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span>!/bin/sh</span><br><span class="line"><span class="meta">#</span>## BEGIN INIT INFO</span><br><span class="line"><span class="meta">#</span> Provides:          apache2</span><br><span class="line"><span class="meta">#</span> Required-Start:    $local_fs $remote_fs $network $syslog</span><br><span class="line"><span class="meta">#</span> Required-Stop:     $local_fs $remote_fs $network $syslog</span><br><span class="line"><span class="meta">#</span> Default-Start:     2 3 4 5</span><br><span class="line"><span class="meta">#</span> Default-Stop:      0 1 6</span><br><span class="line"><span class="meta">#</span> Short-Description: apache2 service</span><br><span class="line"><span class="meta">#</span> Description:       apache2 service daemon</span><br><span class="line"><span class="meta">#</span>## END INIT INFO</span><br><span class="line">start()&#123;</span><br><span class="line">  ssserver -c /etc/shadowsocks_v6.json -d start --pid-file ss1.pid</span><br><span class="line">&#125;</span><br><span class="line">stop()&#123;</span><br><span class="line">  ssserver -c /etc/shadowsocks_v6.json -d stop --pid-file ss1.pid</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line">case "$1" in</span><br><span class="line">start)</span><br><span class="line">  start</span><br><span class="line">  ;;</span><br><span class="line">stop)</span><br><span class="line">  stop</span><br><span class="line">  ;;</span><br><span class="line">restart)</span><br><span class="line">  stop</span><br><span class="line">  start</span><br><span class="line">  ;;</span><br><span class="line">*)</span><br><span class="line">  echo "Uasage: $0 &#123;start|reload|stop&#125;$"</span><br><span class="line">  exit 1</span><br><span class="line">  ;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure>
<p>然后执行下列命令即可：<br>
~#:chmod a+x /etc/init.d/shadowsocks_v4<br>
~#:chmod a+x /etc/init.d/shadowsocks_v6<br>
~#:update-rc.d shadowsocks_v4 defaults<br>
~#:update-rc.d shadowsocks_v6 defaults</p>
<p>至此，服务器端配置完成。</p>
<h2 id="服务端自动配置脚本">服务端自动配置脚本</h2>
<p><a href="https://github.com/mxxhcm/code/tree/master/shell/ss" target="_blank" rel="noopener">地址</a><br>
首先将该文件中所有文件复制到vps上，然后执行<br>
~#:sh install_ss_server.sh<br>
即可</p>
<h3 id="补充说明">补充说明</h3>
<p>该文件夹共包含五个文件<br>
shadowsocks_v4.json为ipv4 ss配置文件，可根据自己的需要修改端口号和密码<br>
shadowsocks_v6.json为ipv6 ss配置文件，可根据自己的需要修改端口号和密码<br>
shadowsocks_v4为ipv4 ss自启动文件，无需修改<br>
shadowsocks_v6为ipv6 ss自启动文件，无需修改<br>
install_ss_server.sh为安装脚本，该脚本同时配置ipv4和ipv6 ss server。可根据自己需要自行选择。</p>
<h2 id="客户端配置">客户端配置</h2>
<h3 id="windows客户端配置">Windows客户端配置</h3>
<h4 id="安装shadowsock客户端">安装shadowsock客户端</h4>
<p>到该网址 <a href="https://github.com/shadowsocks/shadowsocks-windows/releases" target="_blank" rel="noopener">https://github.com/shadowsocks/shadowsocks-windows/releases</a> 下载相应的windows客户端程序。<br>
然后配置服务器即可～</p>
<h3 id="linux客户端配置">Linux客户端配置</h3>
<h4 id="安装shadowsocks程序">安装shadowsocks程序</h4>
<p>~$:sudo pip install shadowsocks</p>
<h4 id="运行shadowsocks客户端程序">运行shadowsocks客户端程序</h4>
<p>~$:sudo vim /etc/shadowsocks.json<br>
填入以下配置文件<br>
{<br>
“server”:“填上自己的shadowsocks server ip地址”,<br>
“server_port”:“8888”,//填上自己的shadowsocks server 端口&quot;<br>
“local_port”:1080,<br>
“password”:“mxxhcm150929”,<br>
“timeout”:600,<br>
“method”:“aes-256-cfb”<br>
}</p>
<p>接下来可以执行以下命令运行shadowsocks客户端：<br>
~$:sudo sslocal -c /etc/shadowsocks.json<br>
然后报错：</p>
<blockquote>
<p>INFO: loading config from /etc/shadowsocks.json<br>
2019-03-04 14:37:49 INFO     loading libcrypto from libcrypto.so.1.1<br>
Traceback (most recent call last):<br>
File “/usr/local/bin/sslocal”, line 11, in <module><br>
load_entry_point(‘shadowsocks==2.8.2’, ‘console_scripts’, ‘sslocal’)()<br>
File “/usr/local/lib/python2.7/dist-packages/shadowsocks/local.py”, line 39, in main<br>
config = shell.get_config(True)<br>
File “/usr/local/lib/python2.7/dist-packages/shadowsocks/shell.py”, line 262, in get_config<br>
check_config(config, is_local)<br>
File “/usr/local/lib/python2.7/dist-packages/shadowsocks/shell.py”, line 124, in check_config<br>
encrypt.try_cipher(config[‘password’], config[‘method’])<br>
File “/usr/local/lib/python2.7/dist-packages/shadowsocks/encrypt.py”, line 44, in try_cipher<br>
Encryptor(key, method)<br>
File “/usr/local/lib/python2.7/dist-packages/shadowsocks/encrypt.py”, line 83, in <strong>init</strong><br>
random_string(self._method_info[1]))<br>
File “/usr/local/lib/python2.7/dist-packages/shadowsocks/encrypt.py”, line 109, in get_cipher<br>
return m[2](method, key, iv, op)<br>
File “/usr/local/lib/python2.7/dist-packages/shadowsocks/crypto/openssl.py”, line 76, in <strong>init</strong><br>
load_openssl()<br>
File “/usr/local/lib/python2.7/dist-packages/shadowsocks/crypto/openssl.py”, line 52, in load_openssl<br>
libcrypto.EVP_CIPHER_CTX_cleanup.argtypes = (c_void_p,)<br>
File “/usr/lib/python2.7/ctypes/<strong>init</strong>.py”, line 379, in <strong>getattr</strong><br>
func = self.<strong>getitem</strong>(name)<br>
File “/usr/lib/python2.7/ctypes/<strong>init</strong>.py”, line 384, in <strong>getitem</strong><br>
func = self._FuncPtr((name_or_ordinal, self))<br>
AttributeError: /usr/lib/x86_64-linux-gnu/libcrypto.so.1.1: undefined symbol: EVP_CIPHER_CTX_cleanup</module></p>
</blockquote>
<p>按照参考文献4的做法，是在openssl 1.1.0版本中放弃了EVP_CIPHER_CTX_cleanup函数</p>
<blockquote>
<p>EVP_CIPHER_CTX was made opaque in OpenSSL 1.1.0. As a result, EVP_CIPHER_CTX_reset() appeared and EVP_CIPHER_CTX_cleanup() disappeared.<br>
EVP_CIPHER_CTX_init() remains as an alias for EVP_CIPHER_CTX_reset().</p>
</blockquote>
<p>将openssl库中的EVP_CIPHER_CTX_cleanup改为EVP_CIPHER_CTX_reset即可。<br>
再次执行以下命令，查看shadowsocks安装位置<br>
~#:pip install shadowsocks<br>
Requirement already satisfied: shadowsocks in /usr/local/lib/python2.7/dist-packages<br>
~#:cd /usr/local/lib/python2.7/dist-packages/shadowsocks<br>
~#:vim crypto/openssl.py<br>
搜索cleanup，将其替换为reset<br>
具体位置在第52行libcrypto.EVP_CIPHER_CTX_cleanup.argtypes = (c_void_p,)和第111行libcrypto.EVP_CIPHER_CTX_cleanup(self._ctx)</p>
<h4 id="手动运行后台挂起">手动运行后台挂起</h4>
<p>将所有的log重定向到~/.log/sslocal.log文件中<br>
~$:mkdir ~/.log<br>
~$:touch ~/.log/ss-local.log<br>
~$:nohup sslocal -c /etc/shadowsocks_v6.json &lt;/dev/null &amp;&gt;&gt;~/.log/ss-local.log &amp;</p>
<h4 id="开机自启shadowsocks-client">开机自启shadowsocks client</h4>
<p>但是这样子的话，每次开机都要重新运行上述命令，太麻烦了。可以写个开机自启脚本。执行以下命令：<br>
~$:sudo vim /etc/init.d/shadowsocks<br>
内容为以下shell脚本</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span>!/bin/sh</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span>## BEGIN INIT INFO</span><br><span class="line"><span class="meta">#</span> Provides:          shadowsocks local</span><br><span class="line"><span class="meta">#</span> Required-Start:    $local_fs $remote_fs $network $syslog</span><br><span class="line"><span class="meta">#</span> Required-Stop:     $local_fs $remote_fs $network $syslog</span><br><span class="line"><span class="meta">#</span> Default-Start:     2 3 4 5</span><br><span class="line"><span class="meta">#</span> Default-Stop:      0 1 6</span><br><span class="line"><span class="meta">#</span> Short-Description: shadowsocks service</span><br><span class="line"><span class="meta">#</span> Description:       shadowsocks service daemon</span><br><span class="line"><span class="meta">#</span>## END INIT INFO</span><br><span class="line">start()&#123;</span><br><span class="line">　　  sslocal -c /etc/shadowsocks.json -d start</span><br><span class="line">&#125;</span><br><span class="line">stop()&#123;</span><br><span class="line">　　  sslocal -c /etc/shadowsocks.json -d stop</span><br><span class="line">&#125;</span><br><span class="line">case “$1” in</span><br><span class="line">start)</span><br><span class="line">　　　start</span><br><span class="line">　　　;;</span><br><span class="line">stop)</span><br><span class="line">　　　stop</span><br><span class="line">　　　;;</span><br><span class="line">reload)</span><br><span class="line">　　　stop</span><br><span class="line">　　　start</span><br><span class="line">　　　;;</span><br><span class="line">\*)</span><br><span class="line">　　　echo “Usage: $0 &#123;start|reload|stop&#125;”</span><br><span class="line">　　　exit 1</span><br><span class="line">　　　;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure>
<p>然后执行以下命令即可：<br>
~$:sudo chomod a+x /etc/init.d/shadowsocks<br>
~$:sudo update_rc.d shadowsocks defaults<br>
上述命令执行完成以后，进行测试<br>
~$:sudo service shadosowcks start</p>
<h4 id="配置代理">配置代理</h4>
<p>上一步的目的是建立了shadowsocks服务的本地客户端，socks5流量会走该通道，但是浏览器的网页的流量是https的，我们需要配置相应的代理，将https流量转换为socks5流量，走ss客户端到达ss服务端。当然，也可以把其他各种流量，如tcp,udp等各种流量都转换为socks5流量，这个可以通过全局代理实现，也可以通过添加特定的代理规则实现。</p>
<h5 id="配置全局代理">配置全局代理</h5>
<p>如下图所示，添加ubuntu socks5系统代理：</p>
<p>然后就可以成功上网了。</p>
<h5 id="使用switchyomega配置chrome代理">使用SwitchyOmega配置chrome代理</h5>
<p>首先到 <a href="https://github.com/FelisCatus/SwitchyOmega/releases" target="_blank" rel="noopener">https://github.com/FelisCatus/SwitchyOmega/releases</a> 下载SyitchyOmega.crx。然后在chrome的地址栏输入chrome://extensions，将刚才下载的插件拖进去。<br>
然后在浏览器右上角就有了这个插件，接下来配置插件。如下图：<br>
<img src="https:" alt="mxx"><br>
直接配置proxy，添加如图所示的规则，这样chrome打开的所有网站都是走代理的。</p>
<h4 id="使用privoxy让terminal走socks5">使用privoxy让terminal走socks5</h4>
<p>~$:sudo apt install privoxy<br>
~$:sudo vim /etc/privoxy/config<br>
取消下列行的注释，或者添加相应条目<br>
forward-socks5 / 127.0.0.1:1080 . # SOCKS5代理地址<br>
listen-address 127.0.0.1:8118     # HTTP代理地址<br>
forward 10.*.*.*/ .               # 内网地址不走代理<br>
forward .abc.com/ .             # 指定域名不走代理<br>
重启privoxy服务<br>
~$:sudo service privoxy restart<br>
在bashrc中添加如下环境变量<br>
export http_proxy=&quot;<a href="http://127.0.0.1:8118" target="_blank" rel="noopener">http://127.0.0.1:8118</a>&quot;<br>
export https_proxy=“<a href="http://127.0.0.1:8118" target="_blank" rel="noopener">http://127.0.0.1:8118</a>”</p>
<p>~$:source ~/.bashrc<br>
~$:curl.gs</p>
<h2 id="参考文献">参考文献</h2>
<ol>
<li><a href="http://godjose.com/2017/06/14/new-article/" target="_blank" rel="noopener">http://godjose.com/2017/06/14/new-article/</a></li>
<li><a href="https://www.polarxiong.com/archives/%E6%90%AD%E5%BB%BAipv6-VPN-%E8%AE%A9ipv4%E4%B8%8Aipv6-%E4%B8%8B%E8%BD%BD%E9%80%9F%E5%BA%A6%E6%8F%90%E5%8D%87%E5%88%B0100M.html" target="_blank" rel="noopener">https://www.polarxiong.com/archives/搭建ipv6-VPN-让ipv4上ipv6-下载速度提升到100M.html</a></li>
<li><a href="https://blog.csdn.net/li1914309758/article/details/86510127" target="_blank" rel="noopener">https://blog.csdn.net/li1914309758/article/details/86510127</a></li>
<li><a href="https://blog.csdn.net/blackfrog_unique/article/details/60320737" target="_blank" rel="noopener">https://blog.csdn.net/blackfrog_unique/article/details/60320737</a></li>
<li><a href="https://blog.csdn.net/qq_31851531/article/details/78410146" target="_blank" rel="noopener">https://blog.csdn.net/qq_31851531/article/details/78410146</a></li>
</ol>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://mxxhcm.github.io/2019/01/06/bayesian-networks/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="马晓鑫爱马荟荟">
      <meta itemprop="description" content="记录硕士三年自己的积累">
      <meta itemprop="image" content="/images/favicon.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="mxxhcm's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/01/06/bayesian-networks/" class="post-title-link" itemprop="http://mxxhcm.github.io/page/14/index.html">Bayesian Networks</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-01-06 14:32:55" itemprop="dateCreated datePublished" datetime="2019-01-06T14:32:55+08:00">2019-01-06</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-08-30 14:31:44" itemprop="dateModified" datetime="2019-08-30T14:31:44+08:00">2019-08-30</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/机器学习/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="介绍">介绍</h2>
<p>贝叶斯网络是一个有向无环图(directed acyclic graphs)，它用节点代表随机变量，用边代表变量之间的依赖关系。</p>
<h2 id="意义">意义</h2>
<p>贝叶斯网络可以用来表示任意的联合分布。</p>
<h2 id="推理">推理</h2>
<p>贝叶斯网络的一个基本任务就是求后验概率。<br>
在AI这本书中，贝叶斯网络中的变量被分为了证据变量(evidence variable)，隐变量(hidden variable)和查询变量(query variable)。<br>
而在PRML这本书中，贝叶斯网络中的变量被分为了观测变量(observed variable)和隐变量(latent variable,hidden variable)。</p>
<p>具体的可以看另外两篇笔记有详细的记录。</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://mxxhcm.github.io/2019/01/06/AI-chapter-14-Probabilistic-reasoning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="马晓鑫爱马荟荟">
      <meta itemprop="description" content="记录硕士三年自己的积累">
      <meta itemprop="image" content="/images/favicon.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="mxxhcm's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/01/06/AI-chapter-14-Probabilistic-reasoning/" class="post-title-link" itemprop="http://mxxhcm.github.io/page/14/index.html">AI chapter 14 Probabilistic reasoning</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-01-06 14:32:16" itemprop="dateCreated datePublished" datetime="2019-01-06T14:32:16+08:00">2019-01-06</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-08-30 13:42:18" itemprop="dateModified" datetime="2019-08-30T13:42:18+08:00">2019-08-30</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/机器学习/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>在这里加一些自己的总结，这一章主要讲的是贝叶斯网络，首先介绍了贝叶斯网络的定义，是一个有向无环图，节点代表随机变量，边代表因果关系。这里给出了贝叶斯公式的两个意义，一个是数值意义，用贝叶斯网络表示全概率分布，另一个是拓扑意义，给定某个节点的父节点，这个节点条件独立于所有它的非后裔节点，或者给定某个节点的马尔科夫毯，这个节点条件独立于所有其他节点。接下来讲了条件独立的高效表示，噪音或模型表示离散型父节点和离散型子节点之间的关系，用参数化模型表示连续型父节点和连续型子节点之间的关系，用probit模型或者logit模型表示连续型父节点和离散型子节点之间的关系。接下来就介绍了贝叶斯精确推理计算后验分布的集中方法，一种是枚举推理，一种是消元法。因为精确推理的复杂度太高了，没有实际应用价值，所以就给出了一些估计推理的方法，直接采样，拒绝采样，以及可能性加权，还有另一类采样方法，蒙特卡洛算法，主要介绍了吉布森采样，大概就是这些。后面的两个小节没有看。</p>
<p>第$13$章讲的是概率论的基础知识并且强调了在概率表示中独立(independence)和条件独立(conditional independence)之间的关系。本章引入了一个系统的方式–贝叶斯网络去表现独立和条件独立之间的关系。概括的来说，本章的内容可以分为以下五部分：</p>
<ol>
<li>首先定义了贝叶斯网络的语法(syntax)和语义(semantics)，并且展示了如何用贝叶斯网络表示不确定知识。</li>
<li>接下来介绍了概率推理在最坏的情况下是很难计算的(computionally intratable)，但是在很多情况下可以高效的完成。</li>
<li>介绍了一系列在精确推理(exact inference)不可行时可以采用的估计推理算法(approximate inference algorithms)。</li>
<li>介绍了一些在概率论中可以被应用到带对象和关系的世界的方法，即与命题，表示相对的一阶模型。</li>
<li>最后，介绍了一些其它不确定性推理的方法。</li>
</ol>
<h2 id="不确定域的知识表示-representing-knowledge-in-an-uncertain-domain">不确定域的知识表示(Representing knowledge in an uncertain domain)</h2>
<p>我们可以根据联合概率分布(full joint probability distribution)算出任何想要的概率值，但是随着随机变量个数的增加，联合概率分布可能会变得特别大。此外，一个一个的指定可能世界中的概率是不可行的。</p>
<h3 id="贝叶斯网络的定义">贝叶斯网络的定义</h3>
<p>如果在联合概率中引入独立和条件独立，将会显著的减少定义联合概率分布所需要的概率。所以这节就介绍了贝叶斯网络来表示变量之间的依赖关系。本质上贝叶斯网络可以表示任何联合概率分布，而且在很多情况下是非常精确地表示。一个贝叶斯网络是一个有向图，图中的节点包含量化后的概率信息。具体的说明如下：</p>
<ol>
<li>每一个节点对应一个随机变量，这个随机变量可以是离散的也可以是连续的。</li>
<li>有向边或者箭头连接一对节点。如果箭头是从节点$X$到节点$Y$，那么节点$X$称为节点$Y$的父节点。图中不能有环，因此贝叶斯网络是一个有向无环图(directed acyclic graph,DAG)。</li>
<li>每一个节点$X_i$有一个条件概率分布$P(x_i|Parents(X_i))$量化(quantifiy)父节点对其影响。</li>
</ol>
<p>网络的拓扑，即节点和边的集合，指定了条件概率分布之间的关系。箭头的直观意义是节点$X$对节点$Y$有直接的影响，$Y$发生的原因是其父节点的影响。通常对于一个领域(domain)的专家来说，指出该域受哪些因素的直接影响要比直接给出它的概率值简单的多。一旦贝叶斯网络的拓扑结构定了，给出一个变量的父节点，我们仅仅需要给出每个节点的条件概率分布。我们能看出，拓扑和条件概率的组合能计算出所有变量的联合概率分布。</p>
<h3 id="贝叶斯网络的示例">贝叶斯网络的示例</h3>
<h4 id="牙疼和天气">牙疼和天气</h4>
<p>给定一组随机变量牙疼(Toothache)，蛀牙(Cavity)，拔牙(Catch)和天气(Weather)。Weather是独立于另外三个随机变量的，此外，给定Cavity，Catch和Toothache是条件独立的，即给定Cavity，Catch和Toothache是相互不受影响的，如下图所示。正式的：给定Cavity，Toochache和Catch是条件独立的，图中Toothache和Catch之间缺失的边体现出了条件独立。直观上，网络表现出Cavity是Toothache和Catch发生的直接原因，然而在Toothache和Catch之间没有直接的因果关系。<br>
<img src="/2019/01/06/AI-chapter-14-Probabilistic-reasoning/" alt="figure 14.1"></p>
<h4 id="警报和打电话">警报和打电话</h4>
<p>我家里有一个新安装的防盗警报(burglar alarm)，这个警报对于小偷的检测是相当可靠的，但是也会对偶然发生的微小的地震响应。我有两个邻居(Mary和John)，他们听到警报后会打电话给我。John有时会把电话铃和警报弄混了，也会打电话。Mary听音乐很大声，经常会错过警报。现在给出John或者Mary谁是否打电话，估计警报响了的概率。<br>
<img src="/2019/01/06/AI-chapter-14-Probabilistic-reasoning/" alt="figure 14.2"><br>
该例子的贝叶斯网络如上图所示。该网络体现了小偷和地震两个因素会直接影响警报响的概率，但是John和Mary会不会打电话只取决于警报有没有响。贝叶斯网络展示出了我们的假设，即John和Mary不直接观察小偷有没有来，也不直接观察小的地震是否，也不受之前是否打过电话的影响。上图中的条件概率分布以一个条件概率分布表(conditional probability table,CPT)的形式展现了出来。这个表适合离散型的随机变量，但是不适合连续性随机变量。没有父节点的节点只有一行，用来表示随机变量的可能取值的先验概率(prior probabilities)。<br>
注意到这个网络中没有节点对应Mary听音乐很大时，也没有节点对应John把电话铃声当成了警报。事实上这些因素都被包含在和边Alarm到JohnCalls和MaryCall相关的不确定性中了，概率包含了无数种情况可能让警报失灵（停电，老鼠咬坏了，等等）或者John和Mary没有打电话的原因（吃饭去了，午睡了，休假了等等），这些不确定性都包含在了概率中了。</p>
<h2 id="贝叶斯网络的意义-the-semantics-of-bayesian-networks">贝叶斯网络的意义(the semantics of bayesian networks)</h2>
<p>上一节主要讲的是什么是贝叶斯网络，但是没有讲它的意义。本节主要给出两种方式可以理解贝叶斯网络的意义。第一个是一种数值化的意义，即&quot;numerical semantics&quot;，把它当成联合概率分布的一种表示形式。第二个是一种拓扑的意义，即&quot;topological semantics&quot;，将它看成条件独立的一种编码方式。事实上，这两种方式是等价的，但是第一种方式更有助于理解如何构建贝叶斯网络，第二种方式更有助于设计推理过程。</p>
<h3 id="贝叶斯网络表示联合分布-representing-the-full-joint-distribution">贝叶斯网络表示联合分布(Representing the full joint distribution)</h3>
<h4 id="定义">定义</h4>
<p>一个贝叶斯网络是一个有向无环图，并且每个节点都有一个数值参数。数值方式给出这个网络的意义是，它代表了所有变量的联合概率分布。之前说过节点上的值代表的是条件概率分布$P(X_i|Parents(X_i)$，这是对的，但是当赋予整个网络意义以后，这里我们认为它们只是一些数字$\theta(X_i|Parents(X_i)$。<br>
联合概率中的一个具体项(entry)表示的是每一个随机变量取某个值的联合概率，如$P(X_1=x_1 \wedge \cdots\wedge X_n = x_n)$，缩写为$P(x_1,\cdots,x_n)$。这个项的值可以通过以下公式进行计算：<br>
$$P(x_1,\cdots,x_n) = \prod_{i=1}^n \theta(x_i|parents(X_i)),$$<br>
其中$parents(X_i)$表示节点$X_i$在$x_1,\cdots,x_n$中的父节点。因此，联合概率分布中的每一项都可以用贝叶斯网络中某些条件概率的乘积表示。从定义中可以看出，很容易证明$\theta(x_i|parents(X_i))$就是条件概率$P(x_i|parents(X_i))$，因此，我们可以把上式写成：<br>
$$P(x_1,\cdots,x_n) = \prod_{i=1}^n P(x_i|parents(X_i)),$$<br>
换句话说：根据上上个式子定义的贝叶斯网络的意义，我们之前叫的条件概率表真的是条件概率表。（这句话。。。）</p>
<h4 id="示例">示例</h4>
<p>我们可以计算出警报响了，但是没有小偷或者地震发生，John和Mary都打电话了的概率。即计算联合分布$P(j,m,a,\neg b, \neg e)$（使用小写字母表示变量的值）：<br>
\begin{align*}<br>
P(j,m,a,\neg b, \neg e) &amp;=P(j|a)P(m|a)P(a|\neg b \wedge \neg e)P(\neg b)P(\neg e)\<br>
&amp;=0.90\times 0.70\times 0.001 \times 0.999 \times 0.998\<br>
&amp;=0.000628<br>
\end{align*}</p>
<h4 id="构建贝叶斯网络-constructing-bayesian-networks">构建贝叶斯网络(Constructing Bayesian networks)</h4>
<p>上面给出了贝叶斯网络的一种意义，接下来给出如何根据这种意义去构建一个贝叶斯网络。确定的条件独立可以用来指导网络拓扑的构建。首先，我们把联合概率的项用乘法公式写成条件概率表示：<br>
$$P(x_1,\cdots,x_n) = P(x_n|x_{n-1},\cdots,x_1)P(x_{n-1},\cdots,x_1)$$<br>
接下来重复这个过程，将联合概率(conjunctive probability)分解成一个条件概率和一个更小的联合概率。最后得到下式：<br>
\begin{align*}<br>
P(x_1,\cdots,x_n) &amp;= P(x_n|x_{n-1},\cdots,x_1)P(x_{n-1}|,x_{n-2}\cdots,x_1)\cdots P(x_2|x_1)P(x_1)\<br>
&amp;= \prod_{i=1}^nP(x_i|x_{i-1},\cdots,x_1)<br>
\end{align*}<br>
这个公式被称为链式法则，它对于任意的随机变量集都成立。对于贝叶斯网络中的每一个变量$X_i$，如果给定$Parents(X_i) \subset {X_{i-1},\cdots,X_1}$（每一个节点的序号应该和图结构的偏序结构一致），那么有：<br>
$$P(x_1,\cdots,x_n) = \prod_{i=1}^n P(x_i|parents(X_i)),$$<br>
将它和上式对比，得出：<br>
$$P(X_i|X_{i-1},\cdots,X_1) = P(X_i|Parents(X_i).$$<br>
这个公式成立的条件是给定每个节点的父节点，它条件独立于所有它的非父前置节点。这里给出一个生成贝叶斯网络的方式：</p>
<ol>
<li>节点：首先，确定需要对领域建模所需要的随机变量集合。对它们进行排序：${X_1,\cdots,X_n}$，任意顺序都行，但是如果随机变量的因(causes)在果(effects)之前，最终的结果会更加紧凑。</li>
<li>边：从$i = 1$到$n$，</li>
</ol>
<ul>
<li>从$X_1,\cdots,X_{i-1}$中选出$X_i$的最小父节点集合。</li>
<li>对于每一个父节点，插入一条从父节点到$X_i$的边。</li>
<li>写下条件概率表，$P(X_i| Parents(X_i))$。</li>
</ul>
<p>直观上，$X_i$的父节点应该包含$X_1,\cdots,X_{i-1}$中所有直接影响$X_i$的节点。因为每一个节点都只和它前面的节点相连，这就保证了每个网络都是无环的(acyclic)。此外，贝叶斯网络还不包含冗余的概率值，如果有冗余值，就会产生不一致：不可能生成一个违反概率论公理的贝叶斯网络。</p>
<h4 id="紧凑性和节点顺序-compactness-and-node-ordering">紧凑性和节点顺序(Compactness and node ordering)</h4>
<h5 id="紧凑性-compactness">紧凑性(compactness)</h5>
<p>因为不包含冗余信息，贝叶斯网络会比联合概率分布更加紧凑，这让它能够处理拥有很多变量的任务。贝叶斯网络的紧凑性是稀疏(sparse)系统或者局部结构化(local structured)系统普遍拥有的稀疏性的一个例子。在一个局部结构化系统中，每一个子部件仅仅和有限数量的其他部件进行交互，而不用管整个系统。局部结构化的复杂度通常是线性增加的而不是指数增加的。在贝叶斯网络中，一个随机变量往往最多受$k$个其他随机变量直接影响，这里的$k$是一个常数。为了简化问题，我们假设有$n$个布尔变量，指定一个条件概率表所需要的数字最多是$2<sup>k$个，整个网络则需要$n2</sup>k$个值；作为对比，联合概率分布需要$2^n$个值。举个例子，如果我们有$n=30$个节点，每一个节点至多有五个父节点(k=5)，那么贝叶斯网络只需要$960$个值，而联合概率分布需要超过十亿个值。<br>
但是在某些领域，可能每一个节点都会被所有其他节点直接影响，这时候网络就成了全连接的网络(fully connected)，它和联合概率分布需要同样多的信息。有时候，增加一条边，也就是一个依赖关系，可能会对结果产生影响，但是如果这个依赖很弱(tenuous)，添加这条边的花费比获得的收益还要大，那么就没有必要加这条边了。比如，警报的那个例子，如果John和Mary感受到了地震，他们认为警报是地震引起的，所以就不打电话了。是否添加Earthquake到JohnCalls和MaryCalls这两条边取决于额外的花费和得到更高的警报率之间的关系。</p>
<h5 id="节点顺序-node-ordering">节点顺序(node ordering)</h5>
<p>即使在一个局部结构化的领域，只有当我们选择好的节点顺序的时候，我们才能得到一个紧凑的贝叶斯网络。考虑警报的例子，我们给出下图：<br>
<img src="/2019/01/06/AI-chapter-14-Probabilistic-reasoning/" alt="figure 14.3"><br>
Figure 14.2和Figure 14.3两张图中的三个贝叶斯网络表达的都是同一个联合分布，但是Figure 14.3中的两张图没有表现出来条件独立，尤其是Figure 14.3(b)中的贝叶斯网络，它需要用和联合分布差不多相同个数的值才能表现出来。可以看出来，节点的顺序会影响紧凑性。</p>
<h3 id="贝叶斯网络中的条件独立-conditional-independence-relations-in-bayesion-networks">贝叶斯网络中的条件独立(Conditional independence relations in Bayesion networks)</h3>
<p>贝叶斯网络的一个数值意义(“numerical” semantics)是用来表示联合概率分布。根据这个意义，给定每个节点的父节点，使得每一个节点条件独立于它的父节点之外的节点，我们能构建一个贝叶斯网络。此外我们也可以从用图结构编码整个条件独立关系的拓扑意义出发，然后推导出贝叶斯网络的数值意义。拓扑语义说的是给定每个节点的父节点，则该节点条件独立于所有它的非后裔(non-descendants)节点。举例来说，Figure 14.2的警报例子中，给定alarm后，JohnCalls独立于Burglary,Eqrthquake和MaryCalls。如图Figuree 14.4(a)中所示。从条件独立断言(assertions)和网络参数$\theta(x_i|parents(X_i))$就是条件概率$P(x_i|parents(X_i))$的解释中，联合概率可以计算出来。在这种情况下，数值意义和拓扑语义是相同的。<br>
另一个拓扑意义的重要属性是：给定某个节点的马尔科夫毯(Markov blanket)，即节点的父节点，子节点，子节点的父节点，这个节点条件独立于所有其他的节点。如图Figure 14.4(b)所示。</p>
<h2 id="条件分布的高效表示-efficient-representation-of-conditional-distributions">条件分布的高效表示(Efficient representation of conditional distributions)</h2>
<p>即使每个节点有$k$个父节点，一个节点的CPT还需要$O(2^k)$，最坏的情况下父节点和子节点是任意连接的。一般情况下，这种关系可以用符合一些标准模式(standard pattern)的规范分布(canonical distribution)表示，这样子就可以仅仅提供分布的一些参数就能生成整个CPT。<br>
最简单的例子是确定性节点(deterministic node)。一个确定性节点的值被它的父节点的值精确确定。这个确定性关系可以是逻辑关系：父节点是加拿大，美国和墨西哥，子节点是北美洲，它们之间的关系是子节点是父节点所在的洲。这个关系也可以是数值型的，一条河的流量是流入它的流量减去流出它的流量。<br>
不确定关系通常称为噪音逻辑关系(noisy logical relationships)。一个例子是噪音或(noisy-OR)，它是逻辑或的推广。在命题逻辑中，当且仅当感冒(Cold)，流感(Flu)或者疟疾(Malaria)是真的时候，发烧(Fever)才是真的。噪音或模型允许不确定性，即每一个父节点都有可能让子节点为真，可能父节点和子节点之间的关系被抑制了(inhibited)，可能一个人感冒了，但是没有表现出发烧。这个模型做了两个假设。第一个，它假设所有的原因都被列了出来，有时候会加一个节点(leak node)包含所有的其他原因(miscellaneous causes)。第二个，抑制每一个父节点和子节点之间的原因是独立的，比如抑制疟疾产生发烧和抑制感冒产生发烧的原因是独立的。所以，当且仅当所有的父节点都是假的时候，发烧才一定不会发生。给出以下的假设：<br>
$q_{cold} = P(\neg fever| cold,\neg flu, \neg malaria) = 0.6$<br>
$q_{flu} = P(\neg fever|\neg cold, flu, \neg malaria) = 0.2$<br>
$q_{malaria} = P(\neg fever|\neg cold,\neg flu, malaria) = 0.1$<br>
根据这些信息，以及噪音或的假设，整个CPT可以被创建。一般的规则是：<br>
$P(x_i|parents(X_i)) = 1 - \prod_{j:X_j=ture} q_j.$<br>
最后生成如下的表：</p>
<table>
<thead>
<tr>
<th style="text-align:center">Cold</th>
<th style="text-align:center">Flu</th>
<th style="text-align:center">Malaria</th>
<th style="text-align:center">P(Fever)</th>
<th style="text-align:center">P($\neg$Fever)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">F</td>
<td style="text-align:center">F</td>
<td style="text-align:center">F</td>
<td style="text-align:center">$0.0$</td>
<td style="text-align:center">$1.0$</td>
</tr>
<tr>
<td style="text-align:center">F</td>
<td style="text-align:center">F</td>
<td style="text-align:center">T</td>
<td style="text-align:center">$0.9$</td>
<td style="text-align:center">$0.1$</td>
</tr>
<tr>
<td style="text-align:center">F</td>
<td style="text-align:center">T</td>
<td style="text-align:center">F</td>
<td style="text-align:center">$0.8$</td>
<td style="text-align:center">$0.2$</td>
</tr>
<tr>
<td style="text-align:center">F</td>
<td style="text-align:center">T</td>
<td style="text-align:center">T</td>
<td style="text-align:center">$0.98$</td>
<td style="text-align:center">$0.1\times 0.2=0.02$</td>
</tr>
<tr>
<td style="text-align:center">T</td>
<td style="text-align:center">F</td>
<td style="text-align:center">F</td>
<td style="text-align:center">$0.4$</td>
<td style="text-align:center">$0.6$</td>
</tr>
<tr>
<td style="text-align:center">T</td>
<td style="text-align:center">F</td>
<td style="text-align:center">T</td>
<td style="text-align:center">$0.94$</td>
<td style="text-align:center">$0.6\times 0.1 = 0.06 $</td>
</tr>
<tr>
<td style="text-align:center">T</td>
<td style="text-align:center">T</td>
<td style="text-align:center">F</td>
<td style="text-align:center">$0.88$</td>
<td style="text-align:center">$0.5\times 0.2 = 0.12 $</td>
</tr>
<tr>
<td style="text-align:center">T</td>
<td style="text-align:center">T</td>
<td style="text-align:center">T</td>
<td style="text-align:center">$0.988$</td>
<td style="text-align:center">$0.6\times 0.2\times 0.1 = 0.012$</td>
</tr>
</tbody>
</table>
<p>对于这个表，感觉自己一直有点转不过来圈。就是有症状不一定发烧，也可能不发烧，没有症状一定不发烧。什么时候不发烧呢，只有某个症状表现出来不发烧，如果多个症状的话，直接把有症状表现但不发烧的概率相乘。<br>
一般情况下，噪声逻辑模型中，有$k$个父节点的变量可以用$O(k)$个参数表示而不是$O(2^k)$去表示整个CPT。这让访问(assessment)和学习(learning)更容易了。</p>
<h3 id="连续性随机变量的贝叶斯网络-bayesian-nets-with-continuous-variables">连续性随机变量的贝叶斯网络(Bayesian nets with continuous variables)</h3>
<h4 id="常用方法">常用方法</h4>
<p>现实中很多问题都是连续型的随机变量，它们有无数可能的取值，所以显式的指定每一个条件概率行不通。常用的总共有三种方法，第一个可能的方法是离散(discretization)连续型随机变量，将随机变量的可能取值划分成固定的区间。比如，温度可以分成，小于$0$度的，$0$度到$100$度之间的，大于$100$度的。离散有时候是可行的，但是通常会造成精度的缺失和非常大的CPT。第二个方法也是最常用的方法是通过指定标准概率密度函数的参数，比如指定高斯分布的均值和方差。第三种方法是非参数化(nonparametric)表示，用隐式的距离去定义条件分布。</p>
<h4 id="示例-v2">示例</h4>
<p>一个同时拥有离散型和随机性变量的网络被称为混合贝叶斯网络(hybrid Bayesian network)。为了创建这样一个网络，我们需要两种新的分布。一种是给定离散或者连续的父节点，子节点是连续型随机变量的条件概率，另一种是给定连续的父节点，子节点是离散型随机变量的条件概率。</p>
<h5 id="连续型子节点">连续型子节点</h5>
<p><img src="/2019/01/06/AI-chapter-14-Probabilistic-reasoning/" alt="figure 14.5"><br>
考虑Figure 14.5的例子，一个顾客买了一些水果，买水果的量取决取水果的价格(Cost)，水果的价格取决于收成(Harvest)和政府是否有补助(Subsidy)。其中，Cost是连续型随机变量，他有连续的父节点Harvest和离散的父节点Subsidy，Buys是离散的，有一个连续型的父节点Cost。<br>
对于变量Cost，我们需要指定条件概率$P(Cost|Subsidy,Harvest)$。离散的父节点通过枚举(enumeration)来表示，指定$P(Cost|subsidy,Harvest)$和$P(Cost|\neg subsidy,Harvest)$。为了表示Harvest，可以指定一个分布来表示变量Cost的值$c$取决于连续性随机变量Harvest的值$h$。换句话说，将$c$看做一个$h$的函数，然后给出这个函数的参数即可，最常用的是线性高斯分布。比如这里，我们可以用两个不同参数的高斯分布来表示有补贴和没补贴时Harvest对Cost的影响：<br>
$$P(c|h, subsidy) = N(a_th+b_t,\sigma_t^2)© = \frac{1}{\sigma_t \sqrt{2\pi} }e^{- \frac{1}{2}(\frac{c-(a_th+b_t)}{\sigma_t})^2}$$<br>
$$P(c|h,\neg subsidy) = N(a_fh+b_t,\sigma_f^2)© = \frac{1}{\sigma_f \sqrt{2\pi} }e^{- \frac{1}{2}(\frac{c-(a_fh+b_f)}{\sigma_f})^2}$$<br>
所以，只需要给出$a_t,b_t,\sigma_t,a_f,b_f,\sigma_f$这几个参数就行了，Figure 14.6(a)和(b)就是一个示例图。注意到坡度(slope)是负的，因为随着供应的增加，cost在下降，当然，这个线性模型只有在harvest在很小的一个区间内才成立，而且cost有可能为负。假设有补贴和没补贴的两种可能性相等，是$0.5$，那么就有了Figure 14.6©的图$P(c|h)$。</p>
<h5 id="连续型父节点">连续型父节点</h5>
<p>当离散型随机变量有连续型父节点时，如Figure 14.5中的Buys节点。我们有一个合理的假设是：当cost高的时候，不买，cost底的时候，买，在中间区域买不买是一个变化很平滑的概率。我们可以把条件分布当成一个软阈值函数(soft-threshold)，一种方式是用标准正态分布的积分(intergral)。<br>
$$\Phi(x) = \int_{-\infty}^{x} N(0,1)(x)dx$$<br>
给定Cost买的概率可能是:<br>
$$P(buys|Cost = c) = \Phi((-x+\nu)/ \sigma))$$<br>
其中cost的阈值在$\nu$附近，阈值的区域和正比于$\sigma$，当价格升高的时候，买的概率会下降。这个probit distribution模型如Figure 14.7(a)所示。<br>
另一个可选择的模型是logit distribution，使用logistic function $1/(1+e^{-x})$来生成一个软阈值：<br>
$$P(buys|Cost = c) = \frac{1}{1+exp(-2\frac{-c+u}{\sigma})}.$$<br>
如Figure 14.7(b)所示，这两个分布很像，但是logit有更长的尾巴。probit更符合实际情况，但是logit数学上更好算。它们都可以通过对父节点进行线性组合推广到多个连续性父节点的情况。</p>
<h2 id="贝叶斯网络的精确推理-exact-inference-in-bayesian-networks">贝叶斯网络的精确推理(Exact inference in bayesian networks)</h2>
<p>概率推理系统的基本任务就是给出一些观察到的事件，即给证据变量(evidence variable)赋值，然后计算一系列查询变量(query variable)的后验概率。我们用$X$表示查询变量，用$\mathbf{E}$表示证据变量$E_1,\cdots,E_m$的集合，$\mathbf{e}$是一个特定的观测事件，$\mathbf{Y}$表示既不是证据变量，也不是查询变量的变量$Y_1,\cdots,Y_l$的集合（隐变量,hidden variables)。变量的所有集合是$\mathbf{X}={X}\cup \mathbf{E}\cup \mathbf{Y}$。一个典型的查询是求后验概率$P(X|\mathbf{e})$。<br>
在这一节中主要讨论的是计算后验概率的精确算法以及这些算法的复杂度。事实上，在一般情况下精确推理的复杂度都是很高的，为了降低复杂度，就只能进行估计推理(approximate inference)了，这个会在下一节中介绍到。</p>
<h3 id="枚举实现精确推理-inference-by-enumeration">枚举实现精确推理(Inference by enumeration)</h3>
<p>任何条件概率都可以用联合概率分布的项相加得到，即：<br>
$$P(X|\mathbf{e}) = \alpha P(X,\mathbf{e}) = \alpha \sum_{\mathbf{y}}P(X,\mathbf{e},\mathbf{y})$$<br>
贝叶斯网络给出了所有的联合概率分布，任何项$P(x,\mathbf{e},\mathbf{y})$都可以用贝叶斯网络中的条件概率的乘积表示出来。比如警报例子中的查询$P(Burglary|JohnCalls=true,MaryCalls=true)$。隐变量是Earthquake和Alarm，我们可以算出：<br>
$$P(B|j,m) = \alpha P(B,j,m) = \alpha \sum_{e}\sum_{a}P(B,j,m,e,a).$$<br>
贝叶斯网络已经给出了所有CPT项的表达式，比如当Burglary = true时：<br>
$$P(b|j,m) = \alpha \sum_e\sum_aP(b,j,m,e,a) = \alpha \sum_e\sum_aP(b)P(e)P(a|b,e)P(j|a)P(m|a).$$<br>
为了计算这个表达式，我们得计算一个四项的加法，分别是e为true和false,a为true和false对应的$P(b,j,m)$的值，每一项都是五个数的乘法。最坏的情况下，所有的变量都用到了，那么拥有$n$个布尔变量的贝叶斯网络的时间复杂度是$O(n2^n)$。我们可以做一些简化，将一些重复的计算保存下来，比如将上面的式子变成：<br>
$$P(b|j,m) = \alpha \sum_e\sum_aP(b,j,m,e,a) = \alpha P(b) \sum_eP(e)\sum_aP(a|b,e)P(j|a)P(m|a).$$<br>
这样子可以按照顺序进行计算，具体的计算过程如Figure 14.8所示。这种算法叫做ENUMERATION-ASK，它的空间复杂度是线性的，但是它的事件复杂度是$O(2<sup>n)$比$O(n2</sup>n)$要好，却仍然是实际上不可行的。（这里我理解的是$O(2<sup>n)$而不是$O(n2</sup>n)$的原因是，总共有$n$个布尔变量，所以总共有$2^n$个可能的取值，每次算一个，存一个，而原来的是算完之后不存。）<br>
事实上，Figure 14.8中的计算过程还有很多重复计算，比如$P(j|a)P(m|a)$和$P(j|\neg a)P(m|\neg a)$这两项被计算了两次。我原来在想这里是不是和上面一段说的冲突了，事实上是没有的，这$2^n$个值，其中可能会有$P(b,j,m,e,a)$和$P(b,j,m,e,\neg a)$，这两个概率中都用到了$P(j|a)P(m|a)$，但是这里就会计算两次，事实上有很多值都会被重复计算很多次。下面就介绍一个避免这种运算的方法。</p>
<h3 id="消元法-the-variable-elimination-algorithm">消元法(The variable elimination algorithm)</h3>
<p>上面问题的解决思路就是保存已经计算过的值，实际上这是一种动态规划。还有很多其他方法可以解决这个问题，这里介绍了最简单的消元算法。消元法对表达式进行从右至左的计算，而枚举法是自底向上的。所有的中间值被报存起来，最对和每个变量有关的表达式进行求和。例如对于下列表达式：<br>
$$P(B|j,m) = \alpha \underbrace{P(B)}<em>{f_1(B)} \sum_e\underbrace{P(e)}</em>{f_2(E)} \sum_a\underbrace{P(a|B,e)}<em>{f_3(A,B,E)} \underbrace{P(j|a)}</em>{f_4(A)} \underbrace{P(m|a)}_{f_5(A)}.$$<br>
表达式的每一部分都是一个新的因子，每一个因子都是由它的参数变量(argument variables)决定的矩阵，参数变量指定的取值是没有固定的变量。比如因子$f_4(A)$和$f_5(A)$对应$P(j|a)$和$P(m|a)$的表达式只取决于$A$的值因为$J$和$M$在这个查询中都是固定的。它们都是两个元素的向量：<br>
$$f_4(A) = \begin{pmatrix}P(j|a)\P(j|\neg a)\end{pmatrix} = \begin{pmatrix}0.90\0.05\end{pmatrix}$$<br>
$$f_5(A) = \begin{pmatrix}P(m|a)\P(m|\neg a)\end{pmatrix} = \begin{pmatrix}0.70\0.01\end{pmatrix}$$<br>
$f_3(A,B,E)$是一个$2\times 2\times 2$的矩阵。用因子表达的话，查询的表达式变成了：<br>
$$P(B|j,m) = \alpha f_1(B)\times \sum_ef_2(E)\times \sum_af_3(A,B,E)\times f_4(A)\times f_5(A)$$<br>
其中$\times$不是普通的矩阵乘法，而是对应元素相乘(pointwise product)。整个表达式的计算过程可以看成从右到左变量相加的过程，将现有的因子消去产生新的因子，最后只剩下一个因子的过程。具体的步骤如下：<br>
首先先利用$f_3,f_4,f_5$把变量$A$消掉，产生一个新的$2\times 2$的只含有变量$B$和$E$的新因子$f_6(B,E)$：<br>
\begin{align*}<br>
f_6(B,E) &amp;= \sum_af_3(A,B,E)\times f_4(A) \times f_5(A)\<br>
&amp;= (f_3(a,B,E)\times f_4(a) \times f_5(a)) + (f_3(\neg a,B,E)\times f_4(\neg a)\times f_5(\neg a)<br>
\end{align*}<br>
这样目标变成了：<br>
$$P(B|j,m) = \alpha f_1(B)\times \sum_ef_2(E)\times \sum_af_6(B,E)$$<br>
利用$f_2,f_6$消去$E$：<br>
\begin{align*}<br>
f_7(B) &amp;= \sum_ef_2(E)\times \sum_af_6(B,E)\<br>
&amp; = f_2(e)\times f_6(B,e) + f_2(\neg e)\times f_6(B,\neg e)<br>
\end{align*}<br>
将表达式化成：<br>
$$P(B|j,m) = \alpha f_1(B)\times f_7(B)$$<br>
显然，根据这个表达式就可以计算出我们想要的结果了。上面的过程可以总结成两步，第一步是point-wise的因子乘法，第二步是利用因子的乘法进行消元。</p>
<h4 id="因子运算-operations-on-factors">因子运算(Operations on factors)</h4>
<p>两个因子$f_1$和$f_2$进行point-wise乘法运算产生新的因子(factor)$f$的变量是$f_1$和$f_2$变量的并，新的因子中的元素的值是$f_1$和$f_2$中对应项的积。假设两个因子有公共变量$Y_1,\cdots,Y_k$，那么就有：<br>
$$f(X_1,\cdots,X_j,Y_1,\cdots,Y_k,Z_1,\cdots,Z_l)=f_1(X_1,\cdots,X_j,Y_1,\cdots,Y_k)f_2(Y_1,\cdots,Y_k,Z_1,\cdots,Z_l).$$<br>
如果所有的变量都是二值化的，那么$f_1$和$f_2$各有$2<sup>{j+l}$和$2</sup>{l+k}$项，$f$有$2^{j+l+k}$项。比如，$f_1(A,B),f_2(B,C)$，那么point-wise乘法产生的$f_3(A,B,C)=f_1\times f_2$有$8$项，如Figure 14.10所示。<br>
<img src="/2019/01/06/AI-chapter-14-Probabilistic-reasoning/" alt="figure 14.10"><br>
根据图中给出的值，消去$f_3(A,B,C)$中的$A$：<br>
\begin{align*}<br>
f(B,C) &amp;= \sum_af_3(A,B,C)\<br>
&amp;= f_3(a,B,C) + f_3(\neg a,B,C)\<br>
&amp;= \begin{pmatrix} 0.06&amp;0.24\0.42&amp;0.28\end{pmatrix} + \begin{pmatrix}0.18&amp;0.72\0.06&amp;0.04\end{pmatrix}\<br>
&amp;= \begin{pmatrix}0.24&amp;0.96\048&amp;0.32\end{pmatrix}<br>
\end{align*}<br>
产生新的因子用的是pointwise乘法，消元用的是累乘。给定pointwise乘法和消元函数，消元算法就变得很简单，一个消元算法如Figure 14.11所示。<br>
<img src="/2019/01/06/AI-chapter-14-Probabilistic-reasoning/" alt="figure 14.11"></p>
<h4 id="变量顺序和变量相关性-variable-ordering-and-variable-relevance">变量顺序和变量相关性(Variable ordering and variable relevance)</h4>
<p>Figure 14.11中的算法包含一个没有给出具体实现的排序函数Order()对要消去的变量进行排序，每一种排序选择都会产生一组有效的算法，但是不同的消元顺序会产生不同的中间因子。一般情况下，消元法的时间和空间复杂度是由算法产生的最大因子决定的，这个最大因子是由消元的顺序和贝叶斯网络的结构决定的，选取最优的消元顺序是很困难的，但是有一些小的技巧：总是消去让新产生的因子最小的变量。<br>
另一个属性是：每一个不是查询变量或者证据变量的祖先变量都和这次查询无关，在实现消元算法的时候可以把这些变量都去掉。（具体的示例可以看第十四章，在$528$页）。</p>
<h3 id="精确推理的复杂度-the-complexity-of-exact-inference">精确推理的复杂度(The complexity of exact inference)</h3>
<p>贝叶斯网络的精确推理跟网络的结构有很大的关系。<br>
Figure 14.2中警报贝叶斯网络中的复杂度是线性的。该网络中任意两个节点只有一条路径，这种网络称为单连接的(singly-connected)或者多树(polytrees)，这种结构有一个很好的属性就是：多树结构中精确推理的时间，空间复杂度对于网络大小来说都是线性关系，这里网络大小指的是CPT项的个数。如果每一个节点的父节点都是一个有界的常数，那么复杂度和节点数之间也是线性关系。<br>
对于多连接(multiply connected)的网络，如Figure 14.12(a)所示，最坏情况下，即使每一个节点的父节点个数都是有界常数，消元法的时间和空间复杂度也都是指数级别的。因为贝叶斯网络的推理也是NP难问题。<br>
<img src="/2019/01/06/AI-chapter-14-Probabilistic-reasoning/" alt="figure 14.12"></p>
<h3 id="聚类算法-clustering-algorithms">聚类算法(clustering algorithms)</h3>
<p>用消元法来计算单个的后验概率是简单而高效的，但是如果要计算网络中所有变量的后验概率是很低效的。例如：在单连接的网络中，每一个查询都是$O(n)$，总共有$O(n)$个查询，所以总共的代价是$O(n^2)$。使用聚类算法(clustering algorithms)，代价可以降到$O(n)$，因此贝叶斯网络中的聚类算法已经被广泛商用。（这里不明白为什么？）。<br>
聚类算法的基本思想是将网络中的一些节点连接成聚点(cluster nodes)，最后形成一个多树(polytree)结构。例如Figure 14.12(a)中的多连接网络可以转换成Figure 14.12(b)所示的多树，Sprinkler和Rain节点形成了SPrinkler+Rain聚点，这两个布尔变量被一个大节点(meganode)取代，这个大节点有四个可能的取值：$tt,tf,ft,ff$。一旦一个多树形式的网络生成了以后，就需要特殊的推理算法进行推理了，因为普通的推理算法不能处理共享变量的大节点，有了这样一个特殊的算法，后验概率的时间复杂度就是线性于聚类网络的大小。但是，NP问题并没有消失，如果消元需要指数级别的时间和空间复杂度，聚类网络中的CPT也是指数级别大小。</p>
<h2 id="贝叶斯网络的估计推理-approximate-inference-in-bayesian-networks">贝叶斯网络的估计推理(Approximate inference in bayesian networks)</h2>
<p>因为多连接网络中的推理是不可行的，所以用估计推理取代精确推理是很有用的。这一节会介绍随机采样算法，也叫蒙特卡洛算法(Monte Carlo)，它的精确度取决于生成的样本数量。我们的目的是采样用于计算后验概率。这里给出了两类算法，直接采样(direct sampling)和马尔科夫链采样(Markov chain sampling)。变分法(variational methods)和循环传播(loopy propagation)将会在本章的最后进行介绍。</p>
<h3 id="直接采样-direct-sampling-methods">直接采样(Direct sampling methods)</h3>
<p>任何采样算法都是通过一个已知的先验概率分布生成样本。比如一个公平的硬币，服从一个先验分布$P(coin) = &lt;0.5,0.5 &gt; $，从这个分布中采样就像抛硬币。<br>
一个最简单的从贝叶斯网络中进行随机采样的方法就是：从没有证据和它相关的网络中生成事件，即按照拓扑顺序对每一个变量进行采样。如Figure 14.13所示的算法，每一个变量的采样都取决于前之前已经采样过了的父节点变量的值。按照Figure 14.13中的算法对Figure 14.12(a)中的网络进行采样，假设一个采样顺序是[Cloudy,Sprinkler,Rain,WetGrass]：</p>
<ol>
<li>从$P(Cloudy)=&lt;0.5,0.5&gt;$中采样，采样值是true；</li>
<li>从$P(Sprinkler|Cloudy=true) = &lt;0.1,0.9&gt;$中采样，采样值是false；</li>
<li>从$P(Rain|Cloudy=true)=&lt;0.8,0.2&gt;$中采样，采样值是true；</li>
<li>从$P(WetGrass|Sprinkler=false,Rain=true)=&lt;0.9,0.1&gt;$中采样，采样值是true；</li>
</ol>
<p>这个例子中，PRIOR-SAMPLE算法返回事件[true,false,true,true]。可以看出来，PRIOR-SAMPLE算法根据贝叶斯网络指定的先验联合分布生成样本。假设$S_{PS}(x_1,\cdot,x_n)$是PRIOR-SAMPLE算法生成的一个样本事件，从采样过程中我们可以得出：<br>
$$S_{PS}(x_1,\cdots,x_n) = \prod_{i=1}^nP(x_i|parents(X_i))$$<br>
即每一步采样都只取决于父节点的值。这个式子和贝叶斯网络的联合概率分布是一样的，所以，我们可以得到：<br>
$$S_{PS} = P(x_1,\cdots,x_n).$$<br>
通过采样让这个联合分布的求解很简单。<br>
<img src="/2019/01/06/AI-chapter-14-Probabilistic-reasoning/" alt="figure 14.13"><br>
事实上在任何采样算法中，结果都是通过对产生的样本进行计数得到的。假设生成了$N$个样本，$N_{PS}(x_1,\cdots,x_n)$是样本集中的一个具体事件$(x_1,\cdots,x_n)$发生的次数。我们希望这个值比上样本总数取极限和采样概率$S_{PS}$是一样的，即：<br>
$$ lim_{N\rightarrow \infty}\frac{N_{PS}(x_1,\cdots,x_n)}{N} = S_{PS}(x_1,\cdots,x_n) = P(x_1,\cdots,x_n).$$<br>
例如之前利用PRIOR-SAMPLE算法产生的事件[true,false,true,true]，这个事件的采样概率是：<br>
$$S_{PS}(true,false,true,true) = 0.5 \times 0.9 \times 0.8 \times 0.9 = 0.324.$$<br>
即当$N$取极限时，我们希望有$32.4%$的样本都是这个事件。(这里为什么要用采样进行计算呢，我的想法是因为实际情况中，采样概率$S_{PS}$是很难计算的，就通过不断的采样，计算出某个样本出现的概率。)<br>
我们用$\approx$表示估计概率(estimated probability)在样本数量$N$取极限时和真实概率一样的估计，这叫一致(consistent)估计。比如，对于任意的含有隐变量的事件(partially spefified event)，$x_1,\cdots,x_m,m\le n$，会产生一个一致估计：<br>
$$P(x_1,\cdots,x_m)\approx N_{PS}(x_1,\cdots,x_m)/N.$$<br>
这个事件的概率可以看成所有满足观测变量条件的样本事件（隐变量所有值都可以取）比上所有样本事件的比值。比如在Spinkler网络中，生成$1000$个样本，其中有$511$个样本的Rain=true，那么rain的估计概率就是$\hat{P}(Rain=true) = 0.511.$</p>
<h4 id="贝叶斯网络的拒绝采样-rejection-sampling-in-bayesian-networks">贝叶斯网络的拒绝采样(Rejection sampling in Bayesian networks)</h4>
<h5 id="算法">算法</h5>
<p><img src="/2019/01/06/AI-chapter-14-Probabilistic-reasoning/" alt="figure 14.14"><br>
拒绝采样(rejection sampling)利用容易采样的分布来生成难采样分布的样本，计算后验概率$P(X|\mathbf{e})$，算法流程如Figure 14.14所示，首先根据贝叶斯网络的先验分布生成样本，接下来拒绝(reject)那些和证据变量不匹配的结果，最后在剩下的样本中统计每个$X=x$出现的概率，估计$\hat{P}(X|\mathbf{e}).$<br>
用$\hat{P}(X|\mathbf{e})$表示估计概率分布，利用拒绝采样算法的定义计算：<br>
$$\hat{P}(X|\mathbf{e}) = \alpha N_{PS}(X,\mathbf{e}) = \frac{N_{PS}(X,\mathbf{e})}{N_{PS}(\mathbf{e})}.$$<br>
而根据$P(x_1,\cdots,x_m)\approx N_{PS}(x_1,\cdots,x_m)/N$，就有：<br>
$$\hat{P}(X|\mathbf{e}) = \alpha N_{PS}(X,\mathbf{e}) = \frac{N_{PS}(X,\mathbf{e})}{N_{PS}(\mathbf{e})} =  \frac {P(X,\mathbf{e})}{P(\mathbf{e})} = P(X|\mathbf{e}).$$<br>
所以，拒绝采样产生了真实概率的一个一致估计(consistent estimate)，但是这个一致估计和无偏估计还不一样。</p>
<h5 id="示例-v3">示例</h5>
<p>举一个例子来说明，假设我们要估计概率$P(Rain|Sprinkler=true)$，生成了$100$个样本，其中$73$个是$Sprinkler=false$，$27$是$Sprinkler=true$，这$27$个中有$8$个$Rain=true$，有$19$个$Rain=false$，因此：<br>
$$P(Rain|Sprinkler=true)\approx NORMALIZE \lt\lt 8,19&gt;&gt; = &lt;0.296,0.704&gt;.$$<br>
正确答案是$&lt;0.3,0.7&gt;$，可以看出来，估计值和真实值差的不多。生成的样本越多，估计值就会和正确值越接近，概率的估计误差和$1/\sqrt{n}$成比例，$n$是用来估计概率的样本数量。</p>
<h5 id="不足">不足</h5>
<p>拒绝采样最大的问题是它拒绝了很多样本，随着证据变量的增加，和证据$\mathbf{e}$一致的样本指数速度减少，所以这个方法对于复杂的问题是不可行的。拒绝假设和现实生活中条件概率是很像的，比如估计观测到晚上天空是红的，第二天下雨的概率$P(Rain|RedSkyAtNight=ture)$，这个条件概率的估计就是根据日常生活的观察实现的。但是如果天空很少是红的，就需要很长时间才能估计它的值，这就是拒绝假设的缺点。</p>
<h4 id="可能性加权-likelihood-weighting">可能性加权(Likelihood weighting)</h4>
<h5 id="算法-v2">算法</h5>
<p>可能性加权(Likelihood weighting)只产生和证据$\mathbf{e}$一致的事件，因此避免了拒绝采样的低效。它是统计学中重要性采样的一个例子，专门为贝叶斯推理设计的。<br>
如Figure 14.15所示，加权似然固定证据变量$\mathbf{E}$的值，只对非证据变量进行采样，这就保证了每一个事件都是和证据一致的。但是，不是所有的事件权重都是一样的。给定每一个证据变量的父节点，它的可能性(likelihood)是证据变量的条件概率的乘积，每一个事件都根据证据的可能性进行加权。</p>
<h5 id="示例-v4">示例</h5>
<p>对于Figure 14.12(a)中的例子，计算后验概率$P(Rain|Cloudy=true,WetGrass=true)$，采样顺序是Cloudy,Sprinkler,Rain,WetGrass。过程如下，首先，权重$w$设为$1$，一个事件生成过程如下：</p>
<ol>
<li>Cloudy是一个证据变量，它的值是true,因此，令：<br>
$$w\leftarrow w\times P(cloudy=true) = 0.5.$$</li>
<li>Sprinkler是隐变量，所以从$P(Sprinkler|Cloudy=true)=&lt;0.1,0.9&gt;$中采样，假设采样结果是false；</li>
<li>Rain是隐变量，从$P(Rain|Cloudy=true)=&lt;0.8,0.2&gt;$中采样，假设采样结果是true；</li>
<li>WetGrass是证据变量，值是true,令：<br>
$$w\leftarrow w\times P(WetGrass=true|Sprinkler=false,Rain=true) = 0.45.$$</li>
</ol>
<p>所以WEIGHTED-SAMPLE算法生成事件[true,false,true,true]，相应的权重是$0.45$。</p>
<h5 id="原理">原理</h5>
<p>用$S_{WS}$表示WEIGHTED-SAMPLE算法中事件的采样概率，证据变量$\mathbf{E}$的取值$\mathbf{e}$是固定的，用$\mathbf{Z}$表示非证据变量，包括隐变量$\mathbf{Y}$和查询变量$\mathbf{X}$。给定变量$\mathbf{Z}$的父节点，算法对变量$\mathbf{Z}$进行采样：<br>
$$S_{WS}(\mathbf{z},\mathbf{e}) = \prod_{i=1}^lP(z_i|parents(Z_i)).$$<br>
其中$Parents(Z_i)$可能同时包含证据变量和非证据变量。<br>
和先验分布$P(\mathbf{z})$不同的是，每一个变量$Z_i$的取值会受到$Z_i$的祖先(ancestor)变量的影响。比如，对Sprinkler进行采样的时候，算法会受到它的父节点中的证据变量Cloudy=true的影响，而先验分布不会。另一方面，$S_{WS}$比后验分布$P(\mathbf{z}|\mathbf{e})$受证据的影响更小，因为对$Z_i$的采样忽略了$Z_i$的非祖先(non-ancestor)变量中的证据。比如，对Sprinkler和Rain进行采样的时候，算法忽略了子节点中的证据变量WetGrass=true，事实上这个证据已经排除了(rule out)Sprinkler=false和Rain=false的情况，但是WEIGHTED-SAMPLE还会产生很多这样的样本事件。<br>
理想情况下，我们想要一个采样分布和真实的后验概率$P(\mathbf{z}|\mathbf{e})$相等，不幸的是不存在这样的多项式时间的算法。如果有这样的算法的话，我们可以用多项式数量的样本以任意精度逼近想要求的概率值。<br>
可能性权重$w$弥补了实际的分布和我们想要的分布之间的差距。一个由$\mathbf{z}$和$\mathbf{e}$组成的样本$\mathbf{x}$的权重是给定了父节点的证据变量的可能性乘积：<br>
$$w(\mathbf{z},\mathbf{e}) = \prod_{i=1}^mP(e_i|parents(E_i)).$$<br>
将上面的两个式子乘起来，可以得到一个样本的加权概率(weighted probability)是：<br>
$$S_{WS}(\mathbf{z},\mathbf{e})w(\mathbf{z},\mathbf{e}) = \prod_{i=1}<sup>lP(z_i|parents(Z_i))\prod_{i=1}</sup>mP(e_i|parents(E_i)) = P(\mathbf{z},\mathbf{e}).$$<br>
可能性加权估计是一致估计。对于任意的$x$，估计的后验概率按下式计算：<br>
\begin{align*}<br>
\hat{P}(x|\mathbf{e}) &amp;= \alpha \sum_{\mathbf{y}} N_{WS}(x,\mathbf{y},\mathbf{e})w(x,\mathbf{y},\mathbf{e})\<br>
&amp;\approx \alpha’\sum_{\mathbf{y}}S_{WS}(x,\mathbf{y},\mathbf{e})w(x,\mathbf{y},\mathbf{e})\<br>
&amp;=\alpha’\sum_{\mathbf{y}}P(x,\mathbf{y},\mathbf{e})\<br>
&amp;=\alpha’\sum_{\mathbf{y}}P(x,\mathbf{y},\mathbf{e})\<br>
&amp;=P(x|\mathbf{e})<br>
\end{align*}<br>
算法中真实实现的是第一行，即统计出用WEIGHTED-SAMPLE产生的样本$(x,\mathbf{y},\mathbf{e})$数量$N_{WS}$，以及对应的权重$w(x,\mathbf{y},\mathbf{e})$，后面的都是理论推导，当$N$取极限的时候$lim_{N\rightarrow \infty}\frac{N_{WS}(x_1,\cdots,x_n)}{N} = S_{WS}(x_1,\cdots,x_n)$，后面的都是为了证明算法是一致估计。</p>
<h5 id="不足-v2">不足</h5>
<p>可能性加权算法使用了所有生成的样本，它比拒绝假设算法更高效。然而，随着证据变量的增加，算法性能会退化(degradation)，这是因为很多样本的权重都会很小，因此加权估计可能会受一小部分权重很大的样本的影响(dominated)。如果证据变量在非证据变量的后边，这个问题会加剧，因为它们的父节点或者祖先节点没有证据变量来指导样本的生成。这就意味着生成的样本和证据变量支撑的真实情况可能差距很大(bear little resemblance)。</p>
<h3 id="马尔科夫链仿真推理-inference-by-markov-chain-simulation">马尔科夫链仿真推理(Inference by Markov chain simulation)</h3>
<p><a href="https://mxxhcm.github.io/2019/08/01/Monte-Carlo-Markov-Chain/">马尔科夫链蒙特卡洛(Markov chain Monte Carlo,MCMC)</a>算法和拒绝采样以及可能性加权很不一样。那两个方法每次都从头开始生成样本，而MCMC算法在之前的样本上做一些随机的变化。可以将MCMC算法看成指定了每一个变量值的特殊当前状态(current state)，通过对当前状态(current state)做任意的改变生成下一个状态(next state)。这一节要介绍的一种MCMC算法是吉布森采样(Gibbs sampling)。</p>
<h4 id="贝叶斯网络中的吉布森采样-gibbs-sampling-in-bayesian-networks">贝叶斯网络中的吉布森采样(Gibbs sampling in Bayesian networks)</h4>
<h5 id="算法-v3">算法</h5>
<p>贝叶斯网络中的吉布森采样从任意一个状态开始，其中证据变量的取值固定为观测值，通过随机选取非证据变量$X_i$的值生成下一个状态。变量$X_i$的采样取决于变量$X_i$的马尔科夫毯的当前值。算法在状态空间（所有非证据变量的全部可能取值空间）中随机采样，每次采样都保持证据变量不变，一次改变一个非证据变量的值。完整的算法如Figure 14.16所示。<br>
<img src="/2019/01/06/AI-chapter-14-Probabilistic-reasoning/" alt="figure 14.16"></p>
<h5 id="示例-v5">示例</h5>
<p>Figure 14.12(a)中的查询(query)$P(Rain|Sprinkler=true, WetGrass=true)$，证据变量Spinkler和WetGrass取它们的观测值不变，非证据变量Cloudy和Rain随机初始化，假设取的是true和false。那么初始状态就是[true,true,false,true]，接下来对非证据变量进行重复的随机采样。<br>
比如第一次对Cloudy采样（也可以对Rain采样），给定它的马尔科夫毯变量，然后从$P(Cloudy|Sprinkler=true,Rain=false)$中进行采样，假设采样结果是false，新的状态就是[false,true,false,true]。接下来随机可以对Rain采样（也可以对Cloudy采样），给定Rain的马尔科夫毯变量的取值，从$P(Rain|Cloudy=false,Sprinkler=true,WetGrass=true)$中进行采样，假设采样值是true,那么新的状态是[true,true,false,false]。接下来可以一直进行采样。。最终利用生成的样本计算出相应的概率。</p>
<h4 id="为什么吉布森采样有用-why-gibbs-sampling-works">为什么吉布森采样有用(Why Gibbs sampling works)</h4>
<p>接下来给出为什么吉布森采样计算后验概率是一致估计。基本的解释是直截了当的：采样过程建立了一个动态平衡，每个状态花费的时间长期来说和它的后验概率是成比例的。<br>
具体的，不想看了。。。就随缘吧</p>
<h2 id="关系和一阶概率模型-relational-and-first-order-probability-models">关系和一阶概率模型(Relational and first-order probability models)</h2>
<h2 id="其他不确定性推理的方法-other-approaches-to-uncertain-reasoning">其他不确定性推理的方法(Other approaches to uncertain reasoning)</h2>
<h2 id="参考文献">参考文献</h2>
<p>1.<a href="http://aima.cs.berkeley.edu/" target="_blank" rel="noopener">Artificial Intelligence A Modern Approach Third Edition,Stuart Russell,Peter Norvig.</a><br>
2.<a href="https://en.wikipedia.org/wiki/Chain_rule_(probability)" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Chain_rule_(probability)</a><br>
3.<a href="https://en.wikipedia.org/wiki/Consistent_estimator" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Consistent_estimator</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://mxxhcm.github.io/2019/01/03/linear-algebra-singular-value-decomposition/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="马晓鑫爱马荟荟">
      <meta itemprop="description" content="记录硕士三年自己的积累">
      <meta itemprop="image" content="/images/favicon.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="mxxhcm's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/01/03/linear-algebra-singular-value-decomposition/" class="post-title-link" itemprop="http://mxxhcm.github.io/page/14/index.html">singular value decomposition（奇异值分解）</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-01-03 15:19:54" itemprop="dateCreated datePublished" datetime="2019-01-03T15:19:54+08:00">2019-01-03</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-09-09 16:53:25" itemprop="dateModified" datetime="2019-09-09T16:53:25+08:00">2019-09-09</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/线性代数/" itemprop="url" rel="index"><span itemprop="name">线性代数</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="特征值分解-eigen-value-decomposition">特征值分解(eigen value decomposition)</h2>
<p>要谈奇异值分解，首先要从特征值分解(eigen value decomposition, EVD)谈起。<br>
矩阵的作用有三个：一个是旋转，一个是拉伸，一个是平移，都是线性操作。如果一个$n\times n$方阵$A$对某个向量$x$只产生拉伸变换，而不产生旋转和平移变换，那么这个向量就称为方阵$A$的特征向量(eigenvector)，对应的伸缩比例叫做特征值(eigenvalue)，即满足等式$Ax = \lambda x$。其中$A$是方阵，$x$是方阵$A$的一个特征向量，$\lambda$是方阵$A$对应特征向量$x$的特征值。<br>
假设$S$是由方阵$A$的$n$个线性无关的特征向量构成的方阵，$\Lambda$是方阵$A$的$n$个特征值构成的对角矩阵，则$A=S\Lambda S^{-1}$，这个过程叫做对角化过程。<br>
证明：<br>
因为$Ax_1 = \lambda_1 x_1,\cdots,Ax_n = \lambda_n x_n$,<br>
所以<br>
\begin{align*}AS &amp;= A\begin{bmatrix}x_1&amp; \cdots&amp;x_n\end{bmatrix}\\<br>
&amp;=\begin{bmatrix} \lambda_1x_1&amp;\cdots&amp;\lambda x_n\end{bmatrix}\\<br>
&amp;= \begin{bmatrix}x_1&amp; \cdots&amp;x_n\end{bmatrix} \begin{bmatrix}\lambda_1&amp; &amp; &amp;\\&amp;\lambda_2&amp;&amp;\\&amp;&amp;\cdots&amp;\\&amp;&amp;&amp;\lambda_n\end{bmatrix}\<br>
&amp;= S\Lambda<br>
\end{align*}<br>
所以$AS=S\Lambda, A=S\Lambda S^{-1}, S^{-1}AS=\Lambda$。<br>
若方阵$A$为对称矩阵，矩阵$A$的特征向量是正交的，将其单位化为$Q$，则$A=Q\Lambda Q^T$，这个过程就叫做特征值分解。</p>
<h2 id="奇异值分解-singular-value-decomposition">奇异值分解(singular value decomposition)</h2>
<p>特征值分解是一个非常好的分解，因为它能把一个方阵分解称两类非常好的矩阵，一个是正交阵，一个是对角阵，这些矩阵都便于进行各种计算，但是它对于原始矩阵的要求太严格了，必须要求矩阵是对称正定矩阵，这是一个很苛刻的条件。所以就产生了奇异值分解，奇异值分解可以看作特征值分解在$m\times n$维矩阵上的推广。对于对称正定矩阵来说，有特征值，对于其他一般矩阵，有奇异值。</p>
<p>奇异值分解可以看作将一组正交基映射到另一组正交基的变换。普通矩阵$A$不是对称正定矩阵，但是$AA^T $和$A^TA $一定是对称矩阵，且至少是半正定的。从对$A^TA $进行特征值分解开始，$A^T A=V\Sigma_1V^T $，$V$是一组正交的单位化特征向量${v_1,\cdots,v_n}$，则$Av_1,\cdots,Av_n$也是正交的。<br>
证明：<br>
\begin{align*}Av_1\cdot Av_2 &amp;=(Av_1)^T Av_2\\<br>
&amp;=v_1^T A^T Av_2\\<br>
&amp;=v_1^T \lambda v_2\\<br>
&amp;=\lambda v_1^T v_2\\<br>
&amp;=0<br>
\end{align*}<br>
所以$Av_1,Av_2$是正交的，同理可得$Av_1,\cdots,Av_n$都是正交的。<br>
而：<br>
\begin{align*}<br>
Av_i\cdot Av_i &amp;= v_i^T A^T Av_i\\<br>
&amp;=v_i \lambda v_i\\<br>
&amp;=\lambda v_i^2\\<br>
&amp;=\lambda<br>
\end{align*}<br>
将$Av_i$单位化为$u_i$，得$u_i = \frac{Av_i}{|Av_i|} = \frac{Av_i}{\sqrt{\lambda_i}}$，所以$Av_i = \sqrt{\lambda_i}u_i$。<br>
将向量组${v_1,\cdots,v_r}$扩充到$R^n $中的标准正交基${v_1,\cdots,v_n}$，将向量组${u_1,\cdots,u_r}$扩充到$R^n $中的标准正交基${u_1,\cdots,u_n}$，则$AV = U\Sigma$，$A=U\sigma V^T $。</p>
<p>事实上，奇异值分解可以看作将行空间的一组正交基加上零空间的一组基映射到列空间的一组正交基加上左零空间的一组基的变换。对一矩阵$A,A\in \mathbb{R}^{m\times n} $，若$r(A)=r$，取行空间的一组特殊正交基${v_1,\cdots,v_r}$，当矩阵$A$作用到这组基上，会得到另一组正交基${u_1,\cdots,u_r}$，即$Av_i = \sigma_iu_i$。<br>
矩阵表示是：<br>
\begin{align*}<br>
AV &amp;= A\begin{bmatrix}v_1&amp;\cdots&amp;v_r\end{bmatrix}\\<br>
&amp;= \begin{bmatrix}\sigma_1u_1 &amp; \cdots &amp; \sigma_ru_r\end{bmatrix}\\<br>
&amp;= \begin{bmatrix}u_1&amp;u_2&amp;\cdots&amp;u_r\end{bmatrix}\begin{bmatrix}\sigma_1&amp;&amp;&amp;\\&amp;\sigma_2&amp;&amp;\\&amp;&amp;\cdots&amp;\\&amp;&amp;&amp;\sigma_n\end{bmatrix}\\<br>
&amp;=U\Sigma<br>
\end{align*}<br>
其中$A\in \mathbb{R}^{m\times n}, V\in \mathbb{R}^{n\times r},U\in \mathbb{R}^{m\times r}, \Sigma \in \mathbb{R}^{r\times n}$。<br>
当有零空间的时候，行空间的一组基是$r$维，加上零空间的$n-r$维，构成$R^n $空间中的一组标准正交基。列空间的一组基也是$r$维的，加上左零空间的$m-r$维，构成$R^m $空间的一组标准正交基。零空间中的向量在对角矩阵$\Sigma$中体现为$0$，<br>
则$A=U\Sigma V^{-1} $，$V$是正交的，所以$A=U\Sigma V^T $，其中$V\in \mathbb{R}^{n\times n}, U\in \mathbb{R}^{m\times m}, \Sigma \in \mathbb{R}^{m\times n}$。</p>
<p>$A=U\Sigma V^T $,<br>
$A^T = V\Sigma^T U^T $,<br>
$AA^T = U\Sigma V^T V\Sigma^T U^T $,<br>
$A^T A = V\Sigma^T U^T U\Sigma V^T $<br>
对$A A^T $和$A^T A$作特征值分解，则$A A^T = U\Sigma_1U^T $,$A^T A=V\Sigma_2V^T $，所以对$AA^T $作特征值分解求出来的$U$和对$A^T A$作特征值分解求出来的$V$就是对$A$作奇异值分解求出来的$U$和$V$，$AA^T $和$A^T A$作特征值分解求出来的$\Sigma$的非零值是相等的，都是对$A$作奇异值分解的$\Sigma$的平方。</p>
<h3 id="a-t-a-和-aa-t-的非零特征值是相等的">$A^T A$和$AA^T $的非零特征值是相等的</h3>
<p>证明：对于任意的$m\times n$矩阵$A$，$A^T A$和$AA^T $的非零特征值相同的。 设$A^T A$的特征值为$\lambda_i$，对应的特征向量为$v_i$，即$A^T Av_i = \lambda_i v_i$。<br>
则$AA^T Av_i = A\lambda_iv_i = \lambda_i Av_i$。<br>
所以$AA^T $的特征值为$\lambda_i$，对应的特征向量为$Av_i$。<br>
因此$A^T A$和$AA^T $的非零特征值相等。</p>
<h3 id="几何意义">几何意义</h3>
<p>对于任意一个矩阵，找到其行空间(加上零空间)的一组正交向量，使得该矩阵作用在该向量序列上得到的新的向量序列保持两两正交。奇异值的几何意义就是这组变化后的新的向量序列的长度。</p>
<h3 id="物理意义">物理意义</h3>
<p>奇异值往往对应着矩阵隐含的重要信息，且重要性和奇异值大小正相关。每个矩阵都可以表示为一系列秩为$1$的“小矩阵”的和，而奇异值则衡量了这些秩一矩阵对$A$的权重。<br>
奇异值分解的物理意义可以通过图像压缩表现出来。给定一张$m\times n$像素的照片$A$，用奇异值分解将矩阵分解为若干个秩一矩阵之和，即：<br>
\begin{align*}<br>
A&amp;=\sigma_1 u_1v_1^T +\sigma_2 u_2v_2^T +\cdots+\sigma_r u_rv_r^T\\<br>
&amp;= \begin{bmatrix}u_1&amp;u_2&amp;\cdots&amp;u_r\end{bmatrix}\begin{bmatrix}\sigma_1&amp;&amp;&amp;\&amp;\sigma_2&amp;&amp;\&amp;&amp;\cdots&amp;\&amp;&amp;&amp;\sigma_n\end{bmatrix}\begin{bmatrix}v_1<sup>T\v_2</sup>T\ \vdots\v_r^T\end{bmatrix}\\<br>
&amp;=U\Sigma V^T<br>
\end{align*}</p>
<p>这个也叫部分奇异值分解。其中$V\in R^{r\times n}, U\in R^{m\times r}, \Sigma \in R^{r\times r}$。因为不含有零空间和左零空间的基，如果加上零空间的$n-r$维和左零空间的$m-r$维，就是奇异值分解。<br>
较大的奇异值保存了图片的主要信息，特别小的奇异值有时可能是噪声，或者对于图片的整体信息不是特别重要。做图像压缩的时候，可以只取一部分较大的奇异值，比如取前八个奇异值作为压缩后的图片：<br>
$$A = \sigma_1 u_1v_1^T +\sigma_2 u_2v_2^T + \cdots + \sigma_8 u_8v_8^T$$<br>
现实中常用的做法有两个：</p>
<ol>
<li>保留矩阵中$90%$的信息：将奇异值平方和累加到总值的%90%为止。</li>
<li>当矩阵有上万个奇异值的时候，取前面的$2000$或者$3000$个奇异值。。</li>
</ol>
<h2 id="参考文献-references">参考文献(references)</h2>
<p>1.Gilbert Strang, MIT Open course：Linear Algebra<br>
2.<a href="https://www.cnblogs.com/pinard/p/6251584.html" target="_blank" rel="noopener">https://www.cnblogs.com/pinard/p/6251584.html</a><br>
3.<a href="http://www.ams.org/publicoutreach/feature-column/fcarc-svd" target="_blank" rel="noopener">http://www.ams.org/publicoutreach/feature-column/fcarc-svd</a><br>
4.<a href="https://www.zhihu.com/question/22237507/answer/53804902" target="_blank" rel="noopener">https://www.zhihu.com/question/22237507/answer/53804902</a><br>
5.<a href="http://charleshm.github.io/2016/03/Singularly-Valuable-Decomposition/" target="_blank" rel="noopener">http://charleshm.github.io/2016/03/Singularly-Valuable-Decomposition/</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/13/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/13/">13</a><span class="page-number current">14</span><a class="page-number" href="/page/15/">15</a><a class="extend next" rel="next" href="/page/15/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <div class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/favicon.jpg" alt="马晓鑫爱马荟荟">
            
              <p class="site-author-name" itemprop="name">马晓鑫爱马荟荟</p>
              <p class="site-description motion-element" itemprop="description">记录硕士三年自己的积累</p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">146</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  <a href="/categories/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">12</span>
                    <span class="site-state-item-name">分类</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  <a href="/tags/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">176</span>
                    <span class="site-state-item-name">标签</span>
                  </a>
                </div>
              
            </nav>
          

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="https://github.com/mxxhcm" title="GitHub &rarr; https://github.com/mxxhcm" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="mailto:mxxhcm@gmail.com" title="E-Mail &rarr; mailto:mxxhcm@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                </span>
              
            </div>
          

          

          
          

          
            
          
          

        </div>
      </div>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">马晓鑫爱马荟荟</span>

  

  
</div>


  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v3.8.0</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> v6.6.0</div>




        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  
    <script src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  


  


  <script src="/js/src/utils.js?v=6.6.0"></script>

  <script src="/js/src/motion.js?v=6.6.0"></script>



  
  


  <script src="/js/src/affix.js?v=6.6.0"></script>

  <script src="/js/src/schemes/pisces.js?v=6.6.0"></script>



  

  


  <script src="/js/src/bootstrap.js?v=6.6.0"></script>



  



  






<!-- LOCAL: You can save these files to your site and update links -->
    
        
        <link rel="stylesheet" href="https://aimingoo.github.io/gitmint/style/default.css">
        <script src="https://aimingoo.github.io/gitmint/dist/gitmint.browser.js"></script>
    
<!-- END LOCAL -->

    

    







  





  

  

  

  

  
  

  
  
    
      
    
      
        
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
  

  
    
      <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      },
      TeX: {equationNumbers: { autoNumber: "AMS" }}
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<script src="//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<style>
.MathJax_Display {
    overflow: auto hidden;
}
</style>

    
  


  
  

  

  

  

  

  

  

</body>
</html>

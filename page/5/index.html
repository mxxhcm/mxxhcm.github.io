<!DOCTYPE html>












  


<html class="theme-next pisces use-motion" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
































<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=6.6.0" rel="stylesheet" type="text/css">



  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.jpg?v=6.6.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.jpg?v=6.6.0">










<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '6.6.0',
    sidebar: {"position":"left","display":"always","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="记录硕士三年自己的积累">
<meta property="og:type" content="website">
<meta property="og:title" content="mxxhcm&#39;s blog">
<meta property="og:url" content="http://mxxhcm.github.io/page/5/index.html">
<meta property="og:site_name" content="mxxhcm&#39;s blog">
<meta property="og:description" content="记录硕士三年自己的积累">
<meta property="og:locale" content="zh-CN">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="mxxhcm&#39;s blog">
<meta name="twitter:description" content="记录硕士三年自己的积累">



  <link rel="alternate" href="/atom.xml" title="mxxhcm's blog" type="application/atom+xml">




  <link rel="canonical" href="http://mxxhcm.github.io/page/5/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>mxxhcm's blog</title>
  












  <noscript>
  <style>
    .use-motion .motion-element,
    .use-motion .brand,
    .use-motion .menu-item,
    .sidebar-inner,
    .use-motion .post-block,
    .use-motion .pagination,
    .use-motion .comments,
    .use-motion .post-header,
    .use-motion .post-body,
    .use-motion .collection-title { opacity: initial; }

    .use-motion .logo,
    .use-motion .site-title,
    .use-motion .site-subtitle {
      opacity: initial;
      top: initial;
    }

    .use-motion .logo-line-before i { left: initial; }
    .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">mxxhcm's blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
    
      
    

    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>标签</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">

    
    
    
      
    

    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>分类</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>归档</a>

  </li>

      
      
    </ul>
  

  

  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
            

          
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://mxxhcm.github.io/2019/05/08/pytorch-basic/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="马晓鑫爱马荟荟">
      <meta itemprop="description" content="记录硕士三年自己的积累">
      <meta itemprop="image" content="/images/favicon.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="mxxhcm's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/05/08/pytorch-basic/" class="post-title-link" itemprop="http://mxxhcm.github.io/page/5/index.html">pytorch</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-05-08 22:07:42" itemprop="dateCreated datePublished" datetime="2019-05-08T22:07:42+08:00">2019-05-08</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-05-17 16:14:01" itemprop="dateModified" datetime="2019-05-17T16:14:01+08:00">2019-05-17</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/pytorch/" itemprop="url" rel="index"><span itemprop="name">pytorch</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="torch"><a href="#torch" class="headerlink" title="torch"></a>torch</h2><p>torch提供了很多基础操作，包括数学操作等等。</p>
<h2 id="torch-cat"><a href="#torch-cat" class="headerlink" title="torch.cat"></a>torch.cat</h2><h3 id="函数原型"><a href="#函数原型" class="headerlink" title="函数原型"></a>函数原型</h3><p>将多个tensor在某一个维度上（默认是第0维）拼接到一起（除了拼接的维度上，其他维度的shape必须一定），最后返回一个tensor。<br>torch.cat(tensors, dim=0, out=None) → Tensor</p>
<blockquote>
<p>Concatenates the given sequence of seq tensors in the given dimension. All tensors must either have the same shape (except in the concatenating dimension) or be empty.</p>
</blockquote>
<h3 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h3><p>tensors (sequence of Tensors) – 任意类型相同python序列或者tensor<br>dim (int, optional) - 在第几个维度上进行拼接(只有在拼接的维度上可以不同，其余维度必须相同。<br>out (Tensor, optional) – 输出的tensor</p>
<h3 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x1 = torch.randn(<span class="number">3</span>, <span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line">x2 = torch.randn(<span class="number">3</span>, <span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line">x = torch.cat([x1, x2], <span class="number">1</span>)</span><br><span class="line">print(x.size())</span><br></pre></td></tr></table></figure>
<p>输出如下：</p>
<blockquote>
<p>torch.Size([3, 5, 4])</p>
</blockquote>
<h2 id="torch中图像-img-格式"><a href="#torch中图像-img-格式" class="headerlink" title="torch中图像(img)格式"></a>torch中图像(img)格式</h2><p>torch中图像的shape是(‘RGB’,width, height)，而numpy和matplotlib中都是(width, height, ‘RGB’)<br>matplotlib.pyplot.imshow()需要的参数是图像矩阵，如果矩阵中是整数，那么它的值需要在区间[0,255]之内，如果是浮点数，需要在[0,1]之间。</p>
<blockquote>
<p>Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).</p>
</blockquote>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>1.<a href="https://pytorch.org/docs/stable/torch.html" target="_blank" rel="noopener">https://pytorch.org/docs/stable/torch.html</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://mxxhcm.github.io/2019/05/08/pytorch-initialize-parameters/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="马晓鑫爱马荟荟">
      <meta itemprop="description" content="记录硕士三年自己的积累">
      <meta itemprop="image" content="/images/favicon.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="mxxhcm's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/05/08/pytorch-initialize-parameters/" class="post-title-link" itemprop="http://mxxhcm.github.io/page/5/index.html">pytorch initialize parameters</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-05-08 22:01:24" itemprop="dateCreated datePublished" datetime="2019-05-08T22:01:24+08:00">2019-05-08</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-05-17 16:21:56" itemprop="dateModified" datetime="2019-05-17T16:21:56+08:00">2019-05-17</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/pytorch/" itemprop="url" rel="index"><span itemprop="name">pytorch</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="神经网络参数初始化"><a href="#神经网络参数初始化" class="headerlink" title="神经网络参数初始化"></a>神经网络参数初始化</h2><h3 id="方法-1-Model-apply-fn"><a href="#方法-1-Model-apply-fn" class="headerlink" title="方法$1$.Model.apply(fn)"></a>方法$1$.Model.apply(fn)</h3><p><a href="https://github.com/mxxhcm/myown_code/blob/master/pytorch/tutorials/initialize/apply.py" target="_blank" rel="noopener">示例</a>如下<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_weights</span><span class="params">(m)</span>:</span></span><br><span class="line">  print(m)</span><br><span class="line">  <span class="keyword">if</span> type(m) == nn.Linear:</span><br><span class="line">    m.weight.data.fill_(<span class="number">1.0</span>)</span><br><span class="line">    print(m.weight)</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(nn.Linear(<span class="number">2</span>, <span class="number">2</span>), nn.Linear(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">net.apply(init_weights)</span><br></pre></td></tr></table></figure></p>
<p>输出结果如下：</p>
<blockquote>
<p>Linear(in_features=2, out_features=2, bias=True)<br>Parameter containing:<br>tensor([[1., 1.],<br>        [1., 1.]], requires_grad=True)<br>Linear(in_features=2, out_features=2, bias=True)<br>Parameter containing:<br>tensor([[1., 1.],<br>        [1., 1.]], requires_grad=True)<br>Sequential(<br>  (0): Linear(in_features=2, out_features=2, bias=True)<br>  (1): Linear(in_features=2, out_features=2, bias=True)<br>)<br>Linear(in_features=2, out_features=2, bias=True)<br>Linear(in_features=2, out_features=2, bias=True)</p>
</blockquote>
<p>其中最后两行为net对象调用self.children()函数返回的模块，就是模型中所有网络的参数。事实上，调用net.apply(fn)函数，会对self.children()中的所有模块应用fn函数，</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2>
          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://mxxhcm.github.io/2019/05/08/pytorch-optim/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="马晓鑫爱马荟荟">
      <meta itemprop="description" content="记录硕士三年自己的积累">
      <meta itemprop="image" content="/images/favicon.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="mxxhcm's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/05/08/pytorch-optim/" class="post-title-link" itemprop="http://mxxhcm.github.io/page/5/index.html">pytorch optim</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-05-08 21:58:19" itemprop="dateCreated datePublished" datetime="2019-05-08T21:58:19+08:00">2019-05-08</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-05-17 16:20:33" itemprop="dateModified" datetime="2019-05-17T16:20:33+08:00">2019-05-17</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/pytorch/" itemprop="url" rel="index"><span itemprop="name">pytorch</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="torch-optim"><a href="#torch-optim" class="headerlink" title="torch.optim"></a>torch.optim</h2><h3 id="基类class-Optimizer-object"><a href="#基类class-Optimizer-object" class="headerlink" title="基类class Optimizer(object)"></a>基类class Optimizer(object)</h3><p>Optimizer是所有optimizer的基类。<br>调用任何优化器都要先初始化Optimizer类，这里拿Adam优化器举例子。Adam optimizer的init函数如下所示：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Adam</span><span class="params">(Optimizer)</span>:</span></span><br><span class="line">	<span class="string">"""</span></span><br><span class="line"><span class="string">        params (iterable): iterable of parameters to optimize or dicts defining</span></span><br><span class="line"><span class="string">            parameter groups</span></span><br><span class="line"><span class="string">        lr (float, optional): learning rate (default: 1e-3)</span></span><br><span class="line"><span class="string">        betas (Tuple[float, float], optional): coefficients used for computing</span></span><br><span class="line"><span class="string">            running averages of gradient and its square (default: (0.9, 0.999))</span></span><br><span class="line"><span class="string">        eps (float, optional): term added to the denominator to improve</span></span><br><span class="line"><span class="string">            numerical stability (default: 1e-8)</span></span><br><span class="line"><span class="string">        weight_decay (float, optional): weight decay (L2 penalty)</span></span><br><span class="line"><span class="string">        amsgrad (boolean, optional): whether to use the AMSGrad variant of this</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, params, lr=<span class="number">1e-3</span>, betas=<span class="params">(<span class="number">0.9</span>, <span class="number">0.999</span>)</span>, eps=<span class="number">1e-8</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 weight_decay=<span class="number">0</span>, amsgrad=False)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="number">0.0</span> &lt;= lr:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">"Invalid learning rate: &#123;&#125;"</span>.format(lr))</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="number">0.0</span> &lt;= eps:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">"Invalid epsilon value: &#123;&#125;"</span>.format(eps))</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="number">0.0</span> &lt;= betas[<span class="number">0</span>] &lt; <span class="number">1.0</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">"Invalid beta parameter at index 0: &#123;&#125;"</span>.format(betas[<span class="number">0</span>]))</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="number">0.0</span> &lt;= betas[<span class="number">1</span>] &lt; <span class="number">1.0</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">"Invalid beta parameter at index 1: &#123;&#125;"</span>.format(betas[<span class="number">1</span>]))</span><br><span class="line">        defaults = dict(lr=lr, betas=betas, eps=eps,</span><br><span class="line">                        weight_decay=weight_decay, amsgrad=amsgrad)</span><br><span class="line">        super(Adam, self).__init__(params, defaults)</span><br></pre></td></tr></table></figure></p>
<p>上述代码将学习率lr,beta,epsilon,weight_decay,amsgrad等封装在一个dict中，然后将其传给Optimizer的init函数，其代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Optimizer</span><span class="params">(object)</span>:</span></span><br><span class="line">    .. warning::</span><br><span class="line">        Parameters need to be specified <span class="keyword">as</span> collections that have a deterministic</span><br><span class="line">        ordering that <span class="keyword">is</span> consistent between runs. Examples of objects that don<span class="string">'t</span></span><br><span class="line"><span class="string">        satisfy those properties are sets and iterators over values of dictionaries.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        params (iterable): an iterable of :class:`torch.Tensor` s or</span></span><br><span class="line"><span class="string">            :class:`dict` s. Specifies what Tensors should be optimized.</span></span><br><span class="line"><span class="string">        defaults: (dict): a dict containing default values of optimization</span></span><br><span class="line"><span class="string">            options (used when a parameter group doesn'</span>t specify them).</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    def __init__(self, params, defaults):</span></span><br><span class="line"><span class="string">        self.defaults = defaults</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        if isinstance(params, torch.Tensor):</span></span><br><span class="line"><span class="string">            raise TypeError("params argument given to the optimizer should be "</span></span><br><span class="line"><span class="string">                            "an iterable of Tensors or dicts, but got " +</span></span><br><span class="line"><span class="string">                            torch.typename(params))</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        self.state = defaultdict(dict)</span></span><br><span class="line"><span class="string">        self.param_groups = []</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        param_groups = list(params)</span></span><br><span class="line"><span class="string">        if len(param_groups) == 0:</span></span><br><span class="line"><span class="string">            raise ValueError("optimizer got an empty parameter list")</span></span><br><span class="line"><span class="string">        if not isinstance(param_groups[0], dict):</span></span><br><span class="line"><span class="string">            param_groups = [&#123;'params': param_groups&#125;]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        for param_group in param_groups:</span></span><br><span class="line"><span class="string">            self.add_param_group(param_group)</span></span><br></pre></td></tr></table></figure>
<p>从这里可以看出来，每个pytorch给出的optimizer至少有以下三个属性和四个函数：<br>属性：</p>
<ul>
<li>self.defaults # 字典类型，主要包含学习率等值</li>
<li>self.state # defaultdict(\<class 'dict'\>, {}) state存放的是</class></li>
<li>self.param_gropus # \<class 'list'\>:[]，prama_groups是一个字典类型的列表，用来存放parameters。</class></li>
</ul>
<p>函数：</p>
<ul>
<li>self.zero_grad()  # 将optimizer中参数的梯度置零</li>
<li>self.step()  # 将梯度应用在参数上</li>
<li>self.state_dict() # 返回optimizer的state,包括state和param_groups。</li>
<li>self.load_state_dict()  # 加载optimizer的state。</li>
<li>self.add_param_group()  # 将一个param group添加到param_groups。可以用在fine-tune上，只添加我们需要训练的层数，然后其他层不动。</li>
</ul>
<p>如果param已经是一个字典列表的话，就无需操作，否则就需要把param转化成一个字典param_groups。然后对param_groups中的每一个param_group调用add_param_group(param_group)函数将param_group字典和defaults字典拼接成一个新的param_group字典添加到self.param_groups中。</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>1.<a href="https://pytorch.org/docs/stable/optim.html" target="_blank" rel="noopener">https://pytorch.org/docs/stable/optim.html</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://mxxhcm.github.io/2019/05/08/pytorch-utils-data/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="马晓鑫爱马荟荟">
      <meta itemprop="description" content="记录硕士三年自己的积累">
      <meta itemprop="image" content="/images/favicon.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="mxxhcm's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/05/08/pytorch-utils-data/" class="post-title-link" itemprop="http://mxxhcm.github.io/page/5/index.html">pytorch utils data</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-05-08 21:56:15" itemprop="dateCreated datePublished" datetime="2019-05-08T21:56:15+08:00">2019-05-08</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-05-17 15:45:03" itemprop="dateModified" datetime="2019-05-17T15:45:03+08:00">2019-05-17</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/pytorch/" itemprop="url" rel="index"><span itemprop="name">pytorch</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="torch-utils-data"><a href="#torch-utils-data" class="headerlink" title="torch.utils.data"></a>torch.utils.data</h2><h3 id="Dataloader"><a href="#Dataloader" class="headerlink" title="Dataloader"></a>Dataloader</h3><h4 id="原型"><a href="#原型" class="headerlink" title="原型"></a>原型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">torch</span>.<span class="title">utils</span>.<span class="title">data</span>.<span class="title">DataLoader</span><span class="params">(</span></span></span><br><span class="line"><span class="class"><span class="params">	dataset, 	# 从哪加载数据</span></span></span><br><span class="line"><span class="class"><span class="params">	batch_size=<span class="number">1</span>, 	# batch大小 <span class="params">(default: <span class="number">1</span>)</span>.</span></span></span><br><span class="line"><span class="class"><span class="params">	shuffle=False,	# 每个epoch的数据是否打乱 <span class="params">(default: False)</span>.</span></span></span><br><span class="line"><span class="class"><span class="params">	sampler=None, 	# 定义采样策略。如果指定这个参数, shuffle必须是False.</span></span></span><br><span class="line"><span class="class"><span class="params">	batch_sampler=None, </span></span></span><br><span class="line"><span class="class"><span class="params">	num_workers=<span class="number">0</span>,		# 多少个子进程用来进行数据加载。<span class="number">0</span>代表使用主进程加载数据 <span class="params">(default: <span class="number">0</span>)</span></span></span></span><br><span class="line"><span class="class"><span class="params">	collate_fn=&lt;function default_collate&gt;, </span></span></span><br><span class="line"><span class="class"><span class="params">	pin_memory=False, </span></span></span><br><span class="line"><span class="class"><span class="params">	drop_last=False, </span></span></span><br><span class="line"><span class="class"><span class="params">	timeout=<span class="number">0</span>, </span></span></span><br><span class="line"><span class="class"><span class="params">	worker_init_fn=None</span></span></span><br><span class="line"><span class="class"><span class="params">)</span></span></span><br></pre></td></tr></table></figure>
<h4 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line">dataset = torchvision.datasets.CIFAR100(root=<span class="string">'./data'</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=<span class="literal">None</span>)</span><br><span class="line">train_loader = torch.utils.data.DataLoader(dataset, bath_size=<span class="number">16</span>, shuffle=<span class="literal">False</span>, num_worker=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<h4 id="如何访问DataLoader返回值"><a href="#如何访问DataLoader返回值" class="headerlink" title="如何访问DataLoader返回值"></a>如何访问DataLoader返回值</h4><p>train_loader不是整数，所以不能用range，这里用enumerate()，i是<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i, data <span class="keyword">in</span> enumerate(train_loader):</span><br><span class="line">    images, labels = data</span><br></pre></td></tr></table></figure></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://mxxhcm.github.io/2019/05/08/pytorch-torchvision/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="马晓鑫爱马荟荟">
      <meta itemprop="description" content="记录硕士三年自己的积累">
      <meta itemprop="image" content="/images/favicon.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="mxxhcm's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/05/08/pytorch-torchvision/" class="post-title-link" itemprop="http://mxxhcm.github.io/page/5/index.html">pytorch torchvision</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-05-08 21:55:57" itemprop="dateCreated datePublished" datetime="2019-05-08T21:55:57+08:00">2019-05-08</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-05-17 16:12:15" itemprop="dateModified" datetime="2019-05-17T16:12:15+08:00">2019-05-17</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/pytorch/" itemprop="url" rel="index"><span itemprop="name">pytorch</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="torchvision"><a href="#torchvision" class="headerlink" title="torchvision"></a>torchvision</h2><p>torchvision是pytorch提供的一些工具包，主要包含下列几个模块</p>
<ul>
<li>torchvision.datasets</li>
<li>torchvision.utils</li>
<li>torchvision.transforms</li>
<li>torchvision.models</li>
</ul>
<h2 id="torchvision-datasets"><a href="#torchvision-datasets" class="headerlink" title="torchvision.datasets"></a>torchvision.datasets</h2><p>torchvision提供了很多数据集<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line">print(torchvision.datasets.__all__)</span><br></pre></td></tr></table></figure></p>
<blockquote>
<p>(‘LSUN’, ‘LSUNClass’, ‘ImageFolder’, ‘DatasetFolder’, ‘FakeData’, ‘CocoCaptions’,     ‘CocoDetection’, ‘CIFAR10’, ‘CIFAR100’, ‘EMNIST’, ‘FashionMNIST’, ‘MNIST’, ‘STL10’,     ‘SVHN’, ‘PhotoTour’, ‘SEMEION’, ‘Omniglot’)</p>
</blockquote>
<h3 id="CIFAR10"><a href="#CIFAR10" class="headerlink" title="CIFAR10"></a>CIFAR10</h3><h4 id="原型"><a href="#原型" class="headerlink" title="原型"></a>原型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">calss torchvision.datasets.CIFAR10(root, train=<span class="literal">True</span>, transform=<span class="literal">None</span>, target_transform=<span class="literal">None</span>, download=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<h4 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h4><p>root (string) – cifar-10-batches-py的存放目录或者download设置为True时将会存放的目录。<br>train (bool, optional) – 设置为True的时候, 从training set创建dataset, 否则从test set创建dataset.<br>transform (callable, optional) – 输入是一个 PIL image，返回一个transformed的版本。如，transforms.RandomCrop<br>target_transform (callable, optional) – A function/transform that takes in the target and transforms it.<br>download (bool, optional) – If true, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again.</p>
<h4 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line">trainset = torchvision.datasets.CIFAR100(root=<span class="string">"./datasets"</span>, train=<span class="literal">True</span>, transform=    <span class="literal">None</span>, download=<span class="literal">True</span>)</span><br><span class="line">testset = torchvision.datasets.CIFAR100(root=<span class="string">"./datasets"</span>, train=<span class="literal">False</span>, transform=    <span class="literal">None</span>, download=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h2 id="torchvision-models"><a href="#torchvision-models" class="headerlink" title="torchvision.models"></a>torchvision.models</h2><p>模型</p>
<h2 id="torchvision-transforms"><a href="#torchvision-transforms" class="headerlink" title="torchvision.transforms"></a>torchvision.transforms</h2><p>transform</p>
<h2 id="torchvision-utils"><a href="#torchvision-utils" class="headerlink" title="torchvision.utils"></a>torchvision.utils</h2><p>一些工具包</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>1.<a href="https://pytorch.org/docs/stable/torchvision/index.html" target="_blank" rel="noopener">https://pytorch.org/docs/stable/torchvision/index.html</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://mxxhcm.github.io/2019/05/08/pytorch-nn-functional/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="马晓鑫爱马荟荟">
      <meta itemprop="description" content="记录硕士三年自己的积累">
      <meta itemprop="image" content="/images/favicon.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="mxxhcm's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/05/08/pytorch-nn-functional/" class="post-title-link" itemprop="http://mxxhcm.github.io/page/5/index.html">pytorch nn functional</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-05-08 21:55:37" itemprop="dateCreated datePublished" datetime="2019-05-08T21:55:37+08:00">2019-05-08</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-05-17 16:17:14" itemprop="dateModified" datetime="2019-05-17T16:17:14+08:00">2019-05-17</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/pytorch/" itemprop="url" rel="index"><span itemprop="name">pytorch</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="torch-nn-functional"><a href="#torch-nn-functional" class="headerlink" title="torch.nn.functional"></a>torch.nn.functional</h2><p>该包提供了很多网络函数</p>
<h2 id="convoludion-functions"><a href="#convoludion-functions" class="headerlink" title="convoludion functions"></a>convoludion functions</h2><h3 id="conv2d"><a href="#conv2d" class="headerlink" title="conv2d"></a>conv2d</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torch.nn as nn</span><br><span class="line">import torch.nn.functional as F</span><br><span class="line">from torch.autograd import Variable</span><br><span class="line"></span><br><span class="line">inputs = Variable(torch.randn(64,3,32,32))</span><br><span class="line"></span><br><span class="line">filters1 = Variable(torch.randn(16,3,3,3))</span><br><span class="line">output1 = F.conv2d(inputs,filters1)</span><br><span class="line">print(output1.size())</span><br><span class="line"></span><br><span class="line">filters2 = Variable(torch.randn(16,3,3,3))</span><br><span class="line">output2 = F.conv2d(inputs,filters2,padding=1)</span><br><span class="line">print(output2.size())</span><br></pre></td></tr></table></figure>
<p>输出</p>
<blockquote>
<p>torch.Size([64, 16, 30, 30])<br>torch.Size([64, 16, 32, 32])</p>
</blockquote>
<h2 id="relu-functions"><a href="#relu-functions" class="headerlink" title="relu functions"></a>relu functions</h2><h2 id="pooling-functions"><a href="#pooling-functions" class="headerlink" title="pooling functions"></a>pooling functions</h2><h2 id="dropout-functions"><a href="#dropout-functions" class="headerlink" title="dropout functions"></a>dropout functions</h2><h3 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"> </span><br><span class="line">x = torch.randn(<span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line">y = F.dropout(x, <span class="number">0.5</span>, <span class="literal">True</span>)</span><br><span class="line">y = F.dropout2d(x, <span class="number">0.5</span>)</span><br><span class="line"> </span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure>
<p>注意$2$中说的问题，不过可能已经被改正了，注意一些就是了。</p>
<h2 id="linear-functions"><a href="#linear-functions" class="headerlink" title="linear functions"></a>linear functions</h2><h2 id="loss-functions"><a href="#loss-functions" class="headerlink" title="loss functions"></a>loss functions</h2><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>1.<a href="https://pytorch.org/docs/stable/nn.html#torch-nn-functional" target="_blank" rel="noopener">https://pytorch.org/docs/stable/nn.html#torch-nn-functional</a><br>2.<a href="https://pytorch.org/docs/stable/nn.html#torch-nn-functional" target="_blank" rel="noopener">https://pytorch.org/docs/stable/nn.html#torch-nn-functional</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://mxxhcm.github.io/2019/05/08/pytorch-nn/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="马晓鑫爱马荟荟">
      <meta itemprop="description" content="记录硕士三年自己的积累">
      <meta itemprop="image" content="/images/favicon.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="mxxhcm's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/05/08/pytorch-nn/" class="post-title-link" itemprop="http://mxxhcm.github.io/page/5/index.html">pytorch nn</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-05-08 21:55:18" itemprop="dateCreated datePublished" datetime="2019-05-08T21:55:18+08:00">2019-05-08</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-06-07 18:14:29" itemprop="dateModified" datetime="2019-06-07T18:14:29+08:00">2019-06-07</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/pytorch/" itemprop="url" rel="index"><span itemprop="name">pytorch</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="Parameter"><a href="#Parameter" class="headerlink" title="Parameter"></a>Parameter</h2><h3 id="一句话介绍"><a href="#一句话介绍" class="headerlink" title="一句话介绍"></a>一句话介绍</h3><p>torch.Tensor的子类，nn.Paramter()声明的变量被赋值给module的属性时，这个变量会自动添加到moudule的parameters list中，parameters()等函数返回的迭代器中可以访问。</p>
<h3 id="API"><a href="#API" class="headerlink" title="API"></a>API</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Parameter</span><span class="params">(torch.Tensor)</span></span></span><br><span class="line">    # data是weights,requires_grad是是否需要梯度</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__new__</span><span class="params">(cls, data=None, requires_grad=True)</span></span></span><br></pre></td></tr></table></figure>
<h3 id="代码示例"><a href="#代码示例" class="headerlink" title="代码示例"></a>代码示例</h3><p>下面的代码实现了和nn.Conv2d同样的功能，使用nn.Parameter()将手动创建的变量设置为module的paramters。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CNN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(CNN, self).__init__()</span><br><span class="line">        </span><br><span class="line">        self.cnn1_weight = nn.Parameter(torch.rand(<span class="number">16</span>, <span class="number">1</span>, <span class="number">5</span>, <span class="number">5</span>))</span><br><span class="line">        self.bias1_weight = nn.Parameter(torch.rand(<span class="number">16</span>))</span><br><span class="line">        </span><br><span class="line">        self.cnn2_weight = nn.Parameter(torch.rand(<span class="number">32</span>, <span class="number">16</span>, <span class="number">5</span>, <span class="number">5</span>))</span><br><span class="line">        self.bias2_weight = nn.Parameter(torch.rand(<span class="number">32</span>))</span><br><span class="line">        </span><br><span class="line">        self.linear1_weight = nn.Parameter(torch.rand(<span class="number">4</span> * <span class="number">4</span> * <span class="number">32</span>, <span class="number">10</span>))</span><br><span class="line">        self.bias3_weight = nn.Parameter(torch.rand(<span class="number">10</span>))</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = x.view(x.size(<span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line">        out = F.conv2d(x, self.cnn1_weight, self.bias1_weight)</span><br><span class="line">        out = F.relu(out)</span><br><span class="line">        out = F.max_pool2d(out)</span><br><span class="line">        </span><br><span class="line">        out = F.conv2d(x, self.cnn2_weight, self.bias2_weight)</span><br><span class="line">        out = F.relu(out)</span><br><span class="line">        out = F.max_pool2d(out)</span><br><span class="line">        </span><br><span class="line">        out = F.linear(x, self.linear1_weight, self.bias3_weight)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure></p>
<h2 id="Container"><a href="#Container" class="headerlink" title="Container"></a>Container</h2><h3 id="Module"><a href="#Module" class="headerlink" title="Module"></a>Module</h3><p>Module是所有模型的基类。<br>它有以下几个常用的函数和常见的属性。</p>
<h4 id="常见的函数"><a href="#常见的函数" class="headerlink" title="常见的函数"></a>常见的函数</h4><ul>
<li>add_module(name, module)  # 添加一个子module到当前module</li>
<li>apply(fn) # 对模型中的每一个submodule（调用.children()得到的）都调用fn函数</li>
<li>_apply() # in-place</li>
<li>children() # 返回module中所有的子module，不包含整个module，<a href="https://github.com/mxxhcm/code/blob/master/pytorch/pytorch_test/torch_module_child_parameter.py" target="_blank" rel="noopener">详情可见</a></li>
<li>buffers(recurse=True)     # 返回module buffers的迭代器 </li>
<li>cuda(device=None)    # 将model para和buffer转换到gpu</li>
<li>cpu()     # 将model para和buffer转换到cpu</li>
<li>double()  #将float的paramters和buffer转换成double</li>
<li>float()</li>
<li>forward(*input) # 前向传播</li>
<li>eval()    # 将module置为evaluation mode，只影响特定的traing和evaluation modules表现不同的module，比如Dropout和BatchNorm，一般Dropout在训练时使用，在测试时关闭。</li>
<li>train(mode=True)  # 设置模型为train mode</li>
<li>load_state_dict(state_dict, strict=True)  # 加载模型参数</li>
<li>modules()  # 返回network中所有的module的迭代器，包含整个module，<a href="https://github.com/mxxhcm/code/blob/master/pytorch/pytorch_test/torch_module_child_parameter.py" target="_blank" rel="noopener">详情可见</a></li>
<li>named_modules() # 同时返回包含module和module名字的迭代器</li>
<li>named_children() # 同时返回包含子module和子module名字的迭代器</li>
<li>named_parameters() # 同时返回包含parameter和paramter名字的迭代器</li>
<li>parameters() # 返回模型参数的迭代器</li>
<li>state_dict(destination=None, prefix=’’, keep_vars=False)  # 返回整个module的state，包含parameters和buffers。</li>
<li>zero_grad()   # 设置model parameters的gradients为$0$</li>
<li>to(*args, **kwargs) # 移动或者改变parameters和buffer的类型或位置</li>
</ul>
<h4 id="常见的属性"><a href="#常见的属性" class="headerlink" title="常见的属性"></a>常见的属性</h4><ul>
<li>self._backend = thnn_backend</li>
<li>self._parameters = OrderedDict()</li>
<li>self._buffers = OrderedDict()</li>
<li>self._backward_hooks = OrderedDict()</li>
<li>self._forward_hooks = OrderedDict()</li>
<li>self._forward_pre_hooks = OrderedDict()</li>
<li>self._state_dict_hooks = OrderedDict()</li>
<li>self._load_state_dict_pre_hooks = OrderedDict()</li>
<li>self._modules = OrderedDict()</li>
<li>self.training = True</li>
</ul>
<h4 id="代码示例-1"><a href="#代码示例-1" class="headerlink" title="代码示例"></a>代码示例</h4><h5 id="apply"><a href="#apply" class="headerlink" title="apply"></a>apply</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_weights</span><span class="params">(m)</span>:</span></span><br><span class="line">    print(m)</span><br><span class="line">    <span class="keyword">if</span> type(m) == nn.Linear:</span><br><span class="line">        m.weight.data.fill_(<span class="number">1.0</span>)</span><br><span class="line">        print(m.weight)</span><br><span class="line">net = nn.Sequential(nn.Linear(<span class="number">2</span>, <span class="number">2</span>), nn.Linear(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">net.apply(init_weights)</span><br></pre></td></tr></table></figure>
<h5 id="module"><a href="#module" class="headerlink" title="module"></a>module</h5><p>关于module,children_modules,parameters的<a href="https://github.com/mxxhcm/code/blob/master/pytorch/pytorch_test/torch_module_child_parameter.py" target="_blank" rel="noopener">代码</a></p>
<h3 id="Sequential"><a href="#Sequential" class="headerlink" title="Sequential"></a>Sequential</h3><h2 id="Convolution-Layers"><a href="#Convolution-Layers" class="headerlink" title="Convolution Layers"></a>Convolution Layers</h2><h3 id="torch-nn-Conv2d"><a href="#torch-nn-Conv2d" class="headerlink" title="torch.nn.Conv2d"></a>torch.nn.Conv2d</h3><h4 id="API-1"><a href="#API-1" class="headerlink" title="API"></a>API</h4><p>2维卷积。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.Conv2d(</span><br><span class="line">    in_channels,    <span class="comment"># int，输入图像的通道</span></span><br><span class="line">    out_channels,   <span class="comment"># int，卷积产生的输出通道数（也就是有几个kernel） </span></span><br><span class="line">    kernel_size,    <span class="comment"># int or tuple – kernel的大小 </span></span><br><span class="line">    stride=<span class="number">1</span>,       <span class="comment"># int or tuple, 可选，步长，默认为$1$</span></span><br><span class="line">    padding=<span class="number">0</span>,      <span class="comment"># int or tuple, 可选，向各边添加Zero-padding的数量，默认为$0$</span></span><br><span class="line">    dilation=<span class="number">1</span>,     <span class="comment"># int or tuple, 可选，Spacing between kernel elements. Default: 1</span></span><br><span class="line">    groups=<span class="number">1</span>,       <span class="comment"># int, 可选， Number of blocked connections from input channels to output channels. Default: 1</span></span><br><span class="line">    bias=<span class="literal">True</span>       <span class="comment"># bool，可选，如果为True,给output添加一个可以学习的bias</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure></p>
<h4 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h4><h5 id="例子1"><a href="#例子1" class="headerlink" title="例子1"></a>例子1</h5><p>用$6$个$5\times 5$的filter处理维度为$32\times 32\times 1$的图像。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">model = torch.nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">input = torch.randn(<span class="number">16</span>, <span class="number">1</span>, <span class="number">32</span>, <span class="number">32</span>)</span><br><span class="line">output = model(input)</span><br><span class="line">print(output.size())</span><br><span class="line"><span class="comment"># output: torch.Size([16, 6, 28, 28])</span></span><br></pre></td></tr></table></figure></p>
<h5 id="例子2，stride和padding"><a href="#例子2，stride和padding" class="headerlink" title="例子2，stride和padding"></a>例子2，stride和padding</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"></span><br><span class="line">inputs = Variable(torch.randn(<span class="number">64</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>))</span><br><span class="line"></span><br><span class="line">m1 = nn.Conv2d(<span class="number">3</span>, <span class="number">16</span>, <span class="number">3</span>)</span><br><span class="line">print(m1)</span><br><span class="line"><span class="comment"># output: Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))</span></span><br><span class="line">output1 = m1(inputs)</span><br><span class="line">print(output1.size())</span><br><span class="line"><span class="comment"># output: torch.Size([64, 16, 30, 30])</span></span><br><span class="line"></span><br><span class="line">m2 = nn.Conv2d(<span class="number">3</span>, <span class="number">16</span>, <span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">print(m2)</span><br><span class="line"><span class="comment"># output: Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span></span><br><span class="line">output2 = m2(inputs)</span><br><span class="line">print(output2.size())</span><br><span class="line"><span class="comment"># output: torch.Size([64, 16, 32, 32])</span></span><br></pre></td></tr></table></figure>
<h2 id="Pooling-Layers"><a href="#Pooling-Layers" class="headerlink" title="Pooling Layers"></a>Pooling Layers</h2><h3 id="MaxPool2dd"><a href="#MaxPool2dd" class="headerlink" title="MaxPool2dd"></a>MaxPool2dd</h3><h4 id="API-2"><a href="#API-2" class="headerlink" title="API"></a>API</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">torch</span>.<span class="title">nn</span>.<span class="title">MaxPool2d</span><span class="params">(</span></span></span><br><span class="line"><span class="class"><span class="params">    kernel_size,</span></span></span><br><span class="line"><span class="class"><span class="params">    stride=None,</span></span></span><br><span class="line"><span class="class"><span class="params">    padding=<span class="number">0</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    dilation=<span class="number">1</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    return_indices=False,</span></span></span><br><span class="line"><span class="class"><span class="params">    ceil_mode=False</span></span></span><br><span class="line"><span class="class"><span class="params">)</span></span></span><br></pre></td></tr></table></figure>
<p>MaxPool2d默认layer stride默认是和kernel_size相同的</p>
<h4 id="代码示例-2"><a href="#代码示例-2" class="headerlink" title="代码示例"></a>代码示例</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch import nn</span><br><span class="line">from torch.autograd import Variable</span><br><span class="line"># maxpool2d</span><br><span class="line"></span><br><span class="line">input = Variable(torch.randn(30,20,32,32))</span><br><span class="line">print(input.size())</span><br><span class="line"># outputtorch.Size([30, 20, 32, 32])</span><br><span class="line"></span><br><span class="line">m1 = nn.MaxPool2d(2)</span><br><span class="line">output = m1(input)</span><br><span class="line">print(output.size())</span><br><span class="line"># output: torch.Size([30, 20, 16, 16])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">m2 = nn.MaxPool2d(5)</span><br><span class="line">print(m2)</span><br><span class="line"># output: MaxPool2d (size=(5, 5), stride=(5, 5), dilation=(1, 1))</span><br><span class="line"></span><br><span class="line">for param in m2.parameters():</span><br><span class="line">  print(param)</span><br><span class="line"></span><br><span class="line">print(m2.state_dict().keys())</span><br><span class="line"># output: []</span><br><span class="line"></span><br><span class="line">output = m2(input)</span><br><span class="line">print(output.size())</span><br><span class="line"># output: torch.Size([30, 20, 6, 6])</span><br></pre></td></tr></table></figure>
<h2 id="Padding-Layers"><a href="#Padding-Layers" class="headerlink" title="Padding Layers"></a>Padding Layers</h2><h2 id="Linear-layers"><a href="#Linear-layers" class="headerlink" title="Linear layers"></a>Linear layers</h2><h3 id="Linear"><a href="#Linear" class="headerlink" title="Linear"></a>Linear</h3><h4 id="API-3"><a href="#API-3" class="headerlink" title="API"></a>API</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.Linear(</span><br><span class="line">    in_features, </span><br><span class="line">    out_features, </span><br><span class="line">    bias=<span class="literal">True</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h4 id="代码示例-3"><a href="#代码示例-3" class="headerlink" title="代码示例"></a>代码示例</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">m = nn.Linear(<span class="number">20</span>, <span class="number">30</span>)</span><br><span class="line">input = torch.randn(<span class="number">128</span>, <span class="number">20</span>)</span><br><span class="line">output = m(input)</span><br><span class="line">print(output.size())</span><br><span class="line">torch.Size([<span class="number">128</span>, <span class="number">30</span>])</span><br></pre></td></tr></table></figure>
<h2 id="Dropout-layers"><a href="#Dropout-layers" class="headerlink" title="Dropout layers"></a>Dropout layers</h2><h3 id="Drop2D"><a href="#Drop2D" class="headerlink" title="Drop2D"></a>Drop2D</h3><h4 id="API-4"><a href="#API-4" class="headerlink" title="API"></a>API</h4><h4 id="代码示例-4"><a href="#代码示例-4" class="headerlink" title="代码示例"></a>代码示例</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line">m = nn.Dropout2d(<span class="number">0.3</span>)</span><br><span class="line">print(m)</span><br><span class="line">inputs = torch.randn(<span class="number">1</span>,<span class="number">28</span>,<span class="number">28</span>)</span><br><span class="line">outputs = m(inputs)</span><br><span class="line">print(outputs)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<blockquote>
<p>Dropout2d(p=0.3)<br>([[[ 0.8535,  1.0314,  2.7904,  1.2136,  2.7561, -2.0429,  0.0772,<br>     -1.9372, -0.0864, -1.4132, -0.1648,  0.2403,  0.5727,  0.8102,<br>      0.4544,  0.1414,  0.1547, -0.9266, -0.6033,  0.5813, -1.3541,<br>     -0.0536,  0.9574,  0.0554,  0.8368,  0.7633, -0.3377, -1.4293],<br>    [ 0.0000,  0.0000, -0.0000, -0.0000,  0.0000, -0.0000,  0.0000,<br>      0.0000,  0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000,<br>      0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000,<br>     -0.0000,  0.0000,  0.0000, -0.0000, -0.0000, -0.0000, -0.0000],<br>      …<br>    [ 0.6452, -0.6455,  0.2370,  0.1088, -0.5421, -0.5120, -2.2915,<br>      0.2061,  1.6384,  2.2276,  2.4022,  0.2033,  0.6984,  0.1254,<br>      1.1627,  1.0699, -2.1868,  1.1293, -0.7030,  0.0454, -1.5428,<br>      -2.4052, -0.3204, -1.5984,  0.1282,  0.2127, -2.3506, -2.2395]]])</p>
</blockquote>
<p>会发现输出的数组中有很多被置为$0$了。</p>
<h2 id="Loss-function"><a href="#Loss-function" class="headerlink" title="Loss function"></a>Loss function</h2>
          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://mxxhcm.github.io/2019/05/08/pytorch-Variable/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="马晓鑫爱马荟荟">
      <meta itemprop="description" content="记录硕士三年自己的积累">
      <meta itemprop="image" content="/images/favicon.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="mxxhcm's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/05/08/pytorch-Variable/" class="post-title-link" itemprop="http://mxxhcm.github.io/page/5/index.html">pytorch Variable(torch.autograd.Variable)</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-05-08 21:54:53" itemprop="dateCreated datePublished" datetime="2019-05-08T21:54:53+08:00">2019-05-08</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-05-17 16:14:27" itemprop="dateModified" datetime="2019-05-17T16:14:27+08:00">2019-05-17</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/pytorch/" itemprop="url" rel="index"><span itemprop="name">pytorch</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="Variable-class-torch-autograd-Variable"><a href="#Variable-class-torch-autograd-Variable" class="headerlink" title="Variable(class torch.autograd.Variable)"></a>Variable(class torch.autograd.Variable)</h3><h4 id="声明一个tensor"><a href="#声明一个tensor" class="headerlink" title="声明一个tensor"></a>声明一个tensor</h4><ul>
<li>torch.zeros</li>
<li>torch.ones</li>
<li>torch.rand</li>
<li>torch.full()</li>
<li>torch.empyt()</li>
<li>torch.rand()</li>
<li>torch.randn()</li>
<li>torch.ones_like()</li>
<li>torch.zeros_like()</li>
<li>torch.randn_like()</li>
<li>torch.Tensor</li>
</ul>
<h4 id="代码示例"><a href="#代码示例" class="headerlink" title="代码示例"></a>代码示例</h4><p><a href="https://github.com/mxxhcm/code/blob/master/pytorch/pytorch_test/torch_tensor.py" target="_blank" rel="noopener">代码地址</a><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">torch.manual_seed(<span class="number">5</span>)</span><br><span class="line">x = torch.empty(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">print(torch.empty(<span class="number">5</span>, <span class="number">3</span>)) <span class="comment"># construct a 5x3 matrix, uninitialized</span></span><br><span class="line"><span class="comment"># tensor([[4.6179e-38, 4.5845e-41, 4.6179e-38],</span></span><br><span class="line"><span class="comment">#         [4.5845e-41, 6.3010e-36, 6.3010e-36],</span></span><br><span class="line"><span class="comment">#         [2.5204e-35, 6.3010e-36, 1.0082e-34],</span></span><br><span class="line"><span class="comment">#         [6.3010e-36, 6.3010e-36, 6.6073e-30],</span></span><br><span class="line"><span class="comment">#         [6.3010e-36, 6.3010e-36, 6.3010e-36]])</span></span><br><span class="line"></span><br><span class="line">print(torch.rand(<span class="number">3</span>, <span class="number">4</span>))  <span class="comment"># construct a 4x3 matrix, uniform [0,1] </span></span><br><span class="line"><span class="comment"># tensor([[0.8303, 0.1261, 0.9075, 0.8199],</span></span><br><span class="line"><span class="comment">#         [0.9201, 0.1166, 0.1644, 0.7379],</span></span><br><span class="line"><span class="comment">#         [0.0333, 0.9942, 0.6064, 0.5646]])</span></span><br><span class="line"></span><br><span class="line">print(torch.randn(<span class="number">5</span>, <span class="number">3</span>)) <span class="comment"># construct a 5x3 matrix, normal distribution</span></span><br><span class="line"><span class="comment"># tensor([[-1.4017, -0.7626,  0.6312],</span></span><br><span class="line"><span class="comment">#         [-0.8991, -0.5578,  0.6907],</span></span><br><span class="line"><span class="comment">#         [ 0.2225, -0.6662,  0.6846],</span></span><br><span class="line"><span class="comment">#         [ 0.5740, -0.5829,  0.7679],</span></span><br><span class="line"><span class="comment">#         [ 0.5740, -0.5829,  0.7679],</span></span><br><span class="line"></span><br><span class="line">print(torch.randn(<span class="number">2</span>, <span class="number">3</span>).type())</span><br><span class="line"><span class="comment"># torch.FloatTensor</span></span><br><span class="line"></span><br><span class="line">print(torch.zeros(<span class="number">5</span>, <span class="number">3</span>)) <span class="comment"># construct a 5x3 matrix filled zeros</span></span><br><span class="line"><span class="comment"># tensor([[0., 0., 0.],</span></span><br><span class="line"><span class="comment">#         [0., 0., 0.],</span></span><br><span class="line"><span class="comment">#         [0., 0., 0.],</span></span><br><span class="line"><span class="comment">#         [0., 0., 0.],</span></span><br><span class="line"><span class="comment">#         [0., 0., 0.]])</span></span><br><span class="line"></span><br><span class="line">print(torch.ones(<span class="number">5</span>, <span class="number">3</span>)) <span class="comment"># construct a 5x3 matrix filled ones</span></span><br><span class="line"><span class="comment"># tensor([[1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 1., 1.]])</span></span><br><span class="line"></span><br><span class="line">print(torch.ones(<span class="number">5</span>, <span class="number">3</span>, dtype=torch.long)) <span class="comment"># construct a tensor with dtype=torch.long</span></span><br><span class="line"><span class="comment"># tensor([[1, 1, 1],</span></span><br><span class="line"><span class="comment">#         [1, 1, 1],</span></span><br><span class="line"><span class="comment">#         [1, 1, 1],</span></span><br><span class="line"><span class="comment">#         [1, 1, 1],</span></span><br><span class="line"><span class="comment">#         [1, 1, 1]])</span></span><br><span class="line"></span><br><span class="line">print(torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])) <span class="comment"># construct a tensor direct from data</span></span><br><span class="line"><span class="comment"># tensor([1, 2, 3])</span></span><br><span class="line"></span><br><span class="line">print(x.new_ones(<span class="number">5</span>,<span class="number">4</span>)) <span class="comment"># constuct a tensor has the same property as x</span></span><br><span class="line"><span class="comment"># tensor([[1., 1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 1., 1., 1.]])</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(torch.full([<span class="number">4</span>,<span class="number">3</span>],<span class="number">9</span>))  <span class="comment"># construct a tensor with a value </span></span><br><span class="line"><span class="comment"># tensor([[9., 9., 9.],</span></span><br><span class="line"><span class="comment">#         [9., 9., 9.],</span></span><br><span class="line"><span class="comment">#         [9., 9., 9.],</span></span><br><span class="line"><span class="comment">#         [9., 9., 9.]])</span></span><br><span class="line"></span><br><span class="line">print(x.new_ones(<span class="number">5</span>,<span class="number">4</span>,dtype=torch.int)) <span class="comment"># construct a tensor with the same property as x, and also can have the specified type.</span></span><br><span class="line"><span class="comment"># tensor([[1, 1, 1, 1],</span></span><br><span class="line"><span class="comment">#         [1, 1, 1, 1],</span></span><br><span class="line"><span class="comment">#         [1, 1, 1, 1],</span></span><br><span class="line"><span class="comment">#         [1, 1, 1, 1],</span></span><br><span class="line"><span class="comment">#         [1, 1, 1, 1]], dtype=torch.int32)</span></span><br><span class="line"></span><br><span class="line">print(torch.randn_like(x,dtype=torch.float)) <span class="comment"># construct a tensor with the same shape with x, </span></span><br><span class="line"><span class="comment"># tensor([[ 0.4699, -1.9540, -0.5587],</span></span><br><span class="line"><span class="comment">#         [ 0.4295, -2.2643, -0.2017],</span></span><br><span class="line"><span class="comment">#         [ 1.0677,  0.3246, -0.0684],</span></span><br><span class="line"><span class="comment">#         [-0.9959,  1.1563, -0.3992],</span></span><br><span class="line"><span class="comment">#         [ 1.2153, -0.8115, -0.8848]])</span></span><br><span class="line"></span><br><span class="line">print(torch.ones_like(x))</span><br><span class="line"><span class="comment"># tensor([[1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 1., 1.]])</span></span><br><span class="line"></span><br><span class="line">print(torch.zeros_like(x))</span><br><span class="line"><span class="comment"># tensor([[0., 0., 0.],</span></span><br><span class="line"><span class="comment">#         [0., 0., 0.],</span></span><br><span class="line"><span class="comment">#         [0., 0., 0.],</span></span><br><span class="line"><span class="comment">#         [0., 0., 0.],</span></span><br><span class="line"><span class="comment">#         [0., 0., 0.]])</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(torch.Tensor(<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line"><span class="comment"># tensor([[-3.8809e-21,  3.0948e-41,  2.3822e-44,  0.0000e+00],</span></span><br><span class="line"><span class="comment">#         [        nan,  7.2251e+28,  1.3733e-14,  1.8888e+31],</span></span><br><span class="line"><span class="comment">#         [ 4.9656e+28,  4.5439e+30,  7.1426e+22,  1.8759e+28]])</span></span><br><span class="line"></span><br><span class="line">print(torch.Tensor(<span class="number">3</span>,<span class="number">4</span>).uniform_(<span class="number">0</span>,<span class="number">1</span>))</span><br><span class="line"><span class="comment"># tensor([[0.8437, 0.1399, 0.2239, 0.3462],</span></span><br><span class="line"><span class="comment">#         [0.5668, 0.3059, 0.1890, 0.4087],</span></span><br><span class="line"><span class="comment">#         [0.2560, 0.5138, 0.1299, 0.3750]])</span></span><br><span class="line"></span><br><span class="line">print(torch.Tensor(<span class="number">3</span>,<span class="number">4</span>).normal_(<span class="number">0</span>,<span class="number">1</span>))</span><br><span class="line"><span class="comment"># tensor([[-0.5490, -0.0838, -0.1387, -0.5289],</span></span><br><span class="line"><span class="comment">#         [-0.4919, -0.4646, -0.0588,  1.2624],</span></span><br><span class="line"><span class="comment">#         [ 1.1935,  1.5696, -0.8977, -0.1139]])</span></span><br><span class="line"></span><br><span class="line">print(torch.Tensor(<span class="number">3</span>,<span class="number">4</span>).fill_(<span class="number">5</span>))</span><br><span class="line"><span class="comment"># tensor([[5., 5., 5., 5.],</span></span><br><span class="line"><span class="comment">#         [5., 5., 5., 5.],</span></span><br><span class="line"><span class="comment">#         [5., 5., 5., 5.]])</span></span><br><span class="line"></span><br><span class="line">print(torch.arange(<span class="number">1</span>, <span class="number">3</span>, <span class="number">0.4</span>))</span><br><span class="line"><span class="comment"># tensor([1.0000, 1.4000, 1.8000, 2.2000, 2.6000])</span></span><br></pre></td></tr></table></figure></p>
<h2 id="tensor的各种操作"><a href="#tensor的各种操作" class="headerlink" title="tensor的各种操作"></a>tensor的各种操作</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">a = torch.ones(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">b = torch.ones(<span class="number">2</span>,<span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<h3 id="加操作"><a href="#加操作" class="headerlink" title="加操作"></a>加操作</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">print(a+b)                <span class="comment">#方法1</span></span><br><span class="line">c = torch.add(a,b)    <span class="comment">#方法2</span></span><br><span class="line">torch.add(a,b,result)    <span class="comment">#方法3</span></span><br><span class="line">a.add(b)                    <span class="comment">#方法4,将a加上b，且a不变</span></span><br><span class="line">a.add_(b)                <span class="comment">#方法5,将a加上b并将其赋值给a</span></span><br></pre></td></tr></table></figure>
<h3 id="转置操作"><a href="#转置操作" class="headerlink" title="转置操作"></a>转置操作</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(a.t())               <span class="comment"># 打印出tensor a的转置</span></span><br><span class="line">print(a.t_())                 <span class="comment">#将tensor a 转置，并将其赋值给a</span></span><br></pre></td></tr></table></figure>
<h3 id="求最大行和列"><a href="#求最大行和列" class="headerlink" title="求最大行和列"></a>求最大行和列</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.max(tensor,dim)</span><br><span class="line">np.max(array,dim)</span><br></pre></td></tr></table></figure>
<h3 id="和relu功能比较类似。"><a href="#和relu功能比较类似。" class="headerlink" title="和relu功能比较类似。"></a>和relu功能比较类似。</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.clamp(tensor, min, max,out=<span class="literal">None</span>)</span><br><span class="line">np.maximun(x1, x2)  <span class="comment"># x1 and x2 must hava the same shape</span></span><br></pre></td></tr></table></figure>
<h2 id="tensor和numpy转化"><a href="#tensor和numpy转化" class="headerlink" title="tensor和numpy转化"></a>tensor和numpy转化</h2><h3 id="convert-tensor-to-numpy"><a href="#convert-tensor-to-numpy" class="headerlink" title="convert tensor to numpy"></a>convert tensor to numpy</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = torch.ones(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">b = a.numpy()</span><br></pre></td></tr></table></figure>
<h3 id="convert-numpy-to-tensor"><a href="#convert-numpy-to-tensor" class="headerlink" title="convert numpy to tensor"></a>convert numpy to tensor</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a =  numpy.ones(<span class="number">4</span>,<span class="number">3</span>)</span><br><span class="line">b = torch.from_numpy(a)</span><br></pre></td></tr></table></figure>
<h2 id="Variable和Tensor"><a href="#Variable和Tensor" class="headerlink" title="Variable和Tensor"></a>Variable和Tensor</h2><p>Variable<br>图1.Variable</p>
<h3 id="属性"><a href="#属性" class="headerlink" title="属性"></a>属性</h3><p>如图1,Variable wrap a Tensor,and it has six attributes,data,grad,requies_grad,volatile,is_leaf and grad_fn.We can acess the raw tensor through .data operation, we can accumualte gradients w.r.t this Variable into .grad,.Finally , creator attribute will tell us how the Variable were created,we can acess the creator attibute by .grad_fn,if the Variable was created by the user,then the grad_fn is None,else it will show us which Function created the Variable.<br>if the grad_fn is None,we call them graph leaves<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Variable.shape  <span class="comment">#查看Variable的size</span></span><br><span class="line">Variable.size()</span><br></pre></td></tr></table></figure></p>
<h3 id="parameters"><a href="#parameters" class="headerlink" title="parameters"></a>parameters</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.autograd.Variable(data,requires_grad=<span class="literal">False</span>,volatile=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p>requires_grad : indicate whether the backward() will ever need to be called</p>
<h3 id="backward"><a href="#backward" class="headerlink" title="backward"></a>backward</h3><p>backward(gradient=None,retain_graph=None,create_graph=None,retain_variables=None)<br>如果Variable是一个scalar output，我们不需要指定gradient，但是如果Variable不是一个scalar，而是有多个element，我们就需要根据output指定一下gradient，gradient的type可以是tensor也可以是Variable，里面的值为梯度的求值比例，例如<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = Variable(torch.Tensor([<span class="number">3</span>,<span class="number">6</span>,<span class="number">4</span>]),requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = Variable(torch.Tensor([<span class="number">5</span>,<span class="number">3</span>,<span class="number">6</span>]),requires_grad=<span class="literal">True</span>)</span><br><span class="line">z = x+y</span><br><span class="line">z.backward(gradient=torch.Tensor([<span class="number">0.1</span>,<span class="number">1</span>,<span class="number">10</span>]))</span><br></pre></td></tr></table></figure></p>
<p>这里[0.1,1,10]分别表示的是对正常梯度分别乘上$0.1,1,10$，然后将他们累积在leaves Variable上<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">detach()    <span class="comment">#</span></span><br><span class="line">detach_()</span><br><span class="line">register_hook()</span><br><span class="line">register_grad()</span><br></pre></td></tr></table></figure></p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>1.<a href="https://pytorch.org/docs/stable/tensors.html" target="_blank" rel="noopener">https://pytorch.org/docs/stable/tensors.html</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://mxxhcm.github.io/2019/05/08/pytorch-Function/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="马晓鑫爱马荟荟">
      <meta itemprop="description" content="记录硕士三年自己的积累">
      <meta itemprop="image" content="/images/favicon.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="mxxhcm's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/05/08/pytorch-Function/" class="post-title-link" itemprop="http://mxxhcm.github.io/page/5/index.html">pytorch Function(torch.autograd.Function)</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-05-08 21:54:22" itemprop="dateCreated datePublished" datetime="2019-05-08T21:54:22+08:00">2019-05-08</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-05-09 13:37:26" itemprop="dateModified" datetime="2019-05-09T13:37:26+08:00">2019-05-09</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/pytorch/" itemprop="url" rel="index"><span itemprop="name">pytorch</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="Function-class-torch-autograd-Funtion"><a href="#Function-class-torch-autograd-Funtion" class="headerlink" title="Function(class torch.autograd.Funtion)"></a>Function(class torch.autograd.Funtion)</h3><h4 id="用法"><a href="#用法" class="headerlink" title="用法"></a>用法</h4><p>Function一般只定义一个操作，并且它无法保存参数，一般适用于激活函数，pooling等，它需要定义三个方法，<strong>init</strong>(),forward(),backward()（这个需要自己定义怎么求导）<br>Model保存了参数，适合定义一层，如线性层(Linear layer),卷积层(conv layer),也适合定义一个网络。<br>和Model的区别，model只需要定义<strong>init()</strong>,foward()方法，backward()不需要我们定义，它可以由自动求导机制计算。</p>
<p>Function定义只是一个函数，forward和backward都只与这个Function的输入和输出有关</p>
<h4 id="functions"><a href="#functions" class="headerlink" title="functions"></a>functions</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyReLU</span><span class="params">(torch.autograd.Function)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    We can implement our own custom autograd Functions by subclassing</span></span><br><span class="line"><span class="string">    torch.autograd.Function and implementing the forward and backward passes</span></span><br><span class="line"><span class="string">    which operate on Tensors.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        In the forward pass we receive a Tensor containing the input and return a</span></span><br><span class="line"><span class="string">        Tensor containing the output. You can cache arbitrary Tensors for use in the</span></span><br><span class="line"><span class="string">        backward pass using the save_for_backward method.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.save_for_backward(input)</span><br><span class="line">        <span class="keyword">return</span> input.clamp(min=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(self, grad_output)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        In the backward pass we receive a Tensor containing the gradient of the loss</span></span><br><span class="line"><span class="string">        with respect to the output, and we need to compute the gradient of the loss</span></span><br><span class="line"><span class="string">        with respect to the input.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        input, = self.saved_tensors</span><br><span class="line">        grad_input = grad_output.clone()</span><br><span class="line">        grad_input[input &lt; <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">        <span class="keyword">return</span> grad_input</span><br><span class="line"></span><br><span class="line">dtype = torch.FloatTensor</span><br><span class="line"><span class="comment"># dtype = torch.cuda.FloatTensor # Uncomment this to run on GPU</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></span><br><span class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></span><br><span class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create random Tensors to hold input and outputs, and wrap them in Variables.</span></span><br><span class="line">x = Variable(torch.randn(N, D_in).type(dtype), requires_grad=<span class="literal">False</span>)</span><br><span class="line">y = Variable(torch.randn(N, D_out).type(dtype), requires_grad=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create random Tensors for weights, and wrap them in Variables.</span></span><br><span class="line">w1 = Variable(torch.randn(D_in, H).type(dtype), requires_grad=<span class="literal">True</span>)</span><br><span class="line">w2 = Variable(torch.randn(H, D_out).type(dtype), requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">1e-6</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</span><br><span class="line">    <span class="comment"># Construct an instance of our MyReLU class to use in our network</span></span><br><span class="line">    relu = MyReLU()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Forward pass: compute predicted y using operations on Variables; we compute</span></span><br><span class="line">    <span class="comment"># ReLU using our custom autograd operation.</span></span><br><span class="line">    y_pred = relu(x.mm(w1)).mm(w2)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute and print loss</span></span><br><span class="line">    loss = (y_pred - y).pow(<span class="number">2</span>).sum()</span><br><span class="line">    print(t, loss.data[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Use autograd to compute the backward pass.</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update weights using gradient descent</span></span><br><span class="line">    w1.data -= learning_rate * w1.grad.data</span><br><span class="line">    w2.data -= learning_rate * w2.grad.data</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Manually zero the gradients after updating weights</span></span><br><span class="line">    w1.grad.data.zero_()</span><br><span class="line">    w2.grad.data.zero_()</span><br></pre></td></tr></table></figure>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://mxxhcm.github.io/2019/05/08/pytorch-distributions/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="马晓鑫爱马荟荟">
      <meta itemprop="description" content="记录硕士三年自己的积累">
      <meta itemprop="image" content="/images/favicon.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="mxxhcm's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/05/08/pytorch-distributions/" class="post-title-link" itemprop="http://mxxhcm.github.io/page/5/index.html">pytorch distributions</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-05-08 21:53:46" itemprop="dateCreated datePublished" datetime="2019-05-08T21:53:46+08:00">2019-05-08</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-05-17 16:15:22" itemprop="dateModified" datetime="2019-05-17T16:15:22+08:00">2019-05-17</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/pytorch/" itemprop="url" rel="index"><span itemprop="name">pytorch</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="torch-distributions"><a href="#torch-distributions" class="headerlink" title="torch.distributions"></a>torch.distributions</h2><p>这个库和gym.space库很相似，都是提供一些分布，然后从中采样。<br>常见的有ExponentialFamily,Bernoulli,Binomial,Categorical,Exponential,Gamma,Independent,Laplace,Multinomial,MultivariateNormal。这里不做过程陈述，可以看<a href="http://localhost:4000/2019/04/12/gym%E4%BB%8B%E7%BB%8D/" target="_blank" rel="noopener">gym</a>中。</p>
<h3 id="Categorical"><a href="#Categorical" class="headerlink" title="Categorical"></a>Categorical</h3><p>对应tensorflow中的<a href="https://github.com/mxxhcm/myown_code/blob/master/tf/some_ops/tf_multinominal.py" target="_blank" rel="noopener">tf.multinomial</a>。<br>类原型：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CLASS torch.distributions.categorical.Categorical(probs=<span class="literal">None</span>, logits=<span class="literal">None</span>, validate_args=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure></p>
<p>参数probs只能是$1$维或者$2$维，而且必须是非负，有限非零和的，然后将其归一化到和为$1$。<br>这个类和torch.multinormal是一样的，从$\{0,\cdots, K-1\}$中按照probs的概率进行采样，$K$是probs.size(-1)，即是size()矩阵的最后一列，$2$维时把第$1$维当成了batch。</p>
<p>举一个简单的例子，<a href="https://github.com/mxxhcm/code/blob/master/pytorch/pytorch_test/torch_distribution.py" target="_blank" rel="noopener">代码</a>。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.distributions <span class="keyword">as</span> diss</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">torch.manual_seed(<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">m = diss.Categorical(torch.tensor([<span class="number">0.25</span>, <span class="number">0.25</span>, <span class="number">0.25</span>, <span class="number">0.25</span> ]))</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">    print(m.sample())</span><br><span class="line"></span><br><span class="line">m = diss.Categorical(torch.tensor([[<span class="number">0.5</span>, <span class="number">0.25</span>, <span class="number">0.25</span>], [<span class="number">0.25</span>, <span class="number">0.25</span>, <span class="number">0.5</span>]]))</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">    print(m.sample())</span><br></pre></td></tr></table></figure></p>
<p>输出结果如下：</p>
<blockquote>
<p>tensor(2)<br>tensor(1)<br>tensor(1)<br>tensor(1)<br>tensor(1)<br>tensor([2, 2])<br>tensor([1, 2])<br>tensor([0, 1])<br>tensor([0, 2])<br>tensor([0, 0])</p>
</blockquote>
<p>作为对比，gym.spaces.Discrete示例如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> gym <span class="keyword">import</span> spaces</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.Discrete</span></span><br><span class="line"><span class="comment"># 取值是&#123;0, 1, ..., n - 1&#125;</span></span><br><span class="line">dis = spaces.Discrete(<span class="number">5</span>)</span><br><span class="line">dis.seed(<span class="number">5</span>)</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">    print(dis.sample())</span><br></pre></td></tr></table></figure></p>
<p>输出结果是：</p>
<blockquote>
<p>3<br>0<br>1<br>0<br>4</p>
</blockquote>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>1.<a href="https://pytorch.org/docs/stable/distributions.html" target="_blank" rel="noopener">https://pytorch.org/docs/stable/distributions.html</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/4/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/4/">4</a><span class="page-number current">5</span><a class="page-number" href="/page/6/">6</a><span class="space">&hellip;</span><a class="page-number" href="/page/15/">15</a><a class="extend next" rel="next" href="/page/6/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <div class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/favicon.jpg" alt="马晓鑫爱马荟荟">
            
              <p class="site-author-name" itemprop="name">马晓鑫爱马荟荟</p>
              <p class="site-description motion-element" itemprop="description">记录硕士三年自己的积累</p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">149</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  <a href="/categories/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">13</span>
                    <span class="site-state-item-name">分类</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  <a href="/tags/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">146</span>
                    <span class="site-state-item-name">标签</span>
                  </a>
                </div>
              
            </nav>
          

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="https://github.com/mxxhcm" title="GitHub &rarr; https://github.com/mxxhcm" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="mailto:mxxhcm@gmail.com" title="E-Mail &rarr; mailto:mxxhcm@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                </span>
              
            </div>
          

          

          
          

          
            
          
          

        </div>
      </div>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">马晓鑫爱马荟荟</span>

  

  
</div>


  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v3.8.0</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> v6.6.0</div>




        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  
    <script src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  


  


  <script src="/js/src/utils.js?v=6.6.0"></script>

  <script src="/js/src/motion.js?v=6.6.0"></script>



  
  


  <script src="/js/src/affix.js?v=6.6.0"></script>

  <script src="/js/src/schemes/pisces.js?v=6.6.0"></script>



  

  


  <script src="/js/src/bootstrap.js?v=6.6.0"></script>



  



  






<!-- LOCAL: You can save these files to your site and update links -->
    
        
        <link rel="stylesheet" href="https://aimingoo.github.io/gitmint/style/default.css">
        <script src="https://aimingoo.github.io/gitmint/dist/gitmint.browser.js"></script>
    
<!-- END LOCAL -->

    

    







  





  

  

  

  

  
  

  
  
    
      
    
      
    
      
    
      
    
      
        
      
    
      
    
      
    
      
    
      
    
      
    
  

  
    
      <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      },
      TeX: {equationNumbers: { autoNumber: "AMS" }}
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<script src="//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<style>
.MathJax_Display {
    overflow: auto hidden;
}
</style>

    
  


  
  

  

  

  

  

  

  

</body>
</html>

<!DOCTYPE html>












  


<html class="theme-next pisces use-motion" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
































<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=6.6.0" rel="stylesheet" type="text/css">



  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.jpg?v=6.6.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.jpg?v=6.6.0">










<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '6.6.0',
    sidebar: {"position":"left","display":"always","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="记录硕士三年自己的积累">
<meta property="og:type" content="website">
<meta property="og:title" content="mxxhcm&#39;s blog">
<meta property="og:url" content="http://mxxhcm.github.io/page/28/index.html">
<meta property="og:site_name" content="mxxhcm&#39;s blog">
<meta property="og:description" content="记录硕士三年自己的积累">
<meta property="og:locale" content="zh-CN">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="mxxhcm&#39;s blog">
<meta name="twitter:description" content="记录硕士三年自己的积累">



  <link rel="alternate" href="/atom.xml" title="mxxhcm's blog" type="application/atom+xml">




  <link rel="canonical" href="http://mxxhcm.github.io/page/28/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>mxxhcm's blog</title>
  












  <noscript>
  <style>
    .use-motion .motion-element,
    .use-motion .brand,
    .use-motion .menu-item,
    .sidebar-inner,
    .use-motion .post-block,
    .use-motion .pagination,
    .use-motion .comments,
    .use-motion .post-header,
    .use-motion .post-body,
    .use-motion .collection-title { opacity: initial; }

    .use-motion .logo,
    .use-motion .site-title,
    .use-motion .site-subtitle {
      opacity: initial;
      top: initial;
    }

    .use-motion .logo-line-before i { left: initial; }
    .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">mxxhcm's blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
    
      
    

    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>标签</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">

    
    
    
      
    

    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>分类</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>归档</a>

  </li>

      
      
    </ul>
  

  

  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
            

          
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://mxxhcm.github.io/2019/03/13/cnn/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="马晓鑫爱马荟荟">
      <meta itemprop="description" content="记录硕士三年自己的积累">
      <meta itemprop="image" content="/images/favicon.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="mxxhcm's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/03/13/cnn/" class="post-title-link" itemprop="http://mxxhcm.github.io/page/28/index.html">CNN</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-03-13 15:21:27" itemprop="dateCreated datePublished" datetime="2019-03-13T15:21:27+08:00">2019-03-13</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-12-17 15:05:22" itemprop="dateModified" datetime="2019-12-17T15:05:22+08:00">2019-12-17</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/机器学习/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="cnn">CNN</h2>
<h3 id="图片的表示">图片的表示</h3>
<p>图像在计算机中是一堆按顺序排列的顺子，数值为0到255。0表示最暗，255表示最亮。我们可以把这堆数字用一个长长的一维数组来表示，但是这样会失去平面结构的信息，为保留该结构信息，我们通常会选择矩阵的表示方式，用一个nn的矩阵来表示一个图像。对于黑白颜色的灰度图来说，我们只需要一个nn的矩阵表示即可。对于一个彩色图像，我们会选择RGB颜色模型来表示。<br>
在彩色图像中，我们需要用三个矩阵去表示一张图，也可以理解为一个三维张量，每一个矩阵叫做这张图片的一个channel。这个三维张量可以表示为(width,length,depth),一张图片就可以用这样一个张量来表示。</p>
<h3 id="卷积神经网络-cnn">卷积神经网络(CNN)</h3>
<h4 id="作用">作用</h4>
<p>让权重在不同位置共享</p>
<h4 id="filter和stride">filter和stride</h4>
<p>filter又叫做kernel或者feature detector。filter会对输入的局部区域进行处理，filter处理的局部区域的范围叫做filter size。比如说一个filter的大小为(3,3),那么这个filter会一次处理width=3，length = 3的区域。卷积神经网络会用filter对整个输入进行扫描，一次移动的多少叫做stride。filter处理一次的输出为一个feature map。</p>
<h4 id="depth">depth</h4>
<p>对于filter来说，我们一般说它的大小为（3，3）只说了它在平面的大小，但是输入的图片一般是一个RGB的三维张量，对于deepth这一个维度，如果为1的话，那么filter是（3,3），但是如果deepth大于1的话，这个filter的deepth维度一般是和张量中的deepth维度一样的。<br>
deepth=1时，filter=（3,3），处理输入中33 个节点的值<br>
deepth=2时，filter=（3,3），会处理输入中332个节点的值<br>
deepth=n时，filter=（3,3），会处理输入中$33\times n$个节点的值</p>
<h4 id="zero-paddings">zero paddings</h4>
<p>因为经过filter处理后，输入的矩阵维度会变小，所以，如果经过很多层filter处理后，就会变得越来越少，因此，为了解决这个问题，提出了zero paddings，zero padding是在filter要处理的输入上，在输入的最外层有选择的加上一行（列）或多行（列）0，从而保持输入经过filter处理之后形状不变。</p>
<h4 id="feature-map">feature map</h4>
<p>一个filter的输出就是一个feature map，该feature map的width和height为：$(input_size + 2\times padding_size - filter_size)/stride + 1$<br>
一个filter可以提取一个feature，得到一个feature map，为了提取多个feature，需要使用多个filters，最后可以得到多个feature map。</p>
<p>所以说，feature map是一类值，因为它对应的是一个filter，给定不同的输入images，一个feature map可以有不同的取值。这个问题是我在看ZFNet中遇到的，因为它在原文中说<br>
“For a given feature map, we show the top 9 activations”。给定一个feature map，这里应该是在所有样本中选择最大的$9$个activations对应的images。<br>
“the strongest activation (across all training examples) within a given feature map”。给定一个feature map，在所有样本中选择一个最强的activation。</p>
<h4 id="activate-function">activate function</h4>
<p>一般使用非线性激活函数relu对feature map进行变化</p>
<h4 id="pooling">pooling</h4>
<h5 id="maxpooling">maxpooling</h5>
<p>它基本上采用一个filter和一个同样长度的stride通常是（2,2）和2，然后把它应用到输入中，输出filter卷积计算的每个区域中的最大数字，这个pooling是在各个维度上分别进行的。<br>
比如一个 22422464的input，经过一个（2,2）的maxpooling会输出一个11211232的张量</p>
<h5 id="averagepooling">averagepooling</h5>
<h4 id="fc-layers">fc layers</h4>
<h2 id="alexnet-2012">Alexnet(2012)</h2>
<p>论文名称：ImageNet Classification with Deep Convolutional Neural Networks<br>
论文地址：<a href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" target="_blank" rel="noopener">http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf</a></p>
<h3 id="概述">概述</h3>
<p>作者提出了一个卷积神经网络架构对Imagenet中$1000$类中的$120$万张图片进行分类。网络架构包含$5$个卷积层，$3$个全连接层，和一个$1000$-way的softmax层，整个网络共有$6000$万参数，$65000$个神经元。作者提出了一些方法提高性能和减少训练的时间，并且介绍了一些防止过拟合的技巧。最后在imagenet测试集上，跑出$37.5%$的top-1 error以及$17.0%$的top-5 error。<br>
本文主要的contribution：</p>
<ol>
<li>给出了一个benchmark－Imagenet</li>
<li>提出了一个CNN架构</li>
<li>ReLU激活函数</li>
<li>dropout的使用</li>
<li>数据增强，四个角落和中心的crop以及对应的horizontial 翻转。</li>
</ol>
<h3 id="问题">问题</h3>
<p>1.数据集太小，都是数以万计的，需要更大的数据集。</p>
<h3 id="创新">创新</h3>
<h4 id="relu非线性激活函数">ReLU非线性激活函数</h4>
<h5 id="作用-v2">作用</h5>
<p>作者说实验表明ReLU可以加速训练过程。</p>
<h5 id="saturating-nonlinearity">saturating nonlinearity</h5>
<p>一个饱和的激活函数会将输出挤压到一个区间内。</p>
<blockquote>
<p>A saturating activation function squeezes the input.</p>
</blockquote>
<p><strong>定义</strong><br>
f是non-saturating 当且仅当$|lim_{z\rightarrow -\infty} f(z)| \rightarrow + \infty$或者$|lim_{z\rightarrow +\infty} f(z)| \rightarrow + \infty$<br>
f是saturating 当且仅当f不是non-saturating<br>
<strong>例子</strong><br>
ReLU就是non-saturating nonlinearity的激活函数，因为$f(x) = max(0, x)$，如下图所示。<br>
<img src="/2019/03/13/cnn/relu.png" alt="relu"><br>
当$x$趋于无穷时，$f(x)$也趋于无穷。<br>
sigmod和tanh是saturating nonlinearity激活函数，如下图所示。<br>
<img src="/2019/03/13/cnn/sigmod.png" alt="sigmo"><br>
<img src="/2019/03/13/cnn/tanh.png" alt="tanh"></p>
<h4 id="多块gpu并行">多块GPU并行</h4>
<p>作者使用了两块GPU一块运行，每个GPU中的参数个数是一样的，在一些特定层中，两个GPU中的参数信息可以进行通信。</p>
<h4 id="overlapping-pooling">Overlapping Pooling</h4>
<p>就是Pooling kernel的size要比stride大。比如一个$12\times 12$的图片，用$5\times 5$的pooling kernel，步长为$3$，步长要比kernel核小，即$3$比$5$小。<br>
为什么这能减小过拟合？</p>
<ul>
<li>可能是减小了Pooling过程中信息的丢失。</li>
</ul>
<blockquote>
<p>If the pooling regions do not overlap, the pooling regions are disjointed and if that is the case, more information is lost in each pooling layer. If some overlap is allowed the pooling regions overlap with some degree and less spatial information is lost in each layer.[4]</p>
</blockquote>
<h4 id="数据增强">数据增强</h4>
<p>目的：防止过拟合</p>
<h5 id="裁剪和翻转">裁剪和翻转</h5>
<p>输入是$256\times 256 \times 3$的图像。<br>
训练：对每张图片都提取多个$224\times 224$大小的patch，这样子总共就多产生了$(256-224)\times (256-224) = 1024$个样本，然后对每个patch做一个水平翻转，就有$1024\times 2 = 2048$个样本。<br>
测试：通过对每张图片裁剪五个（四个角落加中间）$224\times 224$的patches，并且对它们做翻转，也就是有$10$个patches，网络对十个patch的softmax层输出做平均作为预测结果。</p>
<h5 id="在图片上调整rgb通道的密度">在图片上调整RGB通道的密度</h5>
<p>使用PCA对RGB值做主成分分析。对于每张训练图片，加上主成分，其大小正比于特征值乘上一个均值为$0$，方差为$0.1$的高斯分布产生的随机变量。对于一张图片$x,y$点处的像素值$I_{xy}=[I_{xy}^R, I_{xy}<sup>G,I_{xy}</sup>B]^T$，加上$[\bold{p_1},\bold{p_2},\bold{p_3}][\alpha_1\lambda_1,\alpha_2\lambda_2,\alpha_3\lambda_3]$，其中$[\bold{p_1},\bold{p_2},\bold{p_3}]$是特征向量，$\lambda_i$是特征值，$\alpha_i$就是前面说的随机变量。</p>
<h4 id="dropout">Dropout</h4>
<p>通过学习鲁棒的特征防止过拟合。<br>
在训练的时候，每个隐藏单元的输出有$p$的概率被设置为$0$，在该次训练中，如果这个神经元的输出被设置为$0$，它就对loss函数没有贡献，反向传播也不会被更新。对于一层有$N$个神经单元的全连接层，总共有$2^N$种神经元的组合结果，这就相当于训练了一系列共享参数的模型。<br>
在测试的时候，所有隐藏单元的输出都不丢弃，但是会乘上$p$的概率，相当于对一系列集成模型取平均。具体可见<a href="https://mxxhcm.github.io/2019/03/23/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-dropout/">dropout</a><br>
在该模型中，作者在三层全连接层的前两层输出上加了dropout。</p>
<h4 id="局部响应归一化-local-response-normalizaiton">局部响应归一化(Local Response Normalizaiton)</h4>
<p>事实上，后来发现这个东西没啥用。但是这里还是给出一个公式。</p>
<p>$$ b^i_{x,y} = \frac{a^i_{x,y}}{(k+\alpha \sum<sup>{min(N-1,\frac{i+n}{2})}_{j=max(0,\frac{i-n}{2})}(a</sup>j_{x,y})^2)^{\beta}}$$<br>
其中$a^i_{x,y}$是在点$(x,y)$处使用kernel $i$之后，在经过ReLU激活函数。$k,n,\alpha,\beta$是超参数。</p>
<blockquote>
<p>It seems that these kinds of layers have a minimal impact and are not used any more. Basically, their role have been outplayed by other regularization techniques (such as dropout and batch normalization), better initializations and training methods.</p>
</blockquote>
<h3 id="整体架构">整体架构</h3>
<h4 id="目标函数">目标函数</h4>
<p>多峰logistic回归。</p>
<h4 id="并行框架">并行框架</h4>
<p>下图是并行的架构，分为两层，上面一层用一个GPU，下面一层用一个GPU，它们只在第三个卷积层有交互。<br>
<img src="/2019/03/13/cnn/alexnet.png" alt="alexnet"></p>
<h4 id="简化框架">简化框架</h4>
<p>下图是简化版的结构，不需要使用两个GPU。<br>
<img src="/2019/03/13/cnn/alexnet_simple.png" alt="alexnet_simple"></p>
<h4 id="数据流-简化框架">数据流（简化框架）</h4>
<p>输入是$224\times 224 \times 3$的图片，第一层是$96$个stride为$4$的$11\times 11\times 3$卷积核构成的卷积层，输出经过max pooling(步长为2，kernel size为3)输入到第二层；第二层有$256$个$5\times 5\times 96$个卷积核，输出经过max pooling(步长为2，kernel size为3)输入到第三层；第三层到第四层，第四层到第五层之间没有经过pooling和normalization)，第三层有384个$3\times 3\times 256$个卷积核，第四层有$384$个$3\times 3\times 384$个卷积核，第五层有$256$个$3\times 3\times 384$个卷积核。然后接了两个$2048$个神经元的全连接层和一个$1000$个神经元的全连接层。</p>
<h3 id="实验">实验</h3>
<h4 id="datasets">Datasets</h4>
<p>ILSVRC-2010</p>
<h4 id="baselines">Baselines</h4>
<ul>
<li>Sparse coding</li>
<li>SIFT+FV</li>
<li>CNN</li>
</ul>
<h4 id="metric">Metric</h4>
<ul>
<li>top-1 error rate</li>
<li>top-5 error rate</li>
</ul>
<h3 id="代码">代码</h3>
<p>pytorch实现<br>
<a href="https://github.com/mxxhcm/myown_code/blob/master/CNN/alexnet.py" target="_blank" rel="noopener">https://github.com/mxxhcm/myown_code/blob/master/CNN/alexnet.py</a></p>
<h2 id="maxout-networks">Maxout networks</h2>
<p>论文名称：Maxout Networks<br>
下载地址：<a href="https://arxiv.org/pdf/1302.4389.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1302.4389.pdf</a></p>
<h2 id="nin">NIN</h2>
<p>论文名称：Network In Network<br>
论文地址：<a href="https://arxiv.org/pdf/1312.4400.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1312.4400.pdf</a></p>
<h3 id="摘要">摘要</h3>
<p>这篇文章作者使用更复杂的micro神经网络代替CNN，用一个mlp实例化micro nn。CNN中的filter用的是generalized linear model(GLM)。本文使用nonlinear的FA，作者用一个multi layers perceptron 取代GLM。通过和cnn类似的操作对input进行sliding得到feature maps，然后传入下一层，deep NIN通过堆叠多层类似的结构生成。同时作者使用average pooling取代最后的fullcy connected layer。<br>
本文的两个contribution是：</p>
<ol>
<li>使用MLP代替CNN中linear model，引入$1\times 1$的filter</li>
<li>使用average pooling代替fully connected layer。</li>
</ol>
<p>在传统的CNN中，一个concept的不同variation可能需要多个filters，这样子会让下一层的的计算量太大。高层CNN的filters对应input的区域更大，高层的concept是通过对底层的concepts进行组合得到的。这里作者在每一层都对local patch进行组合，而不是在高层才开始进行组合，在每一层中，micro network计算更加local patches更abstract的特征。</p>
<h3 id="network-in-network">Network in Network</h3>
<h4 id="mlp-convolution-layers">MLP convolution layers</h4>
<p>为什么使用MLP代替GLP？</p>
<ol>
<li>MLP和CNN的结构兼容，可以使用BP进行训练；</li>
<li>MLP本身就是一个deep model，满足feature复用的想法。</li>
</ol>
<p>如下图所示，是MLP CNN和GLP CNN的区别。<br>
<img src="/2019/03/13/cnn/mlp_vs_linear.png" alt="mvl_vs_glp"></p>
<p>MLP的公式如下。<br>
<img src="/2019/03/13/cnn/equ.png" alt="equ"><br>
从cross channel(feature maps)的pooling角度来看，上面的公式相当于在一个正常的conv layer上进行多次的parametric pooling，每一个pooling layer对输入的feature map进行线性加权，经过一个relu层之后在下一层继续进行pooling。Cross channel pooled的feature maps在接下来的层中多次进行cross channel pooling。这个cross channel pooling的结构的作用是学习复杂的cross channel信息。<br>
其实整个cross channel的paramteric pooling结构相当于一个普通的卷积加上了多个$1\times 1$的卷积，如下图所示：<br>
<img src="/2019/03/13/cnn/11filter.png" alt="11filter"></p>
<h4 id="global-average-pooling">Global average pooling</h4>
<p>FC layers证明是容易过拟合的，dropout被提出来正则化fc layers的参数。<br>
本文提出的global average pooling取代了CNN的fc layers，直接在最后一个mlpconv layer中对应于分类任务中的每个类别生成一个feature map。然后用在feature maps上的average pooling代替fc layers，然后把它送入softmax layer。原来的CNN是将feature map reshape成一个一维向量，现在是对每一个feature map进行一个average pooling，有多少个feature map就有多少个pooling，相当于一个feature map对应与一个类型。<br>
这样做有以下几个好处：</p>
<ol>
<li>在fc layers上的global average pooling让feature map和categories对应起来，feature map可以看成类别的置信度。</li>
<li>直接进行average pooling不用优化fc layer的参数，也就没有过拟合问题。</li>
<li>global average pooling对全局信息进行了加和，对于input的spatial信息更加鲁邦。</li>
</ol>
<h4 id="nin-v2">NIN</h4>
<p>如下图所示，是NIN的整体架构。<br>
<img src="/2019/03/13/cnn/nin.png" alt="nin"><br>
下图是一个具体参数化的示例<br>
<img src="/2019/03/13/cnn/instance.png" alt="instance"></p>
<h3 id="实验-v2">实验</h3>
<h2 id="overfeat-2013">OverFeat(2013)</h2>
<p>论文名称：OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks<br>
论文地址：<a href="https://arxiv.org/pdf/1312.6229.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1312.6229.pdf</a></p>
<h3 id="概述-v2">概述</h3>
<p>本文提出了一个可用于classification, localization和detection等任务的CNN框架。<br>
ImageNet数据集中大部分选择的是几乎填满了整个image中心的object，image中我们感兴趣的objects的大小和位置也可能变化很大。为了解决这个问题，作者提出了三个方法：</p>
<ol>
<li>用sliding window和multiple scales在image的多个位置apply ConvNet。即使这样，许多window中可能包含能够完美识别object类型的一部分，比如一个狗头。。。最后的结果是classfication很好，但是localization和detection结果很差。</li>
<li>训练一个网络不仅仅预测每一个window的category distribution，还预测包含object的bounding box相对于window的位置和大小。</li>
<li>在每个位置和大小累加每个category的evidence</li>
</ol>
<h3 id="vision任务">Vision任务</h3>
<p>classification，localization和detection。classification和localization通常只有一个很大的object，而detection需要找到很多很小的objects。<br>
classification任务中，每个image都有一个label对应image中主要的object的类型。为了找到正确的label，每个图片可以猜$5$次（图片中可能包含了没有label的数据）。localization任务中，不仅要给出label，还需要找到这个label对应的bouding box，bounding box和groundtruth至少要有$50$匹配，label和bounding box也需要匹配。detection和localization不同的是，detection任务中可以有任何数量的objects，false positive会使用mean average precison measure。localization任务可以看成classification到detection任务的一个中间步。</p>
<h3 id="fcn">FCN</h3>
<p>用卷积层代替全连接层。具体是什么意思呢。<br>
alexnet中，有5层卷积层，3层全连接层。假设第五层的输出是$5\times 5 \times 512$，$512$是output channels number，$5\times 5$是第五层的feature maps的大小。如果使用全连接的话，假设第六层的输出单元是$N$个，第六层权重总共是$(5\times 5\times 512) * (N)$，对于一个训练好的网络，图片的输入大小是固定的，因为第六层是一个全连接层，输入的大小是需要固定的。如果输入一个其他大小的图片，网络就会出错，所以就有了Fully Convolutional networks，它可以处理不同大小的输入图片。<br>
如下所示，使用某个大小的image训练的网络，在classifier处用卷积层替换全连接层，如果使用全连接层，首先将$(5, 5, out_channels)$的feature map进行flatten $5\times 5\times out_channels$，然后经过三层全连接，最后输出一个softmax的结果。而fcn使用卷积层代替全连接，使用$N$个$5\times 5$的卷积核，直接得到$1\tims 1 \times N$的结果，最后得到一个$1\times 1\times C$的输出，$C$代表图像类别，$N$代表全连接层中隐藏节点的数量。<br>
<img src="/2019/03/13/cnn/fcn.png" alt="fcn"><br>
事实上，FCN和全连接的本质上都是一样的，只不过一个进行了flatten，一个直接对feature map进行操作，直接对feature map操作可以处理不同大小的输入，而flatten不行。<br>
当输入图片大小发生变化时，输出大小也会改变，但是网络并不会出错，如下所示：<br>
<img src="/2019/03/13/cnn/fcn2.png" alt="fcn2"><br>
最后输出的结果是$2\times 2 \times C$的结果，可以直接对它们取平均，最后得到一个$1\times 1\times C$的分类结果。</p>
<h3 id="offset-max-pooling">offset Max pooling</h3>
<p>我们之前做max pooling的时候，设$kernel_size=3, stride_size=1$，如果feature map是$3$的倍数，那么只有一个pooling的结果，但是如果不是$3$的倍数，max pooling会很多个结果，比如有个$20\times 20$的feature map，在$x,y$上做max pooling分别有三种结果，分别从$x,y$的位置$0$开始，位置$1$开始，位置$2$开始，排列组合有$9$中情况，这九种情况的结果是不同的。<br>
如下图所示，在一维的长为$20$的pixels上做maxpooling，有三种情况。<br>
<img src="/2019/03/13/cnn/offset_maxpooling.png" alt="offset_maxpooling"></p>
<h3 id="overfeat">overfeat</h3>
<p>这两个方法中，fcn是在输入图片上进行的window sliding，而offset maxpooling是在feature map进行的window sliding，这两个方法结合起来就是overfeat，要比alexnet直接在输入图片上进行window sliding 要好。</p>
<h3 id="classification">Classification</h3>
<h4 id="training">training</h4>
<ul>
<li>datset<br>
Image 2012 trainign set（1.2million iamges，C=$1000$ classes)。</li>
<li>data argumented<br>
对每张图片进行下采样，所以每个图片最小的dimension需要是$256$。<br>
提取$5$个random crops以及horizaontal flips，总共$10$个$221\times 221$的图片</li>
<li>batchsize<br>
$128$</li>
<li>初始权重<br>
$(\mu, \sigma)= (0, 1\times 10^{-2})$</li>
<li>momentum<br>
0.6</li>
<li>l2 weigth decay<br>
$1\times 10^{-5}$</li>
<li>lr<br>
初始是$5\times 10^{-2}$，在$(30,50,60,70,80)$个epoches后，乘以$0.5$</li>
<li>non-spatial<br>
这个说的是什么呢，在test的时候，会输出多个output maps，对他们的结果做平均，而在training的时候，output maps是$1\times 1$。</li>
</ul>
<h4 id="model架构">model架构</h4>
<p>下图展示的是fast model，spatial input size在train和test时候是不同的，这里展示的是train时的spatial seize。layer 5是最上层的CNN，receptive filed最大。后续是FC layers，在test时候使用了sliding window。在spatial设置中，FC-layers替换成了$1\times 1$的卷积。<br>
<img src="/2019/03/13/cnn/overfeat_fast.png" alt="overfeat_fast"><br>
下图给出了accuracy model的结构，<br>
<img src="/2019/03/13/cnn/overfeat_accuracy.png" alt="overfeat_accuracy"><br>
总的来说，这两个模型都在alexnet上做了一些修改，但是整体架构没有大的创新。</p>
<h4 id="多scale-classification">多scale classification</h4>
<p>alexnet中，对一张照片的$10$个views（中间，四个角和horizontal flip)的结果做了平均，这种方式可能会忽略很多趋于，同时如果不同的views有重叠的话，计算很redundant。此外，alexnet中只使用了一个scale。<br>
作者对每个iamge的每一个location和多个scale都进行计算。<br>
如下图，对应了不同大小的输入图片，layer 5 post pool中$(m\times n)\time(3\times 3)$，前面$m\times n$是fcn得到的不同位置的feature map，后面$3\times 3$是$kernel_size=3$的offset max pooling得到的featrue map。乘起来是所有的预测结果。<br>
<img src="/2019/03/13/cnn/multi_scale.png" alt="multi_scale"></p>
<h3 id="localization">localization</h3>
<h3 id="detection">Detection</h3>
<h2 id="zfnet-2014">ZFNet(2014)</h2>
<p>论文名称：Visualizing and Understanding Convolutional Networks<br>
论文地址：<a href="https://cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf" target="_blank" rel="noopener">https://cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf</a><br>
为什么叫ZFNet，两个作者名字首字母的拼写。</p>
<p>首先我有一个问题？就是什么是一个activation。在原文的$2.1$节，有这样一个介绍：</p>
<blockquote>
<p>We present a novel way to map these activities back to the input pixel space, showing what input pattern originally caused a given activation in the feature maps.<br>
我的理解是一个activation就是feature map中的一个unit。事实上，feature map也叫activation map，因为它是image中不同parts的acttivation，而叫feature map是因为它是image中找到特定的feature。</p>
</blockquote>
<h3 id="概述-v3">概述</h3>
<p>这篇文章从可视化的角度给出中间特征的和classifier的特点，分析如何改进alexnet来提高imagenet classification的accuracy。<br>
为什么CNN结果这么好？</p>
<ol>
<li>training set越来越大</li>
<li>GPU的性能越来越好</li>
<li>Dropout等正则化技术</li>
</ol>
<p>但是CNN还是一个黑盒子，我们不知道它为什么表现这么好？这篇文章给出了一个可视化方法可视化任意层的feature。</p>
<p>那么本文的contribution是什么呢？使用deconvnet进行可视化，通过分析特征行为，对alexnet进行fine tune提升模型性能。</p>
<h3 id="使用deconvnet可视化">使用deconvnet可视化</h3>
<p>什么是deconvnet？可以看成和convnet拥有同样组成部分（pooling, filter)等，但是是反过来进行的。如下图所示，convnet是把pixels映射到feature，或者到底层features映射到高层features，而deconvnet是把高层features映射到底层features，或者把features映射到pixels。在测试convnet中给定feature maps的一个activation时，设置所有其他的activation为0，将这个feature map传入deconvnet网络中。<br>
<img src="/2019/03/13/cnn/fig1.png" alt="fig1"><br>
图片左上为deconv，右上为conv。conv的流程为filter-&gt;rectify-&gt;pooling；deconv的流程为unpool-&gt;rectify-&gt;filter。</p>
<h4 id="unpooling">Unpooling</h4>
<p>convnet中的max pooling是不可逆的，这里作者使用switch variables记录下max pooling后的元素在没有pooling时的位置，进行近似的恢复。</p>
<h4 id="rectification">Rectification</h4>
<p>convnet使用relu non-linearities。deconvnet还是使用relu，这里我有些不理解，为什么？为什么deconve还是使用relu</p>
<h4 id="filtering">Filtering</h4>
<p>deconvnet使用convnet中filters的transposed版本。</p>
<h3 id="training-v2">Training</h3>
<h4 id="整体架构-v2">整体架构</h4>
<p><img src="/2019/03/13/cnn/fig3.png" alt="fig3.png"></p>
<ul>
<li>training set<br>
1.3百万张图片，1000类</li>
<li>processed<br>
每个RGB图像resized成最小边维度为$256$，cropping中间的$256 \times 256$，减去所有像素的平均值。crops$10$个$224\times 224$（四个角落和中心以及horizontal flips)</li>
<li>优化方法<br>
带momentumSGD</li>
<li>batch size<br>
128</li>
<li>lr<br>
初始是$10^{-2}$,然后手动anneal</li>
<li>momentum<br>
0;9</li>
<li>Dropout<br>
layer 6和layer 7,0.5</li>
<li>weights和biases初始化<br>
weights设置为$10^{-2}$，biases设置为$0$</li>
<li>normalizaiton<br>
对第一层的filter，如果RMS超过了$10^{-1}$就设置为$10^{-1}$</li>
<li>训练次数<br>
70epochs</li>
</ul>
<h3 id="visualizaiton">Visualizaiton</h3>
<h4 id="feature-visualization">Feature visualization</h4>
<p>如下图所示，使用deconvnet可视化一些feacutre activation。给定一个feature map，选择其中最大的$9$个activations对应的样本，一个feature map是通过一个filter得到的，而一个filter提取的是一个特征，所以这$9$个activations都是一个filter提取的不同图片中的同一个特征。然后将它们输入deconvnet，得到pixel spaces，可以查看哪些不同的结构（哪些原始）产生了这个feature，展现这个filter对于输入deformation的invariance。在黑白图像的旁边有对应的图像原图，他们要比feature的variation更多，因为feature关注的是图像的invariance。比如layer 5的第一行第二列的九个图，这几个patch看起来差异很大，但是却在同一个feature map中，因为这个feature map关注的是背景中的草，并不是其他objects。更多的我们可以看出来，第二层对应corner和edge等，第三次对应更复杂的invariances，比如textures和text等。第四层更class-specific，第五层是object variation。<br>
<img src="/2019/03/13/cnn/fig2.png" alt="fig2"></p>
<h4 id="feature-evolution-durign-training">Feature evolution durign training</h4>
<p>下图随机选择了几个不同的feature，然后展示了他们在不同layer不同epochs（1, 2, 5, 10, 20, 30, 40, 64）的可视化结果。<br>
<img src="/2019/03/13/cnn/fig4.png" alt="fig4"></p>
<h4 id="架构选择">架构选择</h4>
<p>通过可视化alexnet的first layer和second layer，有了各种各样的问题。First layer中主要是high和low frequency的信息，而2nd layer有很多重复的，因为使用stride为$4$而不是$2$。作者做了两个改进：</p>
<ol>
<li>将first layer的filter size从$11\times 11$改成了$7\times 7$</li>
<li>卷积的步长从$4$改成了$2$</li>
</ol>
<p>如下图所示：<br>
<img src="/2019/03/13/cnn/fig5.png" alt="fig5"></p>
<h4 id="occlusion-sensitivity">Occlusion Sensitivity</h4>
<p>model是否真的识别了object在image中的位置，还是仅仅使用了上下文信息？下图中的例子证明了model真的locate了object,当遮挡住物体的部分增大时，给出正确分类的概率就减小了。移动遮挡方块的位置，给出一个和方块位置相关的分类概率函数，我们可以看出来，model really works。<br>
<img src="/2019/03/13/cnn/fig6.png" alt="fig6"></p>
<h3 id="实验-v3">实验</h3>
<p>第一个实验通过使用，证明了前面的特征提取层和fc layers都是有用的。<br>
第二个实验保留前面的特征提取层和fc layers，将最后的softmax替换。</p>
<h2 id="vggnet-2014">VGGNet(2014)</h2>
<p>论文名称：VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION<br>
论文地址：<a href="https://arxiv.org/pdf/1409.1556.pdf%20http://arxiv.org/abs/1409.1556.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1409.1556.pdf http://arxiv.org/abs/1409.1556.pdf</a><br>
VGG是Visual Geometry Group的缩写</p>
<h3 id="概述-v4">概述</h3>
<p>这篇文章主要研究了CNN深度对大规模图像识别问题精度的影响。本文的主要contribution就是使用多层的$3\times 3$ filters替换大的filter，增加网络深度，提高识别精度。</p>
<h3 id="方案">方案</h3>
<h4 id="架构">架构</h4>
<p><strong>训练</strong>，输入$224\times 224$大小的RGB图片。对每张图片减去训练集上所有图片RGB 像素的均值。预处理后的图片被输入多层CNN中，CNN的filter是$3\times 3$的，作何也试了$1\times 1$的filter，相当于对输入做了一个线性变换，紧跟着一个non-linear 激活函数，这里的$1\times 1$的filter没有用于dimention reduction。stride设为$1$，添加padding使得卷积后的输出大小不变。同时使用了$5$个max-pooling层（并不是每一层cnn后面都有max-pooling)，max-pooling的window是$2\times 2$，stride是$2$。<br>
在训练的时候CNN后面接的是三个FC layers，前两个是$4096$单元，最后一层是$1000$个单元的softmax。所有隐藏层都使用ReLu非线性激活函数。<br>
在测试的时候使用fcn而不是直接flatten。</p>
<h4 id="配置">配置</h4>
<p>这篇文章给出了五个网络架构，用$A-E$表示，它们只有在深度上有所不同：从$11$层($8$个conv layers和$3$个FC layers)到$19$层（$16$个conv layers和$3$个FC layers）。Conv layers的channels很小，从第一层的$64$，每过一个max pooling layers，变成原理啊的两倍，直到$512$。具体如下表所示。<br>
<img src="/2019/03/13/cnn/vgg_conf.png" alt="vgg_conf"><br>
网络的参数个数如下表所示。<br>
<img src="/2019/03/13/cnn/vgg_weights_num.png" alt="vgg_weights_num"><br>
网络$A$的参数计算：<br>
\begin{align*}<br>
64\times 3\times 3\times 3 + \\<br>
128\times 3\times 3\times 64 + \\<br>
256\times 3\times 3\times 128 + \\<br>
256\times 3\times 3\times 256 + \\<br>
512\times 3\times 3\times 256 + \\<br>
512\times 3\times 3\times 512 + \\<br>
2\times 512\times 3\times 3\times 512 + \\<br>
7\times 7\times 512\times 4096 + \\<br>
4096\times 4096 + \\<br>
4096\times 1000 = \\<br>
132851392<br>
\end{align*}<br>
网络$B$的参数计算：<br>
\begin{align*}<br>
64\times 3\times 3\times 3 + \\<br>
128\times 3\times 3\times 64 + \\<br>
128\times 3\times 3\times 128 + \\<br>
256\times 3\times 3\times 128 + \\<br>
256\times 3\times 3\times 256 + \\<br>
256\times 3\times 3\times 256 + \\<br>
512\times 3\times 3\times 256 + \\<br>
512\times 3\times 3\times 512 + \\<br>
2\times 512\times 3\times 3\times 512 + \\<br>
7\times 7\times 512\times 4096 + \\<br>
4096\times 4096 + \\<br>
4096\times 1000 = \\<br>
133588672<br>
\end{align*}<br>
其实主要的网络参数还是在全连接层，$7\times 7\times 512\times 4096=102760448<br>
$。</p>
<h4 id="卷积核作用">卷积核作用</h4>
<ol>
<li>为什么要用三个$3\times 3$的conv layers替换$7\times 7$个conv layers？</li>
</ol>
<ul>
<li>使用三个激活函数而不是一个，让整个决策更discriminative。</li>
<li>减少了网络参数，三个有$C$个通道的$3\times 3$conv layers,总的参数是$3\tims(3<sup>2C</sup>2)=27C^2$，而一个$C$通道的$7\times 7$ conv layers，总参数是$49C^2$。可以看成是一种正则化。</li>
</ul>
<ol start="2">
<li>$1\times 1$ conv layers用来增加非线性程度，本文中使用的$1\times 1$的conv layers可以看成加了非线性激活函数的投影。</li>
</ol>
<h3 id="分类框架">分类框架</h3>
<h4 id="training-v3">training</h4>
<ul>
<li>目标函数<br>
多峰logistic regression</li>
<li>训练方法<br>
mini-batch gradient descent with momentum</li>
<li>batch size<br>
256</li>
<li>momentum<br>
0.9</li>
<li>正则化<br>
$L_2$参数正则化(5\codt 10^{-4})<br>
0.5 dorpout 用于前两个FC layers</li>
<li>lr<br>
初始值为$10^{-2}$，当验证集的accuracy不再提升时，除以$10$。学习率总共降了$3$次，$370K$次迭代后停止。</li>
<li>图像预处理<br>
从rescaled中随机cropped $224\times 224$的RGB图像。<br>
使用alexnet中的随机horizontal flipping和随机RGB colour shift。</li>
<li>iamge rescale<br>
用$S$表示training image的小边的大小，$S$也叫作train sacle。网络的输入是从training image中cropped得到的$224\times 224$的图像。所以只要$S$取任何不小于$224$的值即可，如果$S=224$，那么crop在统计上会captuer整个图片，完全包含training image最小的那边；$S&gt;&gt;224$的时候，crop会产生很小一部分的图像。<br>
作者尝试了固定$S$和不固定的$S$。对于固定$S$，设置$S=256$和$S=384$，首先在$S=256$上训练，然后用$S=256$训练的参数初始化$S=384$的参数，使用更小的初始学习率$10^{-3}$。不固定$S$时，$S$从$[S_{min}, S_{max}](S_{max}=512,S_{min}=256)$任意采样，然后crop。</li>
<li>VGG vs alexnet<br>
VGG参数多，深度深，但是收敛快，原因：</li>
</ul>
<ol>
<li>更小的filter带来的implicit regularisation</li>
<li>某些层的预先初始化。<br>
这个解决的是网络深度过深，某些初值使得网络不稳定的问题。解决方法：先随机初始化不是很深的网络A，进行训练。在训练更深网络的时候，使用A网络的值初始化前$4$个卷基层和最后三个FC layers。随机初始化的网络参数，从均值为$0$，方差为$10^{-2}$的高斯分布中采样得到。</li>
</ol>
<h4 id="testing">testing</h4>
<ol>
<li>测试的时候先把input image的窄边缩放到$Q$，$Q$也叫test scale，$Q$和$S$不一定需要相等。</li>
<li>这里和overfeat模型一样，在卷积网络之后采用了fcn，而不是fc layers。</li>
</ol>
<h3 id="classfication">classfication</h3>
<p>ILSVRC-2012，training($1.3M$张图片)，validation($50K张$)，testing($100K$张)<br>
两个metrics：top-1和top-5 error。top-1 error是multi-class classification error，不正确分类图像占的比例；top-5 error是预测的top-5都不是ground-truth。</p>
<h4 id="single-scale-evaluation">single scale evaluation</h4>
<p>$S$固定时，设置test image size $Q=S=256$；<br>
$S$抖动时，设置test image size $Q=0.5(S_{min}+S_{max})=0.5(256+512)=384$，$S\in [S_{min},S_{max}]$。</p>
<h4 id="multi-scale-evaluation">multi scale evaluation</h4>
<p>用同一个模型对不同rescaled大小的图片多次test，即对于不同的$Q$。<br>
固定$S$时，在三个不同大小的test image size $Q={S-32,S,S+32}$评估。<br>
$S$抖动时，模型是在$S\in [S_{min},S_{max}]$上训练的，在$Q={S_{min}, 0.5(S_{min}+S_{max}), S_{max}}$上进行test。</p>
<h4 id="多个crop-evaluation">多个crop evaluation</h4>
<p>这个是为了和alexnet做对比，alexnet网络在testing时，对每一张图片都进行多次cropped，对testing的结果做平均。</p>
<h4 id="convnet-funsion">convnet funsion</h4>
<p>之前作者的evaluation都是在单个的网络上进行的，作者还试了将不同网络的softmax输出做了平均。</p>
<h2 id="inception-v1-googlelenet">Inception V1(GoogleLeNet)</h2>
<h3 id="摘要-v2">摘要</h3>
<p>提出一种方法能够在不增加太多计算代价的同时增加网络的深度和宽度。</p>
<h3 id="motivation">motivation</h3>
<p>直接增加网络的深度和宽度有两个缺点：</p>
<ol>
<li>参数更多，容易过拟合，尤其是训练集太小的情况下，高质量的训练集很难生成。</li>
<li>需要更多的计算资源。比如两层CNN，即使每一层中线性增加filters的个数也会造成计算代价指数级增加。如果增加的权重接近$0$的话，计算代价就浪费了。而现实中的计算资源是有限的。</li>
</ol>
<p>如何解决这个问题呢？使用sparsity layers取代fully connetcted layers。但是现在的计算资源在处理non-uniform 的sparse data时是非常低效的，即使数值操作减小$100$倍，查找的时间也是很多的。而针对CPU和GPU的dense matrix计算能够加快fc layer的学习。现在绝大部分的机器学习视觉模型在sparsity spatial domain都仅仅利用了CNN，而convolution是和前一层patches的dense connection。1998年的convnet为了打破网络对称性，改善学习结果，使用的是random和sparse连接，而在alexnet中为了并行优化计算，使用了全连接。当前cv的state-of-the-art架构使用的都是unifrom structure，为了高效的进行dense计算，filters和batch size的数量都是很大的。<br>
稀疏性可以解决过拟合和资源消耗过多的问题，而稠密连接可以提高计算效率。所以接下来要做的是一个折中，利用filter维度的稀疏结构，同时利用硬件在dense matrices上的计算进行加速。<br>
Inception架构就是使用一个dense组件去逼近sparse结构的例子。</p>
<h3 id="算法">算法</h3>
<p>Inception的idea是使用dense组件近似卷积的局部稀疏结构。本文的旋转不变型是利用convolutional building blocks完成的，找到optimal local construction，然后不断堆叠。文章[11]中建议layer-by-layer的构建，分析上一层之间的关系，并将具有高相关性的units进行分组。这些相关的units cluster构建成了下一层的units，并且和上一层的units相连接。假设之前层中的每一个unit都对应输入图片中的一些region，这些units分组构成filter banks。这就意味着在靠近输入的层中我们会得到很多关于local regions相关的units。通过在下一层中使用$1\times 1$的卷积，可以找到关注于同一个region的很多个clusters。（这里加一些我自己的理解，$1\times 1$的卷积层可以找到那些重复的feature map？？）当然，也有可能有更大的cluster可以通过在更大的patches上进行卷积得到，所以这里同时在一层中同时使用$1\times 1, 3\times 3, 5\times 5$的filters，使用这些大小的filter仅仅是因为方便，然后将他们的输出进行组合当做下一层的输入。当然可以加上pooling，如下图所示。<br>
<img src="/2019/03/13/cnn/naive_inception.png" alt="naive inception"><br>
但是，这样子计算量还是很大，大量$3 \times 3, 5\times 5$在卷积时的计算量，如果再加上输入shape和输出shape相等的max pooling操作，下一层的输入维度相当大，计算开销j就爆炸了。这就使用了本文的第二个idea：使用$1\times 1$的filter降维减少计算量。在$3\times 3, 5\times 5$大小filter之前添加$1\times 1$的卷积进行降维。<br>
<img src="/2019/03/13/cnn/dr_inception.png" alt="dimension reduction inception"></p>
<p>这个架构的好处：</p>
<ol>
<li>在每一层都可以增加units的数量而不用担心计算量暴增。首先将上一层大量filters的输出进行进行降维，然后输入到下一层。</li>
<li>visual信息用不同的scales进行处理，然后拼接起来，这样子在下一层可以同时从不同scales中提出features。</li>
</ol>
<h3 id="googlenet">GoogLeNet</h3>
<p>作者给出了Inception的一个示例，叫GoogLeNet。网络具体配置如下：<br>
<img src="/2019/03/13/cnn/GoogLeNet.png" alt="GoogLeNet"><br>
其中，&quot;#$3 \times 3$ reduce&quot;和&quot;#$5 \times 5$ reduce&quot;表示在$3\times 3, 5\times 5$卷积之前使用$1\times 1$的filters个数，pool proj这一列表示在max pooling之后的$1\times 1$的filters个数。<br>
作者在GoogLeNet中还使用了两个额外的分类层辅助训练。通过观察得知相对shallower的网络有很好的性能，那么在反向传播时，深层网络的中间特征应该是很有判别力的。<br>
通过在网络中间添加辅助的classfiers，作者想要让网络底层也有判别力。在训练的时候，在$4a$和$4d$模块后添加分类器，然后将所有的loss乘上一个权重加到总的loss上，在test时，这些辅助网络被扔掉。</p>
<h2 id="batch-normalization">Batch Normalization</h2>
<p>论文名称：Batch Normalization: Accelerating Deep Network Training b<br>
y Reducing Internal Covariate Shift<br>
论文地址：<a href="https://arxiv.org/pdf/1502.03167.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1502.03167.pdf</a></p>
<h3 id="概述-v5">概述</h3>
<p>在训练深度神经网络的时候，随着训练的不断进行，网络权重在不停的变，除了第一层之外的每层输入也在不停的变，所以就使得权重每次都要去适应新的输入distributions。这就导致训练速度很慢，学习率的要很小，很难使用saturaing nonlinearities激活函数训练。作者把这个问题叫做internal covariate shift，提出了batch normalization解决该问题，bn对于参数初始化的要求没那么高，允许使用更高的学习率。<br>
BN可以看成一种正则化手段。</p>
<h3 id="简介">简介</h3>
<p>SGD相对于单个样本的GD来说，使用mini-batch的梯度作为整个训练集的估计值，效果更好；同时并行计算提高了效率。之前的工作使用ReLU，更好的初始化以及小的学习率来解决梯度消失问题，而本文作者的想法是让非线性输入的分布尽可能稳定，从而解决梯度饱和等问题，加快训练。本文提出的batch normalization通过固定每一层输入的均值和方差减少internal covariate shift，同时减少了gradients对于初始参数的依赖性。在使用了BN的网络中，也可以使用如sigmod和tanh的saturating nonlirearities激活函数，并不是一定要用relu激活函数。</p>
<h3 id="mini-batch-normalization">Mini-Batch Normalization</h3>
<p>Whitening每一层的所有inputs需要很大的代价，而且并不是每个地方都是可导的。作者进行了两个简化。第一个是并不是对所有输入的features进行whiten，而是对每一个feautre单独的normalization，将他们转化成均值为0，方差为1的数据。对于一个d维的输入$x=(x^1,\cdots, x^d)，对每一维进行normalize：<br>
$$\hat{x}^k= \frac{x^k - \mathbb{E}\left[x<sup>k\right]}{\sqrt{Var\left[x</sup>k\right]}}$$<br>
其中的期望和方差是整个training set 的期望和方差。但是仅仅normalize每一层的输入可能改变这一层的表示。比如normalize sigmod的输入会将它们的输出限制在非线性的线性区域。为了解决这个问题，在网络中添加的这个transformation应该能够表示identity transform，作者对每个activation $x<sup>k$引入了一对参数，$\gamma</sup>k, \beta^k$，它们对normalized value进行scale和shift：<br>
$$y^k = \gamma^k \hat{x}^k + \beta^k$$<br>
这些参数和模型参数一块，都是学习出来的，如果学习到$\gamma<sup>k=\sqrt{Var\left[x</sup>k\right]},\beta^k = \mathbb{E}\left[x^k\right]$，就可以表示恒等变换了。。<br>
上面说的是使用整个training set的方差和期望进行normaliza，事实上，在sgd中这是不切合实际的。因此，就引入了第二个简化，使用每个mini-batch的方差和期望进行normalize，并且方差和期望是针对于每一个维度计算的。给出一个大小为$m$的batch $B$，normalization独立的应用于每一个维度。用$\hat{x}_{1,\cdots, m}$表示normalized values，以及它们的linear transformation：$y_{1,\cdots,m}$。这个transform表示为：$BN_{\gamma, \beta}:x_{1,\cdots, m} \rightarrow y_{1,\cdots,m}$，称为Batch Normalization Transform，完整的算法如下：<br>
算法1 Batch Normalizing Transform<br>
输入：　mini-batch：$B={x_{1,\cdots, m}}，要学习的参数$\gamma,\beta$<br>
输出：${y_i=BN_{\gamma,\beta}(x_i)}$<br>
$\mu\leftarrow \frac{1}{m}\sum_{i=1}^mx_i$  计算batch的mean<br>
$\sigma^2_B\leftarrow \sum_{i=1}<sup>m(x_i-\mu_B)</sup>2$  计算batch的variance<br>
$\hat{x}_i\leftarrow \frac{x_i-\mu_B}{\sqrt{\simga^2_B+\epsilon}}$ normalize<br>
$y_i\leftarrow \gamma \hat{x}_i+ \beta \equiv BN_{\gamma, \beta}(x_i)$ scale以及shift。<br>
整个过程的loss还可以通过backpropagate进行传播，即它是可导的。</p>
<h3 id="none"></h3>
<h3 id="batch-normalized-cnn">Batch-Normalized CNN</h3>
<p>原来的CNN是<br>
$$ z= g(Wu+b)$$<br>
现在在nonlinearity前加上BN transform。<br>
$$ z= g(BN(Wu+b))$$<br>
但是事实上，Wu+b和Wu的效果是一样的，因为normalized的时候会减去均值，所以最后就是：<br>
$$ z= g(BN(Wu))$$<br>
BN在Wu的每一个维度上单独使用BN，每一个维度有一对$\gamma<sup>k,\beta</sup>k$。</p>
<h3 id="bn能使用更大的学习率">BN能使用更大的学习率</h3>
<h3 id="bn正则化模型">BN正则化模型</h3>
<h2 id="residual-network-2015">Residual Network(2015)</h2>
<p>论文名称：Deep Residual Learning for Image Recognition<br>
论文地址：<a href="https://arxiv.org/pdf/1512.03385.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1512.03385.pdf</a></p>
<h3 id="概述-v6">概述</h3>
<p>作者提出了参差网络，容易优化，仅仅增加深度就能得到更高的accuracy。在Imagenet上使用比VGG深八倍的152层的residual网络，但是计算复杂度更低。<br>
网络是不是越深越好？并不是！事实上，随着网络的加深，会出现退化问题－即增加网络的深度，accuracy反而会下降。导致这个问题的原因并不是过拟合，至于是什么原因？<br>
在这篇文章中，作者提出了deep residual network。使用一些stacked non-linear layers你和一个residual mapping，而不是直接学习一个underlying mapping。用$H(x)$表示一个underlying mapping，我们的目标是学习一个residual mapping：$F(x) = H(x)-x$，underlying mapping可以写成$H(x) = F(x)+x$。在某种情况下，如果identity mapping是optimal，那么让$F(x)$接近于$0$可能比让stacked non linear layers拟合一个identity mapping要简单。。如下图所示，$H(x)$可以用下图表示，由一个feedforward nn加上shortcut connections（skip one or more layers的connection）组成：<br>
<img src="/2019/03/13/cnn/residual_block.png" alt="residual block"><br>
shortcut connection在这里就是一个identity mapping，不需要额外的参数和计算量，shorcut的输出和$F(x)$的输出再一块经过relu激活函数。</p>
<p>本文的contribution是什么？<br>
加了一个恒等映射让深度网络的训练变得更容易。具体原理是什么？可以从这样一个角度看，在每一层都可以把不同维度的feature进行重组。residual connection是skip的一种方式？？</p>
<h3 id="residual-learning">Residual Learning</h3>
<p>用$H(x)$表示stacked non linear layers拟合的一个underlying mapping，$x$为stacked layers的输入。原来我们用这些layers逼近一个复杂的函数，现在我们用它逼近residual function，即$F(x) = H(x) -x$（假设输入和输出的维度是一样的），原来想要拟合的函数变成了$F(x)+x$，它们的意义是一样的，但是对于learning的帮助却有很大差别。<br>
如网络degradation问题中，如果更深的网络中添加的新layers是identity mapping，那么这个更深的网络的training error至少也要和浅一些的网络一样，然而事实上并不是这样的。在degradation问题中，说明multip nonlinear layers在近似identity mappings时效果并不是很好。而在residual learnign中，如果identity mapping是optimal，那么可以让non linear layers的权重接近于0，最后得到一个indetity mappings。虽然在real cases中，identity mapping几乎不可能是optimal的，但是如果optimal function更接近identity mapping而不是zero ampping，residual learning的效果就要更好。<br>
<img src="/2019/03/13/cnn/residual_block.png" alt="residual block"></p>
<h3 id="identity-mapping-by-shortcuts">Identity Mapping by Shortcuts</h3>
<p>本文中采用的residual block如上上图所示，用公式表示为：<br>
$$y = F(x, {W_i}) + x$$<br>
其中$x,y$是输入和输出向量，函数$F(x, {W_i})$表示要学习的residual mapping，residual block块中有两层，$F=W_2\sigma(W_1x)$表示第一层和第二层，然后$F+x$表示shortcut connection以及element-wise addition。如果$x$和$F(x)$的维度不一样的话，可以进行一个linear projection：<br>
$$y=F(x,{W_i}) + W_sx$$<br>
$W_s$表示线性变换的矩阵。如果必要的话，$W_s$可以走一样线性变换，事实上，实验表明如果维度一样的话，identity mapping足够解决degradation问题，$W_s$就是用来进行dimension matting。<br>
$F$的形式是很灵活的，可以像本文一样使用linear layers，当然也可以使用更多layers，无所谓。</p>
<h3 id="网络架构">网络架构</h3>
<p>作者给出了三个网络架构，一个是VGG，一个是VGG修改得到的网络，另一个是这个修改的网络加上shortcut connection，如图所示。基于VGG的修改有以下两个原则：</p>
<ol>
<li>feature map的大小不变的话，filters的数量不变</li>
<li>feature map的大小减半的话，filters的数量变为原来的$2$倍，保证每一层的计算复杂度不变。</li>
</ol>
<p>网络最后接一个global average pooling layer和一个1000way的fc layer和softmax。</p>
<h3 id="其他细节">其他细节</h3>
<ol>
<li>image的短边被resize到$[256, 480]$之间。然后从中裁剪一个$224 \times 224$的样本或者它的horizontal filp。</li>
<li>使用标准的颜色增强。</li>
<li>使用BN</li>
<li>从头开始训练网络</li>
<li>使用batch size为$256$的SGD</li>
<li>学习率从$0.1$开始，每到error不再改变时，除以$10$，总共进行$60\times 10^4$次迭代。</li>
<li>权重decay为$0.0001$，mementum为$0.9$。</li>
<li>测试时，对十个crop取平均，使用fcn，对多个scales上的scores进行平均。</li>
</ol>
<h3 id="结论">结论</h3>
<p>14.<a href="https://www.quora.com/How-does-deep-residual-learning-work" target="_blank" rel="noopener">https://www.quora.com/How-does-deep-residual-learning-work</a><br>
15.<a href="https://kharshit.github.io/blog/2018/09/07/skip-connections-and-residual-blocks" target="_blank" rel="noopener">https://kharshit.github.io/blog/2018/09/07/skip-connections-and-residual-blocks</a><br>
16.<a href="https://stats.stackexchange.com/questions/56950/neural-network-with-skip-layer-connections" target="_blank" rel="noopener">https://stats.stackexchange.com/questions/56950/neural-network-with-skip-layer-connections</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://mxxhcm.github.io/2019/03/13/python-常见问题/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="马晓鑫爱马荟荟">
      <meta itemprop="description" content="记录硕士三年自己的积累">
      <meta itemprop="image" content="/images/favicon.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="mxxhcm's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/03/13/python-常见问题/" class="post-title-link" itemprop="http://mxxhcm.github.io/page/28/index.html">python 常见问题（不定期更新）</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-03-13 10:40:03" itemprop="dateCreated datePublished" datetime="2019-03-13T10:40:03+08:00">2019-03-13</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-12-17 15:19:53" itemprop="dateModified" datetime="2019-12-17T15:19:53+08:00">2019-12-17</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/python/" itemprop="url" rel="index"><span itemprop="name">python</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="问题1-dict-values-object-does-not-support-indexing">问题1-‘dict_values’ object does not support indexing’</h2>
<p>参考文献[1,2,3]</p>
<h3 id="报错">报错</h3>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&apos;dict_values&apos; object does not support indexing&apos;</span><br></pre></td></tr></table></figure>
<h3 id="原因">原因</h3>
<p>The objects returned by dict.keys(), dict.values() and dict.items() are view objects. They provide a dynamic view on the dictionary’s entries, which means that when the dictionary changes, the view reflects these changes.<br>
python3 中调用字典对象的一些函数，返回值是view objects。如果要转换为list的话，需要使用list()强制转换。<br>
而python2的返回值直接就是list。</p>
<h3 id="代码示例">代码示例</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">m_dict = &#123;<span class="string">'a'</span>: <span class="number">10</span>, <span class="string">'b'</span>: <span class="number">20</span>&#125;</span><br><span class="line">values = m_dict.values()</span><br><span class="line">print(type(values))</span><br><span class="line">print(values)</span><br><span class="line">print(<span class="string">"\n"</span>)</span><br><span class="line"></span><br><span class="line">items = m_dict.items()</span><br><span class="line">print(type(items))</span><br><span class="line">print(items)</span><br><span class="line">print(<span class="string">"\n"</span>)</span><br><span class="line"></span><br><span class="line">keys = m_dict.keys()</span><br><span class="line">print(type(keys))</span><br><span class="line">print(keys)</span><br><span class="line">print(<span class="string">"\n"</span>)</span><br></pre></td></tr></table></figure>
<p>如果使用python3执行以上代码，输出结果如下所示：</p>
<blockquote>
<p>class 'dict_values’<br>
dict_values([10, 20])<br>
class 'dict_items’<br>
dict_items([(‘a’, 10), (‘b’, 20)])<br>
class 'dict_keys’<br>
dict_keys([‘a’, ‘b’])</p>
</blockquote>
<p>如果使用python2执行以上代码，输出结果如下所示：</p>
<blockquote>
<p>type ‘list’<br>
[10, 20]<br>
type ‘list’<br>
[(‘a’, 10), (‘b’, 20)]<br>
type ‘list’<br>
[‘a’, ‘b’]</p>
</blockquote>
<h2 id="问题2-timelimit-object-has-no-attribute-ale">问题2-‘TimeLimit’ object has no attribute ‘ale’</h2>
<p>参考文献[4,5,6]</p>
<h3 id="问题描述">问题描述</h3>
<p>运行github clone 下来的<a href="https://github.com/devsisters/DQN-tensorflow" target="_blank" rel="noopener">DQN-tensorflow</a>，报错:</p>
<blockquote>
<p>AttributeError: ‘TimeLimit’ object has no attribute ‘ale’.</p>
</blockquote>
<h3 id="原因-v2">原因</h3>
<p>是因为gym版本原因，在gym 0.7版本中，可以使用env.ale.lives()访问ale属性，但是0.8版本以及以上，就没有了该属性，可以在系列函数中添加如下修改：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, config)</span>:</span></span><br><span class="line">    self.step_info = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_step</span><span class="params">(self, action)</span>:</span></span><br><span class="line">    self._screen, self.reward, self.terminal, self.step_info = self.env.step(action)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lives</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> self.step_info <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> self.step_info[<span class="string">'ale.lives'</span>]</span><br></pre></td></tr></table></figure>
<h3 id="ale属性是什么">ale属性是什么</h3>
<p>我看官方文档也没有看清楚，但是我觉得就是生命值是否没有了</p>
<blockquote>
<p>info (dict): diagnostic information useful for debugging. It can sometimes be useful for learning (for example, it might contain the raw probabilities behind the environment’s last state change). However, official evaluations of your agent are not allowed to use this for learning.</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> gym</span><br><span class="line">env = gym.make(<span class="string">'CartPole-v0'</span>)</span><br><span class="line"><span class="keyword">for</span> i_episode <span class="keyword">in</span> range(<span class="number">20</span>):</span><br><span class="line">    observation = env.reset()</span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">        env.render()</span><br><span class="line">        print(observation)</span><br><span class="line">        action = env.action_space.sample()</span><br><span class="line">        observation, reward, done, info = env.step(action)</span><br><span class="line">        <span class="keyword">if</span> done:</span><br><span class="line">            print(<span class="string">"Episode finished after &#123;&#125; timesteps"</span>.format(t+<span class="number">1</span>))</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">env.close()</span><br></pre></td></tr></table></figure>
<h2 id="问题3-cannot-import-name">问题3-cannot import name ***</h2>
<p>参考文献[7]</p>
<h3 id="报错-v2">报错</h3>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cannot import name tqdm</span><br></pre></td></tr></table></figure>
<h3 id="问题原因">问题原因</h3>
<p>谷歌了半天，没有发现原因，然后百度了一下，发现了原因，看来还是自己太菜了。。<br>
因为自己起的文件名就叫tqdm，然后就和库中的tqdm冲突了，这也太蠢了吧。。。</p>
<h2 id="问题4-linux下python执行shell脚本输出重定向">问题4-linux下python执行shell脚本输出重定向</h2>
<p><a href="https://mxxhcm.github.io/2019/06/03/linux-python%E8%B0%83%E7%94%A8shell%E8%84%9A%E6%9C%AC%E5%B9%B6%E5%B0%86%E8%BE%93%E5%87%BA%E9%87%8D%E5%AE%9A%E5%90%91%E5%88%B0%E6%96%87%E4%BB%B6/">详细介绍</a></p>
<h2 id="问题4-importerror-no-module-named-conda-cli">问题4-ImportError: No module named conda.cli’</h2>
<h3 id="问题描述-v2">问题描述</h3>
<p>anaconda的python版本是3.7，执行了conda install python=3.6之后，运行conda命令出错。报错如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from conda.cli import main </span><br><span class="line">ModuleNotFoundError: No module named &apos;conda&apos;</span><br></pre></td></tr></table></figure>
<h2 id="解决方案">解决方案</h2>
<p>找到anaconda安装包，加一个-u参数，如下所示。重新安装anaconda自带的package，自己安装的包不会丢失。<br>
~$:sh <a href="http://xxx.sh" target="_blank" rel="noopener">xxx.sh</a> -u</p>
<h2 id="问题5-python-pip使用国内源">问题5-python-pip使用国内源</h2>
<h3 id="暂时使用国内pip源">暂时使用国内pip源</h3>
<p>使用清华源<br>
~$:pip install -i <a href="https://pypi.tuna.tsinghua.edu.cn/simple" target="_blank" rel="noopener">https://pypi.tuna.tsinghua.edu.cn/simple</a> package-name<br>
使用阿里源<br>
~$:pip install -i <a href="https://mirrors.aliyun.com/pypi/simple" target="_blank" rel="noopener">https://mirrors.aliyun.com/pypi/simple</a> package-name</p>
<h3 id="将国内pip源设为默认">将国内pip源设为默认</h3>
<p>~$:pip install pip -U<br>
~$:pip config set global.index-url <a href="https://pypi.tuna.tsinghua.edu.cn/simple" target="_blank" rel="noopener">https://pypi.tuna.tsinghua.edu.cn/simple</a><br>
~$:pip config set global.timeout 60</p>
<blockquote>
<p>Writing to /home/username/.config/pip/pip.conf</p>
</blockquote>
<h4 id="查看pip配置文件">查看pip配置文件</h4>
<p>~$:find / -name pip.conf<br>
我的是在/home/username/.config/pip/pip.conf</p>
<h2 id="问题6-importerror-lib-x86-64-linux-gnu-libc-so-6-version-glibc-2-28-not-found">问题6-ImportError: /lib/x86_64-linux-gnu/libc.so.6: version GLIBC_2.28 not found</h2>
<h3 id="问题描述-v3">问题描述</h3>
<p>安装roboschool之后，出现ImportError。报错如下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ImportError: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.28&apos; not found (required by /usr/local/lib/python3.6/dist-packages/roboschool/.libs/libQt5Core.so.5)</span><br></pre></td></tr></table></figure>
<h3 id="解决方案-v2">解决方案</h3>
<p>在roboschool上找到一个issue，说从1.0.49版本退回到1.0.48即可。我退回之后，又出现以下错误：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ImportError: libpcre16.so.3: cannot open shared object file: No such file or directory</span><br></pre></td></tr></table></figure>
<p>安装相应的库即可。完整的命令如下</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">~$:pip install roboschool==1.0.48</span><br><span class="line">~$:sudo apt install libpcre3-dev</span><br></pre></td></tr></table></figure>
<h2 id="参考文献">参考文献</h2>
<p>1.<a href="https://www.cnblogs.com/timxgb/p/8905290.html" target="_blank" rel="noopener">https://www.cnblogs.com/timxgb/p/8905290.html</a><br>
2.<a href="https://docs.python.org/3/library/stdtypes.html#dictionary-view-objects" target="_blank" rel="noopener">https://docs.python.org/3/library/stdtypes.html#dictionary-view-objects</a><br>
3.<a href="https://stackoverflow.com/questions/43663206/typeerror-unsupported-operand-types-for-dict-values-and-int" target="_blank" rel="noopener">https://stackoverflow.com/questions/43663206/typeerror-unsupported-operand-types-for-dict-values-and-int</a><br>
4.<a href="https://github.com/devsisters/DQN-tensorflow/issues/29" target="_blank" rel="noopener">https://github.com/devsisters/DQN-tensorflow/issues/29</a><br>
5.<a href="https://gym.openai.com/docs" target="_blank" rel="noopener">https://gym.openai.com/docs</a><br>
6.<a href="https://github.com/openai/baselines/issues/42" target="_blank" rel="noopener">https://github.com/openai/baselines/issues/42</a><br>
7.<a href="https://blog.csdn.net/m0_37561765/article/details/78714603" target="_blank" rel="noopener">https://blog.csdn.net/m0_37561765/article/details/78714603</a><br>
8.<a href="https://blog.csdn.net/u014432608/article/details/79066813" target="_blank" rel="noopener">https://blog.csdn.net/u014432608/article/details/79066813</a><br>
9.<a href="https://mirrors.tuna.tsinghua.edu.cn/help/pypi/" target="_blank" rel="noopener">https://mirrors.tuna.tsinghua.edu.cn/help/pypi/</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://mxxhcm.github.io/2019/03/09/markdown帮助/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="马晓鑫爱马荟荟">
      <meta itemprop="description" content="记录硕士三年自己的积累">
      <meta itemprop="image" content="/images/favicon.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="mxxhcm's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/03/09/markdown帮助/" class="post-title-link" itemprop="http://mxxhcm.github.io/page/28/index.html">markdown帮助</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-03-09 19:53:32" itemprop="dateCreated datePublished" datetime="2019-03-09T19:53:32+08:00">2019-03-09</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-10-25 19:42:39" itemprop="dateModified" datetime="2019-10-25T19:42:39+08:00">2019-10-25</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/工具/" itemprop="url" rel="index"><span itemprop="name">工具</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="引用">引用</h2>
<h3 id="代码引用">代码引用</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br></pre></td></tr></table></figure>
<h3 id="文字引用">文字引用</h3>
<blockquote>
<p>实际是人类进步的阶梯。　－－高尔基</p>
</blockquote>
<h2 id="表格">表格</h2>
<table>
<thead>
<tr>
<th style="text-align:center">name</th>
<th style="text-align:center">age</th>
<th style="text-align:center">gender</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Alice</td>
<td style="text-align:center">11</td>
<td style="text-align:center">female</td>
</tr>
<tr>
<td style="text-align:center">Bob</td>
<td style="text-align:center">82</td>
<td style="text-align:center">male</td>
</tr>
</tbody>
</table>
<h2 id="表情">表情</h2>
<h3 id="安装过程">安装过程</h3>
<p>第一步，卸载hexo默认的hexo-renderer-marked markdown渲染器<br>
~$:npm un hexo-renderer-marked --save<br>
第二步，安装支持emoji的markdown渲染器<br>
~$:npm i hexo-renderer-markdown-it --save<br>
第三步，修改博客根目录下的_config.yml文件，添加下列内容：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"># Markdown-it config</span><br><span class="line">## Docs: https://github.com/celsomiranda/hexo-renderer-markdown-it/wiki</span><br><span class="line">markdown:</span><br><span class="line">  render:</span><br><span class="line">    html: true</span><br><span class="line">    xhtmlOut: false</span><br><span class="line">    breaks: true</span><br><span class="line">    linkify: true</span><br><span class="line">    typographer: true</span><br><span class="line">    quotes: &apos;“”‘’&apos;</span><br><span class="line">  plugins:</span><br><span class="line">    - markdown-it-abbr</span><br><span class="line">    - markdown-it-footnote</span><br><span class="line">    - markdown-it-ins</span><br><span class="line">    - markdown-it-sub</span><br><span class="line">    - markdown-it-sup</span><br><span class="line">    - markdown-it-emoji  ## add emoji</span><br><span class="line">  anchors:</span><br><span class="line">    level: 2</span><br><span class="line">    collisionSuffix: &apos;v&apos;</span><br><span class="line">    # If `true`, creates an anchor tag with a permalink besides the heading.</span><br><span class="line">    permalink: false  </span><br><span class="line">    permalinkClass: header-anchor</span><br><span class="line">    # The symbol used to make the permalink</span><br><span class="line">    permalinkSymbol: ¶</span><br></pre></td></tr></table></figure>
<p>然后重新生成部署即可。<br>
测试：<br>
😄<br>
😆<br>
👃</p>
<h2 id="测试">测试</h2>
<table>
   <tr>
      <td></td>
   </tr>
   <tr>
      <td>Coding</td>
   </tr>
   <tr>
      <td>Content</td>
   </tr>
   <tr>
      <td>描述性提炼性质的研究</td>
   </tr>
   <tr>
      <td>第一部分：</td>
   </tr>
   <tr>
      <td>文献综述</td>
   </tr>
   <tr>
      <td>（对话）</td>
   </tr>
   <tr>
      <td>SPL</td>
   </tr>
   <tr>
      <td>本文的文献综述贯穿在行文的过程中</td>
   </tr>
   <tr>
      <td>（1）关于分家的原因：</td>
   </tr>
   <tr>
      <td>敌军：①弗里德曼兄弟之间的利害冲突②许烺光夫妻纽带强于父子之间的纽带→概括为家庭内摩擦</td>
   </tr>
   <tr>
      <td>作者（部分认同敌军基础上提出自己的观点）：分家逐渐演化成一种“文化现象”</td>
   </tr>
   <tr>
      <td>（2）分家中的“继”与“合”</td>
   </tr>
   <tr>
      <td>敌军：①孔迈隆以家产正式分才算分家的定义②分灶</td>
   </tr>
   <tr>
      <td>评论：①经济上的考虑多于社会上的考虑②认为分家是家庭的破裂以及兄弟没有继承一个完整的家庭</td>
   </tr>
   <tr>
      <td>作者：分家中也有垂直关系的“继承”、横纵一体的“合”</td>
   </tr>
   <tr>
      <td></td>
   </tr>
   <tr>
      <td>CPL</td>
   </tr>
   <tr>
      <td>①对现有文献的理解和看法②作者在哪个细分领域展开研究</td>
   </tr>
   <tr>
      <td>理论基础</td>
   </tr>
   <tr>
      <td>RAT</td>
   </tr>
   <tr>
      <td>上面两个部分的完善是为这个部分做准备</td>
   </tr>
   <tr>
      <td>第二部分：</td>
   </tr>
   <tr>
      <td>机制和结构</td>
   </tr>
   <tr>
      <td>F（x）</td>
   </tr>
   <tr>
      <td>结构： </td>
   </tr>
   <tr>
      <td>（1）概念界定：分家的基本内容</td>
   </tr>
   <tr>
      <td>什么是分家？分家时财产按照“股”分割；分家的原因</td>
   </tr>
   <tr>
      <td>（2）分家带来的影响（案例分析）：分家带来了社会流动</td>
   </tr>
   <tr>
      <td>  借用说“分家三年显高低”、“富不过三代”、“父子一条心，黄土变成金”三句俚语来说明分家对社会变化的影响</td>
   </tr>
   <tr>
      <td>（3）分家的中“继”与“合”</td>
   </tr>
   <tr>
      <td>  继：赡养老人、继宗祧（tiao 1声），对应儒的孝、父子一体观念</td>
   </tr>
   <tr>
      <td>  合：生产生活上的合作</td>
   </tr>
   <tr>
      <td>（4）结语</td>
   </tr>
   <tr>
      <td>机制：对应儒的孝、父子一体观念；生产生活上的合作</td>
   </tr>
   <tr>
      <td>Argument</td>
   </tr>
   <tr>
      <td>CA</td>
   </tr>
   <tr>
      <td>分中有继也有合</td>
   </tr>
   <tr>
      <td>第三部分：</td>
   </tr>
   <tr>
      <td>问题和发展</td>
   </tr>
   <tr>
      <td>    （1） 文献综述找敌军可以借鉴，以及文献评述</td>
   </tr>
   <tr>
      <td>    （2） 文献综述可以加一些友军</td>
   </tr>
   <tr>
      <td>    （3） 文章可能写的太早了，不太符合现在的文章写作规范。不太理解第二部分“分家对社会发展的影响”对整篇文章有什么关系？？是不是没有必要占这么大篇幅</td>
   </tr>
   <tr>
      <td>    （4） 内容方面：</td>
   </tr>
   <tr>
      <td>随着时间的演变，他们的合越来越局限于小，男女双方的直系亲属，直系的兄弟关系和姻亲关系，大家族的联系越来越少；大家族即使祖坟放在一起，也难以通过祭祀的手段联系起来，慢慢农村也形成原子化的家庭单位，对于同村的人来说，地缘关系、邻里关系是比血缘关系更重要的存在</td>
   </tr>
   <tr>
      <td></td>
   </tr>
</table>
<h2 id="参考文献">参考文献</h2>
<p>1.<a href="https://daringfireball.net/projects/markdown/syntax" target="_blank" rel="noopener">https://daringfireball.net/projects/markdown/syntax</a><br>
2.<a href="https://www.webfx.com/tools/emoji-cheat-sheet/" target="_blank" rel="noopener">https://www.webfx.com/tools/emoji-cheat-sheet/</a><br>
3.<a href="https://guides.github.com/features/mastering-markdown/" target="_blank" rel="noopener">https://guides.github.com/features/mastering-markdown/</a><br>
4.<a href="https://github.com/mxxhcm/use_vim_as_ide#8.4" target="_blank" rel="noopener">https://github.com/mxxhcm/use_vim_as_ide#8.4</a><br>
5.<a href="https://chaxiaoniu.oschina.io/2017/07/10/HexoAddEmoji/" target="_blank" rel="noopener">https://chaxiaoniu.oschina.io/2017/07/10/HexoAddEmoji/</a><br>
6.<a href="https://www.markdownguide.org/basic-syntax/" target="_blank" rel="noopener">https://www.markdownguide.org/basic-syntax/</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://mxxhcm.github.io/2019/03/07/tensorflow-problems/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="马晓鑫爱马荟荟">
      <meta itemprop="description" content="记录硕士三年自己的积累">
      <meta itemprop="image" content="/images/favicon.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="mxxhcm's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/03/07/tensorflow-problems/" class="post-title-link" itemprop="http://mxxhcm.github.io/page/28/index.html">tensorflow 常见问题（不定期更新）</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-03-07 14:51:01" itemprop="dateCreated datePublished" datetime="2019-03-07T14:51:01+08:00">2019-03-07</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-07-18 20:27:16" itemprop="dateModified" datetime="2019-07-18T20:27:16+08:00">2019-07-18</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/tensorflow/" itemprop="url" rel="index"><span itemprop="name">tensorflow</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="问题1-the-value-of-a-feed-cannot-be-a-tf-tensor-object">问题1-The value of a feed cannot be a tf.Tensor object</h2>
<h3 id="报错">报错</h3>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">TypeError: The value of a feed cannot be a tf.Tensor object</span><br></pre></td></tr></table></figure>
<h3 id="问题原因">问题原因</h3>
<p>sess.run(op, feed_dict={})中的feed value不能是tf.Tensor类型。</p>
<h3 id="解决方法">解决方法</h3>
<p>sess.run(train, feed_dict={x:images, y:labels}的输入不能是tensor，可以使用sess.run(tensor)得到numpy.array形式的数据再喂给feed_dict。</p>
<blockquote>
<p>Once you have launched a sess, you can use your_tensor.eval(session=sess) or sess.run(your_tensor) to get you feed tensor into the format of numpy.array and then feed it to your placeholder.</p>
</blockquote>
<h2 id="问题2-could-not-create-cudnn-handle-cudnn-status-internal-error">问题2-Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR</h2>
<h3 id="配置">配置</h3>
<p>环境配置如下：</p>
<ul>
<li>Ubuntu 18.04</li>
<li>CUDA 10.0</li>
<li>CuDNN 7.4.2</li>
<li>Python3.7.3</li>
<li>Tensorflow 1.13.1</li>
<li>Nvidia Drivers 430.09</li>
<li>RTX2070</li>
</ul>
<h3 id="报错-v2">报错</h3>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">2019-05-12 14:45:59.355405: E tensorflow/stream_executor/cuda/cuda_dnn.cc:334] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR</span><br><span class="line">2019-05-12 14:45:59.357698: E tensorflow/stream_executor/cuda/cuda_dnn.cc:334] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<h3 id="问题原因-v2">问题原因</h3>
<p>GPU不够用了。</p>
<h3 id="解决方法-v2">解决方法</h3>
<p>在代码中添加下面几句：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">config = tf.ConfigProto()</span><br><span class="line">config.gpu_options.allow_growth = <span class="literal">True</span></span><br><span class="line">session = InteractiveSession(config=config)</span><br></pre></td></tr></table></figure>
<h2 id="问题3-libcublas-so-10-0-cannot-open-shared-object-file-no-such-file-or-directory">问题3-libcublas.so.10.0: cannot open shared object file: No such file or directory</h2>
<p>在命令行或者pycharm中import tensorflow报错</p>
<h3 id="报错-v3">报错</h3>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ImportError: libcublas.so.10.0: cannot open shared object file: No such file or directory</span><br><span class="line">Failed to load the native TensorFlow runtime.</span><br></pre></td></tr></table></figure>
<h3 id="问题原因-v3">问题原因</h3>
<p>没有配置CUDA环境变量</p>
<h3 id="解决方法-v3">解决方法</h3>
<h4 id="命令行中">命令行中</h4>
<p>在.bashrc文件中加入下列语句：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export PATH=/usr/local/cuda/bin$&#123;PATH:+:$&#123;PATH&#125;&#125;</span><br><span class="line">export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$&#123;LD_LIBRARY_PATH:+:$&#123;LD_LIBRARY_PATH&#125;&#125;</span><br><span class="line">export CUDA_HOME=/usr/local/cuda</span><br></pre></td></tr></table></figure>
<h4 id="pycharm中">pycharm中</h4>
<h5 id="方法1-这种方法我没有实验成功-不知道为什么">方法1（这种方法我没有实验成功，不知道为什么）</h5>
<p>在左上角选中<br>
File&gt;&gt;Settings&gt;&gt;Build.Execution,Deployment&gt;&gt;Console&gt;&gt;Python Console<br>
在Environment下的Environment variables中添加<br>
LD_LIBRARY_PATH=/usr/local/cuda/lib64:${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}即可。</p>
<h5 id="方法2">方法2</h5>
<p>修改完.bashrc文件后从终端中运行pycharm。</p>
<h2 id="问题4-dlerror-libcupti-so-10-0-cannot-open-shared-object-file-no-such-file-or-directory">问题4-dlerror: libcupti.so.10.0: cannot open shared object file: No such file or directory</h2>
<p>执行mnist_with_summary代码时报错</p>
<h3 id="报错-v4">报错</h3>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">I tensorflow/stream_executor/dso_loader.cc:142] Couldn&apos;t open CUDA library libcupti.so.10.0. LD_LIBRARY_PATH: /usr/local/cuda/lib64:</span><br><span class="line">2019-05-13 23:04:10.620149: F tensorflow/stream_executor/lib/statusor.cc:34] Attempting to fetch value instead of handling error Failed precondition: could not dlopen DSO: libcupti.so.10.0; dlerror: libcupti.so.10.0: cannot open shared object file: No such file or directory</span><br><span class="line">Aborted (core dumped)</span><br></pre></td></tr></table></figure>
<h3 id="问题问题问题问题问题问题问题问题问题原因">问题问题问题问题问题问题问题问题问题原因</h3>
<p>libcupti.so.10.0包没找到</p>
<h3 id="解决方法-v4">解决方法</h3>
<p>执行以下命令，找到相关的依赖包：<br>
~$:find /usr/local/cuda/ -name libcupti.so.10.0<br>
输出如下：</p>
<blockquote>
<p>/usr/local/cuda/extras/CUPTI/lib64/libcupti.so.10.0</p>
</blockquote>
<p>然后修改~/.bashrc文件中相应的环境变量:<br>
export LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64/😒{LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}<br>
重新运行即可。</p>
<h2 id="问题5-unhashable-type-list">问题5-unhashable type: ‘list’</h2>
<p>sess.run(op, feed_dict={})中feed的数据中包含有list的时候会报错。</p>
<h3 id="报错-v5">报错</h3>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">TypeError: unhashable type: &apos;list&apos;</span><br></pre></td></tr></table></figure>
<h3 id="问题原因-v4">问题原因</h3>
<p>feed_dict中不能的value不能是list。</p>
<h3 id="解决方法-v5">解决方法</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">feed_dict = &#123;</span><br><span class="line">               placeholder : value </span><br><span class="line">                  <span class="keyword">for</span> placeholder, value <span class="keyword">in</span> zip(placeholder_list, inputs_list))</span><br><span class="line">            &#125;</span><br></pre></td></tr></table></figure>
<h3 id="代码示例">代码示例</h3>
<p><a href="https://github.com/mxxhcm/code/blob/master/tf/ops/tf_placeholder_list.py" target="_blank" rel="noopener">代码地址</a></p>
<h2 id="问题6-attempting-to-use-uninitialized-value">问题6-Attempting to use uninitialized value</h2>
<p>tf.Session()和tf.InteractiveSession()混用问题。</p>
<h3 id="报错-v6">报错</h3>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensorflow.python.framework.errors_impl.FailedPreconditionError: Attempting to use uninitialized value prediction/l1/w</span><br><span class="line">	 [[&#123;&#123;node prediction/l1/w/read&#125;&#125;]]</span><br><span class="line">	 [[&#123;&#123;node prediction/LogSoftmax&#125;&#125;]]</span><br></pre></td></tr></table></figure>
<h3 id="问题原因-v5">问题原因</h3>
<p>声明了如下session:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sess = tf.Session()</span><br><span class="line">sess.run(tf.global_variables_initializer())</span><br></pre></td></tr></table></figure>
<p>在接下来的代码中，因为我声明的是tf.Session()，使用了op.eval()函数，这种用法是tf.InteractiveSession的用法，所以就相当于没有初始化。<br>
result = op.eval(feed_dict={})<br>
然后就报了未初始化的错误。<br>
把代码改成：<br>
result = sess.run([op], feeed_dct={})<br>
即可，即上下文使用的session应该一致。</p>
<h3 id="解决方案">解决方案</h3>
<p>使用统一的session类型</p>
<h2 id="问题7-setting-an-array-element-with-a-sequence">问题7-setting an array element with a sequence</h2>
<p>feed_dict键值对中中值必须是numpy.ndarray，不能是其他类型。</p>
<h3 id="报错-v7">报错</h3>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">value error setting an array element with a sequence,</span><br></pre></td></tr></table></figure>
<h3 id="问题原因-v6">问题原因</h3>
<p>feed_dict中key-value的value必须是numpy.ndarray，不能是其他类型，尤其不能是tf.Variable。</p>
<h3 id="解决方法-v6">解决方法</h3>
<p>检查sess.run(op, feed_dict={})中的feed_dict，确保他们的类型，不能是tf.Variable()类型的对象，需要是numpy.ndarray。</p>
<h2 id="问题8-访问tf-variable-的值">问题8-访问tf.Variable()的值</h2>
<p>如何获得tf.Variable()对象的值</p>
<h3 id="解决方法-v7">解决方法</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">x = tf.Varialbe([<span class="number">1.0</span>, <span class="number">2.0</span>])</span><br><span class="line">sess = tf.Session()</span><br><span class="line">sess.run(tf.global_variables_initializer())</span><br><span class="line">value = sess.run(x)</span><br></pre></td></tr></table></figure>
<p>或者</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">x = tf.Varialbe([1.0, 2.0])</span><br><span class="line">sess = tf.InteractiveSession()</span><br><span class="line">sess.run(tf.global_variables_initializer())</span><br><span class="line">x.eval()</span><br></pre></td></tr></table></figure>
<h2 id="问题9-can-not-convert-a-ndarray-into-a-tensor-or-operation">问题9-Can not convert a ndarray into a Tensor or Operation</h2>
<h3 id="报错-v8">报错</h3>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Can not convert a ndarray into a Tensor or Operation.</span><br></pre></td></tr></table></figure>
<h3 id="问题原因-v7">问题原因</h3>
<p>原因是sess.run()前后参数名重了，比如outputs = sess.run(outputs)，outputs本来是自己定义的一个op，但是sess.run(outputs)之后outputs就成了一个变量，就把定义的outputs op覆盖了。</p>
<h3 id="解决方法-v8">解决方法</h3>
<p>换个变量名字就行</p>
<h2 id="问题10-本地使用gpu-server的tensorboard">问题10-本地使用gpu server的tensorboard</h2>
<h3 id="问题描述">问题描述</h3>
<p>在gpu server跑的实验结果，然后summary的记录也在server上，但是又没办法可视化，只好在本地可视化。</p>
<h3 id="解决方法-v9">解决方法</h3>
<p>使用ssh进行映射好了。</p>
<h4 id="本机设置">本机设置</h4>
<p>~$:ssh -L 12345:10.1.114.50:6006 <a href="mailto:mxxmhh@127.0.0.1" target="_blank" rel="noopener">mxxmhh@127.0.0.1</a><br>
将本机的12345端口映射到10.1.114.50的6006端口，中间服务器使用的是本机。<br>
或者可以使用10.1.114.50作为中间服务器。<br>
~$:ssh -L 12345:10.1.114.50:6006 <a href="mailto:liuchi@10.1.114.50" target="_blank" rel="noopener">liuchi@10.1.114.50</a><br>
或者可以使用如下方法：<br>
~$:ssh -L 12345:127.0.0.1:6006 <a href="mailto:liuchi@10.1.114.50" target="_blank" rel="noopener">liuchi@10.1.114.50</a><br>
从这个方法中，可以看出127.0.0.1这个ip是中间服务器可以访问的ip。<br>
以上三种方法中，-L后的端口号12345可以随意设置，只要不冲突即可。</p>
<h4 id="服务端设置">服务端设置</h4>
<p>然后在服务端运行以下命令：<br>
~$:tensorboard --logdir logdir -port 6006<br>
这个端口号也是可以任意设置的，不冲突即可。</p>
<h4 id="运行">运行</h4>
<p>然后在本机访问<br>
<a href="https://127.0.0.1:12345" target="_blank" rel="noopener">https://127.0.0.1:12345</a>即可。</p>
<h2 id="问题11-每一步summary一个list的每一个元素">问题11-每一步summary一个list的每一个元素</h2>
<h3 id="问题原因-v8">问题原因</h3>
<p>有一个tf list的placeholder，但是每一步只能生成其中的一个元素，所以怎么样summary中其中的某一个？</p>
<h3 id="解决方法-v10">解决方法</h3>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">number = 3</span><br><span class="line">x_ph_list = []</span><br><span class="line">for i in range(number):</span><br><span class="line">    x_ph_list.append(tf.placeholder(tf.float32, shape=None))</span><br><span class="line"></span><br><span class="line">x_summary_list = []</span><br><span class="line">for i in range(number):</span><br><span class="line">    x_summary_list.append(tf.summary.scalar("x%s" % i, x_ph_list[i]))</span><br><span class="line"></span><br><span class="line">writer = tf.summary.FileWriter("./tf_summary/scalar_list_summary/sep")</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    scope = 10</span><br><span class="line">    inputs = np.arange(scope*number)</span><br><span class="line">    inputs = inputs.reshape(scope, number)</span><br><span class="line">    # inputs = np.random.randn(scope, number)</span><br><span class="line">    for i in range(scope):</span><br><span class="line">        for j in range(number):</span><br><span class="line">            out, xj_s = sess.run([x_ph_list[j], x_summary_list[j]], feed_dict=&#123;x_ph_list[j]: inputs[i][j]&#125;)</span><br><span class="line">            writer.add_summary(xj_s, global_step=i)</span><br></pre></td></tr></table></figure>
<h2 id="问题12-for-value-in-summary-value-attributeerror-list-object-has-no-attribute-value">问题12- for value in summary.value: AttributeError: ‘list’ object has no attribute ‘value’</h2>
<h3 id="问题描述-v2">问题描述</h3>
<p>writer.add_summary时报错</p>
<h3 id="报错-v9">报错</h3>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">File &quot;/home/mxxmhh/anaconda3/lib/python3.7/site-packages/tensorflow/python/summary/writer/writer.py&quot;, line 127, in add_summary</span><br><span class="line">    for value in summary.value:</span><br><span class="line">AttributeError: &apos;list&apos; object has no attribute &apos;value&apos;</span><br></pre></td></tr></table></figure>
<h3 id="问题原因-v9">问题原因</h3>
<p>执行以下代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">s_ = sess.run([loss_summary], feed_dict=&#123;p_losses_ph: inputs1, q_losses_ph: inputs2&#125;)</span><br><span class="line">writer.add_summary(s_, global_step=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>因为[loss_summary]加了方括号，就把它当成了一个list。。返回值也是list，就报错了</p>
<h3 id="解决方法-v11">解决方法</h3>
<ul>
<li>方法1，在等号左边加一个逗号，取出list中的值</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">s_, = sess.run([loss_summary], feed_dict=&#123;p_losses_ph: inputs1, q_losses_ph: inputs2&#125;)</span><br></pre></td></tr></table></figure>
<ul>
<li>方法2，去掉loss_summary外面的中括号。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">s_ = sess.run(loss_summary, feed_dict=&#123;p_losses_ph: inputs1, q_losses_ph: inputs2&#125;)</span><br></pre></td></tr></table></figure>
<h2 id="问题13-tf-get-default-session-always-returns-none-type">问题13- tf.get_default_session() always returns None type:</h2>
<h3 id="问题描述-v3">问题描述</h3>
<p>调用tf.get_default_session()时，返回的是None</p>
<h3 id="报错-v10">报错</h3>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">    tf.get_default_session().run(y)</span><br><span class="line">AttributeError: &apos;NoneType&apos; object has no attribute &apos;run&apos;</span><br></pre></td></tr></table></figure>
<h3 id="问题原因-v10">问题原因</h3>
<p>只有在设定default session之后，才能使用tf.get_default_session()获得当前的默认session，在我们写代码的时候，一般会按照下面的方式写：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    some operations</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure>
<p>这种情况下已经把tf.Session()生成的session当做了默认session，但是如果仅仅使用以下代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">sess =  tf.Session()</span><br><span class="line">sess.run(some operations)</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>是没有把tf.Session()当成默认session的，即只有在with block内，才会将这个session当做默认session。</p>
<h3 id="解决方案-v2">解决方案</h3>
<h2 id="参考文献">参考文献</h2>
<p>1.<a href="https://github.com/tensorflow/tensorflow/issues/4842" target="_blank" rel="noopener">https://github.com/tensorflow/tensorflow/issues/4842</a><br>
2.<a href="https://github.com/tensorflow/tensorflow/issues/24496" target="_blank" rel="noopener">https://github.com/tensorflow/tensorflow/issues/24496</a><br>
3.<a href="https://github.com/tensorflow/tensorflow/issues/9530" target="_blank" rel="noopener">https://github.com/tensorflow/tensorflow/issues/9530</a><br>
4.<a href="https://stackoverflow.com/questions/51128427/how-to-feed-list-of-values-to-a-placeholder-list-in-tensorflow" target="_blank" rel="noopener">https://stackoverflow.com/questions/51128427/how-to-feed-list-of-values-to-a-placeholder-list-in-tensorflow</a><br>
5.<a href="https://github.com/tensorflow/tensorflow/issues/11897" target="_blank" rel="noopener">https://github.com/tensorflow/tensorflow/issues/11897</a><br>
6.<a href="https://stackoverflow.com/questions/34156639/tensorflow-python-valueerror-setting-an-array-element-with-a-sequence-in-t" target="_blank" rel="noopener">https://stackoverflow.com/questions/34156639/tensorflow-python-valueerror-setting-an-array-element-with-a-sequence-in-t</a><br>
7.<a href="https://stackoverflow.com/questions/33679382/how-do-i-get-the-current-value-of-a-variable" target="_blank" rel="noopener">https://stackoverflow.com/questions/33679382/how-do-i-get-the-current-value-of-a-variable</a><br>
8.<a href="https://blog.csdn.net/michael__corleone/article/details/79007425" target="_blank" rel="noopener">https://blog.csdn.net/michael__corleone/article/details/79007425</a><br>
9.<a href="https://stackoverflow.com/questions/47721792/tensorflow-tf-get-default-session-after-sess-tf-session-is-none" target="_blank" rel="noopener">https://stackoverflow.com/questions/47721792/tensorflow-tf-get-default-session-after-sess-tf-session-is-none</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://mxxhcm.github.io/2019/03/04/linux-查看python-package的安装位置/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="马晓鑫爱马荟荟">
      <meta itemprop="description" content="记录硕士三年自己的积累">
      <meta itemprop="image" content="/images/favicon.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="mxxhcm's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/03/04/linux-查看python-package的安装位置/" class="post-title-link" itemprop="http://mxxhcm.github.io/page/28/index.html">linux-查看python package的安装位置</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-03-04 14:52:16" itemprop="dateCreated datePublished" datetime="2019-03-04T14:52:16+08:00">2019-03-04</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-05-12 12:03:26" itemprop="dateModified" datetime="2019-05-12T12:03:26+08:00">2019-05-12</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/linux/" itemprop="url" rel="index"><span itemprop="name">linux</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>使用pip install package-name之后，不知道该包存在了哪个路径下。<br>
可以再次使用pip install package-name，这时候就会给出该包存放在哪个路径下。</p>
<h2 id="参考文献">参考文献</h2>
<ol>
<li><a href="https://blog.csdn.net/weixin_41712059/article/details/82940516" target="_blank" rel="noopener">https://blog.csdn.net/weixin_41712059/article/details/82940516</a></li>
</ol>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://mxxhcm.github.io/2019/03/04/linux-shadowsocks服务端以及客户端配置/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="马晓鑫爱马荟荟">
      <meta itemprop="description" content="记录硕士三年自己的积累">
      <meta itemprop="image" content="/images/favicon.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="mxxhcm's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/03/04/linux-shadowsocks服务端以及客户端配置/" class="post-title-link" itemprop="http://mxxhcm.github.io/page/28/index.html">shadowsocks服务端以及客户端配置</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-03-04 13:03:57" itemprop="dateCreated datePublished" datetime="2019-03-04T13:03:57+08:00">2019-03-04</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-12-17 15:13:05" itemprop="dateModified" datetime="2019-12-17T15:13:05+08:00">2019-12-17</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/linux/" itemprop="url" rel="index"><span itemprop="name">linux</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="服务器端配置">服务器端配置</h2>
<p>首先需要有一个VPS账号，vultr,digitalocean,搬瓦工等等都行。<br>
首先到下面两个网站检测22端口是否开启，如果关闭的话，vps换个ip把。。<br>
<a href="http://tool.chinaz.com/port" target="_blank" rel="noopener">http://tool.chinaz.com/port</a><br>
<a href="https://www.yougetsignal.com/tools/open-ports/" target="_blank" rel="noopener">https://www.yougetsignal.com/tools/open-ports/</a></p>
<h3 id="启用bbr加速">启用BBR加速</h3>
<p>~#:apt update<br>
~#:apt upgrade<br>
~#:echo “net.core.default_qdisc=fq” &gt;&gt; /etc/sysctl.conf<br>
~#:echo “net.ipv4.tcp_congestion_control=bbr” &gt;&gt; /etc/sysctl.conf<br>
~#:sysctl -p<br>
上述命令就完成了BBR加速，执行以下命令验证：<br>
~#:lsmod |grep bbr<br>
看到输出包含tcp_bbr就说明已经成功了。</p>
<h3 id="搭建shadowsocks-server">搭建shadowsocks server</h3>
<h4 id="安装shadowsocks-server">安装shadowsocks server</h4>
<p>~#:apt install python-pip<br>
~#:pip install shadowsocks<br>
需要说一下的是，shadowsocks目前还不支持python3.5及以上版本，上次我把/usr/bin/python指向了python3.6，就是系统默认的python指向了python3.6，然后就gg了。一定要使用Python 2.6,2.7,3.3,3.4中的一个版本才能使用。。</p>
<h4 id="创建shadowsocks配置文件">创建shadowsocks配置文件</h4>
<p>如果你的VPS支持ipv6的话，那么可以开多进程分别运行ipv4和ipv6的shadowsocks server。本地只有ipv4的话，可以用本地ipv4访问ipv6，从而访问byr等网站，但是六维空间对此做了屏蔽。如果本地有ipv6的话，还可以用本地的ipv6访问ipv6实现校园网不走ipv4流量。</p>
<h5 id="ipv4配置">ipv4配置</h5>
<p>~#:vim /etc/shadowsocks_v4.json<br>
配置文件如下</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line"><span class="attr">"server"</span>:<span class="string">"0.0.0.0"</span>,</span><br><span class="line"><span class="attr">"server_port"</span>:<span class="string">"你的端口号"</span>,</span><br><span class="line"><span class="attr">"local_address"</span>:<span class="string">"127.0.0.1"</span>,</span><br><span class="line"><span class="attr">"local_port"</span>:<span class="number">1080</span>,</span><br><span class="line"><span class="attr">"password"</span>:<span class="string">"你的密码"</span>,</span><br><span class="line"><span class="attr">"timeout"</span>:<span class="number">600</span>,</span><br><span class="line"><span class="attr">"method"</span>:<span class="string">"aes-256-cfb"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="ipv6配置">ipv6配置</h5>
<p>~#:vim /etc/shadowsocks_v6.json<br>
配置文件如下</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line"><span class="attr">"server"</span>:<span class="string">"::"</span>,</span><br><span class="line"><span class="attr">"server_port"</span>:<span class="string">"你的端口号"</span>,</span><br><span class="line"><span class="attr">"local_address"</span>:<span class="string">"127.0.0.1"</span>,</span><br><span class="line"><span class="attr">"local_port"</span>:<span class="number">1080</span>,</span><br><span class="line"><span class="attr">"password"</span>:<span class="string">"你的密码"</span>,</span><br><span class="line"><span class="attr">"timeout"</span>:<span class="number">600</span>,</span><br><span class="line"><span class="attr">"method"</span>:<span class="string">"aes-256-cfb"</span></span><br><span class="line">&#125;</span><br><span class="line">``` </span><br><span class="line">注意这两个文件的server_port一定要不同，以及双引号必须是英文引号。</span><br><span class="line">##### 1.2.2.3.手动运行shadowsocks server</span><br><span class="line">~#:ssserver -c /etc/shadowsock_v4.json -d start --pid-file ss1.pid</span><br><span class="line">~#:ssserver -c /etc/shadowsock_v6.json -d start --pid-file ss2.pid</span><br><span class="line">注意这里要给两条命令分配不同的进程号。</span><br><span class="line"></span><br><span class="line">### 设置shadowsocks server开机自启</span><br><span class="line">如果重启服务器的话，就需要重新手动执行上述命令，这里我们可以把它写成开机自启脚本。</span><br><span class="line">~#:vim /etc/init.d/shadowsocks_v4</span><br><span class="line">内容如下：</span><br><span class="line">``` shell</span><br><span class="line">#!/bin/sh</span><br><span class="line">### BEGIN INIT INFO</span><br><span class="line"># Provides:          apache2</span><br><span class="line"># Required-Start:    $local_fs $remote_fs $network $syslog</span><br><span class="line"># Required-Stop:     $local_fs $remote_fs $network $syslog</span><br><span class="line"># Default-Start:     2 3 4 5</span><br><span class="line"># Default-Stop:      0 1 6</span><br><span class="line"># Short-Description: apache2 service</span><br><span class="line"># Description:       apache2 service daemon</span><br><span class="line">### END INIT INFO</span><br><span class="line">start()&#123;</span><br><span class="line">  ssserver -c /etc/shadowsocks_v4.json -d start --pid-file ss2.pid</span><br><span class="line">&#125;</span><br><span class="line">stop()&#123;</span><br><span class="line">  ssserver -c /etc/shadowsocks_v4.json -d stop --pid-file ss2.pid</span><br><span class="line">&#125;</span><br><span class="line">case "$1" in</span><br><span class="line">start)</span><br><span class="line">  start</span><br><span class="line">  ;;</span><br><span class="line">stop)</span><br><span class="line">  stop</span><br><span class="line">  ;;</span><br><span class="line">restart)</span><br><span class="line">  stop</span><br><span class="line">  start</span><br><span class="line">  ;;</span><br><span class="line">*)</span><br><span class="line">  echo "Uasage: $0 &#123;start|reload|stop&#125;$"</span><br><span class="line">  exit 1</span><br><span class="line">  ;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure>
<p>~#:vim /etc/init.d/shadowsocks_v6<br>
内容如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span>!/bin/sh</span><br><span class="line"><span class="meta">#</span>## BEGIN INIT INFO</span><br><span class="line"><span class="meta">#</span> Provides:          apache2</span><br><span class="line"><span class="meta">#</span> Required-Start:    $local_fs $remote_fs $network $syslog</span><br><span class="line"><span class="meta">#</span> Required-Stop:     $local_fs $remote_fs $network $syslog</span><br><span class="line"><span class="meta">#</span> Default-Start:     2 3 4 5</span><br><span class="line"><span class="meta">#</span> Default-Stop:      0 1 6</span><br><span class="line"><span class="meta">#</span> Short-Description: apache2 service</span><br><span class="line"><span class="meta">#</span> Description:       apache2 service daemon</span><br><span class="line"><span class="meta">#</span>## END INIT INFO</span><br><span class="line">start()&#123;</span><br><span class="line">  ssserver -c /etc/shadowsocks_v6.json -d start --pid-file ss1.pid</span><br><span class="line">&#125;</span><br><span class="line">stop()&#123;</span><br><span class="line">  ssserver -c /etc/shadowsocks_v6.json -d stop --pid-file ss1.pid</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line">case "$1" in</span><br><span class="line">start)</span><br><span class="line">  start</span><br><span class="line">  ;;</span><br><span class="line">stop)</span><br><span class="line">  stop</span><br><span class="line">  ;;</span><br><span class="line">restart)</span><br><span class="line">  stop</span><br><span class="line">  start</span><br><span class="line">  ;;</span><br><span class="line">*)</span><br><span class="line">  echo "Uasage: $0 &#123;start|reload|stop&#125;$"</span><br><span class="line">  exit 1</span><br><span class="line">  ;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure>
<p>然后执行下列命令即可：<br>
~#:chmod a+x /etc/init.d/shadowsocks_v4<br>
~#:chmod a+x /etc/init.d/shadowsocks_v6<br>
~#:update-rc.d shadowsocks_v4 defaults<br>
~#:update-rc.d shadowsocks_v6 defaults</p>
<p>至此，服务器端配置完成。</p>
<h2 id="服务端自动配置脚本">服务端自动配置脚本</h2>
<p><a href="https://github.com/mxxhcm/code/tree/master/shell/ss" target="_blank" rel="noopener">地址</a><br>
首先将该文件中所有文件复制到vps上，然后执行<br>
~#:sh install_ss_server.sh<br>
即可</p>
<h3 id="补充说明">补充说明</h3>
<p>该文件夹共包含五个文件<br>
shadowsocks_v4.json为ipv4 ss配置文件，可根据自己的需要修改端口号和密码<br>
shadowsocks_v6.json为ipv6 ss配置文件，可根据自己的需要修改端口号和密码<br>
shadowsocks_v4为ipv4 ss自启动文件，无需修改<br>
shadowsocks_v6为ipv6 ss自启动文件，无需修改<br>
install_ss_server.sh为安装脚本，该脚本同时配置ipv4和ipv6 ss server。可根据自己需要自行选择。</p>
<h2 id="客户端配置">客户端配置</h2>
<h3 id="windows客户端配置">Windows客户端配置</h3>
<h4 id="安装shadowsock客户端">安装shadowsock客户端</h4>
<p>到该网址 <a href="https://github.com/shadowsocks/shadowsocks-windows/releases" target="_blank" rel="noopener">https://github.com/shadowsocks/shadowsocks-windows/releases</a> 下载相应的windows客户端程序。<br>
然后配置服务器即可～</p>
<h3 id="linux客户端配置">Linux客户端配置</h3>
<h4 id="安装shadowsocks程序">安装shadowsocks程序</h4>
<p>~$:sudo pip install shadowsocks</p>
<h4 id="运行shadowsocks客户端程序">运行shadowsocks客户端程序</h4>
<p>~$:sudo vim /etc/shadowsocks.json<br>
填入以下配置文件<br>
{<br>
“server”:“填上自己的shadowsocks server ip地址”,<br>
“server_port”:“8888”,//填上自己的shadowsocks server 端口&quot;<br>
“local_port”:1080,<br>
“password”:“mxxhcm150929”,<br>
“timeout”:600,<br>
“method”:“aes-256-cfb”<br>
}</p>
<p>接下来可以执行以下命令运行shadowsocks客户端：<br>
~$:sudo sslocal -c /etc/shadowsocks.json<br>
然后报错：</p>
<blockquote>
<p>INFO: loading config from /etc/shadowsocks.json<br>
2019-03-04 14:37:49 INFO     loading libcrypto from libcrypto.so.1.1<br>
Traceback (most recent call last):<br>
File “/usr/local/bin/sslocal”, line 11, in <module><br>
load_entry_point(‘shadowsocks==2.8.2’, ‘console_scripts’, ‘sslocal’)()<br>
File “/usr/local/lib/python2.7/dist-packages/shadowsocks/local.py”, line 39, in main<br>
config = shell.get_config(True)<br>
File “/usr/local/lib/python2.7/dist-packages/shadowsocks/shell.py”, line 262, in get_config<br>
check_config(config, is_local)<br>
File “/usr/local/lib/python2.7/dist-packages/shadowsocks/shell.py”, line 124, in check_config<br>
encrypt.try_cipher(config[‘password’], config[‘method’])<br>
File “/usr/local/lib/python2.7/dist-packages/shadowsocks/encrypt.py”, line 44, in try_cipher<br>
Encryptor(key, method)<br>
File “/usr/local/lib/python2.7/dist-packages/shadowsocks/encrypt.py”, line 83, in <strong>init</strong><br>
random_string(self._method_info[1]))<br>
File “/usr/local/lib/python2.7/dist-packages/shadowsocks/encrypt.py”, line 109, in get_cipher<br>
return m[2](method, key, iv, op)<br>
File “/usr/local/lib/python2.7/dist-packages/shadowsocks/crypto/openssl.py”, line 76, in <strong>init</strong><br>
load_openssl()<br>
File “/usr/local/lib/python2.7/dist-packages/shadowsocks/crypto/openssl.py”, line 52, in load_openssl<br>
libcrypto.EVP_CIPHER_CTX_cleanup.argtypes = (c_void_p,)<br>
File “/usr/lib/python2.7/ctypes/<strong>init</strong>.py”, line 379, in <strong>getattr</strong><br>
func = self.<strong>getitem</strong>(name)<br>
File “/usr/lib/python2.7/ctypes/<strong>init</strong>.py”, line 384, in <strong>getitem</strong><br>
func = self._FuncPtr((name_or_ordinal, self))<br>
AttributeError: /usr/lib/x86_64-linux-gnu/libcrypto.so.1.1: undefined symbol: EVP_CIPHER_CTX_cleanup</module></p>
</blockquote>
<p>按照参考文献4的做法，是在openssl 1.1.0版本中放弃了EVP_CIPHER_CTX_cleanup函数</p>
<blockquote>
<p>EVP_CIPHER_CTX was made opaque in OpenSSL 1.1.0. As a result, EVP_CIPHER_CTX_reset() appeared and EVP_CIPHER_CTX_cleanup() disappeared.<br>
EVP_CIPHER_CTX_init() remains as an alias for EVP_CIPHER_CTX_reset().</p>
</blockquote>
<p>将openssl库中的EVP_CIPHER_CTX_cleanup改为EVP_CIPHER_CTX_reset即可。<br>
再次执行以下命令，查看shadowsocks安装位置<br>
~#:pip install shadowsocks<br>
Requirement already satisfied: shadowsocks in /usr/local/lib/python2.7/dist-packages<br>
~#:cd /usr/local/lib/python2.7/dist-packages/shadowsocks<br>
~#:vim crypto/openssl.py<br>
搜索cleanup，将其替换为reset<br>
具体位置在第52行libcrypto.EVP_CIPHER_CTX_cleanup.argtypes = (c_void_p,)和第111行libcrypto.EVP_CIPHER_CTX_cleanup(self._ctx)</p>
<h4 id="手动运行后台挂起">手动运行后台挂起</h4>
<p>将所有的log重定向到~/.log/sslocal.log文件中<br>
~$:mkdir ~/.log<br>
~$:touch ~/.log/ss-local.log<br>
~$:nohup sslocal -c /etc/shadowsocks_v6.json &lt;/dev/null &amp;&gt;&gt;~/.log/ss-local.log &amp;</p>
<h4 id="开机自启shadowsocks-client">开机自启shadowsocks client</h4>
<p>但是这样子的话，每次开机都要重新运行上述命令，太麻烦了。可以写个开机自启脚本。执行以下命令：<br>
~$:sudo vim /etc/init.d/shadowsocks<br>
内容为以下shell脚本</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span>!/bin/sh</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span>## BEGIN INIT INFO</span><br><span class="line"><span class="meta">#</span> Provides:          shadowsocks local</span><br><span class="line"><span class="meta">#</span> Required-Start:    $local_fs $remote_fs $network $syslog</span><br><span class="line"><span class="meta">#</span> Required-Stop:     $local_fs $remote_fs $network $syslog</span><br><span class="line"><span class="meta">#</span> Default-Start:     2 3 4 5</span><br><span class="line"><span class="meta">#</span> Default-Stop:      0 1 6</span><br><span class="line"><span class="meta">#</span> Short-Description: shadowsocks service</span><br><span class="line"><span class="meta">#</span> Description:       shadowsocks service daemon</span><br><span class="line"><span class="meta">#</span>## END INIT INFO</span><br><span class="line">start()&#123;</span><br><span class="line">　　  sslocal -c /etc/shadowsocks.json -d start</span><br><span class="line">&#125;</span><br><span class="line">stop()&#123;</span><br><span class="line">　　  sslocal -c /etc/shadowsocks.json -d stop</span><br><span class="line">&#125;</span><br><span class="line">case “$1” in</span><br><span class="line">start)</span><br><span class="line">　　　start</span><br><span class="line">　　　;;</span><br><span class="line">stop)</span><br><span class="line">　　　stop</span><br><span class="line">　　　;;</span><br><span class="line">reload)</span><br><span class="line">　　　stop</span><br><span class="line">　　　start</span><br><span class="line">　　　;;</span><br><span class="line">\*)</span><br><span class="line">　　　echo “Usage: $0 &#123;start|reload|stop&#125;”</span><br><span class="line">　　　exit 1</span><br><span class="line">　　　;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure>
<p>然后执行以下命令即可：<br>
~$:sudo chomod a+x /etc/init.d/shadowsocks<br>
~$:sudo update_rc.d shadowsocks defaults<br>
上述命令执行完成以后，进行测试<br>
~$:sudo service shadosowcks start</p>
<h4 id="配置代理">配置代理</h4>
<p>上一步的目的是建立了shadowsocks服务的本地客户端，socks5流量会走该通道，但是浏览器的网页的流量是https的，我们需要配置相应的代理，将https流量转换为socks5流量，走ss客户端到达ss服务端。当然，也可以把其他各种流量，如tcp,udp等各种流量都转换为socks5流量，这个可以通过全局代理实现，也可以通过添加特定的代理规则实现。</p>
<h5 id="配置全局代理">配置全局代理</h5>
<p>如下图所示，添加ubuntu socks5系统代理：</p>
<p>然后就可以成功上网了。</p>
<h5 id="使用switchyomega配置chrome代理">使用SwitchyOmega配置chrome代理</h5>
<p>首先到 <a href="https://github.com/FelisCatus/SwitchyOmega/releases" target="_blank" rel="noopener">https://github.com/FelisCatus/SwitchyOmega/releases</a> 下载SyitchyOmega.crx。然后在chrome的地址栏输入chrome://extensions，将刚才下载的插件拖进去。<br>
然后在浏览器右上角就有了这个插件，接下来配置插件。如下图：<br>
<img src="https:" alt="mxx"><br>
直接配置proxy，添加如图所示的规则，这样chrome打开的所有网站都是走代理的。</p>
<h4 id="使用privoxy让terminal走socks5">使用privoxy让terminal走socks5</h4>
<p>~$:sudo apt install privoxy<br>
~$:sudo vim /etc/privoxy/config<br>
取消下列行的注释，或者添加相应条目<br>
forward-socks5 / 127.0.0.1:1080 . # SOCKS5代理地址<br>
listen-address 127.0.0.1:8118     # HTTP代理地址<br>
forward 10.*.*.*/ .               # 内网地址不走代理<br>
forward .abc.com/ .             # 指定域名不走代理<br>
重启privoxy服务<br>
~$:sudo service privoxy restart<br>
在bashrc中添加如下环境变量<br>
export http_proxy=&quot;<a href="http://127.0.0.1:8118" target="_blank" rel="noopener">http://127.0.0.1:8118</a>&quot;<br>
export https_proxy=“<a href="http://127.0.0.1:8118" target="_blank" rel="noopener">http://127.0.0.1:8118</a>”</p>
<p>~$:source ~/.bashrc<br>
~$:curl.gs</p>
<h2 id="参考文献">参考文献</h2>
<ol>
<li><a href="http://godjose.com/2017/06/14/new-article/" target="_blank" rel="noopener">http://godjose.com/2017/06/14/new-article/</a></li>
<li><a href="https://www.polarxiong.com/archives/%E6%90%AD%E5%BB%BAipv6-VPN-%E8%AE%A9ipv4%E4%B8%8Aipv6-%E4%B8%8B%E8%BD%BD%E9%80%9F%E5%BA%A6%E6%8F%90%E5%8D%87%E5%88%B0100M.html" target="_blank" rel="noopener">https://www.polarxiong.com/archives/搭建ipv6-VPN-让ipv4上ipv6-下载速度提升到100M.html</a></li>
<li><a href="https://blog.csdn.net/li1914309758/article/details/86510127" target="_blank" rel="noopener">https://blog.csdn.net/li1914309758/article/details/86510127</a></li>
<li><a href="https://blog.csdn.net/blackfrog_unique/article/details/60320737" target="_blank" rel="noopener">https://blog.csdn.net/blackfrog_unique/article/details/60320737</a></li>
<li><a href="https://blog.csdn.net/qq_31851531/article/details/78410146" target="_blank" rel="noopener">https://blog.csdn.net/qq_31851531/article/details/78410146</a></li>
</ol>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://mxxhcm.github.io/2019/03/02/dqn/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="马晓鑫爱马荟荟">
      <meta itemprop="description" content="记录硕士三年自己的积累">
      <meta itemprop="image" content="/images/favicon.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="mxxhcm's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/03/02/dqn/" class="post-title-link" itemprop="http://mxxhcm.github.io/page/28/index.html">DQN</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-03-02 19:29:35" itemprop="dateCreated datePublished" datetime="2019-03-02T19:29:35+08:00">2019-03-02</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-10-24 14:37:58" itemprop="dateModified" datetime="2019-10-24T14:37:58+08:00">2019-10-24</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/强化学习/" itemprop="url" rel="index"><span itemprop="name">强化学习</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="背景">背景</h2>
<ol>
<li>Atari 2600是一个RL benchmark，有2600个游戏，每个agent会得到一个图像输入(60Hz的210 x 160 RGB视频)。本文的目标是设计一个NN架构尽可能学会更多游戏，网络的输入只有视频信息，reward和terminal信号以及可能采取的action，和人类玩游戏时得到的信息是一样的。</li>
<li>Agent与Atari模拟器不断交互，agent不能观测到模拟器的内部状态，只能得到当前屏幕信息的一个图片。这个task可以认为是部分可观测的，因为仅仅从当前的屏幕图像$x_t$上是不能完全理解整个游戏状况的。所有的序列都认为在有限步骤内是会结束的。</li>
<li>注意agent当前的得分取决于整个sequence的action和observation。一个action的feedback可能等到好几千个timesteps之后才能得到。</li>
<li>agent的目标最大化累计reward。定义$t$时刻的回报return为$R_t = \sum^T_{t’=t} \gamma^{t’-t}r_{t’}$，其中$\gamma$是折扣因子，$T$是游戏终止的时间步。</li>
<li>定义最优的动作值函数$Q^{*}(s,a)$是遵循最优策略在状态$s$处采取动作$a$能获得的最大的期望回报，$Q^{*}(s,a) = \max_{\pi}E[R_t|s_t=s,a_t=a,\pi]$。</li>
<li>最优的动作值函数遵循Bellman optimal equation。如果在下个时间步的状态$s’$处，对于所有可能的$a’$，$Q^{*}(s’,a’)$的最优值是已知的（这里就是对于每一个$a’$，都会有一个最优的$Q(s’,a’)$，最优的策略就是选择最大化$r+Q^{*}(s’,a’)$的动作$a’$：<br>
$$Q^{*}(s,a) = E_{s\sim E}[r+ \gamma \max_{a’} Q^{*}(s’,a’)|s,a], \tag{1}$$<br>
强化学习的一个思路就是使用Bellman optimal equation更新动作值函数，$Q_{i+1}(s,a) = E[r + \gamma Q_i(s’,a’)|s,a]$，当$i\rightarrow \infty$时，$Q_i \rightarrow Q^{*}$。</li>
<li>上述例子是state-action pair很少的情况，当有无穷多个的时候，是无法精确计算的。这时候可以采用函数来估计动作值函数，$Q(s,a;\theta) \approx Q^{*}(s,a)$。一般来说，通常采用线性函数进行估计，当然可以采用非线性的函数，如神经网络等等。这里采用的是神经网络，用$\theta$表示网络的参数，这个网络叫做Q网络，Q网络通过最小化下列loss进行训练：<br>
$$L_i(\theta_i) = E_{s,a\sim \rho(\cdot)}\left[(y_i - Q(s,a;\theta_i))^2\right]\tag{2}$$<br>
其中$y_i = E_{s’\sim E}[r+\gamma \max_{a’}Q(s’,a’;\theta_{i-1})]$是第$i$次迭代的target值，其中$\rho(s,a)$是$(s,a)$服从的概率分布。</li>
<li>注意在优化$L_i(\theta_i)$时，上一次迭代的$\theta_{i-1}$是不变的，target取决于网络参数，和监督学习作对比，监督学习的target和网络参数无关。</li>
<li>对Loss函数进行求导，得到下列的gradient信息：<br>
$$\nabla_{\theta_i}L_i(\theta_i) = E_{s,a\sim \rho(\cdot),s’\sim E}\left[(r+\gamma \max_{a’}Q(s’,a’;\theta_{i-1})-Q(s,a;\theta_i))\nabla_{\theta_i}Q(s,a;\theta_i)\right]\tag{3}$$<br>
通过SGD优化loss函数。如果权重是每隔几个timestep进行更新，并且用从分布$\rho$和环境$E$中采样得到的样本取代期望，就可以得到熟悉的Q-learning算法[2]。(这个具体为什么是这样，我也不清楚，可以看参考文献2)</li>
<li>什么是on-polciy算法：</li>
</ol>
<blockquote>
<p>On-policy methods attempt to evaluate or improve the policy that is used to make decisions, whereas  off-policy methods evaluate or improve a policy different from that used to generate the data.</p>
</blockquote>
<p>Sarsa和Q-learning的区别在于更新Q值时的target policy和behaviour policy是否相同。我觉的是是policy evaluation和value iteration的区别，policy evaluation使用动态规划算法更新$V(s)$，但是并没有改变行为策略，更新迭代用的数据都是利用之前的行为策略生成的。而值迭代是policy evaluation+policy improvement，每一步都用贪心策略选择出最大的$a$更新$V(s)$，target policy（greedy）和behaviour policy（$\varepsilon$-greedy）是不同的。</p>
<h2 id="强化学习需要解决的问题">强化学习需要解决的问题</h2>
<ol>
<li>大量有标记的训练数据。</li>
<li>delayed-reward。这个delay存在于action和reward之间，可以达到几千个timesteps那么远，和supervised learnign中输入和输入之间直接的关系相比要复杂的多。</li>
<li>大多数深度学习算法假设样本之间都是独立的，然而强化学习的一个sequence(序列)通常是高度相关的。</li>
<li>强化学习算法学习到的policy变化时，数据服从的分布通常会改变，然而深度学习通常假设数据服从一个固定的分布。</li>
</ol>
<h2 id="dqn">DQN</h2>
<p>论文名称<a href="https://arxiv.org/pdf/1312.5602.pdf" target="_blank" rel="noopener">Playing Atari with Deep Reinforcement Learning</a></p>
<h3 id="概述">概述</h3>
<p>DQN算法使用卷积神经网络代替Q-learning中tabular的值函数，并提出了几个trick促进收敛。DQN agnet的输入是原始的图片，输出是图片表示的state可能采取的action的$Q$值。</p>
<ol>
<li>dqn是Model-Free的，它直接从环境$E$中采样，并没有显式的对环境进行建模。</li>
<li>dqn是一个online的方法，即训练数据不断增加；offline是训练数据固定。</li>
<li>dqn是一个off-policy算法，target policy 是greedy policy，behaviour policy是$\varepsilon$-greedy policy，target policy和greedy policy策略不同。</li>
<li>DQN是不收敛的。</li>
</ol>
<h3 id="解决方案">解决方案</h3>
<h4 id="experience-replay">Experience replay</h4>
<ol>
<li>DQN使用了experience replay，将多个episodes中的经验存储到一个大小为$N$的replay buffer中。在更新$Q$值的时候，从replay buffer中进行采样更新。behaviour policy是$\varepsilon$-greedy策略，保持探索。target policy是$\varepsilon$ greedy 算法，因为replay buffer中存放的都是behaviour policy生成的experience，所以是off-policy算法。<br>
采用experience replay的DQN和Q-learning算法相比有三个好处，第一个是每一个experience可以多次用来更新参数，提高了数据训练效率；第二个是直接从连续的样本中进行学习是低效的，因为样本之间存在强关联性。第三个是on-policy的学习中，当前的参数决定下一次采样的样本，就可能使学习出来的结果发生偏移。</li>
<li>replay buffer中只存储最近N个experience。</li>
</ol>
<h4 id="data-preprocess">Data preprocess</h4>
<ol>
<li>原始图像是$210\times 160$的RGB图像，预处理首先将它变为灰度图，并进行下采样得到一个$110\times 84$的图像，然后从这个图像中截取一个$84\times 84$的图像。</li>
<li>作者使用预处理函数$\phi$处理连续四张的图像而不是一张，然后将这个预处理后的结果输入$Q$函数。</li>
<li>预处理函数$\phi$是一个卷积神经网络，输入是$84\times 84\times 4$的图像矩阵，经过$16$个stride为$4$的$8\times 8$filter，经过relu激活函数，再经过$32$个stride为$2$的$4\times 4$filter，经过relu激活函数，最后接一个256个单元的全连接层。输出层的大小根据不同游戏的动作个数决定。</li>
<li>$Q$网络的输入是预处理后的图像state，输出是所有当前state可能采取的action的$Q$值。</li>
</ol>
<h3 id="网络结构">网络结构</h3>
<p>输入：[batch_size, 84, 84, 4]<br>
第一个隐藏层：16个步长为$4$的$8\times 8$的filters<br>
第二个隐藏层：32个步长为$2$的$4\times 4$的filters<br>
全连接层：256个units<br>
输出层：softmax</p>
<h3 id="算法">算法</h3>
<p>算法 1 Deep Q-learning with Experience Replay<br>
Initialize replay memory D to capacity N<br>
Initialize action-value function Q with random weights<br>
for episode = $1, M$ do<br>
$\ \ \ \ \ \ \ \ $Initialize sequence $s_1 = {x_1}$ and preprocessed sequenced $\phi_1 = \phi(s_1)$<br>
$\ \ \ \ \ \ \ \ $for $t = 1,T$ do<br>
$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $With probability $\varepsilon$ select a random action $a_t$<br>
$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $otherwise select $a_t = \max_a Q^{∗}(\phi(s_t), a; θ)$<br>
$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $Execute action $a_t$ in emulator and observe reward $r_t$ and image $x_{t+1}$<br>
$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $Set $s_{t+1} = s_t, a_t, x_{t+1}$ and preprocess $\phi_{t+1} = \phi(s_{t+1})$<br>
$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $Store transition $(\phi_t, a_t, r_t, \phi_{t+1})$ in D<br>
$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $Sample random minibatch of transitions $(\phi_j, a_j, r_j, \phi_{j+1})$ from D<br>
$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $Set $y_j = \begin{cases}r_j&amp;\ \ \ \ for\ terminal\ \phi_{j+1}\\r_j+\gamma \max_{a’}Q(\phi_{j+1},a’|\theta)&amp;\ \ \ \ for\ non-terminal\ \phi_{j+1}\end{cases}$<br>
$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $Perform a gradient descent step on $(y_j − Q(\phi_j, a_j|θ))^2$<br>
$\ \ \ \ \ \ \ \ $end for<br>
end for</p>
<h3 id="experiments">Experiments</h3>
<h4 id="datasets">Datasets</h4>
<p>七个Atari 2600 games: B.Rider, Breakout, Enduro, Pong, Q bert, Seaquest, S.Invaders。<br>
在六个游戏上DQN是SOTA，在三个游戏上DQN的表现超过了人类。</p>
<h4 id="settings">Settings</h4>
<ol>
<li>不同游戏的reward变化很大，这里把正的reward全部设置为$1$，把负的reward全部设置为$-1$，reward为$0$的保持不变。这样子在不同游戏中也可以统一学习率。</li>
<li>采用RMSProp优化算法，batch size为$32$，behaviour policy采用的是$\varepsilon$-greedy，在前$100$万步内，$\varepsilon$从$1$变到$0.1$，接下来保持不变。</li>
<li>使用了fram-skip技术，每隔$k$步，agent才选择一个action，在中间的$k-1$步中，保持原来的action不变。这里选择了$k=4$，有的游戏设置的为$k=3$。</li>
<li>超参数设置没有说</li>
</ol>
<h4 id="metrics">Metrics</h4>
<p>每个agent训练$10$ millions帧，replay buffer size是$1$ million。每个epoch进行$50000$个minibatch weight updates或者大约$30$分钟的训练（这里有些不理解）。然后使用$\epsilon$-greedy($\epsilon=0.05$) evaluation $10000$个steps。</p>
<h5 id="average-total-reward">average total reward</h5>
<p>第一个metric是在一个episode或者一次游戏内total reward的平均值。这个metric带有很大噪音，因为policy权值一个很小的改变可能就会对policy访问states的分布造成很大的影响。</p>
<h5 id="action-value-function">action value function</h5>
<p>第二个metric是估计的action-value function，这里作者的做法是在训练开始前使用random policy收集一个固定的states set，然后track这个set中states最大预测$Q$值的平均。尽管缺乏理论收敛保证，DQN看起来还不错。</p>
<h4 id="baselines">Baselines</h4>
<ol>
<li>Sarsa</li>
<li>Contingency</li>
<li>DQN</li>
<li>Human</li>
</ol>
<h3 id="代码">代码</h3>
<p><a href="https://github.com/devsisters/DQN-tensorflow" target="_blank" rel="noopener">https://github.com/devsisters/DQN-tensorflow</a></p>
<h2 id="nature-dqn">Nature DQN</h2>
<h3 id="非线性拟合函数不收敛的原因">非线性拟合函数不收敛的原因</h3>
<ol>
<li>序列中状态的高度相关性。</li>
<li>$Q$值的一点更新就会对policy改变造成很大的影响，从而改变数据的分布。</li>
<li>待优化的$Q$值和target value(目标Q值)之间的关系，每次优化时的目标Q值都是固定上次的参数得来的，优化目标随着优化过程一直在变。<br>
前两个问题是通过DQN中提出的replay buffer解决的，第三个问题是Natura DQN中解决的，在一定时间步内，固定target network参数，更新待network的参数，然后每隔固定步数将network的参数拷贝给target network。</li>
</ol>
<blockquote>
<p>This instability has several causes: the correlations present in the sequence of observations, the fact that small updates to Q may significantly change the policy and therefore change the data distribution, and the correlations between the action-values (Q) and the target values $r+\gamma \max_{a’}Q(s’,a’)$.<br>
We address these instabilities with a novel variant of Q-learning, which uses two key ideas. First, we used a biologically inspired mechanism termed experience replay that randomizes over the data, thereby removing correlations in the observation sequence and smoothing over changes in the data distribution. Second, we used an iterative update that adjusts the action-values (Q) towards target values that are only periodically updated, thereby reducing correlations with the target.</p>
</blockquote>
<h3 id="解决方案-v2">解决方案</h3>
<ol>
<li>预处理的结构变了,CNN的层数增加了一层，</li>
<li>加了target network，</li>
<li>将error限制在$[-1,1]$之间。</li>
</ol>
<blockquote>
<p>clip the error term from the update $r + \gamma \max_{a’} Q(s’,a’;\theta_i^{-} - Q(s,a;\theta_i)$ to be between $-1$ and $1$. Because the absolute value loss function $|x|$ has a derivative of $-1$ for all negative values of $x$ and a derivative of $1$ for all positive values of $x$, clipping the squared error to be between $-1$ and $1$ corresponds to using an absolute value loss function for errors outside of the $(-1,1)$ interval.</p>
</blockquote>
<h3 id="框架和网络结构">框架和网络结构</h3>
<h4 id="框架">框架</h4>
<p>Nature-DNQ的框架如下所示<br>
<img src="/2019/03/02/dqn/nature-dqn.png" alt="ndqn"></p>
<h4 id="网络结构-v2">网络结构</h4>
<p>输入:[batch_size, 84, 84, 4]<br>
三个卷积层，两个全连接层（包含输出层）<br>
第一个隐藏层：$32$个步长为$4$的$8\times 8$filters，以及一个relu<br>
第二个隐藏层：$64$个步长为$2$的$4\times 4$filters，以及一个relu<br>
第三个隐藏层：$64$个步长为$1$的$3\times 3$filters，以及一个relu<br>
第四个隐藏层：$512$个units<br>
输出层：softmax，输出每个action对应的$Q$值</p>
<h3 id="算法-v2">算法</h3>
<p>算法 2 deep Q-learning with experience replay, target network<br>
Initialize replay memory D to capacity N<br>
Initialize action-value function Q with random weights $\theta$<br>
Initialize target action-value function $\hat{Q}$ with weights $\theta^{-}=\theta$<br>
for episode = $1, M$ do<br>
$\ \ \ \ \ \ \ \ $Initialize sequence $s_1 = {x_1}$ and preprocessed sequenced $\phi_1 = \phi(s_1)$<br>
$\ \ \ \ \ \ \ \ $for $t = 1,T$ do<br>
$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $With probability $\varepsilon$ select a random action $a_t$<br>
$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $otherwise select $a_t = \max_a Q^{∗}(\phi(s_t), a; θ)$<br>
$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $Execute action $a_t$ in emulator and observe reward $r_t$ and image $x_{t+1}$<br>
$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $Set $s_{t+1} = s_t, a_t, x_{t+1}$ and preprocess $\phi_{t+1} = \phi(s_{t+1})$<br>
$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $Store transition $(\phi_t, a_t, r_t, \phi_{t+1})$ in D<br>
$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $Sample random minibatch of transitions $(\phi_j, a_j, r_j, \phi_{j+1})$ from D<br>
$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $Set $y_j = \begin{cases}r_j&amp;\ \ \ \ for\ terminal\ \phi_{j+1}\\r_j+\gamma \max_{a’}Q(\phi_{j+1},a’|\theta^{-})&amp;\ \ \ \ for\ non-terminal\ \phi_{j+1}\end{cases}$<br>
$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $Perform a gradient descent step on $(y_j − Q(\phi_j, a_j|θ))^2$ with respect to the network parameters $\theta$<br>
$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $Every $C$ steps reset $\hat{Q} = Q$<br>
$\ \ \ \ \ \ \ \ $end for<br>
end for</p>
<h3 id="experiments-v2">Experiments</h3>
<h4 id="settings-v2">Settings</h4>
<ul>
<li>batch-size: 32</li>
<li>replacy memory size: 1000000 frames</li>
<li>target network update frequency: 10000</li>
<li>discount factor: 0.99</li>
<li>action repeat: 4 # 就是frame skip</li>
<li>history length: 4 # 使用最近的几帧重叠，实际上是16帧</li>
<li>paramteter update frequency: 4 # 执行sgd train的frequency</li>
<li>learning rate: 0.00025</li>
<li>gradient momentum: 0.95</li>
<li>squared gradient momentum: 0.95</li>
<li>min squared gradient: 0.01</li>
<li>initial exploration: 1</li>
<li>final exploration 0.1</li>
<li>final exploration frame: 1000000</li>
<li>replay start size: 50000</li>
<li>no-op max: 30</li>
<li>reward: clipped to [-1, 1]</li>
<li>total train frames: 50 millons frame（实际上是200 millions emulated frames，因为有设置为$4$ frame skip）。</li>
<li>选择random作为一个baseline，因为人类的极限是$10$hz，为了公平起见，random baseline以$10$hz的频率随机选择一个action，atari视频的频率是$60$hz，所以每隔$6$帧，随机选择一个action，在选择action中间的帧中保持这一个action。</li>
</ul>
<h4 id="experiments-v3">Experiments</h4>
<ul>
<li>Average score和average action value<br>
每个training epoch之后进行一次evaluation，记录evaluation过程中average episode reward。总共train $50$million frames，大概有$200$个epoch，也就是一个epoch是$25$万frames，在每个epoch后使用$\epsilon$-greedy($\epsilon =0.05$)策略evaluate $520k$个frames。</li>
<li>Main-Evaluation: Compartion between DQN and other baselines<br>
Baselines有Random play, best linear learner，SARSA，Huamn等。<br>
DQN总共训练了$50$ million frames，replay buffer存放最近的$1$ million framems。在完成训练后，至多执行$30$次no-op，产生随机初始状态，使用$\epsilon$-greedy($\epsilon=0.05$)玩$5$分钟，对多次结果取平均。<br>
Human的数据是在玩家首先进行了$2$个小时训练后，然后玩大约20 episodes，每个episode最长5 min的 average reward。<br>
表格中最后一列还给出了一个百分比，$100\times \frac{\text{DQN score} - \text{random play score}}{\text{human score} - \text{random play score}}$。</li>
<li>Replay buffer和target network的abalation实验<br>
在三个不同的learning rate下使用standard hyperparameters训练DQN $10$ million frames。每隔$250,000$ training frames对每个agent进行$135,000$ frames的validation，记录最高的average episode score。 这些valuation episodes并没有在$5$ min的时候截断，这个实验中Enduro得到的score要比main evaluation中高。这个实验中training的帧数($10$ million frames)要比baseline中training的frames($50$ million frames)少。</li>
<li>DQN和linear function approximator比较<br>
除了function approximator由CNN变成linear的，其他都没有变。<br>
在$5$个validation games上，每个agent在三个不同的learning rates使用标准的参数训练了$10$ million frames。每隔$250000$个training frames对agent进行$135,000$ frames的validation，reported 最高的average episode score。 这些valuation episodes并没有在$5$ min的时候截断，这个实验中Enduro得到的score要比main evaluation中高。这个实验中training的帧数($10$ million frames)要比baseline中training的frames($50$ million frames)少。</li>
</ul>
<h2 id="gorila-dqn">Gorila DQN</h2>
<p>论文名称：<br>
Massively Parallel Methods for Deep Reinforcement Learning<br>
下载地址：<br>
<a href="https://arxiv.org/pdf/1507.04296.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1507.04296.pdf</a></p>
<h3 id="experiments-v4">Experiments</h3>
<h4 id="settings-v3">Settings</h4>
<ul>
<li>网络结构和nature DQN一样。</li>
<li>使用了frame-skip，设置为$4$。</li>
<li>Replay memory是$1$M&gt;</li>
</ul>
<h4 id="evaluation">Evaluation</h4>
<p>Evaluation有两种：</p>
<h5 id="null-op-starts">null op starts</h5>
<p>每个agent在它训练的游戏上evaluated $30$个episodes，每个episode随机的至多执行$30$次no-op之后，评估$5$min的emulator时间($18000$ frames)。然后取这$100$次的平均值。</p>
<h5 id="human-starts">human starts</h5>
<p>Human starts是用来衡量它对于agent可能没有遇到过的state的泛化能力。对于每一个游戏，从一个人类玩家的gameplay中随机取$100$个开始点，使用$\epsilon$-greedy policy玩三十分钟emulator时间（即$108000$frames)。</p>
<h2 id="double-dqn">Double DQN</h2>
<h3 id="dqn中的overestimate问题">DQN中的overestimate问题</h3>
<p>解决overestimate问题，Q-learning中在estimated values上进行了max操作，可能会导致某些更偏爱overestimated value而不是underestimated values。<br>
本文将Double Q-learning的想法推广到了dqn上形成了double-dqn。实验结果表明了overestimated value对于policy有影响，double 会产生好的action value，同时在一些游戏上会得到更高的scores。</p>
<h3 id="contributions">Contributions</h3>
<ol>
<li>解释了在large scale 问题上，Q-learning被overoptimistic的原因是学习固有的estimation errors。</li>
<li>overestimation在实践中是很常见，也很严重的。</li>
<li>Double Q-learning可以减少overoptimism</li>
<li>提出了double-dqn。</li>
<li>double-dqn在某些游戏上可以找到更好的policy。</li>
</ol>
<h3 id="double-q-learning">Double Q-learning</h3>
<p>Q-learning算法计算target value $y$的公式如下：<br>
$$y = r + \gamma \max_a’ Q(s’, a’|\theta_t)\tag{4}$$<br>
在计算target value的时候，使用同一个网络选择和评估action $a’$，这可能会让网络选择一个overestimated value，最后得到一个overoptimistic value estimate。所有就有了double Q-learning，计算公式如下：<br>
$$y = r + \gamma Q(s’, \arg\max_a’ Q(s’,a;\theta_t);\theta’_t)\tag{5}$$<br>
target policy还是greedy policy，通过使用$\theta$对应的网络选择action，然后在计算target value的时候使用$\theta’$对应的网络。<br>
原有的公式可以写成下式，<br>
$$y = r + \gamma Q(s’, \arg\max_a’ Q(s’,a;\theta_t);\theta_t)\tag{6}$$<br>
即选择action和计算target value都是使用的同一个网络。</p>
<h3 id="double-dqn-v2">Double DQN</h3>
<p><img src="/2019/03/02/dqn/double-dqn.png" alt="double-dqn"><br>
Double Q-learnign的做法是分解target action中的max opearation为选择和evaluation。而在Nature-dqn中，提出了target network，所以分别使用network和target network去选择和evaluation action是一个很好的做法，这样子公式就变成了<br>
$$y = r + \gamma Q(s’, \arg\max_a’ Q(s’,a;\theta_t);\theta^{-}_t)\tag{7}$$<br>
和Q-learnign相比，将$\theta’$换成了$\theta^{-}$ evaluate action，target network的更新和nature-dqn一样，过一段时间复制network的参数。</p>
<h3 id="double-q-learning-vs-q-learning">Double Q learning vs Q-learning</h3>
<p>可以在数学上证明，Q-learning是overestimation的，但是double q leraing是无偏的。。。证明留待以后再说。<br>
<todo></todo></p>
<h3 id="网络结构-v3">网络结构</h3>
<p>网络结构和nature DQN一样。</p>
<h3 id="算法-v3">算法</h3>
<p>算法 3: Double DQN Algorithm.<br>
输入: replay buffer $D$, 初始network参数$\theta$,target network参数$\theta^{-}$<br>
输入 : replay buffer的大小$N_r$, batch size $N_b$, target network更新频率$N^{-}$<br>
<strong>for</strong> episode $e \in {1, 2,\cdots, M}$ do<br>
$\qquad$初始化frame sequence $\mathbf{x} \leftarrow ()$<br>
$\qquad$<strong>for</strong> $t \in {0, 1, \cdots}$ do<br>
$\qquad\qquad$设置state $s \leftarrow \mathbf{x}$, 采样 action $a \sim\pi_B$<br>
$\qquad\qquad$给定$(s, a)$，从环境$E$中采样接下来的frame $x_t$,接收reward $r$,在序列$\mathbf{x}$上拼接$x$<br>
$\qquad\qquad$<strong>if</strong> $|\mathbf{x}| \gt N_f$<br>
$\qquad\qquad$<strong>then</strong><br>
$\qquad\qquad\qquad$从$\mathbf{x}$中删除最老的frame $x_{t_min}$<br>
$\qquad\qquad$设置$s’ \leftarrow \mathbf{x}$,添加transition tuple (s, a, r, s 0 ) 到buffer D中，如果$|D| \ge N_r$替换最老的tuple<br>
$\qquad\qquad$采样$N_b$个tuples $(s, a, r, s’) \sim Unif(D)$<br>
$\qquad\qquad$计算target values, one for each of $N_b$ tuples:<br>
$\qquad\qquad$定义$a^{\max}(s’; \theta) = \arg \max_{a’} Q(s’, a’;\theta)$<br>
$\qquad\qquad y_j = \begin{cases}r&amp;\qquad if\ \ s’\ \ is\ \ terminal\\ r+\gamma Q(s’, a^{\max}(s’;\theta);\theta^{-}, &amp;\qquad otherwise\end{cases}$<br>
$\qquad\qquad$利用loss $||y_j − Q(s, a; \theta)||^2$的梯度更新<br>
$\qquad\qquad$每隔$N^{-}$个步骤更新一下target network 参数$\theta^{-}$<br>
$\qquad$<strong>end</strong><br>
<strong>end</strong></p>
<h3 id="experiments-v5">Experiments</h3>
<h4 id="settings-v4">Settings</h4>
<p>Tunned Double DQN，update frequency从$10000$改成了$30000$，训练时$\epsilon$在$1$ millon内从$0.1$退火到$0.01$。Evaluation时是$0.001$。</p>
<h4 id="evaluation-v2">Evaluation</h4>
<p>和Gorila DQN一样，用了两种：no-op和human starts。</p>
<h4 id="training">Training</h4>
<p>在每个游戏上，网络都训练了$200$M frames，也就是$50$M steps。每隔$1$M step进行一次evaluation，从evaluations中选出最好的policy作为输出。</p>
<h4 id="metric">Metric</h4>
<p>提出了一个指标，normalized score，计算公式如下：<br>
$$score_{normalized} = \frac{score_{agent}- score_{random}}{score_{human}-score_{random}}\tag{8}$$<br>
分母是human和random之差，对应$100%$。</p>
<h2 id="prioritized-dqn-per">Prioritized DQN(PER)</h2>
<h3 id="contributions-v2">contributions</h3>
<p>本文提出一种了proritizing experience的框架，在训练过程中多次使用重要的transtions replay进行更新，让训练变得的更有效率。<br>
使用TD-errors作为prioritization mechanism，给出了两种protitization计算方式，提出了一种stochastic prioritization以及importance sampling方法。</p>
<h3 id="prioritized-replay">Prioritized replay</h3>
<p>可以从两个维度上考虑replay memeory的改进，一个是存哪些experiences，一个是使用哪些experiences进行回放。本文是从第二个方向上进行的考虑。</p>
<p>从buffer中随机抽样的方法中，update steps和memory size是线性关系，作者想找一个update steps和memory size是log关系的oracle，但是很遗憾，这是不现实的，所以作者想要找一种比uniform random replay好尽量接近oracle的方法。</p>
<h4 id="prioritizion-with-td-error">Prioritizion with TD-error</h4>
<p>prioritized replay最重要的部分是如何评价每一个transition的重要程度。一个理想的criterion是agent在当前的state可以从某个transition中学到多少。这个measure metric是不确定的，一个替代方案是使用TD error $\delta$，表示how ‘suprising’ 或者upexpected the transition：就是当前的value离next-step bootstrap得到的value相差多少，booststrap就是基于其他估计值进行计算。。这中方法对于incremental,online RL方法，例如SARSA以及Q-learning来说都是很合适的，因为他们会计算TD-error，然后给TD-error一个比例系数用来更新参数。然后当reward是noisy的时候，TD-error效果可能很差。<br>
作者在一个人工设计的环境中使用了greedy TD-error prioritization算法，算法在每次存transition到replay buffer的时候，同时还会存一下该transition最新的TD-error，然后在更新的时候从memory中选择TD-error最大的transition。最新的transition TD-error没有算出来，就给它一个最大的priority，保证所有的experience都至少被看到过一次。<br>
采用二叉堆用实现优先队列，查找复杂度是$O(1)$，更新priorities的复杂度是$O(logN)$。</p>
<h4 id="stochastic-prioritization">Stochastic prioritization</h4>
<p>上述方法有很多问题。第一，每次都sweep整个replay memory的计算量很大，所以只有被replayed的experiences的TD-errors才会被更新。开始时一个TD error很小的transition可能很长一段事件不会被replayed，这就导致了replay buffer的sliding window不起作用了。第二，TD-error对于noise spike很敏感，还会被bootstrap加剧，估计误差可能会是另一个noise。第三，greedy prioritization集中在experiences的一个subset：errors减小的很慢，尤其是使用function appriximation时，这就意味着初始的高error的transitions会被replayed的很频繁，然后会over-fitting因为缺乏diversity。<br>
为了解决这些问题，引入了一个介于pure greedy prioritizaiton以及uniform random sampling之间的stochastic采样方法，priority高的transition有更大的概率被采样，而lowest-priority的transition也有概率被选中，具体的定义transition $i$的概率如下：<br>
$$P(i) = \frac{p_i^{\alpha}}{\sum_kp_k^{\alpha}}\tag{9}$$<br>
$\alpha$确定prioritizaiton的比重，如果$\alpha=0$就是unifrom。</p>
<p>有两种$p_i$的计算方法，一种是直接的proportional prioritization，$p_i = |\delta_i| + \varepsilon$，其中$\varepsilon$是一个小的正整数，确定当$p_i=0$时，该transition仍能被replay；第二种是间接的，$p_i = \frac{1}{rank(i)}$，其中$rank(i)$是所有replay memory中的experiences根据$|\delta_i|$排序后的rank。第二种方法的鲁棒性更好。<br>
在实现上，两种方法都有相应的trick，让复杂度不依赖于memory 大小$N$。Proportional prioritization采用了’sum-tree’数据结构，每一个节点都是它的子节点的children，priorities是leaf nodes。而rank-based方法，使用线性函数估计累计密度函数，具体怎么实现没有细看。</p>
<h4 id="annealing-the-bias">annealing the bias</h4>
<p>因为random sample方法，samples之间没有一点联系，选择每一个sample的概率都是相等的，但是如果加上了priority，就有一个bias toward高priority的samples。IS和prioritized replay的组合在non-learn FA中有一个用处，large steps可能会产生不好的影响，因为梯度信息可能是局部reliable，所以需要使用一个小点的step-size。<br>
在本文中，high-error的样本可能会观测到很多次，使用IS减小gradient的大小，对应于高priority的samples的weight被微调了一下，而对应于低priority的样本基本不变。<br>
weigth的计算公式如下：<br>
$$w_i = (\frac{1}{N}\cdot \frac{1}{P(i)})^{\beta}\tag{10}$$<br>
OK,这里IS的作用有些不明白。。。。<todo></todo></p>
<h3 id="算法-v4">算法</h3>
<p>算法 4<br>
输入: minibatch $k$, 学习率（步长）$\eta$, replay period $K$ and size $N$ , exponents $\alpha$ and $\beta$, budget $T$.<br>
初始化replay memory $H = \emptyset, \Delta = 0, p_1 = 1$<br>
根据$S_0$选择 $A_0 \sim \pi_{\theta}|(S_0)$<br>
<strong>for</strong> $t = 1,\cdots, T$ do<br>
$\qquad$观测$S_t, R_t, \gamma_t$<br>
$\qquad$存储transition $(S_{t−1}, A_{t−1}, R_t , \gamma_t, S_t)$ 到replay memory，以及$p_t$的最大priority $p_t = \max {i\lt t} p_i$<br>
$\qquad$<strong>if</strong> $t ≡ 0$ mod $K$ then<br>
$\qquad\qquad$<strong>for</strong> j = 1 to k do<br>
$\qquad\qquad\qquad$Sample transition $j \sim P(j) = \frac{p_j^{\alpha}}{\sum_i p_i^{\alpha}}$<br>
$\qquad\qquad\qquad$计算importance-sampling weight $w_j = \frac{(N \cdot P(j))^{\beta}}{\max_i w_i}$<br>
$\qquad\qquad\qquad$计算TD-error $\delta_j = R_j + \gamma_j Q_{target} (S_j$, $arg \max_a Q(S_j, a)) − Q(S_{j−1} , A\ {j−1})$<br>
$\qquad\qquad\qquad$更新transition的priority $p_j \leftarrow |\delta_j|$<br>
$\qquad\qquad\qquad$累计weight-change $\Delta \leftarrow \Delta + w_j \cdot \delta_j \cdot \nabla_{\theta} Q(S_{j−1}, A_{j−1})$<br>
$\qquad\qquad$<strong>end for</strong><br>
$\qquad\qquad$更新weights $\theta\leftarrow \theta+ \eta\cdot\Delta$, 重置$\Delta = 0$<br>
$\qquad\qquad$每隔一段时间更新target network $\theta_{target} \leftarrow \theta$<br>
$\qquad$<strong>end if</strong><br>
$\qquad$选择action $A_t \sim \pi_{\theta}(S_t)$<br>
<strong>end for</strong></p>
<h3 id="experiments-v6">Experiments</h3>
<p>两组实验，<br>
一组是DQN和proportional prioritization作比较。<br>
一组是tuned Double DQN和rank-based以及proportional prioritizaiton。</p>
<h4 id="metrics-v2">Metrics</h4>
<p>用的是double dqn提出来的nomalized score，这里在分母上加了绝对值。<br>
主要用的median scores和mean scores。</p>
<h2 id="dueling-dqn">Dueling DQN</h2>
<h3 id="介绍">介绍</h3>
<p>本文作者提出来将dueling网络框架应用在model-free算法上。The dueling architecture能用一个deep model同时表示$V(s)$和优势函数$A(s,a)$，网络的输出将$V$和$A$结合产生$Q(s,a)$。和advantage不一样的是，这种方式在构建时就将他们进行了解耦，因此，dueling architecture可以应用在各种各样的model free RL算法上。<br>
本文的架构是对算法创新的补充，它可以对之前已有的各种DQN算法进行结合。</p>
<h3 id="dueling-network-architecture">dueling network architecture</h3>
<p>这个新的architecture的核心想法是，没有必要估计所有states的action value。在一些states，需要action value去确定执行哪个action，但是在许多其他states，action values并没有什么用。当然，对于bootstrap算法来说，每一个state的value estimation都很重要。<br>
<img src="/2019/03/02/dqn/deuling-dqn.png" alt="dueling-dqn"><br>
作者给出了一个single Q-network的architecture，如图所示。<br>
网络结构和nature-dqn一样，但是这里加了两个fully connected layers，一个用于输出$V$，一个用于输出$A$。然后$A$和$V$结合在一起，产生$Q$，网络的输出和nature dqn一样，对应于某个state的一系列action value。<br>
从$Q$函数的定义$Q^{\pi}(s,a) = V^{\pi}(s)+A^{\pi}(s,a)$以及$Q$和$V$之间的关系$V^{\pi}(s) = \mathbb{E}_{a\sim\pi(s)}\left[Q^{\pi}(s,a)\right] = \pi(a|s)Q^{\pi}(s,a)$，所以有$\mathbb{E}_{a\sim\pi(s)}\left[A^{\pi}(s,a)\right]=0$。此外，对于deterministic policy，$a^{*} = \arg \max_{a’\in A}Q(s,a’)$，有$V(s) = Q(s,a^{*})$，即$A(s,a^{*}) = 0$。<br>
如图所示的network中，一个网络输出scalar $V(s;\theta, \beta)$，一个网络输出一个$|A|$维的vector $A(s,a;\theta, \alpha)$，其中$\theta$是网络参数，$\alpha$和$\beta$是两个全连接层的参数。<br>
根据advantage的定义，可以直接将他们加起来，即：<br>
$$Q(s,a;\theta, \alpha, \beta) = V(s;\theta, \beta) + A(s,a;\theta, \alpha) \tag{11}$$<br>
但是，我们需要知道的一点是，$Q(s, a;\theta, \alpha, \beta)$仅仅是$Q$的一个参数化估计。它由两部分组成，一部分是$V$，一部分是$A$，但是需要注意的是，这里的$V$和$Q$只是我们叫它$V$和$A$，它的实际意义并不是$V$和$A$。给了$Q$，我们可以得到任意的$Q(s, a) = V(s) + A(s,a)$，而$V$和$Q$并不代表value function和advantage functino。<br>
为了解决这个问题，作者提出了选择让advantage为$0$的action，即：<br>
$$Q(s, a; \theta,\alpha, \beta) = V(s; \theta, \beta) + \left(A(s,a;\theta,\alpha) - \max_{a’\in |A|}A(s, a’; \theta, \alpha)\right)\tag{12}$$<br>
选择$a^{*} = \arg \max_{a’\in A} Q(s, a’; \theta, \alpha, \beta) = \arg \max_{a’\in A}A(s, a’;\theta, \alpha)$，我们得到$Q(s,a^{*}; \theta, \alpha,\beta) = V(s;\theta, \beta)$。这个时候，输出$V$的网络给出的真的是state value的估计$V(s;\theta, \beta)$，另一个网络真的给出的是advantage的估计。<br>
另一种方法是用mean取代max操作：<br>
$$Q(s, a; \theta,\alpha, \beta) = V(s; \theta, \beta) + \left(A(s,a;\theta,\alpha)- \frac{1}{|A|}\sum_{a’}A(s, a’; \theta, \alpha)\right)\tag{13}$$<br>
一方面这种方法失去了$V$和$A$的原始语义，因为它们有一个常数的off-target，但是另一方面它增加了优化的稳定性，因为上式中advantage的改变只需要和mean保持一致即可，不需要optimal action’s advantange一有变化就要改变。</p>
<h3 id="算法-v5">算法</h3>
<h2 id="distributed-dqn">Distributed DQN</h2>
<h2 id="noisy-dqn">Noisy DQN</h2>
<h3 id="介绍-v2">介绍</h3>
<p>已有方法的exploration都是通过agent policy的random perturbations，比如常见的$\varepsilon$-greedy等方法。这些方法不能找出环境中efficient exploration的behavioural patterns。常见的方法有以下几种:<br>
第一种方法是optimism in the face of uncertainty，理论上证明可行，但是通常应用在state-action spaces很小的情况下或者linear FA，很难处理non-linearn FA，而且non-linear情况下收敛性没有保证。<br>
另一种方法是添加额外的intrinsic motivation term，该方法的问题是将算法的generalisation mechanism和exploration分割开，即有instrinsic reward和environment reward，它们的比例如何去设置，需要认为指定。如果不仔细调整，optimal policy可能会受intrinsic reward影响很大。此外为了增加exploration的鲁邦性，扰动项仍然是需要的。这些算法很具体也能应用在参数化policy上，但是很低效，而且需要很多次policy evaluation。<br>
本文提出NoisyNet学习网络参数的perturbations，主要想法是参数的一点改变可能会导致policy在很多个timsteps上的consistent，complex, state-dependent的变化，而如$\varepsilon$-greedy的dithering算法中，每一步添加到policy上的noise都是不相关的。pertubations从一个noise分布中进行采样，它的variance可以看成noise的energy，variance的参数和网络参数都是通过loss的梯度进行更新。网络参数中仅仅加入了噪音，没有distribution，可以自动学习。<br>
在高维度上，本文的算法是一个randomised value function，这个函数是neural network，网络的参数并没有加倍，linear 的参数加倍，而参数是noise的一个简单变换。<br>
还有人添加constant Gaussian niose到网络参数，而文本的算法添加的noise并不是限制在Gaussion noise distributions。添加noise辅助训练在监督学习等任务中一直都有，但是这些噪音都是不能训练的，而NoisyNet中的噪音是可以梯度下降更新的。</p>
<h3 id="noisynets">NoisyNets</h3>
<p><img src="/2019/03/02/dqn/noisy_linear_layer.png" alt="noisy_linear_layer"><br>
用$\theta$表示noisy net的参数，输入是$x$，输出是$y$，即$y=f_{\theta}(x)$。$\theta$定义为$\theta=\mu+\Sigma\odot\varepsilon$，其中$\zeta=(\mu,\Sigma)$表示可以学习的参数，$\varepsilon$表示服从固定分布的均值为$0$的噪音,$\varepsilon$是random variable。$\odot$表示element-wise乘法。最后的loss函数是关于$\varepsilon$的期望：$\bar{L}(\zeta)=\mathbb{E}\left[L(\theta)\right]$，然后优化相应的$\zeta$，$\varepsilon$不能被优化，因为它是random variable。<br>
一个有$p$个输入单元，$q$个输出单元的fully-connected layer表示如下：<br>
$$y=wx+b \tag{14}$$<br>
其中$w\in \mathbb{R}^{q\times p}$，$x\in \mathbb{R}^{p}$,$b\in \mathbb{R}^{q}$，对应的noisy linear layer定义如下：<br>
$$y=(\mu^w+\sigma^w\odot\varepsilon^w)x + \mu^b+\sigma^b\odot\varepsilon^b \tag{15}$$<br>
就是用$\mu^w+\sigma^w\odot\varepsilon^w$取代$w$，用$\mu^b+\sigma^b\odot\varepsilon^b$取代$b$。其中$\mu^w,\sigma^w\in \mathbb{R}^{q\times p} $，而$\mu^b,\sigma^b\in\mathbb{R}^{q}$是可以学习的参数，而$\varepsilon^w\in \mathbb{R}^{p\times q},\varepsilon^b \in \mathbb{R}^{q}$是random variable。<br>
作者提出了两种添加noise的方式，一种是Independent Gaussian noise，一种是Factorised Gaussion noise。使用Factorised的原因是减少随机变量的计算时间，这些时间对于单线程的任务来说还是很多的。</p>
<h4 id="independent-gaussian-noise">Independent Gaussian noise</h4>
<p>应用到每一个weight和bias的noise都是independent的，对于$\varepsilon^w$的每一项$\varepsilon_{i,j}^w$来说，它们的值都是从一个unit Gaussion distribution中采样得到的；$varepsilon^b$同理。所以对于一个$p$个输入,$q$个输出的noisy linear layer总共有$pq+q$个noise 变量。</p>
<h4 id="factorised-gaussian-noise">Factorised Gaussian noise</h4>
<p>通过对$\varepsilon_{i,j}^w$来说，可以将其分解成$p$个$\varepsilon_i$用于$p$个输入和$q$个$\varepsilon_j$用于$q$个输出，总共有$p+q$个noiss变量。每一个$\varepsilon_{i,j}^w$和$\varepsilon_{j}^b$可以写成：<br>
$$\varepsilon_{i,j}^w = f(\varepsilon_i)f(\varepsilon_j) \tag{16}$$<br>
$$\varepsilon_{j}^b = f(\varepsilon_j)\tag{17}$$<br>
其中$f$是一个实函数，在第一个式子中$f(x) = sng(x)\sqrt{|x|}$，在第二个式子中可以取$f(x)=x$，这里选择了和第一个式子中一致。<br>
因为noisy network的loss函数是$\bar{L}(\zeta)=\mathbb{E}\left[L(\theta)\right]$，是关于noise的一个期望，梯度如下：<br>
$$\nabla\bar{L}(\zeta)=\nabla\mathbb{E}\left[L(\theta)\right]=\mathbb{E}\left[\nabla_{\mu,\Sigma}L(\mu+\Sigma\odot\varepsilon)\right] \tag{18}$$<br>
使用Monte Carlo估计上述梯度，在每一个step采样一个sample进行optimization:<br>
$$\nabla\bar{L}(\zeta)\approx\nabla_{\mu,\Sigma}L(\mu+\Sigma\odot\varepsilon) \tag{19}$$</p>
<h3 id="noisy-dqn-and-dueling">Noisy DQN and dueling</h3>
<p>相对于DQN和dueling DQN来说，noisy DQN and dueling主要做了两方面的改进：</p>
<ol>
<li>不再使用$\varepsilon$-greedy behaviour policy了，而是使用greedy behaviour policy采样优化randomised action-value function。</li>
<li>网络中的fully connected layers全都换成了参数化的noisy network，noisy network的参数在每一次replay之后从noise服从的distribution中进行采样。这里使用的nose是factorised Gaussian noise。</li>
</ol>
<p>在replay 整个batch的过程中，noisy network parameter sample保持不变。因为DQN和Dueling每执行一个action step之后都会执行一次optimization，每次采样action之前都要重新采样noisy network parameters。</p>
<h4 id="loss">Loss</h4>
<p>$Q(s,a,\epsilon;\zeta)$可以看成$\zeta$的一个random variable，NoisyNet-DQN loss如下：<br>
$$\bar{L}(\zeta) = \mathbb{E}\left[\mathbb{E}_{(x,a,r,y)}\sim D\left[r + \gamma \max_{b\in A}Q(y, b, \varepsilon’;\zeta^{-}) - Q(x,a,\varepsilon;\zeta)\right]^2\right]\tag{20}$$<br>
其中外层的期望是$\varepsilon$相对于noisy value function $Q(x,a, \varepsilon;\zeta)$和$\varepsilon’$相对于noisy target value function $Q(x,a, \varepsilon’;\zeta^{-}$。对于buffer中的每一个transition，计算loss的无偏估计，只需要计算target value和true value即可，为了让target value和true之间没有关联，target network和online network采用independent noises。<br>
就double dqn中的action选择来说，采样一个新的independent sample $\varepsilon^{’’}$计算action value，然后使用greedy操作，NoisyNet-Dueling的loss如下：<br>
$$\bar{L}(\zeta) = \mathbb{E}\left[\mathbb{E}_{(x,a,r,y)}\sim D\left[r + \gamma Q(y, b^{*}(y), \varepsilon’;\zeta^{-} - Q(x,a,\varepsilon;\zeta)\right]^2\right]\tag{21}$$<br>
$$b^{*}(y) = \arg \max_{b\in A} Q(y, b(y), \varepsilon^{’’};\zeta)\tag{22}$$</p>
<h3 id="noisy-a3c">Noisy-A3C</h3>
<p>Noisy-A3C相对于A3C有以下的改进：</p>
<ol>
<li>entropy项被去掉了;</li>
<li>fully-connected layer被替换成了noisy network。</li>
</ol>
<p>A3C算法中没有像$\epsilon$-greedy这样进行action exploration，选中的action通常是从current policy中选的，加入entropy是为了鼓励exploration，而不是选择一个deterministic policy。当添加了noisy weights时，对参数进行采样就表示选择不同的current policy，就已经代表了exploration。NoisyNet相当于直接在policy space中进行exploration，而entropy项就可以去掉了。</p>
<h3 id="noisy-networks的初始化">Noisy Networks的初始化</h3>
<p>在unfactorised noisy networks中，每个$\mu_{i,j}$从独立的均匀分布$U\left[-\sqrt{\frac{3}{p}}, \sqrt{\frac{3}{p}}\right]$中采样初始化，其中$p$是对应linear layer的输入个数，$\sigma_{i,j}$设置为一个常数$0.0017$，这是从监督学习的任务中借鉴的。<br>
在factorised noisy netowrks中，每个$\mu_{i,j}$从独立的均匀分布$U\left[-\sqrt{\frac{1}{p}}, \sqrt{\frac{1}{p}}\right]$中进行采样，$\sigma_{i,j}$设置为$\frac{\sigma_0}{p}$，超参数$\sigma_0$设置为$0.5$。</p>
<h3 id="算法-v6">算法</h3>
<p>算法5 NoisyNet-DQN / NoisyNet-Dueling<br>
输入: Env Environment; $\varepsilon$ random variables of the network的集合<br>
输入: DUELING Boolean; &quot;true&quot;代表NoisyNet-Dueling and &quot;false&quot;代表 NoisyNet-DQN<br>
输入: $B$空replay buffer; $\zeta$初始的network parameters; $\zeta^{-}$初始的target network parameters<br>
输入: replay buffer大小$N_B$; batch size $N_T$; target network更新频率$N^{-}$<br>
输出: $Q(\cdot, \varepsilon; \zeta)$ action-value function<br>
<strong>for</strong> episode $e\in  {1,\cdots , M}$ do<br>
$\qquad$初始化state sequence $x_0 \sim Env$<br>
$\qquad$<strong>for</strong> $t \in {1,\cdots }$ do<br>
$\qquad\qquad$设置$x \leftarrow x_0$<br>
$\qquad\qquad$采样 a noisy network  $\xi\sim \varepsilon$<br>
$\qquad\qquad$选择an action $a \leftarrow \arg \max_{b\in A} Q(x, b, \xi; \zeta)$<br>
$\qquad\qquad$采样 next state $y \sim  P (\cdot|x, a)$, 接收 reward $r \leftarrow R(x, a) $以及$x_0 \leftarrow y$<br>
$\qquad\qquad$将transition (x, a, r, y)添加到replay buffer<br>
$\qquad\qquad$<strong>if</strong> $|B| \gt N_B$ then<br>
$\qquad\qquad\qquad$删掉最老的transition<br>
$\qquad\qquad$<strong>end if</strong><br>
$\qquad$采样一个大小为$N_T$的batch, transitions $((x_j, a_j, r_j, y_j) \sim D)_{j=1}^{N_T}$<br>
$\qquad\qquad$采样noisy variables用于online network $\xi \sim\varepsilon$<br>
$\qquad\qquad$采样noisy variables用于target network $\xi’\sim\varepsilon$<br>
$\qquad\qquad\qquad$<strong>if</strong> DUELING then<br>
$\qquad\qquad\qquad$采样noisy variables用于选择action的network $\xi\sim\varepsilon$<br>
$\qquad\qquad$<strong>end if</strong><br>
$\qquad\qquad$<strong>for</strong> $j \in {1,\cdots, N_T}$ do<br>
$\qquad\qquad\qquad$<strong>if</strong> $y_j$ is a terminal state then<br>
$\qquad\qquad\qquad\qquad$$\hat{Q}\leftarrow r_j$<br>
$\qquad\qquad\qquad$<strong>end if</strong><br>
$\qquad\qquad\qquad$<strong>if</strong> DUELING then<br>
$\qquad\qquad\qquad\qquad b^{*}(y_j) = \arg \max_{b\in A} Q(y_j, b, \xi^{’’}; \zeta)$<br>
$\qquad\qquad\qquad\qquad\qquad \hat{Q}\leftarrow r_j + \gamma Q(y_j, b^{*}(y_j), \xi’;\zeta^{-})$<br>
$\qquad\qquad\qquad$<strong>else</strong><br>
$\qquad\qquad\qquad\qquad$$\hat{Q}\leftarrow r_j + \gamma \max_{b\in A} Q(y_j, b, \xi’;\zeta^{-})$<br>
$\qquad\qquad$<strong>end if</strong><br>
$\qquad\qquad\qquad$利用loss $(\hat{Q}-Q(x_j,a_j, \xi;\zeta))^2$的梯度更新$\zeta$<br>
$\qquad\qquad$<strong>end for</strong><br>
$\qquad\qquad$每隔$N^{-}$步更新target network:$ \zeta^{−}\leftarrow \zeta$<br>
$\qquad$<strong>end for</strong><br>
<strong>end for</strong></p>
<p>算法6 NoisyNet-A3C for each actor-learner thread<br>
输入: Environment Env, 全局共享参数$(\zeta_{\pi},\zeta_{V})$ , 全局共享counter $T$和maximal time $T_{max}$<br>
输入: 每个线程的参数 $(\zeta’_{\pi},\zeta’_{V})$, random variables $\varepsilon$的集合, 每个线程的counter $t$和TD-$\gamma$的长度$t_{max}$<br>
输出: policy $\pi(\cdot; \zeta_{\pi}, \varepsilon)$和value $V(\cdot; \zeta_{V}, \varepsilon)$<br>
初始化线程counter $t \leftarrow 1$<br>
<strong>repeat</strong><br>
$\qquad$重置acumulative gradients: $d\zeta_{\pi}\leftarrow 0$和$d\zeta_V \leftarrow 0$<br>
$\qquad$Synchronise每个线程的parameters: $\zeta’_{\pi}\leftarrow \zeta_{\pi}$和$\zeta_V\leftarrow \zeta_V$<br>
$\qquad$counter $\leftarrow 0$<br>
$\qquad$从Env中得到state $x_t$<br>
$\qquad$采样noise: $\xi\sim\varepsilon$<br>
$\qquad r \leftarrow []$<br>
$\qquad a \leftarrow []$<br>
$\qquad x \leftarrow []$和$x[0] \leftarrow x_t$<br>
$\qquad$<strong>repeat</strong><br>
$\qquad\qquad$采样action: $a_t \sim\pi(\cdot|x_t;\zeta’_{\pi};\xi)$<br>
$\qquad\qquad$$a[−1]\leftarrow a_t$<br>
$\qquad\qquad$接收reward $r_t$和next state $x_{t+1}$<br>
$\qquad\qquad$$r[−1]\leftarrow r_t$和$x[−1]\leftarrow x_t+1$<br>
$\qquad\qquad$$t\leftarrow t + 1$和 $T\leftarrow T + 1$<br>
$\qquad\qquad$$counter = counter + 1$<br>
$\qquad\qquad$<strong>until</strong> $x_t\ \ terminal\ \ or\ \ counter == t_{max} + 1$<br>
$\qquad$<strong>if</strong> $x_t$ is a terminal state then<br>
$\qquad\qquad$$Q = 0$<br>
$\qquad$<strong>else</strong><br>
$\qquad\qquad$$Q = V(x_t; \zeta’_{V}, \xi)$<br>
$\qquad$<strong>end if</strong><br>
$\qquad$<strong>for</strong> $i \in {counter − 1, \cdots, 0}$ do<br>
$\qquad\qquad$更新Q: $Q\leftarrow r[i] + \gamma Q$<br>
$\qquad\qquad$累积policy-gradient: $d\zeta_{\pi} \leftarrow d\zeta_{\pi} + \nabla \zeta’_{\pi}log(\pi(a[i]|x[i]; \zeta’_{\pi}, \xi))[Q − V(x[i]; \zeta’_{\pi}V, \xi)]$<br>
$\qquad\qquad$累积 value-gradient: $d\zeta_V \leftarrow ← d\zeta_V+ \nabla \zeta’_{V}[Q − V(x[i]; \zeta’_{V}, \xi)]^2$<br>
$\qquad$<strong>end for</strong><br>
$\qquad$执行$\zeta_{\pi}$的asynchronous update: $\zeta_{\pi}\leftarrow \zeta_{\pi} + \alpha_{\pi}d\zeta_{\pi}$<br>
$\qquad$执行$\zeta_{V}$的asynchronous update: $\zeta_{V}\leftarrow \zeta_{V} − \alpha_VdV\zeta_{V}$<br>
<strong>until</strong> $T \gt T_{max}$</p>
<h2 id="rainbow">Rainbow</h2>
<h2 id="参考文献">参考文献</h2>
<p>1.<a href="https://blog.csdn.net/yangshaokangrushi/article/details/79774031" target="_blank" rel="noopener">https://blog.csdn.net/yangshaokangrushi/article/details/79774031</a><br>
2.<a href="https://link.springer.com/article/10.1007%2FBF00992698" target="_blank" rel="noopener">https://link.springer.com/article/10.1007%2FBF00992698</a><br>
3.<a href="https://www.jianshu.com/p/b92dac7a4225" target="_blank" rel="noopener">https://www.jianshu.com/p/b92dac7a4225</a><br>
4.<a href="https://datascience.stackexchange.com/questions/20535/what-is-experience-replay-and-what-are-its-benefits/20542#20542" target="_blank" rel="noopener">https://datascience.stackexchange.com/questions/20535/what-is-experience-replay-and-what-are-its-benefits/20542#20542</a><br>
5.<a href="https://stats.stackexchange.com/questions/897/online-vs-offline-learning" target="_blank" rel="noopener">https://stats.stackexchange.com/questions/897/online-vs-offline-learning</a><br>
6.<a href="https://www.freecodecamp.org/news/improvements-in-deep-q-learning-dueling-double-dqn-prioritized-experience-replay-and-fixed-58b130cc5682/" target="_blank" rel="noopener">https://www.freecodecamp.org/news/improvements-in-deep-q-learning-dueling-double-dqn-prioritized-experience-replay-and-fixed-58b130cc5682/</a><br>
7.<a href="https://jaromiru.com/2016/11/07/lets-make-a-dqn-double-learning-and-prioritized-experience-replay/" target="_blank" rel="noopener">https://jaromiru.com/2016/11/07/lets-make-a-dqn-double-learning-and-prioritized-experience-replay/</a><br>
8.<a href="https://datascience.stackexchange.com/questions/32873/prioritized-replay-what-does-importance-sampling-really-do" target="_blank" rel="noopener">https://datascience.stackexchange.com/questions/32873/prioritized-replay-what-does-importance-sampling-really-do</a><br>
9.<a href="https://papers.nips.cc/paper/5249-weighted-importance-sampling-for-off-policy-learning-with-linear-function-approximation.pdf" target="_blank" rel="noopener">https://papers.nips.cc/paper/5249-weighted-importance-sampling-for-off-policy-learning-with-linear-function-approximation.pdf</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://mxxhcm.github.io/2019/01/29/Modeling-Others-using-Oneself-in-Multi-Agent-Reinforcement-Learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="马晓鑫爱马荟荟">
      <meta itemprop="description" content="记录硕士三年自己的积累">
      <meta itemprop="image" content="/images/favicon.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="mxxhcm's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/01/29/Modeling-Others-using-Oneself-in-Multi-Agent-Reinforcement-Learning/" class="post-title-link" itemprop="http://mxxhcm.github.io/page/28/index.html">Modeling Others using Oneself in Multi-Agent Reinforcement Learning</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-01-29 13:19:33" itemprop="dateCreated datePublished" datetime="2019-01-29T13:19:33+08:00">2019-01-29</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-12-17 15:43:35" itemprop="dateModified" datetime="2019-12-17T15:43:35+08:00">2019-12-17</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/强化学习/" itemprop="url" rel="index"><span itemprop="name">强化学习</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="摘要">摘要</h2>
<p>我们考虑使用不完全信息的多智能体强化学习问题，每个智能体的目标是最大化自身的效用。奖励函数取决于两个智能体的隐藏状态（或者目标），每一个智能体必须从它观察到的行为中推断出其他玩家的隐藏目标从而完成任务。我们提出了一种新的方法在这些领域中进行学习：自我其他建模（SOM），智能体使用自己的策略来预测其他智能体的动作并实时更新其他智能体隐藏状态的置信度。我们在三个不同的任务上对该方法进行了评估，结果表明智能体无论在合作还是对抗环境中都能使用他们对其他玩家隐藏状态的估计来学习到更好的策略。</p>
<h2 id="引言">引言</h2>
<p>在多智能体系统中推理其他智能体的意图并预测它们的行为是很重要的，这些智能体可能有不同的甚至是竞争的目标集。由于多智能体系统的不稳定性，这仍然是一个非常具有挑战性的问题。<br>
在本文中，我们介绍了一种从其他智能体的行为中估计对应的未知的目标和并利用这些估计的目标选择动作的新方法。我们证明了在本文提到的任务中，在游戏中显式的对其他玩家进行建模比将其他智能体看做环境的一部分会有更好的性能。我们将问题定义为双人随机游戏，也叫双人马尔可夫游戏，其中环境对于智能体是完全可见的，但是没有关于其他智能体目标的明确知识而且没有沟通信道。每个智能体在回合结束时收到的奖励取决于两个智能体的目标，因此是每个智能体最优的策略都必须考虑到所有智能体的目标。<br>
认知科学研究表明，人类维持与他们联系的其他人的模型，这些模型用来捕捉那些人的目标，信仰或偏好。在某些情况下，人类利用自己的心理过程来模拟他人的行为。这使他们能够理解其他人的意图或动机，并能在社交场合采取相应的行动。受这些研究的启发，关键想法是要理解游戏中其他玩家正在做什么，智能体应该问自己“如果我扮演另一个玩家的角色，我的目标是什么？”。我们通过使用一个多层循环神经网络参数化智能体的动作和值函数来实现这个想法，该神经网络将状态和目标作为输入。当智能体玩游戏时，它通过直接使用自己的动作函数优化目标来最大化对方行动的可能性，从而推断出其他智能体的未知目标。</p>
<h2 id="方法">方法</h2>
<p><strong>背景</strong> 两个智能体的马尔可夫游戏由描述所有智能体的可能配置的一组状态集合$S$，两组动作集合$A_1$，$A_2$和两个智能体的观察$O_1$，$O_2$以及转换函数$\Tau$：$S\times A_1 \times A_2 \rightarrow S$作为当前状态和动作的函数给出下一个状态的概率分布。每个智能体$i$通过从随机策略$\pi_{\theta_i}:S\times A_i\rightarrow [0,1]$中采样选择动作。每个智能体都有一个奖励函数，它取决于智能体的状态和动作：$r_i：S\times A_i\rightarrow R$。每个智能体$i$试图最大化自己的总预期收益$R_i = \sum_{t =0}<sup>T\gamma</sup>tr_i^t$，其中$\gamma$是折扣因子，$T$是时间范围。在本文中，我们考虑了合作以及竞争环境。<br>
接下来介绍自我其他模型（SOM），这是一种在一个回合内以实时方式推断其他智能体的目标并使用这些估计来选择动作的新方法。为了决定一个动作并估计一个状态的值，我们使用一个神经网络$f$将它自己的目标$z_{self}$，另一个玩家的估计目标$\hat{z}<em>{self}$，并且他自己的角度的观察状态$s</em>{self}$作为输入，输出动作$\pi$的一个概率分布和值估计$V$，即对于每个玩游戏的智能体，有：<br>
$$\begin{bmatrix}\pi<sup>i\V</sup>i\end{bmatrix}=f<sup>i(s_{self}</sup>i,z_{self}<sup>i,\hat{z}_{other}</sup>i;\theta^i)$$<br>
其中$\theta_i$是智能体$i$的神经网络$f$的参数，包括一个softmax层输出策略，一个线性层输出值函数，所有非输出层是共享的。动作是从策略$\pi$中采样得到的。观察状态$s_{self}<sup>i$包含$f</sup>i$智能体的位置，以及其他智能体的位置。每个智能体都有两个网络（为了简洁，省略了智能体上标$i$），一个计算它自己的动作和值函数，一个计算其他智能体的估计值，如下：<br>
\begin{equation}<br>
f_{self}(s_{self},z_{self},\hat{z}<em>{other};\theta</em>{self})<br>
\end{equation}<br>
\begin{equation}<br>
f_{other}(s_{other},\hat{z}<em>{other},z</em>{self};\theta_{self})<br>
\end{equation}<br>
这两个网络使用的方式不同：$f_{self}$用于计算智能体自己的行为和价值，并以前馈方式运行。给出其他智能体观察到的动作，智能体使用$f_{other}$通过优化$\hat{z}<em>{other}$推断其他智能体的目标。<br>
我们建议每个智能体使用自己的策略模拟其他玩家的行为，这样$f</em>{other}$的参数与$f_{self}$的参数是相同的。但请注意，两个网络的输入$z_{self}$和$\hat{z}<em>{other}$的相对位置不同。另外，由于环境是完全可观测的，两个智能体的观察状态的不同仅通过地图上智能体的身份体现出来（即，每个智能体将能够区分其自己的位置和另一个智能体的位置）。因此，在acting模式下，$f</em>{self}$网络将$s_{self}$作为输入；在推理模式下，$f_{other}$网络将$s_{other}$作为输入。在游戏的每一步，智能体需要推理$\hat{z}<em>{other}$将其作为(1)的输入并选择其动作。为了实现这个目的，在每一步中，智能体观察另一个智能体采取的行动，并且在下一步中，智能体使用先前观察到的另一个智能体的动作作为监督信号，使用式子(2)反向传播并优化其$\hat{z}</em>{other}$，如图1所示。<br>
推理过程优化器中采取的步数是一个可根据游戏的不同而变化的超参数。因此，在游戏的每一步中其他智能体的目标估计$\hat{z}<em>{other}$会被更新多次。参数$\theta</em>{self}$在每个回合结束时使用和带有智能体获得的奖励信号的Asynchronous Advantage Actor-Critic（A3C）进行更新。<br>
算法1给出了一个回合内训练SOM智能体的伪代码。这里考虑的所有任务的目标都是离散的，智能体的目标$\hat{z}<em>{self}$被<br>
表示独热向量，维度是智能体目标所有可能的情况数。另一个玩家的目标嵌入$\hat{z}</em>{other}$有相同的维度。为了估计经过离散而不可微的变量$\hat{z}<em>{other}$的梯度，我们用Gumbel-Softmax分布上的一个可微样本$\hat{z}</em>{other}^G$代替它。这种重新参数化技巧被证明可以有效地产生低方差偏置的梯度。使用该方法在每一步优化过$\hat{z}<em>{other}$之后，$\hat{z}</em>{other}$通常偏离独热向量。在下一步中，$f_{self}$将对应于先前更新的$z_{other}$ argmax的一个独热向量量$\hat{z}<em>{other}^OH$作为输入。<br>
智能体的策略由长短期记忆（LSTM）单元参数化，以及两个全连接的线性层和指数线性单元（ELU）激活函数。神经网络的权重用半正交矩阵初始化。<br>
由于$f</em>{other}$的循环性，当推理步数$\gt 1$时必须特别小心。在这种情况下，在游戏的每一步中，我们在推理模式中的第一次前向传播之前保存$f_{other}$的循环状态，并且在每个推理步骤将循环状态初始化为此值。这个过程可以确保在动作和推理模式下$f_{other}$可以展开相同数量的步骤。</p>
<h2 id="相关工作">相关工作</h2>
<p>不完全信息的游戏中对手建模一直在被广泛研究。但是，大多数以前的方法都侧重于研究特定领域内的概率先验或参数化策略的模型。相比之下，本文的工作为对手建模提出了一个更通用的框架。给定比赛历史，Davidson使用MLP预测对手的动作，但是智能体无法实时适应对手的行为。Lockett等人设计了一种神经网络结构，通过在给定的一组主要对手上学习权重的值来识别对手类型。然而，游戏并没有在强化学习框架内展开。<br>
大量多智能体深度强化学习的研究中侧重于部分可见的，完全合作和紧急通信等环境。本文不允许智能体之间进行任何沟通，因此玩家必须利用他们观察到的行为间接推理他们对手的意图。作为对比，Leibo等考虑半合作多智能体环境，智能体根据任务类型和奖励结构制定合作和竞争策略。类似地，Lowe等人提出了一种集中AC框架，用于在具有混合策略的环境中进行高效的训练。 Lerer和Peysakhovich通过将针锋相对的著名游戏理论策略推广到多智能体马尔可夫游戏，设计了能够在复杂社会困境中保持合作的强化学习智能体。最近认知科学方面的工作试图通过使用分层的社会智能体模型来理解人类的决策，它能推断出其他人类智能体的意图，从而决定是否采取合作或竞争策略。然而，这些论文都没有设计出能够显式模拟环境中其他人工智能体或者估计他们意图的算法来改善智能体的决策。<br>
逆强化学习领域也与本文考虑的问题有关。逆强化学习的目的是通过观察智能体的行为来推断智能体的奖励函数。相反，我们的方法使用观察到其他玩家的行为以在线方式直接推断他们的目标，然后在环境的acting模式中由智能体使用。这避免为了估计奖励函数收集其他智能体状态-动作对离线样本的需要，然后使用它来学习最大化该效用的单独策略。最近Hadfield-Menell等的论文也关注推理他人意图的问题，但他们关注的是人机交互和价值调整。在类似目标的推动下，Chandrasekaran等人考虑建立人工智能理论的问题，以改善人工智能交互和人工智能系统的可解释性。为了这个目标，他们展示了可以使用少量示例训练人们预测视觉问答模型的响应。<br>
Foerster等人和He等人的工作与我们的工作最接近。Foerster等人设计强化学习智能体在更新自己的策略时同时考虑到环境中其他智能体的学习。这使得智能体能够发现自私而又协作的策略，例如在迭代囚徒困境中的针锋相对策略。虽然我们的工作没有明确地试图塑造其他智能体的学习，但它的优点是智能体可以在一个回合中更新他们的信念并以在线方式更新策略以获得更多奖励。我们的设置也有所不同，它认为每个智能体都有一些其他玩家所需的隐藏信息，以便最大化其回报。<br>
我们的工作非常符合He等人的工作，作者构建了一个用于在强化学习环境中构建其他智能体的一般框架。He等人提出了一个模型，通过将对手的观察使用DQN进行编码，共同学习一个策略和对手的行为对手。他们的混合专家架构能够在两个纯对抗性任务中发现不同对手的策略模式。我们的工作与He等人的工作之间的一个区别在于，我们的目标不是推断其他智能体的策略，而是专注于显式估计他们在环境中的目标。此外，在这项工作中，智能体不是使用其他智能体动作的人工设计特征，而是根据自己的模型端到端的学习其他智能体模型。另一个区别是，在这项工作中，智能体使用优化推断其他智能体的隐藏状态，而不是通过前馈网络推断其他智能体的隐藏状态。在下面的实验中，我们表明SOM优于He等人的方法。</p>
<h2 id="实验">实验</h2>
<p>在本节中，我们在三个任务上评估SOM模型：</p>
<ul>
<li>硬币游戏，这是一个完全合作的任务，智能体的角色是对称的。</li>
<li>配方游戏，它是对抗的，但具有对称角色。</li>
<li>门禁游戏，它是完全合作的，但是两个玩家拥有不对称的角色。</li>
</ul>
<p>我们将SOM与其他三个baselines以及一个可以访问其他智能体目标的ground truth的模型进行比较。所有任务都是在Mazebase gridworld环境中创建的。</p>
<h3 id="baselines">Baselines</h3>
<p>TRUE-OTHER-GOAL（TOG）：我们提供了一个给出的模型性能上限的策略网络，该网络将其他智能体的真正目标$z_{other}$，以及状态特征$s_{self}$和自己的目标$z_{self}$作为输入。因为这个模型可以直接访问其他智能体真正的目标，因此不需要单独的网络来模拟其他智能体的行为。 TOG的结构与SOM的一个策略网络$f_{self}$相同。 NO-OTHER-MODEL（NOM）：我们使用的第一个baseline仅使用观察状态$s_{self}$和自身目标$z_{self}$作为输入。NOM与SOM的一个策略网络$f_{self}$有相同的架构。该baseline没有对其他智能体的显式建模或估计它们的目标。<br>
集成-策略-预测器（IPP）：从NOM的体系结构和输入开始，我们构建了一个更强的baseline IPP，它有一个额外的最终线性层输出另一个智能体下一个动作的概率分布。除了用于训练该网络策略的A3C损失函数，我们还添加交叉熵损失项训练其他智能体的行为的预测。<br>
分离-策略-预测器（SPP）：He等人提出了一个基于DQN的对手建模框架。在他们的方法中，给定对手特有的人工提取的状态信息，训练一个神经网络预测对手的动作。该网络的中间隐藏表示用作Q网络的输入。<br>
我们修改了He等人的模型应用到本文的场景中。特别的，我们使用A3C而不是DQN，我们不使用特定领域的特征表示对手的隐藏状态。<br>
最后产生的SPP模型由两个独立的网络组成，一个策略网络用于决定智能体的动作，一个对手网络用于预测其他智能体的动作。对手网络将世界状态$s$和自己的目标$z_{self}$作为输入，并输出其他智能体在下一步采取动作的概率分布，以及其隐藏状态（由网络的循环给出）。与IPP一样，我们使用其他智能体的真实动作训练对手策略预测器的交叉熵损失。在每一步中，该网络输出的隐藏状态以及智能体观察状态和智能体自身的目标被作为智能体的策略网络的输入。策略网络和对手策略预测器都是与SOM结构相同的LSTM网络。<br>
与SOM作对比，SPP没有显式推断出其他智能体的目标。相反，它通过预测智能体在每个时间步的动作来隐式的构建对手模型。在SOM中，一个参考的目标作为策略网络的附加输入。而在SPP，类似的参考目标是从对手策略预测器得到的隐藏表示，把它作为策略网络的附加输入。<br>
<strong>训练细节</strong>。在我们的所有实验中，我们使用系数为$0.01$的熵，价值损失系数为$0.5$，折扣系数为$0.99$的A3C训练智能体的策略。使用Adam优化智能体商策略的参数，其中$\beta_1= 0.9,\beta_2= 0.999,\epsilonn =1\times 10^{-8}$，权重衰减为$0$。学习率为$0.1$的SGD用于推断另一个智能体的目标，$\hat{z}_{other}$。<br>
硬币和食谱游戏中策略网络的隐藏层维度为$64$，门游戏中为$128$。所有游戏和模型的学习率都是$1\times 10^{-4}$。<br>
观测状态$s$用一些独热向量表示，包括环境中所有物体的位置，以及智能体和另一个智能体的位置。这个输入状态的维度是$1\times n$特征，其中Coin，Recipe和Door游戏的特征数分别为$384$,$192$和$900$。对于每个实验，我们使用5个不同的随机种子训练模型。除非特殊说明，否则论文中展示的所有游戏结果都是每步进行的10次优化更新的结果。</p>
<!--
### 硬币游戏。
首先，我们在一个完全合作的任务上评估模型，在这个任务中，当智能体使用他们两个的目标而不仅仅是他们自己的目标时，他们可以获得更多的奖励。因此，估计其他玩家的目标并在采取行动时使用该信息符合每个智能体人的最佳利益。如图4的左图所示，游戏在8×8网格上进行，该网格包含12个3种不同颜色的硬币（每种颜色4个硬币）。在每集开始时，智能体被随机分配三种颜色中的一种。动作空间包括：上，下，左，右或通过。一旦智能体人踩到硬币，那个硬币就会从网格中消失。游戏在20个步骤后结束（即每个智能体需要10个步骤）。两名特工在比赛结束时收到的奖励由下面的公式给出：
2），其他n其他Cself是自我目标颜色的硬币数量，由其他智能体人收集，而n self Cneither是与自己收集的智能体人目标相对应的硬币数量。对于图4中的示例，智能体1具有Cself =橙色和Cother =青色，而智能体2的Cself是青色而Cother是橙色。对于两种药剂，两者都是红色的。
收集不符合任何智能体人目标的硬币的惩罚的作用是避免收敛到暴力政策，在这种政策中，智能体人可以通过收集其附近的所有硬币而获得不可忽视的奖励金额，而不是关于他们的颜色。为了最大化其回报，每个智能体人需要收集自己的硬币或其合作者的颜色，而不是剩余颜色的硬币。因此，当两个智能体人能够在游戏中尽可能早地高精度地推断其合作者的目标时.
-->
<h2 id="讨论">讨论</h2>
<p>在本文中，我们介绍了一种新方法，用于从其他智能体的行为中推断他们的隐藏状态，并使用这些估计来选择动作。我们证明了智能体能够在合作和竞争环境中估计其他参与者的隐藏目标，这使他们能够收敛到更好的政策并获得更高的回报。在本文提出的任务中，对其他智能体的显式建模比仅仅考虑其他代理成为环境的一部分更好的性能。 	SOM的一个限制是它比其他baseline需要更长的训练时间，因为我们在每一步都进行了反向传播。但是，它的online更新方式对于适应环境中其他智能体的动作变化至关重要。SOM的一些主要优点是简单性和灵活性，它不需要任何额外参数来模拟环境中的其他代理，可以使用任何强化学习算法进行训练，并且可以轻松地与任何策略参数化或网络结构集成。SOM可以适应具有两个以上智能体的环境，因为智能体可以使用自己的策略来模拟任意数量的智能体的动作并推断其目标。而且，它可以很容易地推广到许多不同的环境和任务。<br>
我们计划通过评估更复杂环境中的模型来扩展这项工作，包括两个以上的参与者，混合策略，更多样化的智能体类型（例如具有不同动作空间的智能体，奖励函数，角色或策略），以及假设其他玩家和自己一样的模型偏差。<br>
未来研究的其他重要途径是设计能够适应环境中其他智能体非平稳策略的模型，处理具有分层目标的任务，并在测试时遇到新智能体时表现良好。<br>
最后，许多研究领域可以从拥有其他智能体的模型中受益，这些智能体能够推理其他智能体的意图并预测他们的动作。这些模型可能对人机或师生互动，以及价值对齐问题有恒大帮助。此外，这些方法可用于多智能体任务中基于模型的强化学习，因为前向模型的准确性很大程度上取决于预测其他智能体动作的能力。</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://mxxhcm.github.io/2019/01/26/Policy-Gradient-With-Value-Function-Approximation-For-Collective-Multiagent-Planning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="马晓鑫爱马荟荟">
      <meta itemprop="description" content="记录硕士三年自己的积累">
      <meta itemprop="image" content="/images/favicon.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="mxxhcm's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/01/26/Policy-Gradient-With-Value-Function-Approximation-For-Collective-Multiagent-Planning/" class="post-title-link" itemprop="http://mxxhcm.github.io/page/28/index.html">Policy Gradient With Value Function Approximation For Collective Multiagent Planning</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-01-26 19:33:50" itemprop="dateCreated datePublished" datetime="2019-01-26T19:33:50+08:00">2019-01-26</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-05-07 00:22:27" itemprop="dateModified" datetime="2019-05-07T00:22:27+08:00">2019-05-07</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/强化学习/" itemprop="url" rel="index"><span itemprop="name">强化学习</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="摘要">摘要</h2>
<p>分布式的部分可观测马尔科夫决策过程(Dec POMDP)为解决多智能体系统中的序列决策问题提供了一个框架。考虑到POMDP的计算复杂度，最近的研究主要集中在Dec-POMDP中一些易于处理但是比较实用的子问题。本文解决的就是其中的一个子问题叫做CDec-POMDP其中一系列智能体的共同行为影响了它们公共的reward和环境变化。本文的主要贡献是提出了一个actor-critic(AC)强化学习算法优化CDec-POMDP问题的policy。普通的AC算法对于大型问题收敛的很慢，为了解决这个问题，本文展示了如何将智能体的估计动作值函数进行分解从而产生有效的更新以及推导出一个基于局部奖励信号的新的critic训练方式。通过在一个合成的benchmark以及真实的出租车车队优化问题上和其他方法进行对比，结果表明本文的AC方法提供了比之前最好的方法还要高质量的方法。</p>
<h2 id="引言">引言</h2>
<p>近些年来，分布式的部分可观测马尔科夫决策过程已经发展成了解决多智能体协作的序列决策问题的一个很有前景的(promising)方法。Dec-POMDP对智能体基于环境和其他智能体的不同部分观测最大化一个全局的目标进行建模。Dec-POMDP的具体应用包括协调行星探测，多机器人协调控制以及无线网络的吞吐量优化。然而，解决分布式的部分马尔科夫决策过程是相当困难的，即使对于只有$2$个智能体的问题呢是NP难的。<br>
为了增大规模和提高真实问题中的应用，过去的研究已经探索了智能体之间严格的交互，如状态转换和观测独立，事件驱动的的交互以及智能体之间的弱耦合性。最近，一系列工作开始关注于智能体的身份不影响它们之间的交互上，环境的变化主要受到智能体的共同影响，和著名的阻塞游戏很像。一些城市交通中的问题如出租车调度可以用这样的协同规划模型进行建模。<br>
在本文中，作者着重于集中的Dec-POMDP框架将一类不确定情况下的集中多智能体序列决策问题形式化。Nguyen等人提出了一个采样方法优化CDec-POMDP模型中的policy。之前方法的一个主要缺点是policy是用表格形式展现的，随着智能体的observation spaces改变时，表格形式的policy不能很好的进行扩展。受到最近一些强化学习工作的启发，本文的贡献是一个AC框架的强化学习算法用来优化CDec-POMDP的policy。Policy用函数如神经网络来表示可以避免表格形式的policy的扩展性问题。我们推导出了策略梯度并且基于CDec-POMDP中智能体的交互提出了一个估计的因子动作值函数。普通的AC算法因为学习全局reward的原因，在解决大型多智能体系统问题时收敛的很慢。为了解决这个问题，本文提出了一种新的方式去训练critic，高效利用智能体的局部值函数的估计动作值函数。<br>
我们在一个合成的多机器人导航领域和现实世界中一个亚洲城市的出租车调度问题上测试了本文的方法，结果展示了本文的方法可以扩展到大型多智能体系统上。根据经验，我们因式AC方法比以前最好的方法给出的解决方案都要好。因式AC方法收敛的也比普通的AC方法快很多，验证了我们提出的critic训练方法的有效性。<br>
<strong>相关工作</strong> 我们的工作基于具有近似值函数的策略梯度框架。然而，根据以往的经验显示，直接应用原始的策略梯度到多智能体任务中，尤其是CDec-POMDP模型中会产生较高方差。在本文中，我们展示了一个和CDec-POMDP兼容的估计值函数，它能产生高效且低方差的策略梯度更新。Peshkin很早之前就研究过了应用于分布式policy的强化学习，Guestrin还提出使用REINFORCE从协调图中训练一个因子值函数的softmax策略。然而，这些以前的工作中，策略梯度都是从全局的经验回报而不是分解后的critic中估计的。我们在第四章中展示了一个分解后ciritc和基于训练这个critic得到的一个单个值函数对于高效的采样学习是很重要的。我们的实验结果表明了我们提出的critic训练方式比用全局经验回报训练收敛的还要快。</p>
<h2 id="集中分布式pomdp模型">集中分布式POMDP模型</h2>
<p>我们首先介绍一下Nguyen提出的CDec-POMDP模型。一个对应于这个模型的$T$步的动态贝叶斯网络如图所示。它由以下几个部分组成：</p>
<ul>
<li>一个有限的计划范围$H$</li>
<li>智能的数量$M$，一个智能体m可能处在state space $S$中的任意一个状态，联合state space是$\times_{m=1}^MS$，我们用$i\in S$表示一个state。</li>
<li>每一个智能体m都有一个action spaceA，我们用$j\in A$表示一个action。</li>
<li>用$(s_{1:H},a_{1:H})<sup>m=(s_1</sup>m,a_1<sup>m,\cdots,s_H</sup>m,a_H<sup>m)$表示一个智能体m完整的state-action轨迹。用随机变量$s_t</sup>m,a_t^m$表示智能体$m$在$t$时刻的state和action。不同的指示函数$I_t(\cdot)$如表$1$所示。给定每一个智能体$m\in M$的轨迹，定义以下的计数方式：<br>
$$n_t(i,j,i’) = \sum_{m=1}^M I_t^m(i,j,i’),\forall i,i’\in S,j\in A.$$<br>
如表$1$所示，计数器$n_t(i,j,i’)$表示在$t$时刻处于state $i$，采取action $j$，转换到state $i’$的智能体数量。其他计数器$n_t(i)$和$n_t(i,j)$的定义类似。使用这些计数器，我们可以定义$t$时刻的计数表$\bf{n}<em>{s_t}$和$\bf{n}</em>{s_ta_t}$如表$1$所示。</li>
<li>我们假设一个普遍的部分观测环境，其中智能体基于其他智能体的总体影响可以有不同的ovservation。一个智能体观测到它的局部state $s_t<sup>m$。此外在$t$时刻基于它的局部状态$s_t</sup>m$和计数表$\bf{n}_{s_t}$观测到$o_t^m$。例如，一个智能体m在$t$时刻处于state $i$，可以观测到其他也处在state $i(=n_t(i))$的智能体或者其他处在state $i$临近状态$j$的智能体，即$n_t(j),\forall j\in Nb(i)$。</li>
<li>状态转换函数是$\Phi_t(s_{t+1}<sup>m=i’|s_t</sup>m=i,a_t^m=j,\bf{N}<em>{s_t})$。所有智能体的状态转换函数是一样的，注意它会受到$\bf{n}</em>{s_t}$的影响，而$\bf{n}_{s_t}$依赖于智能体的共同行为。</li>
<li>每一个智能体m有一个不平稳的policy $\pi_t<sup>m(j|i,o_t</sup>m(i,\bf{n}<em>{s_t}))$，表示在$t$时刻给定智能体m的observation $(i,o_t^m(i,\bf{n}</em>{s_t})$之后，智能体采取action $j$的概率。我们用$\pi^m=(\pi_1,\cdots,\pi_H)$表示智能体m水平范围的policy。</li>
<li>一个智能体接收到的reward $r_t^m=r_t(i,j,\bf{n}<em>{s_t}$取决于它的局部state和action，以及计数表$\bf{n}</em>{s_t}$。</li>
<li>初始的state分布，$b_o=(P(i)\forall i \in S)$，对于所有的智能体都是相同的。</li>
</ul>
<p>我们在这里展示了最简单的版本，所有的智能体的类型都相同，并且有相似的state transition，observation和reward模型。模型也可以处理多种类型的智能体，不同类型的智能体有不同的变化。我们还可以引入一个不受智能体action影响的external state，如交通领域的出租车需求。我们的结果也可以扩展到解决类似的问题。<br>
像CDec-POMDP之类的模型对于解决智能体数量很大或者智能体的身份不影响reward或者transition function之类的问题是很有用的。其中一个应用是出租车车队优化问题，这个问题是计算出出租车调度的policy使得车队的利润最大化。一个出租车的决策过程如下。在时刻$t$时，每个出租车观测到它当前的城市空间$z$，不同的空间构成了state space $S$，以及当前空间和它的相邻空间的其他出租车的计数和当前局部请求的一个估计。这构成了出租车基于计数的observation $o(\cdot)$。基于这个observation，出租车必须决定待在当前空间$z$寻找乘客还是移动到下一个空间。这些决策选择取决于不同的因子，如请求比率和当前空间其他出租车的计数。类似的，环境是随机的，在不同时间出租车请求是变化的。使用出租车车队的的GPS记录可以得到这些历史的请求数据。<br>
<strong>基于计数的统计数据用于规划</strong> CDec-POMDP模型的一个关键属性是模型的变换取决于智能体的集中交互而不是智能体的身份。在出租车车队优化问题中，智能体数量可以相当大（大约有$8000$个智能体在现实世界的实验中）。给出这么大数量的智能体个数，为每一个智能体计算出独一无二的policy是不可能的。因此，和之前的工作类似，我们的目标是对所有智能体计算出一个相同的policy $\pi$。因为policy $\pi$取决于计数，它代表了一种富有表现力的policy。<br>
对于一个固定的数量M来说，用${(s_{1:T},a_{1:T})^m\forall m}$表示从图$1$的DBN网络中采样得到的不同智能体的state-action轨迹。用$\mathbf{n}<em>{1:T}={(\mathbf{n}</em>{s_t},\mathbf{n}<em>{s_ta_t},\mathbf{n}</em>{s_ta_ts_{t+1}})\forall t=1:T}$表示每一个时间步$t$的结果计数表的组合向量。Nguyen等人展示了计数器$\mathbf{n}$中拥有足够的统计数据用来规划。也就是说，一个policy $\pi$在水平范围H内的联合值函数可以通过计数器的期望进行计算：<br>
$$V(\pi) = \sum_{m=1}<sup>M\sum_{T=1}</sup>H E[r_T^m] = \sum_{\mathbf{n}\in \Omega_{1:H}}P(\mathbf{n};\pi) \left[\sum_{T=1}^H\sum_{i\in S,j\in A} n_T(i,j)r_T(i,j,\mathbf{n}<em>T)\right]$$<br>
集合$\Omega</em>{1:H}$是所有允许的一致计数表的集合，如下所示：<br>
$$\sum_{i\in S}n_T(i) = M \forall T;$$<br>
$$\sum_{j\in A}n_T(i,j) = n_T(i) = \forall j \forall T;$$<br>
$$\sum_{i’\in S}n_T(i,j,i’) = n_T(i,j)\forall i\in S,\forall j \in A, \forall T;$$<br>
$P(\mathbf{n},\pi)$是计数器的分布。这个结果的一个关键好处是我们可以直接从分布$P(\mathbf{n})$中对计数器$\mathbf{n}$采样而不是对单个不同智能体的轨迹$(s_{1:H},a_{1:H})进行采样来$评估policy $\pi$，这显著节省了计算开销。我们的目标是计算最优的policy $\pi$来最大化$V(\pi)$。我们假设一个集中式学习，分布式执行的强化学习设置。我们假设有一个模拟器可以从$P(\mathbf{n};\pi)$中提供计数器样本。</p>
<h2 id="cdec-pomdp的策略梯度">CDec-POMDP的策略梯度</h2>
<p>之前的工作提出了一个基于采样的EM算法来优化policy $\pi$。这个policy被表示成计数器$\mathbf{n}$空间中的一个线性分段表policy，其中每一个线性片段指定了下一个action的分布。然而，这种表格形式的表示限制了它的表达能力，因为片段的数量是固定的先验，并且每个范围都必须手动定义，这可能会对性能产生不利影响。此外，当observation o是多维的时候，即，一个智能体观测到它位置相邻区域的计数器时，需要指数多个片段。为了解决这个问题，我们的目标是优化函数形式（如神经网络）的policy。<br>
我们首先扩展策略梯度理论到CDec-POMDP上，用$\theta$表示policy参数的向量。我们接下来展示如何计算$\Delta_\theta V(\pi)$。用$\mathbf{s}_t,\mathbf{a}<em>t$表示$t$时刻所有智能体的联合state和联合action。给定一个policy $\pi$，值函数表示形式如下：<br>
$$V_t(\pi)=\sum</em>{\mathbf{s}_t,\mathbf{a}<em>t}P<sup>{\pi}(\mathbf{s}_t,\mathbf{a}_t|b_o,\pi)Q_t</sup>{\pi}(\mathbf{s}<em>t,\mathbf{a}<em>T)$$<br>
其中$P<sup>{\pi}(\mathbf{s}_t,\mathbf{a}_t|b_o)=\sum_{\mathbf{s}_{1:t-1},\mathbf{a}_{1:t-1}}P</sup>{\pi}(\mathbf{s}</em>{1:t},\mathbf{a}</em>{1:t}|b_o)$是policy $\pi$下联合state $\mathbf{s}<em>t$，和联合action $\mathbf{a}<em>t$的分布。值函数$Q_t^{\pi}(\mathbf{s}<em>t,\mathbf{a}<em>t)$的计算过程如下：<br>
$$Q_t^{\pi}(\mathbf{s}<em>t,\mathbf{a}<em>t) = r_t(\mathbf{s}<em>t,\mathbf{a}<em>t)+\sum</em>{\mathbf{s}</em>{t+1},\mathbf{a}</em>{t+1})}P<sup>{\pi}(\mathbf{s}_{t+1},\mathbf{a}_{t+1}|\mathbf{s}_t,\mathbf{a}_t))Q_{t+1}</sup>{\pi}(\mathbf{s}</em>{t+1},\mathbf{a}</em>{t+1})$$<br>
接下来介绍以下CDec-POMDP的策略梯度理论：<br>
<strong>定理1.</strong> 对于任何CDec-POMDP，策略梯度计算公式如下：<br>
$$\Delta</em>{\theta}V_1(\pi)=\sum</em>{t=1}<sup>HE_{\mathbf{s}_t,\mathbf{a}_t)|b_o,\pi}\left[Q_t</sup>{\pi}(\mathbf{s}<em>t,\mathbf{a}<em>t)\sum</em>{i\in S,j\in A}n_t(i,j)\Delta</em>{\theta}log\pi</em>{t}(j|i,o(i,\mathbf{n}</em>{s_t}))\right]$$<br>
这个定理的证明和其他后续结果在附录中。<br>
注意由于许多原因利用上述结果计算策略梯度是不切实际的。联合state-action $\mathbf{a}_t,\mathbf{s}_t$空间是组合的。考虑到智能体的个数可能有很多个，对每一个智能体的轨迹进行采样是计算上不可行的。为了补救，我们接下来会展示类似policy评估直接对计数器$\mathbf{n}~P(\mathbf{n};\pi)$进行采样计算梯度。类似的，也可以使用经验回报作为动作值函数$Q_t<sup>{\pi}(\mathbf{s}_t,\mathbf{a}_t)$的一个近似估计。这是标准的REINFORCE算法在CDec-POMDP上的应用。众所周知，REINFORCE可能比其他使用学习的动作值函数的方法学习的慢。因此，我们提出了一个$Q_t</sup>{\pi}$的近似函数，展示了直接采样计数器$\mathbf{n}$来计算策略梯度。</p>
<h3 id="使用估计动作值函数的策略梯度">使用估计动作值函数的策略梯度</h3>
<p>估计动作值函数$Q_t^{\pi}(\mathbf{s}<em>t,\mathbf{a}<em>t)$有几种不同的方式。我们考虑下列特征形式的近似值函数$f_w$：<br>
$$Q_t^{\pi}(\mathbf{s}<em>t,\mathbf{a}<em>t)\approx f_w(\mathbf{s}<em>t,\mathbf{a}<em>t)=\sum</em>{m=1}<sup>Mf_w</sup>m(s_t<sup>m,o(s_t</sup>m,\mathbf{n</em>{s_t}}),s_t^m)$$<br>
每一个智能体m都定义了一个$f_w<sup>m$，它的输入是智能体的局部state，action和observation。注意不同的$f_w</sup>m$是相关的，因为它们依赖于公共的计数器表$\mathbf{n}</em>{s_t}$。这样的一种分解方式是很有用的，因为它产生了有效的策略梯度计算方式。此外，CDec-POMDP中一类很重要的这种形式的估计值函数是兼容值函数最后会产生一个无偏的策略梯度。<br>
<strong>命题1</strong> CDec-POMDP中的兼容值函数可以分解成：<br>
$$f_w(\mathbf{s}<em>t\mathbf{a}<em>t) = \sum_mf_w<sup>m(s_t</sup>m,o(s_t<sup>m,\mathbf{n}_{s_t}),a</sup>m)$$<br>
我们可以直接用估计值函数$f_w$取代$Q^{\pi}(\cdot)$。经验上来说，我们发现使用这个估计的方差很大。我们利用$f_w$的结构进一步分解策略梯度会有更好的效果。<br>
<strong>定理2</strong> 对于任何具有如下的分解的值函数：<br>
$$f_w(\mathbf{s}<em>t\mathbf{a}<em>t) = \sum_mf_w<sup>m(s_t</sup>m,o(s_t<sup>m,\mathbf{n}_{s_t}),a</sup>m)$$<br>
策略梯度可以写成：<br>
$$\Delta</em>{\theta}V_1(\pi)=\sum</em>{t=1}<sup>HE_{\mathbf{s}_t,\mathbf{a}_t)|b_o,\pi}\left[\sum_m\Delta_{\theta}log\pi(a_t</sup>m|s_t<sup>m,o(s_t</sup>m,\mathbf{n}</em>{s_t}))f_w<sup>m(s_t</sup>m,o(s_t<sup>m,\mathbf{n}_{s_t}),a_t</sup>m)\right]$$<br>
上述结果展示了如果估计值函数被分解了，那么得到的策略梯度也是分解的。上述结果也可以应用到多种类型的智能体上，只要我们假设不同的智能体有不同的函数$f_t^m$。最简单的情况下，所有的智能体都是相同类型的，每一个智能体都有相同的函数$f_w$，推断出下式：<br>
$$f_w(\mathbf{s}<em>t,\mathbf{a}<em>t) = \sum</em>{i,j}n_t(i,j)f_w(i,j,o(i,\mathbf{n}</em>{s_t}))$$<br>
使用上式，我们可以将策略梯度简化成：<br>
$$\Delta</em>{\theta}V_1(\pi) = \sum_tE</em>{\mathbf{s}<em>t,\mathbf{a}<em>t}\left[\sum</em>{i,j}n_t(i,j)\Delta</em>{\theta}log\pi (j|i,o(i,\mathbf{n}</em>{s_t}))f_w(i,j,o(i,\mathbf{n}</em>{s_t}))\right]$$</p>
<h3 id="基于计数器的策略梯度计算">基于计数器的策略梯度计算</h3>
<p>注意在上式中，期望仍然和联合state，action，$(\mathbf{s}<em>t,\mathbf{a}<em>t)$相关，当智能体的个数很大时效率很低。为了解决这个问题是<br>
<strong>定理3</strong> 对于任何拥有形式$f_w(\mathbf{s}<em>t,\mathbf{a}<em>t) = \sum</em>{i,j}n_t(i,j)f_w(i,j,o(i,\mathbf{n}</em>{s_t}))$的值函数，策略梯度都可以用下式计算：<br>
\begin{equation}<br>
E</em>{\mathbf{n}</em>{1:H}\in \Omega_{1:H}} \left[\sum_{t=1}^H\sum_{i\in S,j\in A}n_t(i,j) \Delta_{\theta}log\pi (j|i,o(i,\mathbf{n}_t)) f_w(i,j,o(i,\mathbf{n}<em>t))\right]<br>
\end{equation}<br>
上述结果展示了策略梯度可以类似于计算policy的值函数一样通过从底层分布$P(\cdot)$中采样计数表向量$\mathbf{n}</em>{1:H}$来计算策略梯度，在智能体数量很大的情况下也是可行的。</p>
<h2 id="训练动作值函数">训练动作值函数</h2>
<p>在我们的方法中，在计数器样本$\mathbf{n}<em>{1:H}$生成用来计算策略梯度后，我们还需要调整critic $f_w$的参数。注意对于每一个动作值函数$f_w(\mathbf{s}<em>t,\mathbf{a}<em>t)$只取决于联合state，action $(\mathbf{s}<em>t,\mathbf{a}<em>t)$生成的计数器。训练$f_w$可以通过一个梯度步最下化下列loss函数实现：<br>
\begin{equation}<br>
min_w\sum</em>{\xi=1}<sup>K\sum_{t=1}</sup>H\left(f_w(\mathbf{n}<em>t<sup>{\xi})-R_t</sup>{\xi}\right)^2<br>
\end{equation}<br>
其中$\mathbf{n}</em>{1:H}<sup>{\xi}$是从分布$P(\mathbf{n};\pi)$中生成的一个计数器样本；$f_w(\mathbf{n}_t</sup>{\xi})$是动作值函数，$R_t^{\xi}$是用式子$(1)$计算的$t$时刻的所有经验回报：<br>
\begin{equation}<br>
f_w(\mathbf{n}<em>t^{\xi}) = \sum</em>{i,j}n_t<sup>{\xi}(i,j)f_w(i,j,o(i,\mathbf{n}_t</sup>{\xi});R_t<sup>{\xi}=\sum_{T=t}</sup>H]\sum</em>{i\in S,j\in A}n_T{\xi}(i,j)r_T(i,j,\mathbf{n}<em>T^{\xi})<br>
\end{equation}<br>
然而，我们发现公式$(11)$中的loss函数在训练较大问题的critic时表现并不好。需要一定数量的计数器样本可靠的训练$f_w$，这对于拥有较多数量智能体的大问题的扩展有不利影响。已知在多智能体强化学习中单独利用全局reward信号的算法要比利用局部reward信号的方法多用一些样本。受到这些现象的启发，接下来我们提出了一个基于策略的局部reward信号去训练critic $f_w$。<br>
<strong>单个值函数</strong> 用$\mathbf{n}</em>{1:H}<sup>{\xi}$表示一个计数器样本。给定计数器样本$\mathbf{n}_{1:H}</sup>{\xi}$，用$V_t<sup>{\xi}(i,j)=E\left[\sum_{t’=t}</sup>Hr</em>{t’}<sup>m|s_t</sup>m=i,a_m<sup>t=j,n_{1:H}</sup>{\xi}\right]$表示一个智能体在时刻$t$处于state $i$，采取action $j$，所能得到的所有期望reward。这个单个的值函数可以用动态规划算法来计算。基于这个值函数，我们接下来展示了式子$(12)$中全局经验reward的重新参数化：<br>
<strong>引理(Lemma)1</strong> 给定计数器样本$\mathbf{1:H}<sup>{\xi}$，$t$时刻的经验回报$R_t</sup>{\xi}$可以被重新参数化为：<br>
$$R_t^{\xi} = \sum</em>{i\in S,j\in A}n_t<sup>{\xi}(i,j)V_t</sup>{\xi}(i,j).$$<br>
<strong>基于单个值函数的loss</strong> 给出引理$1$，我们推导出式子$11$中真实loss的上界，它有效利用了单个值函数：<br>
\begin{align*}<br>
&amp;\sum</em>{\xi}\sum_t\left(f_w(\mathbf{n}<sup>{\xi})-R_t</sup>{\xi}\right)^2 \<br>
= &amp;\sum_{\xi}\sum_t\left(\sum_{i,j}n_t<sup>{\xi}(i,j)f_w(i,j,o(i,\mathbf{n}_t</sup>{\xi}))-\sum_{i,j}n_t<sup>{\xi}(i,j)V_t</sup>{\xi}(i,h)\right)^2\<br>
= &amp;\sum_{\xi}\sum_t\left( \sum_{i,j}n_t<sup>{\xi}(i,j)(f_w(i,j,o(i,\mathbf{n}_t</sup>{\xi}))-V_t<sup>{\xi}(i,h))\right)</sup>2\<br>
\le &amp;M\sum_{\xi}\sum_{t,i,j}n_t(i,j)\left(f_w(i,j,o(i,\mathbf{n}_t<sup>{\xi}))-V_t</sup>{\xi}(i,j)\right)^2<br>
\end{align*}<br>
其中最后一部用了柯西施瓦茨不等式。我们用式子(14)中修改过的loss训练critic。按照经验来说，对于较大的问题，式子(14)中的新loss比式子(13)中的原始loss要收敛的快很多。直观上来说，这是因为式子(14)中的新loss尝试调整每一个critic组件$f_w(i,j,o(i,\mathbf{n}_t<sup>{\xi}))$更接近它的经验回报$V_t</sup>{\xi}(i,j)$。然而，原始的式子(13)中的loss着重于最小化全局loss，而不是调整每一个单个的critic因子$f_w(\cdot)$到相对应的每一个经验回报。<br>
算法$1$展示了CDec-POMDP中AC算法的大纲。第$7$行和第$8$行展示了两种不同的方式训练critic。第$7$行代表基于局部值函数的critic更新，也可以称为factored cirtic更新(fC)。第$8$行展示了基于全局reward或者全局critic的更新©。第$10$行展示了使用定理$2$(fA)计算的策略梯度。第$11$行展示了直接使用$f_w$计算的梯度。</p>
<h2 id="实验">实验</h2>
<p>这一节中比较了我们的AC算法和另外两个解决CDec-POMDP问题的算法，Soft-Max based flow update(SMFU)，和期望最大化方法。SMFU只能优化智能体的action依赖于局部state的policy，$\pi(a_t<sup>m|s_t</sup>m)$，因为它通过计算在规划阶段单个最有可能的计数器向量来估计计数器$\mathbf{n}$的作用。EM方法优化基于计数器的分段线性policy，其中$\pi(a_t<sup>m|s_t</sup>m,\cdot)$是所有可能的计数器observation $o_t$空间上的一个分段函数。<br>
算法$1$展示了更新critic的两种方式（第$7$行和第$8$行）和更新actor的两种方式（第$10$行和第$11$行），所以就有四种可能的AC方法－fAfC,AC,FfC,fAC。我们也研究了不同actor-critic方法的属性。在附录中有神经网络的结构和其他一些实验设置。<br>
为了和之前方法公平的进行比较，我们使用了三种不同的模型用于基于计数的observation $o_t$。在$o0$设置中，policy只取决于智能体的局部state $s_t^m$并不需要计数器。在$o1$设置中，policy取决于局部state $s_t^m$和单个计数器observation $n_t(s_t<sup>m)$。也就是说，智能体只能观测到其他也在当前状态$s_t</sup>m$的智能体的计数器。在$oN$设置中，智能体能观测到它的局部state $s_t<sup>m$和当前状态$s_t</sup>m$的局部相邻状态内其他智能体的计数器。$oN$ observation模型提供给智能体最多的信息。然而，它也是最难优化的因为policy有更多的参数。SMFU方法只能在$o0$设置中起作用，EM方法和本文中的AC方法在所有设置中都能起作用。</p>
<!--
**出租车调度** 我们在第二节中介绍的现实世界中的域测试了本文的方法。在这个问题中，目标是计算出租车policy优化整个车队的收入。数据包含亚洲一个大城市超过一年的出租车轨迹数据。我们使用了从数据集中提取到的车辆请求信息。平均来说，每天大概有$8000$辆出租车。整个城市被划分为$81$个空间，时间范围是$24$个小时划分为$48$个半小时的区间。
图$2(a)$中展示了不同方法在不同的观测模型（$'o0','o1','oN'$)上的量化比较。我们测试了$4000$和$8000$辆出租车来验证是否出租车的数量会影响不同方法的性能。$y$轴展示了整个车队每天的利润。在$'o0'$设置下，所有的方法（fAfC-$o0$,SMFU,EM-$o0$）给出质量差不多的解，在$8000$个出租车上fAfC-$o0$和EM-$o0$表现的比SMFU稍微好一些。在$'o1'$设置下，
-->

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://mxxhcm.github.io/2019/01/21/ml-expectatin_maximization/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="马晓鑫爱马荟荟">
      <meta itemprop="description" content="记录硕士三年自己的积累">
      <meta itemprop="image" content="/images/favicon.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="mxxhcm's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/01/21/ml-expectatin_maximization/" class="post-title-link" itemprop="http://mxxhcm.github.io/page/28/index.html">EM(Expectation Maximization)算法</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-01-21 10:22:45" itemprop="dateCreated datePublished" datetime="2019-01-21T10:22:45+08:00">2019-01-21</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-12-17 15:41:00" itemprop="dateModified" datetime="2019-12-17T15:41:00+08:00">2019-12-17</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/机器学习/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="引言-introduction">引言(Introduction)</h2>
<h3 id="什么是期望最大化算法">什么是期望最大化算法</h3>
<p>期望最大化算法(Expectation Maximization,EM)，是利用参数估计的迭代法求解最大似然估计的一种方法。</p>
<h3 id="em和mle关系">EM和MLE关系</h3>
<p>MLE的目标是求解已知分布类型的单个分布的参数。<br>
EM的目标是求解已知分布类型的多个混合分布的参数。<br>
一般我们用到的极大似然估计都是求某种已知分布类型的单个分布的参数，如求高斯分布的均值和方差；而EM算法是用来求解已知分布类型，多个该已知类型分布的混合分布的参数，这句话听起来可能有些拗口，举个最常见的例子，高斯混合分布参数的求解，这个混合分布都是高斯分布，只是每个分布的参数不同而已。如果一个高斯分布，一个卡方分布是没有办法求解的。</p>
<h3 id="为什么叫它em算法">为什么叫它EM算法</h3>
<p>因为这个算法总共有两个迭代步骤，E步和M步。第一步是对多个分布求期望，固定每一个分布的参数，计算出混合分布的参数，即E步，第二步是对这个混合分布利用最大似然估计方法进行参数估计，即M步。</p>
<h2 id="推理过程">推理过程</h2>
<p>假设我们要求一个混合分布p的参数$\theta$，比如校园内男生和女生的身高参数，显然，男生和女生的身高服从的分布类型是相同的，但是参数是不一样的。这里通过引入一个隐变量$z$，求解出对应不同$z$取值的参数$\theta$的值。<br>
\begin{align*}<br>
p(x|\theta) &amp;= \sum_zp(x,z|\theta)\\<br>
&amp;=\sum_zp(z|\theta)p(x|\theta, z) \tag{0}<br>
\end{align*}<br>
如果我们假设男女生的身高分布是一个高斯混合模型，现在要求它的参数$\theta$。混合模型的表达式可以写为：<br>
\begin{align*}<br>
p(x|\theta) &amp;= \sum_zw(z)N(x|\mu_z,\sigma_z)\\<br>
&amp;=\sum_zp(z|\theta)p(x|\theta,z)<br>
\end{align*}<br>
其中$\sum_zw(z) = 1,\theta={w, \mu, \sigma}$，如果用最大似然估计来解该问题的话，log函数内有和式，不好优化，所以就要换种方法。<br>
观测数据：$x=(x_1,\cdots, x_N)$<br>
对应的隐变量：$z=(z_1,\cdots, z_N)$，$z_i$有$c$种取值。</p>
<p>\begin{align*}<br>
l(\theta;x) &amp;= log p(x|\theta) \tag{1}\\<br>
&amp;= log\prod_{i=1}^N\ p(x_i|\theta) \tag{2}\\<br>
&amp;= \sum_{i=1}^Nlog\ p(x_i|\theta) \tag{3}\\<br>
&amp;= \sum_{i=1}^Nlog\sum_zp(x_i,z|\theta) \tag{4}\\<br>
\end{align*}<br>
这里式子(4)中$\sum_zp(x,z|\theta)$该怎么变形，因为现在解不出来了。<br>
最开始我想的是使用条件概率进行展开，即：<br>
$$\sum_zp(x_i, z|\theta) = \sum_zp(x_i|z, \theta)p(z|\theta)$$<br>
但是如果展开成这样子，就变成了文章开头给出的式子(0)，并没有什么用，不能继续化简了。<br>
所以就对式子(4)做个变形<br>
\begin{align*}<br>
&amp;\ \ \ \ \sum_{i=1}^Nlog\sum_zp(x_i,z|\theta) \tag{4}\\<br>
&amp;= \sum_{i=1}^Nlog\sum_zq(z|x_i)\frac{p(x_i,z|\theta)}{q(z|x_i)}, \ \ s.t.\sum_zq(z|x_i)=1 \tag{5}\\<br>
&amp;\ge \sum_{i=1}^N \underbrace{\sum_zq(z|x_i)log\frac{p(x_i,z|\theta)}{q(z|x_i)}}_{L(q,\theta)},\ \ s.t. \sum_zq(z|x_i)=1 \tag{6}\\<br>
\end{align*}<br>
第(4)步到第(5)步引入了一个分布$q(z|x)$，就是给定一个观测数据$x$，隐变量$z$取值的概率分布。注意，$q(z)$是一个函数，但是给定$x$之后，$q(z|x)$是一个变量。然后因为变形之后还是没有求解，就利用杰森不等式做了缩放，将$log(sum())$变成了$sum(log())$，就变成了(6)式。<br>
这里使用Jensen不等式的目的是使得缩放后的值还能取得和原式相等的值，重要的是等号能够取到。</p>
<h3 id="jensen不等式">Jensen不等式</h3>
<p>对于随机变量的Jensen不等式，当函数$f(x)$是凸函数的时候可以用下式表示：<br>
$$f(E(x)) \le E(f(x))$$<br>
当$f(x)$是凹函数的时候，有<br>
$$f(E(x)) \ge E(f(x))$$</p>
<p>接下来我们就要求解使得式子(6)中杰森不等式等号成立的$q$分布的取值。这里有两种方法可以求解。</p>
<h3 id="拉格朗日乘子法">拉格朗日乘子法</h3>
<p>令<br>
$$L(q,\theta) = \sum_z q(z|x_i)log{\frac{p(x_i,z|\theta)}{q(z|x_i)}}, s.t.\sum q(z|x_i) = 1 \tag{7}$$<br>
构建拉格朗日目标函数：<br>
\begin{align*}<br>
L &amp;= L(q, \theta) + \lambda(\sum_zq(z|x)- 1) \tag{8}\\<br>
&amp;= \sum_z q(z|x_i)log{\frac{p(x_i,z|\theta)}{q(z|x_i)}} + \lambda(\sum_z q(z|x_i) - 1)  \tag{9}<br>
\end{align*}</p>
<p>对$L$求导，得到：<br>
$$\frac{\partial L}{\partial q(z|x_i)} = log\frac{p(x_i, z|\theta)}{q(z|x_i)} + q(z|x_i)(-\frac{1}{q(z|x_i)}) + \lambda \tag{10}$$<br>
令$\frac{\partial L}{\partial q(z|x_i)}$等于$0$，得到：$$log\frac{p(x_i, z|\theta)}{q(z|x_i)} = 1 - \lambda$$<br>
两边同取$e$的对数：<br>
$$\frac{p(x_i, z|\theta)}{q(z|x_i)} = e^{1-\lambda} \tag{11}$$<br>
$$q(z|x_i) = e^{\lambda - 1}p(x_i, z|\theta) \tag{12}$$<br>
两边同时求和得：<br>
$$1 = e^{\lambda - 1}\sum_z p(x_i, z|\theta) \tag{13}$$<br>
用$p$表示$e^{\lambda-1}$得到：<br>
$$e^{\lambda-1} = \frac{1}{\sum_z p(x_i, z|\theta)}$$<br>
将其代入式子(12)得：<br>
\begin{align*}<br>
q(z|x_i) &amp;= \frac{p(x_i, z|\theta)}{\sum_z p(x_i, z|\theta)}\\<br>
&amp;= \frac{p(z, x_i|\theta)}{p(x_i|\theta)}\\<br>
&amp;= p(z|x_i, \theta)  \tag{14}<br>
\end{align*}</p>
<p>最后求出来$q(z|x_i) = p(z|x_i, \theta)$。</p>
<h3 id="杰森不等式成立条件">杰森不等式成立条件</h3>
<p>杰森不等式成立条件是常数，即：<br>
$$\frac{p(x_i, z|\theta)}{q(z|x_i)} = c,  s.t. \sum q(z|x_i)=1 \tag{15}$$<br>
则有:<br>
$$p(x, z_i|\theta) = cq(z_i|x) \tag{16}$$<br>
同时对式子左右两边求和，得到：<br>
$$\sum p(x_i, z|\theta) = \sum cq(z|x_i) = c \tag{17}$$<br>
将$c = \sum p(x_i, z|\theta)$代入式子(14)得：<br>
\begin{align*}<br>
q(z|x_i) &amp;= \frac{p(x_i, z|\theta)}{\sum p(x_i,z|\theta)}\\<br>
&amp;= \frac{p(x_i, z)|\theta}{p(x_i|\theta)}\\<br>
&amp;= p(z|x_i, \theta) \tag{18}<br>
\end{align*}</p>
<h3 id="等号成立证明">等号成立证明</h3>
<p>上面两个方法都算出来在$q(z|x_i) = p(z|x_i, \theta)$时$L$能取得最大值。接下来证明这个这个$L$的最大值和$l$相等。<br>
将$q = p(z|x_i, \theta)$代入$L(q, \theta)$得：<br>
\begin{align*}<br>
L(q, \theta) &amp;= L(p(z|x_i, \theta^t), \theta^t)\\<br>
&amp;= \sum_z p(z|x_i, \theta^t) log\frac{p(z, x_i|\theta^t)}{p(z|x_i, \theta^t)} \\<br>
&amp;= \sum_z p(z|x_i, \theta^t) log p(x_i|\theta^t)\\<br>
&amp;= 1\cdot log p(x_i|\theta^t)\\<br>
&amp;= log p(x_i|\theta^t)\\<br>
&amp;= l(\theta^t; x_i)<br>
\end{align*}</p>
<h3 id="另一种等号成立推导">另一种等号成立推导</h3>
<p>\begin{align*}<br>
l(\theta; x) - L(q, \theta) &amp;= l(\theta; x_i) - \sum_z q(z|x_i) log{\frac{p(z, x_i|\theta)}{q(z|x_i)}}\\<br>
&amp;= \sum_z q(z|x_i) log p(x_i|\theta) - \sum_z q(z|x_i) log{\frac{p(z, x_i|\theta)}{q(z|x_i)}}\\<br>
&amp;= \sum_z q(z|x_i)log {\frac{p(x_i|\theta)q(z|x_i)}{p(z, x_i|\theta)}}\\<br>
&amp;= \sum_z q(z|x_i)log {\frac{q(z|x_i)}{p(z|x_i, \theta)}}\\<br>
&amp;= KL(q(z|x_i)||p(z|x_i,\theta))<br>
\end{align*}<br>
最后算出来两个函数之差是一个KL散度，是从$p$到$q$的KL散度。当前仅当$p=q$时取等，否则就非负。</p>
<h3 id="m步">M步</h3>
<p>\begin{align*}<br>
L(q, \theta) &amp; = \sum_z q(z|x_i) log\frac{p(z, x_i|\theta)}{q(z|x_i)} \\<br>
&amp; = \underbrace{\sum_z q(z|x_i)log{p(z, x_i|\theta)}}_{Expected\ complete\ log-likelyhood} - \underbrace{\sum_z q(z|x_i)l{q(z|x_i)}}_{Entropy}<br>
\end{align*}</p>
<h2 id="em流程">EM流程</h2>
<h3 id="计算流程">计算流程</h3>
<p>（１）首先随机初始化模型的不同隐变量对应的参数，<br>
（２）对于每一个观测，首先判断它对应的隐变量的分布。<br>
（３）求期望<br>
（４）最大似然估计求参数<br>
用公式来表示如下：<br>
E步：$q^{t+1} = arg\ max_q L(q, \theta^t)$<br>
M步：$\theta^{t+1} = arg max_{\theta}L(q^{t+1}, \theta)$<br>
E步就是根据$t$时刻的$\theta^t$利用概率$q$求出$L$的期望，然后M步使用最大似然估计计算出新的$\theta$，就这样迭代下去。</p>
<h2 id="em收敛性分析">EM收敛性分析</h2>
<p>EM算法的收敛性就是要证明$L(q=p(z|x_i, \theta^t) , \theta)$的值一直在增大。<br>
\begin{align*}<br>
L(p(z|x_i, \theta^{t+1}) , \theta^{t+1}) - L(p(z|x_i, \theta^{t}) , \theta^{t}) &amp;= log p(x_i|\theta^{t+1}) - log p(x_i|\theta^t)\\<br>
&amp; \ge 0<br>
\end{align*}</p>
<h2 id="例子">例子</h2>
<p>假如有两个硬币A和B，假设随机从A,B中选一个硬币，掷$10$次，重复$5$次实验，分别求出两个硬币正面向上的概率。假设硬币服从二项分布<br>
$5$次实验结果如下：<br>
5H5T<br>
9H1T<br>
8H2T<br>
4H6T<br>
7H3T</p>
<p>这个时候有两种情况</p>
<h3 id="知道每次选的是a还是b">知道每次选的是A还是B</h3>
<p>这个时候就变成了极大似然估计。</p>
<h3 id="不知道每次选的是a还是b">不知道每次选的是A还是B</h3>
<p>这个时候就用EM算法了。<br>
首先随机初始化$\theta_A = 0.5, \theta_B = 0.5$，<br>
对于每一个观测，首先判断它对应的隐变量的分布。<br>
$i={1,2,3,4,5}$，分别代表$5$个实验。<br>
首先求出$\theta_A$的参数。<br>
$$P(z = A|x_i, \theta_A, \theta_B) = \frac{P(z = A|x_i, \theta_A)}{P(z = A|x_i, \theta_A) + P(z = B|x_i, \theta_B)}$$<br>
$$P(z = B|x_i, \theta_A, \theta_B) = 1 - P(z = A|x_i,\theta_A,\theta_B)$$<br>
然后计算下式：<br>
\begin{align*}<br>
L(q,\theta_A) &amp;= \sum_{i=1}^5 \sum_zp(z|x_i, \theta_A, \theta_B)log p(x_i|\theta)\\<br>
&amp;= \sum_{i=1}^5 (p(z=A|x_i, \theta_A)log p(x_i|\theta_A) + p(z=B|x_i, \theta_B)log p(x_i|\theta_B))<br>
\end{align*}<br>
然后利用极大既然估计计算$\theta_A$和$\theta_B$的值。</p>
<h2 id="参考文献">参考文献</h2>
<p>1.<a href="https://www.zhihu.com/question/27976634/answer/153567695" target="_blank" rel="noopener">https://www.zhihu.com/question/27976634/answer/153567695</a><br>
2.<a href="https://en.wikipedia.org/wiki/Jensen%27s_inequality" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Jensen's_inequality</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/27/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/27/">27</a><span class="page-number current">28</span><a class="page-number" href="/page/29/">29</a><span class="space">&hellip;</span><a class="page-number" href="/page/31/">31</a><a class="extend next" rel="next" href="/page/29/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <div class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/favicon.jpg" alt="马晓鑫爱马荟荟">
            
              <p class="site-author-name" itemprop="name">马晓鑫爱马荟荟</p>
              <p class="site-description motion-element" itemprop="description">记录硕士三年自己的积累</p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">302</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  <a href="/categories/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">23</span>
                    <span class="site-state-item-name">分类</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  <a href="/tags/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">152</span>
                    <span class="site-state-item-name">标签</span>
                  </a>
                </div>
              
            </nav>
          

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="https://github.com/mxxhcm" title="GitHub &rarr; https://github.com/mxxhcm" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="mailto:mxxhcm@gmail.com" title="E-Mail &rarr; mailto:mxxhcm@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                </span>
              
            </div>
          

          

          
          

          
            
          
          

        </div>
      </div>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">马晓鑫爱马荟荟</span>

  

  
</div>


  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v3.8.0</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> v6.6.0</div>




        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  
    <script src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  


  


  <script src="/js/src/utils.js?v=6.6.0"></script>

  <script src="/js/src/motion.js?v=6.6.0"></script>



  
  


  <script src="/js/src/affix.js?v=6.6.0"></script>

  <script src="/js/src/schemes/pisces.js?v=6.6.0"></script>



  

  


  <script src="/js/src/bootstrap.js?v=6.6.0"></script>



  



  






<!-- LOCAL: You can save these files to your site and update links -->
    
        
        <link rel="stylesheet" href="https://aimingoo.github.io/gitmint/style/default.css">
        <script src="https://aimingoo.github.io/gitmint/dist/gitmint.browser.js"></script>
    
<!-- END LOCAL -->

    

    







  





  

  

  

  

  
  

  
  
    
      
        
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
  

  
    
      <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      },
      TeX: {equationNumbers: { autoNumber: "AMS" }}
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<script src="//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<style>
.MathJax_Display {
    overflow: auto hidden;
}
</style>

    
  


  
  

  

  

  

  

  

  

</body>
</html>

<!DOCTYPE html>












  


<html class="theme-next pisces use-motion" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
































<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=6.6.0" rel="stylesheet" type="text/css">



  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.jpg?v=6.6.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.jpg?v=6.6.0">










<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '6.6.0',
    sidebar: {"position":"left","display":"always","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="记录硕士三年自己的积累">
<meta property="og:type" content="website">
<meta property="og:title" content="mxxhcm&#39;s blog">
<meta property="og:url" content="http://mxxhcm.github.io/page/7/index.html">
<meta property="og:site_name" content="mxxhcm&#39;s blog">
<meta property="og:description" content="记录硕士三年自己的积累">
<meta property="og:locale" content="zh-CN">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="mxxhcm&#39;s blog">
<meta name="twitter:description" content="记录硕士三年自己的积累">



  <link rel="alternate" href="/atom.xml" title="mxxhcm's blog" type="application/atom+xml">




  <link rel="canonical" href="http://mxxhcm.github.io/page/7/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>mxxhcm's blog</title>
  












  <noscript>
  <style>
    .use-motion .motion-element,
    .use-motion .brand,
    .use-motion .menu-item,
    .sidebar-inner,
    .use-motion .post-block,
    .use-motion .pagination,
    .use-motion .comments,
    .use-motion .post-header,
    .use-motion .post-body,
    .use-motion .collection-title { opacity: initial; }

    .use-motion .logo,
    .use-motion .site-title,
    .use-motion .site-subtitle {
      opacity: initial;
      top: initial;
    }

    .use-motion .logo-line-before i { left: initial; }
    .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">mxxhcm's blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
    
      
    

    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>标签</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">

    
    
    
      
    

    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>分类</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>归档</a>

  </li>

      
      
    </ul>
  

  

  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
            

          
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://mxxhcm.github.io/2019/03/18/jupyter-notebook笔记/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="马晓鑫爱马荟荟">
      <meta itemprop="description" content="记录硕士三年自己的积累">
      <meta itemprop="image" content="/images/favicon.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="mxxhcm's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/03/18/jupyter-notebook笔记/" class="post-title-link" itemprop="http://mxxhcm.github.io/page/7/index.html">jupyter notebook笔记</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-03-18 15:14:33" itemprop="dateCreated datePublished" datetime="2019-03-18T15:14:33+08:00">2019-03-18</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-05-07 00:22:27" itemprop="dateModified" datetime="2019-05-07T00:22:27+08:00">2019-05-07</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/工具/" itemprop="url" rel="index"><span itemprop="name">工具</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="一、安装和运行"><a href="#一、安装和运行" class="headerlink" title="一、安装和运行"></a>一、安装和运行</h2><h3 id="1-安装"><a href="#1-安装" class="headerlink" title="1.安装"></a>1.安装</h3><h4 id="Anaconda安装"><a href="#Anaconda安装" class="headerlink" title="Anaconda安装"></a>Anaconda安装</h4><p>Anaconda自身已经集成了jupyter包，所以如果没有装python的话，可以选择安装Anaconda集成环境</p>
<h4 id="pip安装"><a href="#pip安装" class="headerlink" title="pip安装"></a>pip安装</h4><p>~#:pip install jupyter</p>
<h3 id="2-运行"><a href="#2-运行" class="headerlink" title="2.运行"></a>2.运行</h3><p>~#:jupyter notebook</p>
<h3 id="3-远程访问"><a href="#3-远程访问" class="headerlink" title="3.远程访问"></a>3.远程访问</h3><h4 id="1-直接使用命令"><a href="#1-直接使用命令" class="headerlink" title="(1).直接使用命令"></a>(1).直接使用命令</h4><p>这种方法是建立了一个session，会有一个token，这个会话结束之后，这个token就无效了，需要再重现建立新的session</p>
<h5 id="a-在前台运行以下命令"><a href="#a-在前台运行以下命令" class="headerlink" title="a.在前台运行以下命令"></a>a.在前台运行以下命令</h5><p>~#:jupyter notebook —ip=your_server_ip<br>输出如下</p>
<p>复制这个url到你的客户端浏览器，就可以直接访问服务器端。</p>
<h5 id="b-后台运行"><a href="#b-后台运行" class="headerlink" title="b.后台运行"></a>b.后台运行</h5><p>~#:nohup jupyter notebook —ip=10.4.21.214 &amp;</p>
<h4 id="2-创建配置文件"><a href="#2-创建配置文件" class="headerlink" title="(2).创建配置文件"></a>(2).创建配置文件</h4><h5 id="a-服务器端设置密码"><a href="#a-服务器端设置密码" class="headerlink" title="a.服务器端设置密码"></a>a.服务器端设置密码</h5><p>这里是使用notebook的passwd()函数生成自己设置密码的sha1哈希值。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> notebook.auth <span class="keyword">import</span> passwd</span><br><span class="line">passwd()</span><br></pre></td></tr></table></figure></p>
<p>输入两边自己设置的密码，然后将哈希值复制到下面的配置文件中即可。</p>
<h5 id="b-服务端设置配置文件"><a href="#b-服务端设置配置文件" class="headerlink" title="b.服务端设置配置文件"></a>b.服务端设置配置文件</h5><p>~#:jupyter notebook —generate-config<br>~#:vim ~/.jupyter/jupyter_notebook_config.py<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">c.NotebookApp.ip=&apos;localhost&apos;</span><br><span class="line">c.NotebookApp.password=u&apos;sha1:...&apos;</span><br><span class="line">c.NotebookApp.open_browser=False</span><br><span class="line">c.NotebookApp.port=8888(your_port)</span><br></pre></td></tr></table></figure></p>
<h5 id="c-服务器端启动"><a href="#c-服务器端启动" class="headerlink" title="c.服务器端启动"></a>c.服务器端启动</h5><p>~#:jupyter notebook</p>
<h5 id="d-客户端访问"><a href="#d-客户端访问" class="headerlink" title="d.客户端访问"></a>d.客户端访问</h5><p><a href="http://your_server_ip:port" target="_blank" rel="noopener">http://your_server_ip:port</a><br>输入密码即可</p>
<h2 id="二、使用"><a href="#二、使用" class="headerlink" title="二、使用"></a>二、使用</h2><h3 id="1-创建新的文档"><a href="#1-创建新的文档" class="headerlink" title="1.创建新的文档"></a>1.创建新的文档</h3><h2 id="三、快捷键"><a href="#三、快捷键" class="headerlink" title="三、快捷键"></a>三、快捷键</h2><p>Jupyter Notebook 有两种键盘输入模式。编辑模式，允许你往单元中键入代码或文本；这时的单元框线是绿色的。命令模式，键盘输入运行程序命令；这时的单元框线是灰色。</p>
<h3 id="1-命令模式-按键-Esc-开启"><a href="#1-命令模式-按键-Esc-开启" class="headerlink" title="1.命令模式 (按键 Esc 开启)"></a>1.命令模式 (按键 Esc 开启)</h3><p>Enter : 转入编辑模式<br>Shift-Enter : 运行本单元，选中下个单元<br>Ctrl-Enter : 运行本单元<br>Alt-Enter : 运行本单元，在其下插入新单元<br>Y : 单元转入代码状态<br>M :单元转入markdown状态<br>R : 单元转入raw状态<br>1 : 设定 1 级标题<br>2 : 设定 2 级标题<br>3 : 设定 3 级标题<br>4 : 设定 4 级标题<br>5 : 设定 5 级标题<br>6 : 设定 6 级标题<br>Up : 选中上方单元<br>K : 选中上方单元<br>Down : 选中下方单元<br>J : 选中下方单元<br>Shift-K : 扩大选中上方单元<br>Shift-J : 扩大选中下方单元<br>A : 在上方插入新单元<br>B : 在下方插入新单元<br>X : 剪切选中的单元<br>C : 复制选中的单元<br>Shift-V : 粘贴到上方单元<br>V : 粘贴到下方单元<br>Z : 恢复删除的最后一个单元<br>dd : 删除选中的单元<br>Shift-M : 合并选中的单元<br>Ctrl-S : 文件存盘<br>S : 文件存盘<br>L : 转换行号<br>O : 转换输出<br>Shift-O : 转换输出滚动<br>Esc : 关闭页面<br>Q : 关闭页面<br>H : 显示快捷键帮助<br>I,I : 中断Notebook内核<br>0,0 : 重启Notebook内核<br>Shift : 忽略<br>Shift-Space : 向上滚动<br>Space : 向下滚动</p>
<h3 id="2-编辑模式-Enter-键启动"><a href="#2-编辑模式-Enter-键启动" class="headerlink" title="2.编辑模式 ( Enter 键启动)"></a>2.编辑模式 ( Enter 键启动)</h3><p>Tab : 代码补全或缩进<br>Shift-Tab : 提示<br>Ctrl-] : 缩进<br>Ctrl-[ : 解除缩进<br>Ctrl-A : 全选<br>Ctrl-Z : 复原<br>Ctrl-Shift-Z : 再做<br>Ctrl-Y : 再做<br>Ctrl-Home : 跳到单元开头<br>Ctrl-Up : 跳到单元开头<br>Ctrl-End : 跳到单元末尾<br>Ctrl-Down : 跳到单元末尾<br>Ctrl-Left : 跳到左边一个字首<br>Ctrl-Right : 跳到右边一个字首<br>Ctrl-Backspace : 删除前面一个字<br>Ctrl-Delete : 删除后面一个字<br>Esc : 进入命令模式<br>Ctrl-M : 进入命令模式<br>Shift-Enter : 运行本单元，选中下一单元<br>Ctrl-Enter : 运行本单元<br>Alt-Enter : 运行本单元，在下面插入一单元<br>Ctrl-Shift— : 分割单元<br>Ctrl-Shift-Subtract : 分割单元<br>Ctrl-S : 文件存盘<br>Shift : 忽略<br>Up : 光标上移或转入上一单元<br>Down :光标下移或转入下一单元</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://mxxhcm.github.io/2019/03/18/hdf5笔记/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="马晓鑫爱马荟荟">
      <meta itemprop="description" content="记录硕士三年自己的积累">
      <meta itemprop="image" content="/images/favicon.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="mxxhcm's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/03/18/hdf5笔记/" class="post-title-link" itemprop="http://mxxhcm.github.io/page/7/index.html">hdf5笔记</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-03-18 15:12:03" itemprop="dateCreated datePublished" datetime="2019-03-18T15:12:03+08:00">2019-03-18</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-05-07 00:22:27" itemprop="dateModified" datetime="2019-05-07T00:22:27+08:00">2019-05-07</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/python/" itemprop="url" rel="index"><span itemprop="name">python</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><h3 id="创建和打开hdf5文件"><a href="#创建和打开hdf5文件" class="headerlink" title="创建和打开hdf5文件"></a>创建和打开hdf5文件</h3><p>f = hdf5.File(“pathname”,”w”)<br>w     create file, truncate if exist<br>w- or x  create file,fail if exists<br>r         readonly, file must be exist<br>r+        read/write,file must be exist<br>a        read/write if exists,create othrewise (default)</p>
<h3 id="删除一个dataset或者group"><a href="#删除一个dataset或者group" class="headerlink" title="删除一个dataset或者group"></a>删除一个dataset或者group</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">del</span> group[<span class="string">"dataset_name/group_name"</span>]</span><br></pre></td></tr></table></figure>
<h2 id="dataset"><a href="#dataset" class="headerlink" title="dataset"></a>dataset</h2><h3 id="什么是dataset"><a href="#什么是dataset" class="headerlink" title="什么是dataset"></a>什么是dataset</h3><p>datasets和numpy arrays挺像的</p>
<h3 id="创建一个dataset"><a href="#创建一个dataset" class="headerlink" title="创建一个dataset"></a>创建一个dataset</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">f = hdf5.File(<span class="string">"pathname"</span>,<span class="string">"w"</span>)</span><br><span class="line">f.create_dataset(<span class="string">"dataset_name"</span>, (<span class="number">10</span>,), dtype=<span class="string">'i'</span>)</span><br><span class="line">f.create_dataset(<span class="string">"dataset_name"</span>, (<span class="number">10</span>,), dtype=<span class="string">'c'</span>)</span><br></pre></td></tr></table></figure>
<p>第一个参数是dataset的名字, 第二个参数是dataset的shape, dtype参数是dataset中元素的类型。</p>
<h3 id="如何访问一个dataset"><a href="#如何访问一个dataset" class="headerlink" title="如何访问一个dataset"></a>如何访问一个dataset</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dataset = f[<span class="string">"dataset_name"</span>]                           <span class="comment"># acess like a python dict</span></span><br><span class="line">dataset = f.create_dateset(<span class="string">"dataset_name"</span>)  <span class="comment"># or create a new dataset</span></span><br></pre></td></tr></table></figure>
<h3 id="dataset的属性"><a href="#dataset的属性" class="headerlink" title="dataset的属性"></a>dataset的属性</h3><p>dataset.name        #输出dataset的名字<br>dataset.tdype        #输出dataset中elements的type<br>dataset.shape        #输出dataset的shape<br>dataset.value<br>dataset doesn’t hava attrs like keys,values,items,etc..</p>
<h3 id="给hdf5-dataset复制numpy-array"><a href="#给hdf5-dataset复制numpy-array" class="headerlink" title="给hdf5 dataset复制numpy array"></a>给hdf5 dataset复制numpy array</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">array = np.zero((<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">h[<span class="string">'array'</span>] = array        <span class="comment"># in hdf5 file, you need't to explicit declare the shape of array, just assign it an object of numpy array</span></span><br></pre></td></tr></table></figure>
<h2 id="group"><a href="#group" class="headerlink" title="group"></a>group</h2><h3 id="什么是group"><a href="#什么是group" class="headerlink" title="什么是group"></a>什么是group</h3><p>group和字典挺像的</p>
<h3 id="创建一个group"><a href="#创建一个group" class="headerlink" title="创建一个group"></a>创建一个group</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">group = f.create_group(<span class="string">"group_name"</span>)    <span class="comment">#在f下创建一个group</span></span><br><span class="line">group.create_group(<span class="string">"group_name"</span>)        <span class="comment">#在group下创建一个group</span></span><br><span class="line">group.create_dataset(<span class="string">"dataset_name"</span>)    <span class="comment">#在group下创建一个dataset</span></span><br></pre></td></tr></table></figure>
<h3 id="访问一个group-the-same-as-dataset"><a href="#访问一个group-the-same-as-dataset" class="headerlink" title="访问一个group(the same as dataset)"></a>访问一个group(the same as dataset)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">group = f[<span class="string">"group_name"</span>]                           <span class="comment"># acess like a python dict</span></span><br><span class="line">group = f.create_dateset(<span class="string">"group_name"</span>)  <span class="comment"># or create a new group</span></span><br></pre></td></tr></table></figure>
<h3 id="group的属性和方法"><a href="#group的属性和方法" class="headerlink" title="group的属性和方法"></a>group的属性和方法</h3><p>group.name        #输出group的名字<br>以下内容分为python2和python3版本<br><strong>python 2 版本</strong><br>group.values()    #输出group的value<br>group.keys()        #输出gorup的keys<br>group.items()    #输出group中所有的item，包含group和dataste<br><strong>python 3 版本</strong><br>list(group.keys())<br>list(group.values())<br>list(group.items())</p>
<h2 id="属性"><a href="#属性" class="headerlink" title="属性"></a>属性</h2><h3 id="设置dataset属性"><a href="#设置dataset属性" class="headerlink" title="设置dataset属性"></a>设置dataset属性</h3><p>dataset.attrs[“attr_name”]=”attr_value”    #设置attr<br>print(dataset.attrs[“attr_name”])                #访问attr</p>
<h3 id="设置group属性"><a href="#设置group属性" class="headerlink" title="设置group属性"></a>设置group属性</h3><p>group.attrs[“attr_name”]=”attr_value”    #设置attr<br>print(group.attrs[“attr_name”])                #访问attr</p>
<h2 id="numpy-and-hdf5"><a href="#numpy-and-hdf5" class="headerlink" title="numpy and hdf5"></a>numpy and hdf5</h2><p>f = h5py.File(pathname,”r”)<br>data = f[‘data’]</p>
<h2 id="type-是dataset"><a href="#type-是dataset" class="headerlink" title="type 是dataset"></a>type 是dataset</h2><p>data = f[‘data’][:]</p>
<h2 id="type是numpy-ndarray"><a href="#type是numpy-ndarray" class="headerlink" title="type是numpy ndarray"></a>type是numpy ndarray</h2><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>1.<a href="http://docs.h5py.org/en/latest/index.html" target="_blank" rel="noopener">http://docs.h5py.org/en/latest/index.html</a><br>2.<a href="https://stackoverflow.com/questions/31037088/discovering-keys-using-h5py-in-python3" target="_blank" rel="noopener">https://stackoverflow.com/questions/31037088/discovering-keys-using-h5py-in-python3</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://mxxhcm.github.io/2019/03/18/MongoDB笔记/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="马晓鑫爱马荟荟">
      <meta itemprop="description" content="记录硕士三年自己的积累">
      <meta itemprop="image" content="/images/favicon.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="mxxhcm's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/03/18/MongoDB笔记/" class="post-title-link" itemprop="http://mxxhcm.github.io/page/7/index.html">MongoDB笔记</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-03-18 15:06:56" itemprop="dateCreated datePublished" datetime="2019-03-18T15:06:56+08:00">2019-03-18</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-05-07 00:22:27" itemprop="dateModified" datetime="2019-05-07T00:22:27+08:00">2019-05-07</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/数据库/" itemprop="url" rel="index"><span itemprop="name">数据库</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="一、数据库的安装"><a href="#一、数据库的安装" class="headerlink" title="一、数据库的安装"></a>一、数据库的安装</h2><p>自行下载安装包并安装</p>
<h2 id="二、数据库的运行和连接-以及以下简单的使用"><a href="#二、数据库的运行和连接-以及以下简单的使用" class="headerlink" title="二、数据库的运行和连接,以及以下简单的使用"></a>二、数据库的运行和连接,以及以下简单的使用</h2><h3 id="1-windows命令行下连接"><a href="#1-windows命令行下连接" class="headerlink" title="1.windows命令行下连接"></a>1.windows命令行下连接</h3><h4 id="（1）设置数据库存放目录"><a href="#（1）设置数据库存放目录" class="headerlink" title="（1）设置数据库存放目录"></a>（1）设置数据库存放目录</h4><p>~#:md D:/data/db</p>
<h4 id="（2）运行mongodb服务"><a href="#（2）运行mongodb服务" class="headerlink" title="（2）运行mongodb服务"></a>（2）运行mongodb服务</h4><p>~#:mongod</p>
<h4 id="（3）连接mongodb数据库"><a href="#（3）连接mongodb数据库" class="headerlink" title="（3）连接mongodb数据库"></a>（3）连接mongodb数据库</h4><p>~#:mongo (database_name)<br>如果不输入数据库名会默认连接到mongodb自带的一个数据库test，如果指定了数据库名就会连接到该数据库</p>
<h3 id="2-使用python代码中连接到数据库"><a href="#2-使用python代码中连接到数据库" class="headerlink" title="2.使用python代码中连接到数据库"></a>2.使用python代码中连接到数据库</h3><h4 id="（1）导入python-pacakge"><a href="#（1）导入python-pacakge" class="headerlink" title="（1）导入python pacakge"></a>（1）导入python pacakge</h4><p>使用pip安装即可<br>import pymongo</p>
<h4 id="（2）连接到mongodb"><a href="#（2）连接到mongodb" class="headerlink" title="（2）连接到mongodb"></a>（2）连接到mongodb</h4><p>connection = MongoClient(‘localhost’, 27017)</p>
<h4 id="（3）连接到某个数据库"><a href="#（3）连接到某个数据库" class="headerlink" title="（3）连接到某个数据库"></a>（3）连接到某个数据库</h4><p>db = connection.test  #连接到test数据库<br>db现在指向的是test这个数据库</p>
<h4 id="（4）指向某个collection"><a href="#（4）指向某个collection" class="headerlink" title="（4）指向某个collection"></a>（4）指向某个collection</h4><p>collection = db.collection_one</p>
<h4 id="（5）查看collection中的内容"><a href="#（5）查看collection中的内容" class="headerlink" title="（5）查看collection中的内容"></a>（5）查看collection中的内容</h4><p>items = collection.find()<br>print(items[‘key’])</p>
<h3 id="3-一些简单的操作"><a href="#3-一些简单的操作" class="headerlink" title="3.一些简单的操作"></a>3.一些简单的操作</h3><h4 id="（1）切换数据库"><a href="#（1）切换数据库" class="headerlink" title="（1）切换数据库"></a>（1）切换数据库</h4><p>~#:use database_name</p>
<h4 id="（2）查看所有的数据库"><a href="#（2）查看所有的数据库" class="headerlink" title="（2）查看所有的数据库"></a>（2）查看所有的数据库</h4><p>~#:show databases;</p>
<h4 id="（3）查看所有的collection"><a href="#（3）查看所有的collection" class="headerlink" title="（3）查看所有的collection"></a>（3）查看所有的collection</h4><p>~#:show collections;</p>
<h2 id="三-CRUD操作"><a href="#三-CRUD操作" class="headerlink" title="三.CRUD操作"></a>三.CRUD操作</h2><h3 id="1-id的构成-12-bytes-hex"><a href="#1-id的构成-12-bytes-hex" class="headerlink" title="1.id的构成 12 bytes hex"></a>1.id的构成 12 bytes hex</h3><p>4+3+2+3<br>timestamp + mac address + pid + counter<br>timestamp是unix timestamp，mac address 是 mongd运行的网卡mac address，pid是process id，</p>
<h3 id="2-create-document"><a href="#2-create-document" class="headerlink" title="2. create document"></a>2. create document</h3><h4 id="（1）create-one-document-insertOne"><a href="#（1）create-one-document-insertOne" class="headerlink" title="（1）create one document(insertOne)"></a>（1）create one document(insertOne)</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">db.collection_one.insertOne(&#123;&quot;key_one&quot;:&quot;value&quot;,&quot;key_two&quot;:&quot;value&quot;&#125;)</span><br></pre></td></tr></table></figure>
<h4 id="（2）create-many-documents（有order-insertMany）"><a href="#（2）create-many-documents（有order-insertMany）" class="headerlink" title="（2）create many documents（有order,insertMany）"></a>（2）create many documents（有order,insertMany）</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">db.collection_one.insertMany(</span><br><span class="line">[</span><br><span class="line">&#123;&quot;key_one&quot;:&quot;value&quot;,&quot;key_two&quot;:&quot;value&quot;&#125;,</span><br><span class="line">&#123;&quot;key_one&quot;:&quot;value&quot;,&quot;key_two&quot;:&quot;value&quot;&#125;,</span><br><span class="line">&#123;&quot;key_one&quot;:&quot;value&quot;,&quot;key_two&quot;:&quot;value&quot;&#125;</span><br><span class="line">])</span><br></pre></td></tr></table></figure>
<h4 id="（3）create-many-documents（无order-insertMany）"><a href="#（3）create-many-documents（无order-insertMany）" class="headerlink" title="（3）create many documents（无order,insertMany）"></a>（3）create many documents（无order,insertMany）</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">db.collection_one.insertMany(</span><br><span class="line">db.collection_one.insertMany(</span><br><span class="line">[</span><br><span class="line">&#123;&quot;key_one&quot;:&quot;value&quot;,&quot;key_two&quot;:&quot;value&quot;&#125;,</span><br><span class="line">&#123;&quot;key_one&quot;:&quot;value&quot;,&quot;key_two&quot;:&quot;value&quot;&#125;,</span><br><span class="line">&#123;&quot;key_one&quot;:&quot;value&quot;,&quot;key_two&quot;:&quot;value&quot;&#125;</span><br><span class="line">] , &#123;&quot;ordered&quot;:false&#125;)</span><br></pre></td></tr></table></figure>
<h4 id="（4）upsert"><a href="#（4）upsert" class="headerlink" title="（4）upsert"></a>（4）upsert</h4><p>第一个参数是一个filter选择合适的 document，第二个参数是一个更新操作for the documents were selected，第三个参数是 that if there is no matching result,if the value of upsert is true,then insert a new document,else do nothing.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">db.collection_one.insertMany(</span><br><span class="line">db.movieDetails.updateOne(&#123; name:&quot;mxxhcm&quot;&#125;, &#123; \$set:&#123;lover:&quot;mahuihui&quot;&#125; &#125; , &#123;upsert : true&#125;)</span><br></pre></td></tr></table></figure></p>
<h4 id="（5）有无order的区别"><a href="#（5）有无order的区别" class="headerlink" title="（5）有无order的区别"></a>（5）有无order的区别</h4><p>有order的话遇到inset错误就会停下来，没有order的话在插入document的时候，遇到错误会跳过该条语句执行下一条语句。</p>
<h3 id="3-read-documents-query-documents"><a href="#3-read-documents-query-documents" class="headerlink" title="3.read documents(query documents)"></a>3.read documents(query documents)</h3><p>link:<br><a href="https://docs.mongodb.com/manual/reference/operator/query/" target="_blank" rel="noopener">https://docs.mongodb.com/manual/reference/operator/query/</a></p>
<h4 id="（1）查找document"><a href="#（1）查找document" class="headerlink" title="（1）查找document"></a>（1）查找document</h4><p>查找collection_one这个collection中所有的document<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">db.collection_one.find()</span><br></pre></td></tr></table></figure></p>
<p>查找collection_one这个collection中满足{}中条件的collection，{}中的条件需要满足anded<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">db.collection_one.find(&#123;&#125;)</span><br></pre></td></tr></table></figure></p>
<p>pretty()表示以规范的格式展现出来查询结果<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">db.collection_one.find().pretty()</span><br></pre></td></tr></table></figure></p>
<p>findOne表示只展示出第一条结果<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">db.collection_one.findOne()</span><br></pre></td></tr></table></figure></p>
<p>满足{}中条件的第一条结果<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">db.collection_one.findOne(&#123;&#125;)</span><br></pre></td></tr></table></figure></p>
<h4 id="（2）对document进行计数"><a href="#（2）对document进行计数" class="headerlink" title="（2）对document进行计数"></a>（2）对document进行计数</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">db.collection_one.count()</span><br></pre></td></tr></table></figure>
<h4 id="（3）设置查找的条件-equality-match"><a href="#（3）设置查找的条件-equality-match" class="headerlink" title="（3）设置查找的条件(equality match)"></a>（3）设置查找的条件(equality match)</h4><h5 id="a-scalar-equality-match"><a href="#a-scalar-equality-match" class="headerlink" title="a.scalar equality match"></a>a.scalar equality match</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">db.collection_one.find(&#123;&quot;key&quot;:&quot;value&quot;,&quot;key&quot;,&quot;value&quot;&#125;)</span><br></pre></td></tr></table></figure>
<h5 id="b-nested-documents-equality-match"><a href="#b-nested-documents-equality-match" class="headerlink" title="b.nested documents equality match"></a>b.nested documents equality match</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">db.collection_one.find(&#123;&quot;key.key2.key3&quot;:&quot;value&quot;&#125;)</span><br></pre></td></tr></table></figure>
<h5 id="c-equality-matches-on-arrays"><a href="#c-equality-matches-on-arrays" class="headerlink" title="c.equality matches on arrays"></a>c.equality matches on arrays</h5><h6 id="entire-array-value-match"><a href="#entire-array-value-match" class="headerlink" title="entire array value match"></a>entire array value match</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">db.collection_one.find(&#123;key:[value1,value2]&#125;)</span><br></pre></td></tr></table></figure>
<h6 id="any-array-element-fileds-match-a-specfic-value"><a href="#any-array-element-fileds-match-a-specfic-value" class="headerlink" title="any array element fileds match a specfic value"></a>any array element fileds match a specfic value</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">db.collection_one.find(&#123;key:&quot;value2&quot;&#125;)</span><br></pre></td></tr></table></figure>
<h6 id="a-specfiec-element-fields-match-a-specfic-value"><a href="#a-specfiec-element-fields-match-a-specfic-value" class="headerlink" title="a specfiec element fields match a specfic value"></a>a specfiec element fields match a specfic value</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">db.collection_one.find(&#123;key.0:&quot;value1&quot;&#125;)</span><br></pre></td></tr></table></figure>
<h4 id="（4）cursor"><a href="#（4）cursor" class="headerlink" title="（4）cursor"></a>（4）cursor</h4><h4 id="（5）projection"><a href="#（5）projection" class="headerlink" title="（5）projection"></a>（5）projection</h4><p>by default,mongodb return all fields in all matching documents for query.<br>Projection are supplied as the second argument<br>db.collection_one.find({“key1”:”value”,”key2”,”value”},{“key1”:1,”key2”:1,”key3”:0,”key4”:0}).pretty()</p>
<h4 id="（6）comparison-operation"><a href="#（6）comparison-operation" class="headerlink" title="（6）comparison operation"></a>（6）comparison operation</h4><p>$eq<br>$gt<br>$gte<br>$lt<br>$lte<br>$ne<br>$in<br>$nin</p>
<h5 id="a-在某个范围内"><a href="#a-在某个范围内" class="headerlink" title="a.在某个范围内"></a>a.在某个范围内</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">db.movieDetails.find(&#123; runtime : &#123; \$gt: 70,  \$lte:100 &#125; &#125;).pretty()</span><br></pre></td></tr></table></figure>
<h5 id="b-不等于-ne"><a href="#b-不等于-ne" class="headerlink" title="b.不等于($ne)"></a>b.不等于($ne)</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">db.movieDetails.find(&#123; rated : &#123; \$ne:&quot;unrated&quot; &#125; &#125;).pretty()</span><br></pre></td></tr></table></figure>
<h5 id="c-在-in"><a href="#c-在-in" class="headerlink" title="c.在($in)"></a>c.在($in)</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">db.movieDetails.find(&#123;rated : &#123; \$in : [&quot;G&quot;,&quot;PG&quot;,&quot;PG-13&quot;] &#125;  &#125;).pretty()</span><br></pre></td></tr></table></figure>
<h4 id="（7）element-operator"><a href="#（7）element-operator" class="headerlink" title="（7）element operator"></a>（7）element operator</h4><h5 id="a-存在某个filed-exists"><a href="#a-存在某个filed-exists" class="headerlink" title="a.存在某个filed($exists)"></a>a.存在某个filed($exists)</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">db.movieDetail.find( &#123; filed_name : &#123; \$exists: true|false &#125; &#125; ).pretty()</span><br></pre></td></tr></table></figure>
<h5 id="b-某个字段的类型-type"><a href="#b-某个字段的类型-type" class="headerlink" title="b.某个字段的类型($type)"></a>b.某个字段的类型($type)</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">db.movieDetail.find( &#123; filed_name : &#123; \$type :&quot;string&quot;&#125; &#125;).pretty()</span><br></pre></td></tr></table></figure>
<h4 id="（8）logical-operator"><a href="#（8）logical-operator" class="headerlink" title="（8）logical operator"></a>（8）logical operator</h4><p>$or<br>$and<br>$not<br>$nor </p>
<h5 id="a-逻辑或-or"><a href="#a-逻辑或-or" class="headerlink" title="a.逻辑或($or)"></a>a.逻辑或($or)</h5><p>$or需要数组作为参数<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">db.movieDetails.find( &#123; \$or: [ &#123; field_one : &#123;\$type : &quot;string&quot;&#125; &#125; , &#123;field_two : &#123;\$exist: &quot;name&quot; &#125; &#125; ] &#125; ).pretty()</span><br></pre></td></tr></table></figure></p>
<h5 id="b-逻辑与-and"><a href="#b-逻辑与-and" class="headerlink" title="b.逻辑与($and)"></a>b.逻辑与($and)</h5><p>$and操作支持我们在同一个filed指定多个约束条件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">db.movieDetails.find(&#123; \$and: [ &#123;field_one: &#123;\$ne :null&#125; &#125; , &#123; field_one: &#123;\$gt:60, \$lte: 100&#125; &#125; ] &#125;).pretty()</span><br></pre></td></tr></table></figure></p>
<h4 id="（9）regex-operator"><a href="#（9）regex-operator" class="headerlink" title="（9）regex operator"></a>（9）regex operator</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">db.movieDetails.find(&#123; &quot;awards.text&quot;: &#123; \$regex: /^Won\s/&#125;  &#125;).pretty()</span><br></pre></td></tr></table></figure>
<h4 id="（10）array-operator"><a href="#（10）array-operator" class="headerlink" title="（10）array operator"></a>（10）array operator</h4><p>$all<br>$size<br>$elementMatch</p>
<h5 id="a-all"><a href="#a-all" class="headerlink" title="a.all"></a>a.all</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">db.movieDetails.find(&#123;genres : &#123;\$all :[&quot;comedy&quot;,&quot;crime&quot;,&quot;drama&quot;]&#125; &#125;).pretty()</span><br><span class="line">db.movieDetails.find(&#123;genres :  [&quot;comedy&quot;,&quot;crime&quot;,&quot;drama&quot;]  &#125;).pretty()</span><br></pre></td></tr></table></figure>
<p>上面两个式子是有区别的，第一个式子会匹配genres中包含”comedy”,”crime”,”drama”的document<br>而第二个只会匹配genres为”comedy”,”crime”,”drama”的document。</p>
<h5 id="b-size"><a href="#b-size" class="headerlink" title="b.size"></a>b.size</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">db.movieDetails.find(&#123;country : &#123;\$size : 3&#125; &#125;).pretty()</span><br></pre></td></tr></table></figure>
<h5 id="c-elementMatch"><a href="#c-elementMatch" class="headerlink" title="c.elementMatch"></a>c.elementMatch</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">db.movieDetails.find(&#123; boxOffice: &#123; country: &quot;UK&quot;, revenue: &#123; \$gt: 15 &#125; &#125; &#125;)</span><br></pre></td></tr></table></figure>
<h3 id="9-update-documents"><a href="#9-update-documents" class="headerlink" title="9.update documents"></a>9.update documents</h3><p>link:<br><a href="https://docs.mongodb.com/manual/reference/operator/update/" target="_blank" rel="noopener">https://docs.mongodb.com/manual/reference/operator/update/</a></p>
<h4 id="（0）some-update-operator"><a href="#（0）some-update-operator" class="headerlink" title="（0）some update operator"></a>（0）some update operator</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">db.movieDetails.updateOne( &#123; name : &quot;mxxhcm&quot; &#125; , &#123; \$inc : &#123; age: 1&#125; &#125;)</span><br></pre></td></tr></table></figure>
<h4 id="（1）updateOne"><a href="#（1）updateOne" class="headerlink" title="（1）updateOne"></a>（1）updateOne</h4><h5 id="a-update-for-scalar-fields"><a href="#a-update-for-scalar-fields" class="headerlink" title="a.update for scalar fields"></a>a.update for scalar fields</h5><p>$set<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">db.movieDetails.updateOne( &#123; name : &quot;mxxhcm&quot; &#125; , &#123; \$set : &#123; age: 19&#125; &#125;)</span><br></pre></td></tr></table></figure></p>
<p>$unset<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">db.movieDetails.updateOne( &#123; name : &quot;mxxhcm&quot; &#125; , &#123; \$unset : &#123; age: 19&#125; &#125;)</span><br></pre></td></tr></table></figure></p>
<p>$inc<br>age后是在原来的age上加的数值<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">db.movieDetails.updateOne( &#123; name : &quot;mxxhcm&quot; &#125; , &#123; \$set : &#123; age: 19&#125; &#125;)</span><br></pre></td></tr></table></figure></p>
<p>updateOne has two arguments, the first one is a selector,the second argument is how we want to update the document.</p>
<h5 id="b-update-for-array-fields"><a href="#b-update-for-array-fields" class="headerlink" title="b.update for array fields"></a>b.update for array fields</h5><p>$push<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">db.movieDetails.updateOne(&#123;name:&quot;mxxhcm&quot;&#125; , &#123;\$push: &#123; reviews: &#123; key1:value,key2:value...&#125;  &#125;  &#125; )</span><br><span class="line">db.movieDetails.updateOne(&#123;name:&quot;mxxhcm&quot;&#125; , &#123;\$push: &#123; reviews:</span><br><span class="line">                                                                                                &#123; \$each: [&#123; key1:value,key2:value...&#125; ,                                                                                                                        &#123;key1:value,key2:value...&#125; ]  &#125;  </span><br><span class="line">                                                                                              &#125;   &#125; )</span><br><span class="line">db.movieDetails.updateOne(&#123;name:&quot;mxxhcm&quot;&#125; , &#123;\$push: &#123; reviews:</span><br><span class="line">                                                                                                &#123; \$each: [&#123; key1:value,key2:value...&#125; ,                                                                                                                        &#123;key1:value,key2:value...&#125; ] ,                                                                                                             \$slice:3 &#125;  </span><br><span class="line">                                                                                              &#125;   &#125; )</span><br><span class="line">db.movieDetails.updateOne(&#123;name:&quot;mxxhcm&quot;&#125; , &#123;\$push: &#123; reviews:</span><br><span class="line">                                                                                                &#123; \$each: [&#123; key1:value,key2:value...&#125; ,                                                                                                                        &#123;key1:value,key2:value...&#125; ] ,                                                                                                             \$position:0,  </span><br><span class="line">                                                                                                  \$slice:3 &#125;  </span><br><span class="line">                                                                                              &#125;   &#125; )</span><br></pre></td></tr></table></figure></p>
<h4 id="（2）updateMany"><a href="#（2）updateMany" class="headerlink" title="（2）updateMany"></a>（2）updateMany</h4><p>the same as updateOne</p>
<h4 id="（3）replaceOne"><a href="#（3）replaceOne" class="headerlink" title="（3）replaceOne"></a>（3）replaceOne</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">db.movieDetail.replcaeOne(&#123;&#125;,&#123;&#125;)</span><br></pre></td></tr></table></figure>
<p>the first argument is a filter,the second argument is the thing that replace what the filter choose,it can be a document,or a variable point to a document.</p>
<h3 id="10-using-mongdb-by-pymongo"><a href="#10-using-mongdb-by-pymongo" class="headerlink" title="10. using mongdb by pymongo"></a>10. using mongdb by pymongo</h3><p>见代码</p>
<h4 id="（1）sort，skip，limit"><a href="#（1）sort，skip，limit" class="headerlink" title="（1）sort，skip，limit"></a>（1）sort，skip，limit</h4><p>sort &gt; skip &gt; limit<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cursor.sort(&apos;student_id&apos;,pymongo.ASCENDING).skip(4).limit(3)</span><br><span class="line">in python file:</span><br><span class="line">cursor.sort(  [ (&apos;student_id&apos;,pymongo.ASCENDING) , (&apos;score&apos;,pymongo.DESCENDING) ] ).skip(4).limit(3)</span><br><span class="line">in mongo shell:</span><br><span class="line">cursor.sort(  [ &#123;&apos;student_id&apos;:1&#125;, &#123;&apos;score&apos;,-1)&#125; ] ).skip(4).limit(3)</span><br></pre></td></tr></table></figure></p>
<h4 id="（2）find-find-one-cursors"><a href="#（2）find-find-one-cursors" class="headerlink" title="（2）find,find_one,cursors"></a>（2）find,find_one,cursors</h4><h4 id="（3）project"><a href="#（3）project" class="headerlink" title="（3）project"></a>（3）project</h4><h4 id="（4）regex"><a href="#（4）regex" class="headerlink" title="（4）regex"></a>（4）regex</h4><h4 id="（5）insert"><a href="#（5）insert" class="headerlink" title="（5）insert"></a>（5）insert</h4><h4 id="（6）update"><a href="#（6）update" class="headerlink" title="（6）update"></a>（6）update</h4><h4 id="（7）"><a href="#（7）" class="headerlink" title="（7）"></a>（7）</h4><p>There is a intervening between find and update,so maybe you find and update is not the same one.</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://mxxhcm.github.io/2019/03/14/神经网络-激活函数/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="马晓鑫爱马荟荟">
      <meta itemprop="description" content="记录硕士三年自己的积累">
      <meta itemprop="image" content="/images/favicon.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="mxxhcm's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/03/14/神经网络-激活函数/" class="post-title-link" itemprop="http://mxxhcm.github.io/page/7/index.html">神经网络-激活函数</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-03-14 11:45:46" itemprop="dateCreated datePublished" datetime="2019-03-14T11:45:46+08:00">2019-03-14</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-05-07 00:22:27" itemprop="dateModified" datetime="2019-05-07T00:22:27+08:00">2019-05-07</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/机器学习/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="激活函数的一些问题"><a href="#激活函数的一些问题" class="headerlink" title="激活函数的一些问题"></a>激活函数的一些问题</h2><h3 id="为什么要使用non-linear激活函数不使用linear激活函数？"><a href="#为什么要使用non-linear激活函数不使用linear激活函数？" class="headerlink" title="为什么要使用non-linear激活函数不使用linear激活函数？"></a>为什么要使用non-linear激活函数不使用linear激活函数？</h3><p><img src="/2019/03/14/神经网络-激活函数/fnn.png" alt="fnn"><br>给定一个如图所示的前馈神经网络。有一个输入层，一个隐藏层，一个输出层。输入是$2$维的，有$4$个隐藏单元，输出是$2$维的。<br>则：$ \hat{f}(x) = \sigma(w_1x+b_1)w_2 + b_2$<br>这里$\sigma$是一个线性的激活函数，不妨设$\sigma(x) = x$。<br>那么就有：<br>\begin{align*}<br>\hat{f}(x) &amp;= \sigma(w_1x+b_1)w_2 + b_2\\<br>&amp;= (w_1x+b_1)w_2 + b_2\\<br>&amp;= w_1w_2x + w_2b1 + b_2\\<br>&amp;= (w_1w_2) x + (w_2b1 + b_2)\\<br>&amp;= w’ x + b’<br>\end{align*}<br>因此，当使用线性激活函数的时候，我们可以把一个多层感知机模型化简成一个线性模型。当使用线性激活函数时，增加网络的深度没有用，使用线性激活函数的十层感知机和一层感知机没有区别，并不能增加网络的表达能力。因为任意两个仿射函数的组合还是仿射函数。</p>
<h3 id="为什么ReLU激活函数是non-linear的？"><a href="#为什么ReLU激活函数是non-linear的？" class="headerlink" title="为什么ReLU激活函数是non-linear的？"></a>为什么ReLU激活函数是non-linear的？</h3><p>ReLU的数学表达形式如下：</p>
<script type="math/tex; mode=display">g(x) = max(0, x)</script><p>首先考虑一下什么是linear function,什么是non-linear function。在微积分上，平面内的任意一条直线是线性函数，否则就是非线性函数。<br>考虑这样一个例子，输入数据的维度为$1$，输出数据的维度也为$1$，用$g(ax+b)$表示ReLU激活函数。如果我们使用两个隐藏单元，那么$h_1(x) = g(x)+g(-x)$可以用来表示$f(x)=|x|$，而函数$|x|$是一个非线性函数，函数图像如下所示。<br><img src="/2019/03/14/神经网络-激活函数/absolute.png" alt="f(x)=|x|"><br>我们还可以用ReLU逼近二次函数$f(x) = x^2$，如使用函数$h_2(x) = g(x) + g(-x) + g(2x-2) + g(2x+2)$逼近二次函数，对应的图像如下。<br><img src="/2019/03/14/神经网络-激活函数/quadratic.png" alt="h_2(x)"><br>使用的项越多，最后近似出来的图像也就和我们要逼近的二次函数越像。<br>同理，可以使用ReLU激活函数去逼近任意非线性函数。</p>
<h3 id="为什么ReLU比sigmod还有tanh激活函数要好？"><a href="#为什么ReLU比sigmod还有tanh激活函数要好？" class="headerlink" title="为什么ReLU比sigmod还有tanh激活函数要好？"></a>为什么ReLU比sigmod还有tanh激活函数要好？</h3><p>ReLU收敛的更快，因为梯度更大。<br>当CNN的层数越来越深的时候，实验表明，使用ReLU的CNN要比使用sigmod或者tanh的CNN训练的更容易，更快收敛。<br>为什么会这样，目前有两种理论，见参考文献[4]。<br>第一个，$tanh(x)$有梯度消散问题(vanishing gradient)。当$x$趋向于$\pm\infty$时，$tanh(x)$的导数趋向于$0$。如下图所示。</p>
<blockquote>
<p>Vanishing gradients occur when lower layers of a DNN have gradients of nearly 0 because higher layer units are nearly saturated at -1 or 1, the asymptotes of the tanh function. Such vanishing gradients cause slow optimization convergence, and in some cases the final trained network converges to a poor local minimum.</p>
<p>One way ReLUs improve neural networks is by speeding up training. The gradient computation is very simple (either 0 or 1 depending on the sign of x). Also, the computational step of a ReLU is easy: any negative elements are set to 0.0 — no exponentials, no multiplication or division operations.</p>
</blockquote>
<p><img src="/2019/03/14/神经网络-激活函数/tanh.png" alt="tanh(x)"><br>ReLU是non-saturating nonlinearity的激活函数，sigmod和tanh是saturating nonlinearity激活函数，会将输出挤压到一个区间内。</p>
<blockquote>
<p>f是non-saturating 当且仅当$|lim_{z\rightarrow -\infty} f(z)| \rightarrow + \infty$或者$|lim_{z\rightarrow +\infty} f(z)| \rightarrow + \infty$</p>
</blockquote>
<p>tanh和sigmod将输入都挤压在某一个很小的区间内，比如(0,1)，输入发生很大的变化，经过激活函数以后变化很小，经过好几层之后，基本上就没有差别了。而当网络很深的时候，反向传播主要集中在后几层，而输入层附近的权值没办法好好学习。而对于ReLU来说，任意深度的神经网络，都不存在梯度消失。</p>
<p>第二种理论是说有一些定理能够证明，在某些假设条件下，局部最小就是全局最小。如果使用sigmod或者tanh激活函数的时候，这些假设不能成立，而使用ReLU的话，这些条件就会成立。</p>
<h3 id="为什么发生了梯度消失以后训练结构很差？"><a href="#为什么发生了梯度消失以后训练结构很差？" class="headerlink" title="为什么发生了梯度消失以后训练结构很差？"></a>为什么发生了梯度消失以后训练结构很差？</h3><p>我的想法是，</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>1.<a href="https://stats.stackexchange.com/a/391971" target="_blank" rel="noopener">https://stats.stackexchange.com/a/391971</a><br>2.<a href="https://stats.stackexchange.com/a/299933" target="_blank" rel="noopener">https://stats.stackexchange.com/a/299933</a><br>3.<a href="https://stats.stackexchange.com/a/141978" target="_blank" rel="noopener">https://stats.stackexchange.com/a/141978</a><br>4.<a href="https://stats.stackexchange.com/a/335972" target="_blank" rel="noopener">https://stats.stackexchange.com/a/335972</a><br>5.<a href="https://stats.stackexchange.com/a/174438" target="_blank" rel="noopener">https://stats.stackexchange.com/a/174438</a><br>6.<a href="https://stats.stackexchange.com/questions/391968/relu-vs-a-linear-activation-function" target="_blank" rel="noopener">https://stats.stackexchange.com/questions/391968/relu-vs-a-linear-activation-function</a><br>7.<a href="https://stats.stackexchange.com/questions/141960/why-are-rectified-linear-units-considered-non-linear" target="_blank" rel="noopener">https://stats.stackexchange.com/questions/141960/why-are-rectified-linear-units-considered-non-linear</a><br>8.<a href="https://stats.stackexchange.com/questions/299915/how-does-the-rectified-linear-unit-relu-activation-function-produce-non-linear" target="_blank" rel="noopener">https://stats.stackexchange.com/questions/299915/how-does-the-rectified-linear-unit-relu-activation-function-produce-non-linear</a><br>9.<a href="https://stats.stackexchange.com/questions/226923/why-do-we-use-relu-in-neural-networks-and-how-do-we-use-it/226927#226927" target="_blank" rel="noopener">https://stats.stackexchange.com/questions/226923/why-do-we-use-relu-in-neural-networks-and-how-do-we-use-it/226927#226927</a><br>10.<a href="https://www.zhihu.com/question/264163033" target="_blank" rel="noopener">https://www.zhihu.com/question/264163033</a><br>11.<a href="http://ai.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf" target="_blank" rel="noopener">http://ai.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://mxxhcm.github.io/2019/03/13/CNN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="马晓鑫爱马荟荟">
      <meta itemprop="description" content="记录硕士三年自己的积累">
      <meta itemprop="image" content="/images/favicon.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="mxxhcm's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/03/13/CNN/" class="post-title-link" itemprop="http://mxxhcm.github.io/page/7/index.html">CNN</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-03-13 15:21:27" itemprop="dateCreated datePublished" datetime="2019-03-13T15:21:27+08:00">2019-03-13</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-05-07 00:22:27" itemprop="dateModified" datetime="2019-05-07T00:22:27+08:00">2019-05-07</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/机器学习/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="Alexnet-2012"><a href="#Alexnet-2012" class="headerlink" title="Alexnet(2012)"></a>Alexnet(2012)</h2><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>作者训练了一个深度卷积神经网络用来将Imagenet数据集中1000个类别中的120万张图片进行分类。整个网络结构包含五个卷积层，三个全连接层，以及一个1000-way的softmax层，整个网络共有6000万参数，65000个神经元。此外，作者提出了一些方法用来提高性能和减少训练的时间，并且介绍了一些防止过拟合的技巧。最后，在测试集上，它们跑出$37.5%$的top-1 error以及$17.0%$的top-5 error。</p>
<h3 id="存在的问题"><a href="#存在的问题" class="headerlink" title="存在的问题"></a>存在的问题</h3><p>1.之前的数据集太小，都是数以万计的，需要更大的数据集。</p>
<h3 id="创新"><a href="#创新" class="headerlink" title="创新"></a>创新</h3><h4 id="ReLU非线性激活函数"><a href="#ReLU非线性激活函数" class="headerlink" title="ReLU非线性激活函数"></a>ReLU非线性激活函数</h4><h5 id="作用"><a href="#作用" class="headerlink" title="作用"></a>作用</h5><p>作者说实验表明ReLU可以加速训练过程。</p>
<h5 id="saturating-nonlinearity"><a href="#saturating-nonlinearity" class="headerlink" title="saturating nonlinearity"></a>saturating nonlinearity</h5><p>一个饱和的激活函数会将输出挤压到一个区间内。</p>
<blockquote>
<p>A saturating activation function squeezes the input.</p>
</blockquote>
<p><strong>定义</strong><br>f是non-saturating 当且仅当$|lim_{z\rightarrow -\infty} f(z)| \rightarrow + \infty$或者$|lim_{z\rightarrow +\infty} f(z)| \rightarrow + \infty$<br>f是saturating 当且仅当f不是non-saturating<br><strong>例子</strong><br>ReLU就是non-saturating nonlinearity的激活函数，因为$f(x) = max(0, x)$，如下图所示。<br><img src="/2019/03/13/CNN/relu.png" alt="relu"><br>当$x$趋于无穷时，$f(x)$也趋于无穷。<br>sigmod和tanh是saturating nonlinearity激活函数，如下图所示。<br><img src="/2019/03/13/CNN/sigmod.png" alt="sigmo"><br><img src="/2019/03/13/CNN/tanh.png" alt="tanh"></p>
<h4 id="多块GPU并行"><a href="#多块GPU并行" class="headerlink" title="多块GPU并行"></a>多块GPU并行</h4><p>作者使用了两块GPU一块运行，每个GPU中的参数个数是一样的，在一些特定层中，两个GPU中的参数信息可以进行通信。</p>
<h4 id="Overlapping-Pooling"><a href="#Overlapping-Pooling" class="headerlink" title="Overlapping Pooling"></a>Overlapping Pooling</h4><p>就是Pooling kernel的size要比stride大。比如一个$12\times 12$的图片，用$5\times 5$的pooling kernel，步长为$3$，步长要比kernel核小，即$3$比$5$小。<br>为什么这能减小过拟合？</p>
<ul>
<li>可能是减小了Pooling过程中信息的丢失。</li>
</ul>
<blockquote>
<p>If the pooling regions do not overlap, the pooling regions are disjointed and if that is the case, more information is lost in each pooling layer. If some overlap is allowed the pooling regions overlap with some degree and less spatial information is lost in each layer.[4]</p>
</blockquote>
<h4 id="数据增强"><a href="#数据增强" class="headerlink" title="数据增强"></a>数据增强</h4><p>防止过拟合</p>
<h5 id="裁剪和翻转"><a href="#裁剪和翻转" class="headerlink" title="裁剪和翻转"></a>裁剪和翻转</h5><p>输入是$256\times 256 \times 3$的图像。<br>训练：对每张图片都提取多个$224\times 224$大小的patch，这样子总共就多产生了$(256-224)\times (256-224) = 1024$个样本，然后对每个patch做一个水平翻转，就有$1024\times 2 = 2048$个样本。<br>测试：通过对每张图片裁剪五个（四个角落加中间）$224\times 224$的patches，并且对它们做翻转，也就是有$10$个patches，网络对十个patch的softmax层输出做平均作为预测结果。</p>
<h5 id="在图片上调整RGB通道的密度"><a href="#在图片上调整RGB通道的密度" class="headerlink" title="在图片上调整RGB通道的密度"></a>在图片上调整RGB通道的密度</h5><p>使用PCA对RGB值做主成分分析。对于每张训练图片，加上主成分，其大小正比于特征值乘上一个均值为$0$，方差为$0.1$的高斯分布产生的随机变量。对于一张图片$x,y$点处的像素值$I_{xy}=[I_{xy}^R, I_{xy}^G,I_{xy}^B]^T$，加上$[\bold{p_1},\bold{p_2},\bold{p_3}][\alpha_1\lambda_1,\alpha_2\lambda_2,\alpha_3\lambda_3]$，其中$[\bold{p_1},\bold{p_2},\bold{p_3}]$是特征向量，$\lambda_i$是特征值，$\alpha_i$就是前面说的随机变量。</p>
<h4 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h4><p>通过学习鲁棒的特征防止过拟合。<br>在训练的时候，每个隐藏单元的输出有$p$的概率被设置为$0$，在该次训练中，如果这个神经元的输出被设置为$0$，它就对loss函数没有贡献，反向传播也不会被更新。对于一层有$N$个神经单元的全连接层，总共有$2^N$种神经元的组合结果，这就相当于训练了一系列共享参数的模型。<br>在测试的时候，所有隐藏单元的输出都不丢弃，但是会乘上$p$的概率，相当于对一系列集成模型取平均。具体可见<a href="https://mxxhcm.github.io/2019/03/23/神经网络-dropout/">dropout</a><br>在该模型中，作者在三层全连接层的前两层输出上加了dropout。</p>
<h4 id="局部响应归一化-Local-Response-Normalizaiton"><a href="#局部响应归一化-Local-Response-Normalizaiton" class="headerlink" title="局部响应归一化(Local Response Normalizaiton)"></a>局部响应归一化(Local Response Normalizaiton)</h4><p>事实上，后来发现这个东西没啥用。但是这里还是给出一个公式。</p>
<script type="math/tex; mode=display">b^i_{x,y} = \frac{a^i_{x,y}}{(k+\alpha \sum^{min(N-1,\frac{i+n}{2})}_{j=max(0,\frac{i-n}{2})}(a^j_{x,y})^2)^{\beta}}</script><p>其中$a^i_{x,y}$是在点$(x,y)$处使用kernel $i$之后，在经过ReLU激活函数。$k,n,\alpha,\beta$是超参数。</p>
<blockquote>
<p>It seems that these kinds of layers have a minimal impact and are not used any more. Basically, their role have been outplayed by other regularization techniques (such as dropout and batch normalization), better initializations and training methods.</p>
</blockquote>
<h3 id="整体架构"><a href="#整体架构" class="headerlink" title="整体架构"></a>整体架构</h3><h4 id="目标函数"><a href="#目标函数" class="headerlink" title="目标函数"></a>目标函数</h4><p>多峰logistic回归。</p>
<h4 id="并行框架"><a href="#并行框架" class="headerlink" title="并行框架"></a>并行框架</h4><p>下图是并行的架构，分为两层，上面一层用一个GPU，下面一层用一个GPU，它们只在第三个卷积层有交互。<br><img src="/2019/03/13/CNN/alexnet.png" alt="alexnet"></p>
<h4 id="简化框架"><a href="#简化框架" class="headerlink" title="简化框架"></a>简化框架</h4><p>下图是简化版的结构，不需要使用两个GPU。<br><img src="/2019/03/13/CNN/alexnet_simple.png" alt="alexnet_simple"></p>
<h4 id="数据流（简化框架）"><a href="#数据流（简化框架）" class="headerlink" title="数据流（简化框架）"></a>数据流（简化框架）</h4><p>输入是$224\times 224 \times 3$的图片，第一层是$96$个stride为$4$的$11\times 11\times 3$卷积核构成的卷积层，输出经过max pooling(步长为2，kernel size为3)输入到第二层；第二层有$256$个$5\times 5\times 96$个卷积核，输出经过max pooling(步长为2，kernel size为3)输入到第三层；第三层到第四层，第四层到第五层之间没有经过pooling和normalization)，第三层有384个$3\times 3\times 256$个卷积核，第四层有$384$个$3\times 3\times 384$个卷积核，第五层有$256$个$3\times 3\times 384$个卷积核。然后接了两个$2048$个神经元的全连接层和一个$1000$个神经元的全连接层。</p>
<h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><h4 id="Datasets"><a href="#Datasets" class="headerlink" title="Datasets"></a>Datasets</h4><p>ILSVRC-2010</p>
<h4 id="Baselines"><a href="#Baselines" class="headerlink" title="Baselines"></a>Baselines</h4><ul>
<li>Sparse coding</li>
<li>SIFT+FV</li>
<li>CNN</li>
</ul>
<h4 id="Metric"><a href="#Metric" class="headerlink" title="Metric"></a>Metric</h4><ul>
<li>top-1 error rate</li>
<li>top-5 error rate</li>
</ul>
<h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p>pytorch实现<br><a href="https://github.com/mxxhcm/myown_code/blob/master/CNN/alexnet.py" target="_blank" rel="noopener">https://github.com/mxxhcm/myown_code/blob/master/CNN/alexnet.py</a></p>
<h2 id="Vggnet-2013"><a href="#Vggnet-2013" class="headerlink" title="Vggnet(2013)"></a>Vggnet(2013)</h2><h3 id="概述-1"><a href="#概述-1" class="headerlink" title="概述"></a>概述</h3><h3 id="存在的问题-1"><a href="#存在的问题-1" class="headerlink" title="存在的问题"></a>存在的问题</h3><h3 id="方案"><a href="#方案" class="headerlink" title="方案"></a>方案</h3><h4 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h4><h4 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h4><h4 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h4><h2 id><a href="#" class="headerlink" title=" "></a> </h2><h3 id="概述-2"><a href="#概述-2" class="headerlink" title="概述"></a>概述</h3><h3 id="存在的问题-2"><a href="#存在的问题-2" class="headerlink" title="存在的问题"></a>存在的问题</h3><h3 id="方案-1"><a href="#方案-1" class="headerlink" title="方案"></a>方案</h3><h4 id="背景-1"><a href="#背景-1" class="headerlink" title="背景"></a>背景</h4><h4 id="算法-1"><a href="#算法-1" class="headerlink" title="算法"></a>算法</h4><h4 id="代码-2"><a href="#代码-2" class="headerlink" title="代码"></a>代码</h4><h1 id="-1"><a href="#-1" class="headerlink" title="#"></a>#</h1><h3 id="概述-3"><a href="#概述-3" class="headerlink" title="概述"></a>概述</h3><h3 id="存在的问题-3"><a href="#存在的问题-3" class="headerlink" title="存在的问题"></a>存在的问题</h3><h3 id="方案-2"><a href="#方案-2" class="headerlink" title="方案"></a>方案</h3><h4 id="背景-2"><a href="#背景-2" class="headerlink" title="背景"></a>背景</h4><h4 id="算法-2"><a href="#算法-2" class="headerlink" title="算法"></a>算法</h4><h4 id="代码-3"><a href="#代码-3" class="headerlink" title="代码"></a>代码</h4><h1 id="-2"><a href="#-2" class="headerlink" title="#"></a>#</h1><h3 id="概述-存在的问题"><a href="#概述-存在的问题" class="headerlink" title="概述 ### 存在的问题"></a>概述 ### 存在的问题</h3><h3 id="方案-3"><a href="#方案-3" class="headerlink" title="方案"></a>方案</h3><h4 id="背景-3"><a href="#背景-3" class="headerlink" title="背景"></a>背景</h4><h4 id="算法-3"><a href="#算法-3" class="headerlink" title="算法"></a>算法</h4><h4 id="代码-4"><a href="#代码-4" class="headerlink" title="代码"></a>代码</h4><h1 id="-3"><a href="#-3" class="headerlink" title="#"></a>#</h1><h3 id="概述-4"><a href="#概述-4" class="headerlink" title="概述"></a>概述</h3><h3 id="存在的问题-4"><a href="#存在的问题-4" class="headerlink" title="存在的问题"></a>存在的问题</h3><h3 id="方案-4"><a href="#方案-4" class="headerlink" title="方案"></a>方案</h3><h4 id="背景-4"><a href="#背景-4" class="headerlink" title="背景"></a>背景</h4><h4 id="算法-4"><a href="#算法-4" class="headerlink" title="算法"></a>算法</h4><h4 id="代码-5"><a href="#代码-5" class="headerlink" title="代码"></a>代码</h4><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>1.<a href="https://stats.stackexchange.com/a/174438" target="_blank" rel="noopener">https://stats.stackexchange.com/a/174438</a><br>2.<a href="https://www.zhihu.com/question/264163033/answer/277481519" target="_blank" rel="noopener">https://www.zhihu.com/question/264163033/answer/277481519</a><br>3.<a href="https://stats.stackexchange.com/questions/145768/importance-of-local-response-normalization-in-cnn" target="_blank" rel="noopener">https://stats.stackexchange.com/questions/145768/importance-of-local-response-normalization-in-cnn</a><br>4.<a href="https://stats.stackexchange.com/a/386304" target="_blank" rel="noopener">https://stats.stackexchange.com/a/386304</a><br>5.<a href="https://blog.csdn.net/luoyang224/article/details/78088582/" target="_blank" rel="noopener">https://blog.csdn.net/luoyang224/article/details/78088582/</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://mxxhcm.github.io/2019/03/13/pytorch踩坑（不定期更新）/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="马晓鑫爱马荟荟">
      <meta itemprop="description" content="记录硕士三年自己的积累">
      <meta itemprop="image" content="/images/favicon.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="mxxhcm's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/03/13/pytorch踩坑（不定期更新）/" class="post-title-link" itemprop="http://mxxhcm.github.io/page/7/index.html">pytorch踩坑（不定期更新）</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-03-13 15:10:22" itemprop="dateCreated datePublished" datetime="2019-03-13T15:10:22+08:00">2019-03-13</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-05-07 00:22:27" itemprop="dateModified" datetime="2019-05-07T00:22:27+08:00">2019-05-07</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/python/" itemprop="url" rel="index"><span itemprop="name">python</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="常见问题"><a href="#常见问题" class="headerlink" title="常见问题"></a>常见问题</h2><h3 id="RuntimeError-CUDNN-STATUS-ARCH-MISMATCH"><a href="#RuntimeError-CUDNN-STATUS-ARCH-MISMATCH" class="headerlink" title="RuntimeError: CUDNN_STATUS_ARCH_MISMATCH"></a>RuntimeError: CUDNN_STATUS_ARCH_MISMATCH</h3><p>CUDNN doesn’t support CUDA arch 2.1 cards.<br>CUDNN requires Compute Capability 3.0, at least.<br>意思是GPU的加速能力不够，CUDNN只支持CUDA Capability 3.0以上的GPU加速，实验室主机是GT620的显卡，2.1的加速能力。<br>GPU对应的capability: <a href="https://developer.nvidia.com/cuda-gpus" target="_blank" rel="noopener">https://developer.nvidia.com/cuda-gpus</a><br>所以，对于不能使用cudnn对cuda加速的显卡，我们可以设置cudnn加速为False，这个默认是为True的<br>torch.backends.cudnn.enabled=False<br>但是，由于显卡版本为2.1，太老了，没有二进制版本。所以，还是会报其他错误，因此，就别使用cpu进行加速啦。</p>
<h3 id="查看cuda版本"><a href="#查看cuda版本" class="headerlink" title="查看cuda版本"></a>查看cuda版本</h3><p>~#:nvcc —version</p>
<h2 id="神经网络参数初始化"><a href="#神经网络参数初始化" class="headerlink" title="神经网络参数初始化"></a>神经网络参数初始化</h2><h3 id="方法-1-Model-apply-fn"><a href="#方法-1-Model-apply-fn" class="headerlink" title="方法$1$.Model.apply(fn)"></a>方法$1$.Model.apply(fn)</h3><p><a href="https://github.com/mxxhcm/myown_code/blob/master/pytorch/tutorials/initialize/apply.py" target="_blank" rel="noopener">示例</a>如下<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_weights</span><span class="params">(m)</span>:</span></span><br><span class="line">  print(m)</span><br><span class="line">  <span class="keyword">if</span> type(m) == nn.Linear:</span><br><span class="line">    m.weight.data.fill_(<span class="number">1.0</span>)</span><br><span class="line">    print(m.weight)</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(nn.Linear(<span class="number">2</span>, <span class="number">2</span>), nn.Linear(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">net.apply(init_weights)</span><br></pre></td></tr></table></figure></p>
<p>输出结果如下：</p>
<blockquote>
<p>Linear(in_features=2, out_features=2, bias=True)<br>Parameter containing:<br>tensor([[1., 1.],<br>        [1., 1.]], requires_grad=True)<br>Linear(in_features=2, out_features=2, bias=True)<br>Parameter containing:<br>tensor([[1., 1.],<br>        [1., 1.]], requires_grad=True)<br>Sequential(<br>  (0): Linear(in_features=2, out_features=2, bias=True)<br>  (1): Linear(in_features=2, out_features=2, bias=True)<br>)<br>Linear(in_features=2, out_features=2, bias=True)<br>Linear(in_features=2, out_features=2, bias=True)</p>
</blockquote>
<p>其中最后两行为net对象调用self.children()函数返回的模块，就是模型中所有网络的参数。事实上，调用net.apply(fn)函数，会对self.children()中的所有模块应用fn函数，</p>
<h2 id="torch-multiprocessing"><a href="#torch-multiprocessing" class="headerlink" title="torch.multiprocessing"></a>torch.multiprocessing</h2><h3 id="join"><a href="#join" class="headerlink" title="join"></a>join</h3><p>等待调用join()方法的线程执行完毕，然后继续执行。<br>可参见github<a href="https://github.com/mxxhcm/myown_code/tree/master/pytorch/tutorials/multiprocess_torch/mnist_hogwild" target="_blank" rel="noopener">官方demo</a>。</p>
<h3 id="share-memory"><a href="#share-memory" class="headerlink" title="share_memory_()"></a>share_memory_()</h3><p>在多个线程之间共享参数，如下<a href="https://github.com/mxxhcm/myown_code/blob/master/pytorch/tutorials/multiprocess_torch/share_memory.py" target="_blank" rel="noopener">代码</a>所示。可以用来实现A3C。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.multiprocessing <span class="keyword">as</span> mp</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">proc</span><span class="params">(sec, x)</span>:</span></span><br><span class="line">   print(os.getpid(),<span class="string">"  "</span>, x)</span><br><span class="line">   time.sleep(sec)</span><br><span class="line">   print(os.getpid(), <span class="string">"  "</span>, x)</span><br><span class="line">   x += sec</span><br><span class="line">   print(str(os.getpid()) + <span class="string">"  over.  "</span>, x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">   num_processes = <span class="number">3</span></span><br><span class="line">   processes = []</span><br><span class="line">   x = torch.ones([<span class="number">3</span>,])</span><br><span class="line">   x.share_memory_()</span><br><span class="line">   <span class="keyword">for</span> rank <span class="keyword">in</span> range(num_processes):</span><br><span class="line">     p = mp.Process(target=proc, args=(rank + <span class="number">1</span>, x))</span><br><span class="line">     p.start() </span><br><span class="line">     processes.append(p)</span><br><span class="line">   <span class="keyword">for</span> p <span class="keyword">in</span> processes:</span><br><span class="line">     p.join()</span><br><span class="line">   print(x)</span><br></pre></td></tr></table></figure></p>
<p>输出结果如下所示：</p>
<blockquote>
<p>python share_memory.py<br>7739    tensor([1., 1., 1.])<br>7738    tensor([1., 1., 1.])<br>7737    tensor([1., 1., 1.])<br>7737    tensor([1., 1., 1.])<br>7737  over.   tensor([2., 2., 2.])<br>7738    tensor([2., 2., 2.])<br>7738  over.   tensor([4., 4., 4.])<br>7739    tensor([4., 4., 4.])<br>7739  over.   tensor([7., 7., 7.])<br>tensor([7., 7., 7.])</p>
</blockquote>
<p>我们可以发现$7739$这个线程中，传入的$x$还是和最开始的一样，但是在$7738$线程更新完$x$之后，$7739$使用的$x$就已经变成了更新后的$x$。所以，我猜测这里面应该是有一个对$x$的锁，保证$x$在同一时刻只能被一个线程访问。</p>
<h2 id="torch-distributions"><a href="#torch-distributions" class="headerlink" title="torch.distributions"></a>torch.distributions</h2><p>这个库和gym.space库很相似，都是提供一些分布，然后从中采样。<br>常见的有ExponentialFamily,Bernoulli,Binomial,Categorical,Exponential,Gamma,Independent,Laplace,Multinomial,MultivariateNormal。这里不做过程陈述，可以看<a href="http://localhost:4000/2019/04/12/gym%E4%BB%8B%E7%BB%8D/" target="_blank" rel="noopener">gym</a>中。</p>
<h3 id="Categorical"><a href="#Categorical" class="headerlink" title="Categorical"></a>Categorical</h3><p>对应tensorflow中的<a href="https://github.com/mxxhcm/myown_code/blob/master/tf/some_ops/tf_multinominal.py" target="_blank" rel="noopener">tf.multinomial</a>。<br>类原型：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CLASS torch.distributions.categorical.Categorical(probs=<span class="literal">None</span>, logits=<span class="literal">None</span>, validate_args=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure></p>
<p>参数probs只能是$1$维或者$2$维，而且必须是非负，有限非零和的，然后将其归一化到和为$1$。<br>这个类和torch.multinormal是一样的，从$\{0,\cdots, K-1\}$中按照probs的概率进行采样，$K$是probs.size(-1)，即是size()矩阵的最后一列，$2$维时把第$1$维当成了batch。</p>
<p>举一个简单的例子，<a href>代码</a>。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.distributions <span class="keyword">as</span> diss</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">torch.manual_seed(<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">m = diss.Categorical(torch.tensor([<span class="number">0.25</span>, <span class="number">0.25</span>, <span class="number">0.25</span>, <span class="number">0.25</span> ]))</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">    print(m.sample())</span><br><span class="line"></span><br><span class="line">m = diss.Categorical(torch.tensor([[<span class="number">0.5</span>, <span class="number">0.25</span>, <span class="number">0.25</span>], [<span class="number">0.25</span>, <span class="number">0.25</span>, <span class="number">0.5</span>]]))</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">    print(m.sample())</span><br></pre></td></tr></table></figure></p>
<p>输出结果如下：</p>
<blockquote>
<p>tensor(2)<br>tensor(1)<br>tensor(1)<br>tensor(1)<br>tensor(1)<br>tensor([2, 2])<br>tensor([1, 2])<br>tensor([0, 1])<br>tensor([0, 2])<br>tensor([0, 0])</p>
</blockquote>
<p>作为对比，gym.spaces.Discrete示例如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> gym <span class="keyword">import</span> spaces</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.Discrete</span></span><br><span class="line"><span class="comment"># 取值是&#123;0, 1, ..., n - 1&#125;</span></span><br><span class="line">dis = spaces.Discrete(<span class="number">5</span>)</span><br><span class="line">dis.seed(<span class="number">5</span>)</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">    print(dis.sample())</span><br></pre></td></tr></table></figure></p>
<p>输出结果是：</p>
<blockquote>
<p>3<br>0<br>1<br>0<br>4</p>
</blockquote>
<h2 id="torch"><a href="#torch" class="headerlink" title="torch"></a>torch</h2><h3 id="torch-cat"><a href="#torch-cat" class="headerlink" title="torch.cat"></a>torch.cat</h3><h4 id="函数原型"><a href="#函数原型" class="headerlink" title="函数原型"></a>函数原型</h4><p>将多个tensor在某一个维度上（默认是第0维）拼接到一起（除了拼接的维度上，其他维度的shape必须一定），最后返回一个tensor。<br>torch.cat(tensors, dim=0, out=None) → Tensor</p>
<blockquote>
<p>Concatenates the given sequence of seq tensors in the given dimension. All tensors must either have the same shape (except in the concatenating dimension) or be empty.</p>
</blockquote>
<h4 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h4><p>tensors (sequence of Tensors) – 任意类型相同python序列或者tensor<br>dim (int, optional) - 在第几个维度上进行拼接(只有在拼接的维度上可以不同，其余维度必须相同。<br>out (Tensor, optional) – 输出的tensor</p>
<h4 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x1 = torch.randn(<span class="number">3</span>, <span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line">x2 = torch.randn(<span class="number">3</span>, <span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line">x = torch.cat([x1, x2], <span class="number">1</span>)</span><br><span class="line">print(x.size())</span><br></pre></td></tr></table></figure>
<p>输出如下：</p>
<blockquote>
<p>torch.Size([3, 5, 4])</p>
</blockquote>
<h3 id="torch中图像-img-格式"><a href="#torch中图像-img-格式" class="headerlink" title="torch中图像(img)格式"></a>torch中图像(img)格式</h3><p>torch中图像的shape是(‘RGB’,width, height)，而numpy和matplotlib中都是(width, height, ‘RGB’)<br>matplotlib.pyplot.imshow()需要的参数是图像矩阵，如果矩阵中是整数，那么它的值需要在区间[0,255]之内，如果是浮点数，需要在[0,1]之间。</p>
<blockquote>
<p>Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).</p>
</blockquote>
<h2 id="torch-autograd-torch-autograd-Variable和torch-autograd-Function"><a href="#torch-autograd-torch-autograd-Variable和torch-autograd-Function" class="headerlink" title="torch.autograd(torch.autograd.Variable和torch.autograd.Function)"></a>torch.autograd(torch.autograd.Variable和torch.autograd.Function)</h2><h3 id="Variable-class-torch-autograd-Variable"><a href="#Variable-class-torch-autograd-Variable" class="headerlink" title="Variable(class torch.autograd.Variable)"></a>Variable(class torch.autograd.Variable)</h3><h4 id="声明一个tensor"><a href="#声明一个tensor" class="headerlink" title="声明一个tensor"></a>声明一个tensor</h4><p>torch.zeros,torch.ones,torch.rand,torch.Tensor<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">torch.manual_seed(<span class="number">5</span>)</span><br><span class="line">x = torch.empty(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">print(torch.empty(<span class="number">5</span>, <span class="number">3</span>)) <span class="comment"># construct a 5x3 matrix, uninitialized</span></span><br><span class="line"><span class="comment"># tensor([[4.6179e-38, 4.5845e-41, 4.6179e-38],</span></span><br><span class="line"><span class="comment">#         [4.5845e-41, 6.3010e-36, 6.3010e-36],</span></span><br><span class="line"><span class="comment">#         [2.5204e-35, 6.3010e-36, 1.0082e-34],</span></span><br><span class="line"><span class="comment">#         [6.3010e-36, 6.3010e-36, 6.6073e-30],</span></span><br><span class="line"><span class="comment">#         [6.3010e-36, 6.3010e-36, 6.3010e-36]])</span></span><br><span class="line"></span><br><span class="line">print(torch.rand(<span class="number">3</span>, <span class="number">4</span>))  <span class="comment"># construct a 4x3 matrix, uniform [0,1] </span></span><br><span class="line"><span class="comment"># tensor([[0.8303, 0.1261, 0.9075, 0.8199],</span></span><br><span class="line"><span class="comment">#         [0.9201, 0.1166, 0.1644, 0.7379],</span></span><br><span class="line"><span class="comment">#         [0.0333, 0.9942, 0.6064, 0.5646]])</span></span><br><span class="line"></span><br><span class="line">print(torch.randn(<span class="number">5</span>, <span class="number">3</span>)) <span class="comment"># construct a 5x3 matrix, normal distribution</span></span><br><span class="line"><span class="comment"># tensor([[-1.4017, -0.7626,  0.6312],</span></span><br><span class="line"><span class="comment">#         [-0.8991, -0.5578,  0.6907],</span></span><br><span class="line"><span class="comment">#         [ 0.2225, -0.6662,  0.6846],</span></span><br><span class="line"><span class="comment">#         [ 0.5740, -0.5829,  0.7679],</span></span><br><span class="line"><span class="comment">#         [ 0.5740, -0.5829,  0.7679],</span></span><br><span class="line"></span><br><span class="line">print(torch.randn(<span class="number">2</span>, <span class="number">3</span>).type())</span><br><span class="line"><span class="comment"># torch.FloatTensor</span></span><br><span class="line"></span><br><span class="line">print(torch.zeros(<span class="number">5</span>, <span class="number">3</span>)) <span class="comment"># construct a 5x3 matrix filled zeros</span></span><br><span class="line"><span class="comment"># tensor([[0., 0., 0.],</span></span><br><span class="line"><span class="comment">#         [0., 0., 0.],</span></span><br><span class="line"><span class="comment">#         [0., 0., 0.],</span></span><br><span class="line"><span class="comment">#         [0., 0., 0.],</span></span><br><span class="line"><span class="comment">#         [0., 0., 0.]])</span></span><br><span class="line"></span><br><span class="line">print(torch.ones(<span class="number">5</span>, <span class="number">3</span>)) <span class="comment"># construct a 5x3 matrix filled ones</span></span><br><span class="line"><span class="comment"># tensor([[1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 1., 1.]])</span></span><br><span class="line"></span><br><span class="line">print(torch.ones(<span class="number">5</span>, <span class="number">3</span>, dtype=torch.long)) <span class="comment"># construct a tensor with dtype=torch.long</span></span><br><span class="line"><span class="comment"># tensor([[1, 1, 1],</span></span><br><span class="line"><span class="comment">#         [1, 1, 1],</span></span><br><span class="line"><span class="comment">#         [1, 1, 1],</span></span><br><span class="line"><span class="comment">#         [1, 1, 1],</span></span><br><span class="line"><span class="comment">#         [1, 1, 1]])</span></span><br><span class="line"></span><br><span class="line">print(torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])) <span class="comment"># construct a tensor direct from data</span></span><br><span class="line"><span class="comment"># tensor([1, 2, 3])</span></span><br><span class="line"></span><br><span class="line">print(x.new_ones(<span class="number">5</span>,<span class="number">4</span>)) <span class="comment"># constuct a tensor has the same property as x</span></span><br><span class="line"><span class="comment"># tensor([[1., 1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 1., 1., 1.]])</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(torch.full([<span class="number">4</span>,<span class="number">3</span>],<span class="number">9</span>))  <span class="comment"># construct a tensor with a value </span></span><br><span class="line"><span class="comment"># tensor([[9., 9., 9.],</span></span><br><span class="line"><span class="comment">#         [9., 9., 9.],</span></span><br><span class="line"><span class="comment">#         [9., 9., 9.],</span></span><br><span class="line"><span class="comment">#         [9., 9., 9.]])</span></span><br><span class="line"></span><br><span class="line">print(x.new_ones(<span class="number">5</span>,<span class="number">4</span>,dtype=torch.int)) <span class="comment"># construct a tensor with the same property as x, and also can have the specified type.</span></span><br><span class="line"><span class="comment"># tensor([[1, 1, 1, 1],</span></span><br><span class="line"><span class="comment">#         [1, 1, 1, 1],</span></span><br><span class="line"><span class="comment">#         [1, 1, 1, 1],</span></span><br><span class="line"><span class="comment">#         [1, 1, 1, 1],</span></span><br><span class="line"><span class="comment">#         [1, 1, 1, 1]], dtype=torch.int32)</span></span><br><span class="line"></span><br><span class="line">print(torch.randn_like(x,dtype=torch.float)) <span class="comment"># construct a tensor with the same shape with x, </span></span><br><span class="line"><span class="comment"># tensor([[ 0.4699, -1.9540, -0.5587],</span></span><br><span class="line"><span class="comment">#         [ 0.4295, -2.2643, -0.2017],</span></span><br><span class="line"><span class="comment">#         [ 1.0677,  0.3246, -0.0684],</span></span><br><span class="line"><span class="comment">#         [-0.9959,  1.1563, -0.3992],</span></span><br><span class="line"><span class="comment">#         [ 1.2153, -0.8115, -0.8848]])</span></span><br><span class="line"></span><br><span class="line">print(torch.ones_like(x))</span><br><span class="line"><span class="comment"># tensor([[1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 1., 1.]])</span></span><br><span class="line"></span><br><span class="line">print(torch.zeros_like(x))</span><br><span class="line"><span class="comment"># tensor([[0., 0., 0.],</span></span><br><span class="line"><span class="comment">#         [0., 0., 0.],</span></span><br><span class="line"><span class="comment">#         [0., 0., 0.],</span></span><br><span class="line"><span class="comment">#         [0., 0., 0.],</span></span><br><span class="line"><span class="comment">#         [0., 0., 0.]])</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(torch.Tensor(<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line"><span class="comment"># tensor([[-3.8809e-21,  3.0948e-41,  2.3822e-44,  0.0000e+00],</span></span><br><span class="line"><span class="comment">#         [        nan,  7.2251e+28,  1.3733e-14,  1.8888e+31],</span></span><br><span class="line"><span class="comment">#         [ 4.9656e+28,  4.5439e+30,  7.1426e+22,  1.8759e+28]])</span></span><br><span class="line"></span><br><span class="line">print(torch.Tensor(<span class="number">3</span>,<span class="number">4</span>).uniform_(<span class="number">0</span>,<span class="number">1</span>))</span><br><span class="line"><span class="comment"># tensor([[0.8437, 0.1399, 0.2239, 0.3462],</span></span><br><span class="line"><span class="comment">#         [0.5668, 0.3059, 0.1890, 0.4087],</span></span><br><span class="line"><span class="comment">#         [0.2560, 0.5138, 0.1299, 0.3750]])</span></span><br><span class="line"></span><br><span class="line">print(torch.Tensor(<span class="number">3</span>,<span class="number">4</span>).normal_(<span class="number">0</span>,<span class="number">1</span>))</span><br><span class="line"><span class="comment"># tensor([[-0.5490, -0.0838, -0.1387, -0.5289],</span></span><br><span class="line"><span class="comment">#         [-0.4919, -0.4646, -0.0588,  1.2624],</span></span><br><span class="line"><span class="comment">#         [ 1.1935,  1.5696, -0.8977, -0.1139]])</span></span><br><span class="line"></span><br><span class="line">print(torch.Tensor(<span class="number">3</span>,<span class="number">4</span>).fill_(<span class="number">5</span>))</span><br><span class="line"><span class="comment"># tensor([[5., 5., 5., 5.],</span></span><br><span class="line"><span class="comment">#         [5., 5., 5., 5.],</span></span><br><span class="line"><span class="comment">#         [5., 5., 5., 5.]])</span></span><br><span class="line"></span><br><span class="line">print(torch.arange(<span class="number">1</span>, <span class="number">3</span>, <span class="number">0.4</span>))</span><br><span class="line"><span class="comment"># tensor([1.0000, 1.4000, 1.8000, 2.2000, 2.6000])</span></span><br></pre></td></tr></table></figure></p>
<h4 id="tensor的各种操作"><a href="#tensor的各种操作" class="headerlink" title="tensor的各种操作"></a>tensor的各种操作</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">a = torch.ones(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">b = torch.ones(<span class="number">2</span>,<span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<h5 id="加操作"><a href="#加操作" class="headerlink" title="加操作"></a>加操作</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">print(a+b)                <span class="comment">#方法1</span></span><br><span class="line">c = torch.add(a,b)    <span class="comment">#方法2</span></span><br><span class="line">torch.add(a,b,result)    <span class="comment">#方法3</span></span><br><span class="line">a.add(b)                    <span class="comment">#方法4,将a加上b，且a不变</span></span><br><span class="line">a.add_(b)                <span class="comment">#方法5,将a加上b并将其赋值给a</span></span><br></pre></td></tr></table></figure>
<h5 id="转置操作"><a href="#转置操作" class="headerlink" title="转置操作"></a>转置操作</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(a.t())               <span class="comment"># 打印出tensor a的转置</span></span><br><span class="line">print(a.t_())                 <span class="comment">#将tensor a 转置，并将其赋值给a</span></span><br></pre></td></tr></table></figure>
<h5 id="求最大行和列"><a href="#求最大行和列" class="headerlink" title="求最大行和列"></a>求最大行和列</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.max(tensor,dim)</span><br><span class="line">np.max(array,dim)</span><br></pre></td></tr></table></figure>
<h5 id="和relu功能比较类似。"><a href="#和relu功能比较类似。" class="headerlink" title="和relu功能比较类似。"></a>和relu功能比较类似。</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.clamp(tensor, min, max,out=<span class="literal">None</span>)</span><br><span class="line">np.maximun(x1, x2)  <span class="comment"># x1 and x2 must hava the same shape</span></span><br></pre></td></tr></table></figure>
<h4 id="tensor和numpy转化"><a href="#tensor和numpy转化" class="headerlink" title="tensor和numpy转化"></a>tensor和numpy转化</h4><h5 id="convert-tensor-to-numpy"><a href="#convert-tensor-to-numpy" class="headerlink" title="convert tensor to numpy"></a>convert tensor to numpy</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = torch.ones(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">b = a.numpy()</span><br></pre></td></tr></table></figure>
<h5 id="convert-numpy-to-tensor"><a href="#convert-numpy-to-tensor" class="headerlink" title="convert numpy to tensor"></a>convert numpy to tensor</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a =  numpy.ones(<span class="number">4</span>,<span class="number">3</span>)</span><br><span class="line">b = torch.from_numpy(a)</span><br></pre></td></tr></table></figure>
<h4 id="Variable和Tensor"><a href="#Variable和Tensor" class="headerlink" title="Variable和Tensor"></a>Variable和Tensor</h4><p>Variable<br>图1.Variable</p>
<h5 id="属性"><a href="#属性" class="headerlink" title="属性"></a>属性</h5><p>如图1,Variable wrap a Tensor,and it has six attributes,data,grad,requies_grad,volatile,is_leaf and grad_fn.We can acess the raw tensor through .data operation, we can accumualte gradients w.r.t this Variable into .grad,.Finally , creator attribute will tell us how the Variable were created,we can acess the creator attibute by .grad_fn,if the Variable was created by the user,then the grad_fn is None,else it will show us which Function created the Variable.<br>if the grad_fn is None,we call them graph leaves<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Variable.shape  <span class="comment">#查看Variable的size</span></span><br><span class="line">Variable.size()</span><br></pre></td></tr></table></figure></p>
<h5 id="parameters"><a href="#parameters" class="headerlink" title="parameters"></a>parameters</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.autograd.Variable(data,requires_grad=<span class="literal">False</span>,volatile=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p>requires_grad : indicate whether the backward() will ever need to be called</p>
<h5 id="backward"><a href="#backward" class="headerlink" title="backward"></a>backward</h5><p>backward(gradient=None,retain_graph=None,create_graph=None,retain_variables=None)<br>如果Variable是一个scalar output，我们不需要指定gradient，但是如果Variable不是一个scalar，而是有多个element，我们就需要根据output指定一下gradient，gradient的type可以是tensor也可以是Variable，里面的值为梯度的求值比例，例如<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = Variable(torch.Tensor([<span class="number">3</span>,<span class="number">6</span>,<span class="number">4</span>]),requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = Variable(torch.Tensor([<span class="number">5</span>,<span class="number">3</span>,<span class="number">6</span>]),requires_grad=<span class="literal">True</span>)</span><br><span class="line">z = x+y</span><br><span class="line">z.backward(gradient=torch.Tensor([<span class="number">0.1</span>,<span class="number">1</span>,<span class="number">10</span>]))</span><br></pre></td></tr></table></figure></p>
<p>这里[0.1,1,10]分别表示的是对正常梯度分别乘上$0.1,1,10$，然后将他们累积在leaves Variable上<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">detach()    <span class="comment">#</span></span><br><span class="line">detach_()</span><br><span class="line">register_hook()</span><br><span class="line">register_grad()</span><br></pre></td></tr></table></figure></p>
<h3 id="Function-class-torch-autograd-Funtion"><a href="#Function-class-torch-autograd-Funtion" class="headerlink" title="Function(class torch.autograd.Funtion)"></a>Function(class torch.autograd.Funtion)</h3><h4 id="用法"><a href="#用法" class="headerlink" title="用法"></a>用法</h4><p>Function一般只定义一个操作，并且它无法保存参数，一般适用于激活函数，pooling等，它需要定义三个方法，<strong>init</strong>(),forward(),backward()（这个需要自己定义怎么求导）<br>Model保存了参数，适合定义一层，如线性层(Linear layer),卷积层(conv layer),也适合定义一个网络。<br>和Model的区别，model只需要定义<strong>init()</strong>,foward()方法，backward()不需要我们定义，它可以由自动求导机制计算。</p>
<p>Function定义只是一个函数，forward和backward都只与这个Function的输入和输出有关</p>
<h4 id="functions"><a href="#functions" class="headerlink" title="functions"></a>functions</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyReLU</span><span class="params">(torch.autograd.Function)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    We can implement our own custom autograd Functions by subclassing</span></span><br><span class="line"><span class="string">    torch.autograd.Function and implementing the forward and backward passes</span></span><br><span class="line"><span class="string">    which operate on Tensors.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        In the forward pass we receive a Tensor containing the input and return a</span></span><br><span class="line"><span class="string">        Tensor containing the output. You can cache arbitrary Tensors for use in the</span></span><br><span class="line"><span class="string">        backward pass using the save_for_backward method.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.save_for_backward(input)</span><br><span class="line">        <span class="keyword">return</span> input.clamp(min=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(self, grad_output)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        In the backward pass we receive a Tensor containing the gradient of the loss</span></span><br><span class="line"><span class="string">        with respect to the output, and we need to compute the gradient of the loss</span></span><br><span class="line"><span class="string">        with respect to the input.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        input, = self.saved_tensors</span><br><span class="line">        grad_input = grad_output.clone()</span><br><span class="line">        grad_input[input &lt; <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">        <span class="keyword">return</span> grad_input</span><br><span class="line"></span><br><span class="line">dtype = torch.FloatTensor</span><br><span class="line"><span class="comment"># dtype = torch.cuda.FloatTensor # Uncomment this to run on GPU</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></span><br><span class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></span><br><span class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create random Tensors to hold input and outputs, and wrap them in Variables.</span></span><br><span class="line">x = Variable(torch.randn(N, D_in).type(dtype), requires_grad=<span class="literal">False</span>)</span><br><span class="line">y = Variable(torch.randn(N, D_out).type(dtype), requires_grad=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create random Tensors for weights, and wrap them in Variables.</span></span><br><span class="line">w1 = Variable(torch.randn(D_in, H).type(dtype), requires_grad=<span class="literal">True</span>)</span><br><span class="line">w2 = Variable(torch.randn(H, D_out).type(dtype), requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">1e-6</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</span><br><span class="line">    <span class="comment"># Construct an instance of our MyReLU class to use in our network</span></span><br><span class="line">    relu = MyReLU()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Forward pass: compute predicted y using operations on Variables; we compute</span></span><br><span class="line">    <span class="comment"># ReLU using our custom autograd operation.</span></span><br><span class="line">    y_pred = relu(x.mm(w1)).mm(w2)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute and print loss</span></span><br><span class="line">    loss = (y_pred - y).pow(<span class="number">2</span>).sum()</span><br><span class="line">    print(t, loss.data[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Use autograd to compute the backward pass.</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update weights using gradient descent</span></span><br><span class="line">    w1.data -= learning_rate * w1.grad.data</span><br><span class="line">    w2.data -= learning_rate * w2.grad.data</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Manually zero the gradients after updating weights</span></span><br><span class="line">    w1.grad.data.zero_()</span><br><span class="line">    w2.grad.data.zero_()</span><br></pre></td></tr></table></figure>
<h2 id="torch-nn"><a href="#torch-nn" class="headerlink" title="torch.nn"></a>torch.nn</h2><h3 id="Container"><a href="#Container" class="headerlink" title="Container"></a>Container</h3><h4 id="Module"><a href="#Module" class="headerlink" title="Module"></a>Module</h4><p>Module是所有模型的基类。<br>它有以下几个常用的函数和常见的属性。</p>
<h5 id="常见的函数"><a href="#常见的函数" class="headerlink" title="常见的函数"></a>常见的函数</h5><ul>
<li>Module.forward() # 前向传播</li>
<li>Module.modules()  # 返回module中所有的module，包含整个module，详情可见<a href="htts">module.modules()</a></li>
<li>Module.named_modules() # 同时返回module的名字</li>
<li>Module.children() # 返回module中所有的子module，不包含整个module，详情可见<a href="htts">module.modules()</a></li>
<li>Module.named_children() # 同时返回子module的名字</li>
<li>Module.parameters() # 返回模型的参数</li>
<li>Module.named_parameters() # 同时返回parameter的名字</li>
<li>Module.state_dict()  # 保存模型参数</li>
<li>Module.load_state_dict()  # 加载模型参数</li>
<li>Module.to() # </li>
<li>Module.cuda()</li>
<li>Module.cpu() </li>
<li>Module.apply(fn) # 对模型中的每一个module都调用fn函数</li>
<li>Module._apply()</li>
</ul>
<h5 id="常见的属性"><a href="#常见的属性" class="headerlink" title="常见的属性"></a>常见的属性</h5><ul>
<li>self._backend = thnn_backend</li>
<li>self._parameters = OrderedDict()</li>
<li>self._buffers = OrderedDict()</li>
<li>self._backward_hooks = OrderedDict()</li>
<li>self._forward_hooks = OrderedDict()</li>
<li>self._forward_pre_hooks = OrderedDict()</li>
<li>self._state_dict_hooks = OrderedDict()</li>
<li>self._load_state_dict_pre_hooks = OrderedDict()</li>
<li>self._modules = OrderedDict()</li>
<li>self.training = True</li>
</ul>
<h5 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h5><p>关于module,children_modules,parameters的<a href>代码</a></p>
<h4 id="Sequential"><a href="#Sequential" class="headerlink" title="Sequential"></a>Sequential</h4><h3 id="Convolution-Layers"><a href="#Convolution-Layers" class="headerlink" title="Convolution Layers"></a>Convolution Layers</h3><h4 id="torch-nn-Conv2d"><a href="#torch-nn-Conv2d" class="headerlink" title="torch.nn.Conv2d"></a>torch.nn.Conv2d</h4><h5 id="类声明"><a href="#类声明" class="headerlink" title="类声明"></a>类声明</h5><p>应用2维卷积到输入信号中。<br>torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)</p>
<blockquote>
<p>Applies a 2D convolution over an input signal composed of several input planes.</p>
</blockquote>
<h5 id="参数声明"><a href="#参数声明" class="headerlink" title="参数声明"></a>参数声明</h5><p>in_channels (int) – 输入图像的通道<br>out_channels (int) – 卷积产生的输出通道数（也就是有几个kernel）<br>kernel_size (int or tuple) – kernel的大小<br>stride (int or tuple, optional) – 卷积的步长，默认为$1$<br>padding (int or tuple, optional) – 向输入数据的各边添加Zero-padding的数量，默认为$0$<br>dilation (int or tuple, optional) – Spacing between kernel elements. Default: 1<br>groups (int, optional) – Number of blocked connections from input channels to output channels. Default: 1<br>bias (bool, optional) – If True, adds a learnable bias to the output</p>
<h5 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h5><h6 id="例子1"><a href="#例子1" class="headerlink" title="例子1"></a>例子1</h6><p>用$6$个$5\times 5$的filter处理维度为$32\times 32\times 1$的图像。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">model = torch.nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">input = torch.randn(<span class="number">16</span>, <span class="number">1</span>, <span class="number">32</span>, <span class="number">32</span>)</span><br><span class="line">output = model(input)</span><br><span class="line">print(output.size())</span><br></pre></td></tr></table></figure></p>
<p>输出是：</p>
<blockquote>
<p>torch.Size([16, 6, 28, 28])</p>
</blockquote>
<h6 id="例子2"><a href="#例子2" class="headerlink" title="例子2"></a>例子2</h6><p>stride和padding<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"></span><br><span class="line">inputs = Variable(torch.randn(<span class="number">64</span>,<span class="number">3</span>,<span class="number">32</span>,<span class="number">32</span>))</span><br><span class="line"></span><br><span class="line">m1 = nn.Conv2d(<span class="number">3</span>,<span class="number">16</span>,<span class="number">3</span>)</span><br><span class="line">print(m1)</span><br><span class="line">output1 = m1(inputs)</span><br><span class="line">print(output1.size())</span><br><span class="line"></span><br><span class="line">m2 = nn.Conv2d(<span class="number">3</span>, <span class="number">16</span>, <span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">print(m2)</span><br><span class="line">output2 = m2(inputs)</span><br><span class="line">print(output2.size())</span><br></pre></td></tr></table></figure></p>
<p>输出</p>
<blockquote>
<p>Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))<br>torch.Size([64, 16, 30, 30])<br>Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))<br>torch.Size([64, 16, 32, 32])</p>
</blockquote>
<h3 id="Pooling-Layers"><a href="#Pooling-Layers" class="headerlink" title="Pooling Layers"></a>Pooling Layers</h3><h4 id="MaxPool2dd"><a href="#MaxPool2dd" class="headerlink" title="MaxPool2dd"></a>MaxPool2dd</h4><p>MaxPool2d这个layer stride默认是和kernel_size相同的<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch import nn</span><br><span class="line">from torch.autograd import Variable</span><br><span class="line"></span><br><span class="line"># maxpool2d</span><br><span class="line"># class torch.nn.MaxPool2d(kernel_size,stride=None,padding=0,dilation=1,return_indices=False,ceil_mode=False)</span><br><span class="line"></span><br><span class="line">print(&quot;input:&quot;)</span><br><span class="line">input = Variable(torch.randn(30,20,32,32))</span><br><span class="line">print(input.size())</span><br><span class="line"></span><br><span class="line">m2 = nn.MaxPool2d(5)</span><br><span class="line">print(m2)</span><br><span class="line"></span><br><span class="line">for param in m2.parameters():</span><br><span class="line">  print(param)</span><br><span class="line"></span><br><span class="line">print(m2.state_dict().keys())</span><br><span class="line"></span><br><span class="line">output = m2(input)</span><br><span class="line">print(&quot;output:&quot;)</span><br><span class="line">print(output.size())</span><br></pre></td></tr></table></figure></p>
<p>输出</p>
<blockquote>
<p>input:<br>torch.Size([30, 20, 32, 32])<br>MaxPool2d (size=(5, 5), stride=(5, 5), dilation=(1, 1))<br>[]<br>output:<br>torch.Size([30, 20, 6, 6])</p>
</blockquote>
<h3 id="Padding-Layers"><a href="#Padding-Layers" class="headerlink" title="Padding Layers"></a>Padding Layers</h3><h3 id="Linear-layers"><a href="#Linear-layers" class="headerlink" title="Linear layers"></a>Linear layers</h3><h3 id="Dropout-layers"><a href="#Dropout-layers" class="headerlink" title="Dropout layers"></a>Dropout layers</h3><h4 id="Drop2D"><a href="#Drop2D" class="headerlink" title="Drop2D"></a>Drop2D</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line">m = nn.Dropout2d(<span class="number">0.3</span>)</span><br><span class="line">print(m)</span><br><span class="line">inputs = torch.randn(<span class="number">1</span>,<span class="number">28</span>,<span class="number">28</span>)</span><br><span class="line">outputs = m(inputs)</span><br><span class="line">print(outputs)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<blockquote>
<p>Dropout2d(p=0.3)<br>([[[ 0.8535,  1.0314,  2.7904,  1.2136,  2.7561, -2.0429,  0.0772,<br>     -1.9372, -0.0864, -1.4132, -0.1648,  0.2403,  0.5727,  0.8102,<br>      0.4544,  0.1414,  0.1547, -0.9266, -0.6033,  0.5813, -1.3541,<br>     -0.0536,  0.9574,  0.0554,  0.8368,  0.7633, -0.3377, -1.4293],<br>    [ 0.0000,  0.0000, -0.0000, -0.0000,  0.0000, -0.0000,  0.0000,<br>      0.0000,  0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000,<br>      0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000,<br>     -0.0000,  0.0000,  0.0000, -0.0000, -0.0000, -0.0000, -0.0000],<br>      …<br>    [ 0.6452, -0.6455,  0.2370,  0.1088, -0.5421, -0.5120, -2.2915,<br>      0.2061,  1.6384,  2.2276,  2.4022,  0.2033,  0.6984,  0.1254,<br>      1.1627,  1.0699, -2.1868,  1.1293, -0.7030,  0.0454, -1.5428,<br>      -2.4052, -0.3204, -1.5984,  0.1282,  0.2127, -2.3506, -2.2395]]])</p>
</blockquote>
<p>会发现输出的数组中有很多被置为$0$了。</p>
<h3 id="Loss-function"><a href="#Loss-function" class="headerlink" title="Loss function"></a>Loss function</h3><h2 id="torch-nn-functional"><a href="#torch-nn-functional" class="headerlink" title="torch.nn.functional"></a>torch.nn.functional</h2><h3 id="convoludion-functions"><a href="#convoludion-functions" class="headerlink" title="convoludion functions"></a>convoludion functions</h3><h4 id="conv2d"><a href="#conv2d" class="headerlink" title="conv2d"></a>conv2d</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torch.nn as nn</span><br><span class="line">import torch.nn.functional as F</span><br><span class="line">from torch.autograd import Variable</span><br><span class="line"></span><br><span class="line">inputs = Variable(torch.randn(64,3,32,32))</span><br><span class="line"></span><br><span class="line">filters1 = Variable(torch.randn(16,3,3,3))</span><br><span class="line">output1 = F.conv2d(inputs,filters1)</span><br><span class="line">print(output1.size())</span><br><span class="line"></span><br><span class="line">filters2 = Variable(torch.randn(16,3,3,3))</span><br><span class="line">output2 = F.conv2d(inputs,filters2,padding=1)</span><br><span class="line">print(output2.size())</span><br></pre></td></tr></table></figure>
<p>输出</p>
<blockquote>
<p>torch.Size([64, 16, 30, 30])<br>torch.Size([64, 16, 32, 32])</p>
</blockquote>
<h3 id="relu-functions"><a href="#relu-functions" class="headerlink" title="relu functions"></a>relu functions</h3><h3 id="pooling-functions"><a href="#pooling-functions" class="headerlink" title="pooling functions"></a>pooling functions</h3><h3 id="dropout-functions"><a href="#dropout-functions" class="headerlink" title="dropout functions"></a>dropout functions</h3><h4 id="例子-1"><a href="#例子-1" class="headerlink" title="例子"></a>例子</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"> </span><br><span class="line">x = torch.randn(<span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line">y = F.dropout(x, <span class="number">0.5</span>, <span class="literal">True</span>)</span><br><span class="line">y = F.dropout2d(x, <span class="number">0.5</span>)</span><br><span class="line"> </span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure>
<p>注意$9$中说的问题，不过可能已经被改正了，注意一些就是了。</p>
<h3 id="linear-functions"><a href="#linear-functions" class="headerlink" title="linear functions"></a>linear functions</h3><h3 id="loss-functions"><a href="#loss-functions" class="headerlink" title="loss functions"></a>loss functions</h3><h2 id="torch-optim"><a href="#torch-optim" class="headerlink" title="torch.optim"></a>torch.optim</h2><h3 id="基类class-Optimizer-object"><a href="#基类class-Optimizer-object" class="headerlink" title="基类class Optimizer(object)"></a>基类class Optimizer(object)</h3><p>Optimizer是所有optimizer的基类。<br>调用任何优化器都要先初始化Optimizer类，这里拿Adam优化器举例子。Adam optimizer的init函数如下所示：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Adam</span><span class="params">(Optimizer)</span>:</span></span><br><span class="line">    <span class="string">r"""Implements Adam algorithm.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    It has been proposed in `Adam: A Method for Stochastic Optimization`_.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        params (iterable): iterable of parameters to optimize or dicts defining</span></span><br><span class="line"><span class="string">            parameter groups</span></span><br><span class="line"><span class="string">        lr (float, optional): learning rate (default: 1e-3)</span></span><br><span class="line"><span class="string">        betas (Tuple[float, float], optional): coefficients used for computing</span></span><br><span class="line"><span class="string">            running averages of gradient and its square (default: (0.9, 0.999))</span></span><br><span class="line"><span class="string">        eps (float, optional): term added to the denominator to improve</span></span><br><span class="line"><span class="string">            numerical stability (default: 1e-8)</span></span><br><span class="line"><span class="string">        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)</span></span><br><span class="line"><span class="string">        amsgrad (boolean, optional): whether to use the AMSGrad variant of this</span></span><br><span class="line"><span class="string">            algorithm from the paper `On the Convergence of Adam and Beyond`_</span></span><br><span class="line"><span class="string">            (default: False)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    .. _Adam\: A Method for Stochastic Optimization:</span></span><br><span class="line"><span class="string">        https://arxiv.org/abs/1412.6980</span></span><br><span class="line"><span class="string">    .. _On the Convergence of Adam and Beyond:</span></span><br><span class="line"><span class="string">        https://openreview.net/forum?id=ryQu7f-RZ</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, params, lr=<span class="number">1e-3</span>, betas=<span class="params">(<span class="number">0.9</span>, <span class="number">0.999</span>)</span>, eps=<span class="number">1e-8</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 weight_decay=<span class="number">0</span>, amsgrad=False)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="number">0.0</span> &lt;= lr:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">"Invalid learning rate: &#123;&#125;"</span>.format(lr))</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="number">0.0</span> &lt;= eps:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">"Invalid epsilon value: &#123;&#125;"</span>.format(eps))</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="number">0.0</span> &lt;= betas[<span class="number">0</span>] &lt; <span class="number">1.0</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">"Invalid beta parameter at index 0: &#123;&#125;"</span>.format(betas[<span class="number">0</span>]))</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="number">0.0</span> &lt;= betas[<span class="number">1</span>] &lt; <span class="number">1.0</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">"Invalid beta parameter at index 1: &#123;&#125;"</span>.format(betas[<span class="number">1</span>]))</span><br><span class="line">        defaults = dict(lr=lr, betas=betas, eps=eps,</span><br><span class="line">                        weight_decay=weight_decay, amsgrad=amsgrad)</span><br><span class="line">        super(Adam, self).__init__(params, defaults)</span><br></pre></td></tr></table></figure></p>
<p>上述代码将学习率lr,beta,epsilon,weight_decay,amsgrad等封装在一个dict中，然后将其传给Optimizer的init函数，其代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Optimizer</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">r"""Base class for all optimizers.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    .. warning::</span></span><br><span class="line"><span class="string">        Parameters need to be specified as collections that have a deterministic</span></span><br><span class="line"><span class="string">        ordering that is consistent between runs. Examples of objects that don't</span></span><br><span class="line"><span class="string">        satisfy those properties are sets and iterators over values of dictionaries.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        params (iterable): an iterable of :class:`torch.Tensor` s or</span></span><br><span class="line"><span class="string">            :class:`dict` s. Specifies what Tensors should be optimized.</span></span><br><span class="line"><span class="string">        defaults: (dict): a dict containing default values of optimization</span></span><br><span class="line"><span class="string">            options (used when a parameter group doesn't specify them).</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, params, defaults)</span>:</span></span><br><span class="line">        self.defaults = defaults</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> isinstance(params, torch.Tensor):</span><br><span class="line">            <span class="keyword">raise</span> TypeError(<span class="string">"params argument given to the optimizer should be "</span></span><br><span class="line">                            <span class="string">"an iterable of Tensors or dicts, but got "</span> +</span><br><span class="line">                            torch.typename(params))</span><br><span class="line"></span><br><span class="line">        self.state = defaultdict(dict)</span><br><span class="line">        self.param_groups = []</span><br><span class="line"></span><br><span class="line">        param_groups = list(params)</span><br><span class="line">        <span class="keyword">if</span> len(param_groups) == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">"optimizer got an empty parameter list"</span>)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> isinstance(param_groups[<span class="number">0</span>], dict):</span><br><span class="line">            param_groups = [&#123;<span class="string">'params'</span>: param_groups&#125;]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> param_group <span class="keyword">in</span> param_groups:</span><br><span class="line">            self.add_param_group(param_group)</span><br></pre></td></tr></table></figure></p>
<p>从这里可以看出来，每个pytorch给出的optimizer至少有以下三个属性和四个函数：<br>属性：</p>
<ul>
<li>self.defaults # 字典类型，主要包含学习率等值</li>
<li>self.state # defaultdict(\<class 'dict'\>, {}) state存放的是</class></li>
<li>self.param_gropus # \<class 'list'\>:[]，prama_groups是一个字典类型的列表，用来存放parameters。</class></li>
</ul>
<p>函数：</p>
<ul>
<li>self.zero_grad()  # 将optimizer中参数的梯度置零</li>
<li>self.step()  # 将梯度应用在参数上</li>
<li>self.state_dict() # 返回optimizer的state,包括state和param_groups。</li>
<li>self.load_state_dict()  # 加载optimizer的state。</li>
<li>self.add_param_group()  # 将一个param group添加到param_groups。可以用在fine-tune上，只添加我们需要训练的层数，然后其他层不动。</li>
</ul>
<p>如果param已经是一个字典列表的话，就无需操作，否则就需要把param转化成一个字典param_groups。然后对param_groups中的每一个param_group调用add_param_group(param_group)函数将param_group字典和defaults字典拼接成一个新的param_group字典添加到self.param_groups中。</p>
<h2 id="torchvision"><a href="#torchvision" class="headerlink" title="torchvision"></a>torchvision</h2><h3 id="torchvision-datasets"><a href="#torchvision-datasets" class="headerlink" title="torchvision.datasets"></a>torchvision.datasets</h3><p>torchvision提供了很多数据集<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line">print(torchvision.datasets.__all__)</span><br></pre></td></tr></table></figure></p>
<blockquote>
<p>(‘LSUN’, ‘LSUNClass’, ‘ImageFolder’, ‘DatasetFolder’, ‘FakeData’, ‘CocoCaptions’,     ‘CocoDetection’, ‘CIFAR10’, ‘CIFAR100’, ‘EMNIST’, ‘FashionMNIST’, ‘MNIST’, ‘STL10’,     ‘SVHN’, ‘PhotoTour’, ‘SEMEION’, ‘Omniglot’)</p>
</blockquote>
<h3 id="CIFAR10"><a href="#CIFAR10" class="headerlink" title="CIFAR10"></a>CIFAR10</h3><h4 id="原型"><a href="#原型" class="headerlink" title="原型"></a>原型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">calss torchvision.datasets.CIFAR10(root, train=<span class="literal">True</span>, transform=<span class="literal">None</span>, target_transform=<span class="literal">None</span>, download=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<h4 id="参数-1"><a href="#参数-1" class="headerlink" title="参数"></a>参数</h4><p>root (string) – cifar-10-batches-py的存放目录或者download设置为True时将会存放的目录。<br>train (bool, optional) – 设置为True的时候, 从training set创建dataset, 否则从test set创建dataset.<br>transform (callable, optional) – 输入是一个 PIL image，返回一个transformed的版本。如，transforms.RandomCrop<br>target_transform (callable, optional) – A function/transform that takes in the target and transforms it.<br>download (bool, optional) – If true, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again.</p>
<h4 id="例子-2"><a href="#例子-2" class="headerlink" title="例子"></a>例子</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line">trainset = torchvision.datasets.CIFAR100(root=<span class="string">"./datasets"</span>, train=<span class="literal">True</span>, transform=    <span class="literal">None</span>, download=<span class="literal">True</span>)</span><br><span class="line">testset = torchvision.datasets.CIFAR100(root=<span class="string">"./datasets"</span>, train=<span class="literal">False</span>, transform=    <span class="literal">None</span>, download=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h3 id="torchvision-models"><a href="#torchvision-models" class="headerlink" title="torchvision.models"></a>torchvision.models</h3><p>模型</p>
<h3 id="torchvision-transforms"><a href="#torchvision-transforms" class="headerlink" title="torchvision.transforms"></a>torchvision.transforms</h3><p>transform</p>
<h3 id="torchvision-utils"><a href="#torchvision-utils" class="headerlink" title="torchvision.utils"></a>torchvision.utils</h3><p>一些工具包</p>
<h2 id="torch-utils-data"><a href="#torch-utils-data" class="headerlink" title="torch.utils.data"></a>torch.utils.data</h2><h3 id="Dataloader"><a href="#Dataloader" class="headerlink" title="Dataloader"></a>Dataloader</h3><h4 id="原型-1"><a href="#原型-1" class="headerlink" title="原型"></a>原型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">torch</span>.<span class="title">utils</span>.<span class="title">data</span>.<span class="title">DataLoader</span><span class="params">(dataset, batch_size=<span class="number">1</span>, shuffle=False, sampler=None, batch_sampler=None, num_workers=<span class="number">0</span>, collate_fn=&lt;function default_collate&gt;, pin_memory=False, drop_last=False, timeout=<span class="number">0</span>, worker_init_fn=None)</span></span></span><br></pre></td></tr></table></figure>
<h4 id="参数-2"><a href="#参数-2" class="headerlink" title="参数"></a>参数</h4><p>dataset (Dataset) – 从哪加载数据<br>batch_size (int, optional) – batch大小 (default: 1).<br>shuffle (bool, optional) – 每个epoch的数据是否打乱 (default: False).<br>sampler (Sampler, optional) – 定义采样策略。如果指定这个参数, shuffle必须是False.<br>batch_sampler (Sampler, optional) – like sampler, but returns a batch of indices at a time. Mutually exclusive with batch_size, shuffle, sampler, and drop_last.<br>num_workers (int, optional) – 多少个子进程用来进行数据加载。0代表使用主进程加载数据 (default: 0)<br>collate_fn (callable, optional) – merges a list of samples to form a mini-batch.<br>pin_memory (bool, optional) – If True, the data loader will copy tensors into CUDA pinned memory before returning them.<br>drop_last (bool, optional) – set to True to drop the last incomplete batch, if the dataset size is not divisible by the batch size. If False and the size of dataset is not divisible by the batch size, then the last batch will be smaller. (default: False)<br>timeout (numeric, optional) – if positive, the timeout value for collecting a batch from workers. Should always be non-negative. (default: 0)<br>worker_init_fn (callable, optional) – If not None, this will be called on each worker subprocess with the worker id (an int in [0, num_workers - 1]) as input, after seeding and before data loading. (default: None)</p>
<h4 id="例子-3"><a href="#例子-3" class="headerlink" title="例子"></a>例子</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line">dataset = torchvision.datasets.CIFAR100(root=<span class="string">'./data'</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=<span class="literal">None</span>)</span><br><span class="line">train_loader = torch.utils.data.DataLoader(dataset, bath_size=<span class="number">16</span>, shuffle=<span class="literal">False</span>, num_worker=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<h4 id="如何访问DataLoader返回值"><a href="#如何访问DataLoader返回值" class="headerlink" title="如何访问DataLoader返回值"></a>如何访问DataLoader返回值</h4><p>train_loader不是整数，所以不能用range，这里用enumerate()，i是<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i, data <span class="keyword">in</span> enumerate(train_loader):</span><br><span class="line">    images, labels = data</span><br></pre></td></tr></table></figure></p>
<h2 id="torch-distributed"><a href="#torch-distributed" class="headerlink" title="torch.distributed"></a>torch.distributed</h2><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>1.<a href="https://pytorch.org/docs/stable/torch.html" target="_blank" rel="noopener">https://pytorch.org/docs/stable/torch.html</a><br>2.<a href="https://pytorch.org/docs/stable/nn.html" target="_blank" rel="noopener">https://pytorch.org/docs/stable/nn.html</a><br>3.<a href="http://pytorch.org/tutorials/beginner/pytorch_with_examples.html" target="_blank" rel="noopener">http://pytorch.org/tutorials/beginner/pytorch_with_examples.html</a><br>4.<a href="https://discuss.pytorch.org/t/distributed-model-parallelism/10377" target="_blank" rel="noopener">https://discuss.pytorch.org/t/distributed-model-parallelism/10377</a><br>5.<a href="https://ptorch.com/news/40.html" target="_blank" rel="noopener">https://ptorch.com/news/40.html</a><br>6.<a href="https://discuss.pytorch.org/t/distributed-data-parallel-freezes-without-error-message/8009" target="_blank" rel="noopener">https://discuss.pytorch.org/t/distributed-data-parallel-freezes-without-error-message/8009</a><br>7.<a href="https://discuss.pytorch.org/t/runtimeerror-cudnn-status-arch-mismatch/3580" target="_blank" rel="noopener">https://discuss.pytorch.org/t/runtimeerror-cudnn-status-arch-mismatch/3580</a><br>8.<a href="https://discuss.pytorch.org/t/error-when-using-cudnn/577/7" target="_blank" rel="noopener">https://discuss.pytorch.org/t/error-when-using-cudnn/577/7</a><br>9.<a href="https://www.zhihu.com/question/67209417" target="_blank" rel="noopener">https://www.zhihu.com/question/67209417</a><br>10.<a href="https://pytorch.org/docs/stable/distributions.html#categorical" target="_blank" rel="noopener">https://pytorch.org/docs/stable/distributions.html#categorical</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://mxxhcm.github.io/2019/03/13/dict-values-object-does-not-support-indexing/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="马晓鑫爱马荟荟">
      <meta itemprop="description" content="记录硕士三年自己的积累">
      <meta itemprop="image" content="/images/favicon.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="mxxhcm's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/03/13/dict-values-object-does-not-support-indexing/" class="post-title-link" itemprop="http://mxxhcm.github.io/page/7/index.html">'dict_values' object does not support indexing</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-03-13 10:40:03" itemprop="dateCreated datePublished" datetime="2019-03-13T10:40:03+08:00">2019-03-13</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-05-07 00:22:27" itemprop="dateModified" datetime="2019-05-07T00:22:27+08:00">2019-05-07</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Error/" itemprop="url" rel="index"><span itemprop="name">Error</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="python3-dict"><a href="#python3-dict" class="headerlink" title="python3 dict"></a>python3 dict</h2><p>python3 中调用字典对象的一些函数，返回值是view objects。如果要转换为list的话，需要使用list()强制转换。<br>而python2的返回值直接就是list。</p>
<blockquote>
<p>The objects returned by dict.keys(), dict.values() and dict.items() are view objects. They provide a dynamic view on the dictionary’s entries, which means that when the dictionary changes, the view reflects these changes.</p>
</blockquote>
<h3 id="代码示例"><a href="#代码示例" class="headerlink" title="代码示例"></a>代码示例</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">m_dict = &#123;<span class="string">'a'</span>: <span class="number">10</span>, <span class="string">'b'</span>: <span class="number">20</span>&#125;</span><br><span class="line">values = m_dict.values()</span><br><span class="line">print(type(values))</span><br><span class="line">print(values)</span><br><span class="line">print(<span class="string">"\n"</span>)</span><br><span class="line"></span><br><span class="line">items = m_dict.items()</span><br><span class="line">print(type(items))</span><br><span class="line">print(items)</span><br><span class="line">print(<span class="string">"\n"</span>)</span><br><span class="line"></span><br><span class="line">keys = m_dict.keys()</span><br><span class="line">print(type(keys))</span><br><span class="line">print(keys)</span><br><span class="line">print(<span class="string">"\n"</span>)</span><br></pre></td></tr></table></figure>
<p>如果使用python3执行以上代码，输出结果如下所示：</p>
<blockquote>
<p>class ‘dict_values’<br>dict_values([10, 20])</p>
<p>class ‘dict_items’<br>dict_items([(‘a’, 10), (‘b’, 20)])</p>
<p>class ‘dict_keys’<br>dict_keys([‘a’, ‘b’])</p>
</blockquote>
<p>如果使用python2执行以上代码，输出结果如下所示：</p>
<blockquote>
<p>type ‘list’<br>[10, 20]                                                                                </p>
<p>type ‘list’<br>[(‘a’, 10), (‘b’, 20)]                                                                  </p>
<p>type ‘list’<br>[‘a’, ‘b’]</p>
</blockquote>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>1.<a href="https://www.cnblogs.com/timxgb/p/8905290.html" target="_blank" rel="noopener">https://www.cnblogs.com/timxgb/p/8905290.html</a><br>2.<a href="https://docs.python.org/3/library/stdtypes.html#dictionary-view-objects" target="_blank" rel="noopener">https://docs.python.org/3/library/stdtypes.html#dictionary-view-objects</a><br>3.<a href="https://stackoverflow.com/questions/43663206/typeerror-unsupported-operand-types-for-dict-values-and-int" target="_blank" rel="noopener">https://stackoverflow.com/questions/43663206/typeerror-unsupported-operand-types-for-dict-values-and-int</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://mxxhcm.github.io/2019/03/09/markdown帮助/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="马晓鑫爱马荟荟">
      <meta itemprop="description" content="记录硕士三年自己的积累">
      <meta itemprop="image" content="/images/favicon.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="mxxhcm's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/03/09/markdown帮助/" class="post-title-link" itemprop="http://mxxhcm.github.io/page/7/index.html">markdown帮助</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-03-09 19:53:32" itemprop="dateCreated datePublished" datetime="2019-03-09T19:53:32+08:00">2019-03-09</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-05-07 00:22:27" itemprop="dateModified" datetime="2019-05-07T00:22:27+08:00">2019-05-07</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/工具/" itemprop="url" rel="index"><span itemprop="name">工具</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h2><h3 id="代码引用"><a href="#代码引用" class="headerlink" title="代码引用"></a>代码引用</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br></pre></td></tr></table></figure>
<h3 id="文字引用"><a href="#文字引用" class="headerlink" title="文字引用"></a>文字引用</h3><blockquote>
<p>实际是人类进步的阶梯。　－－高尔基</p>
</blockquote>
<h2 id="表格"><a href="#表格" class="headerlink" title="表格"></a>表格</h2><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">name</th>
<th style="text-align:center">age</th>
<th style="text-align:center">gender</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Alice</td>
<td style="text-align:center">11</td>
<td style="text-align:center">female</td>
</tr>
<tr>
<td style="text-align:center">Bob</td>
<td style="text-align:center">82</td>
<td style="text-align:center">male</td>
</tr>
</tbody>
</table>
</div>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://mxxhcm.github.io/2019/03/07/tensorflow踩坑（不定期更新）/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="马晓鑫爱马荟荟">
      <meta itemprop="description" content="记录硕士三年自己的积累">
      <meta itemprop="image" content="/images/favicon.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="mxxhcm's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/03/07/tensorflow踩坑（不定期更新）/" class="post-title-link" itemprop="http://mxxhcm.github.io/page/7/index.html">tensorflow踩坑（不定期更新）</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-03-07 14:51:01" itemprop="dateCreated datePublished" datetime="2019-03-07T14:51:01+08:00">2019-03-07</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-05-07 00:22:27" itemprop="dateModified" datetime="2019-05-07T00:22:27+08:00">2019-05-07</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/python/" itemprop="url" rel="index"><span itemprop="name">python</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><ol>
<li>TypeError: The value of a feed cannot be a tf.Tensor object<br>Sess.run(train, feed_dict={x:images, y:labels}的输入不能是tensor，可以使用sess.run(tensor)得到numpy.array形式的数据再喂给feed_dict。<blockquote>
<p>Once you have launched a sess, you can use your_tensor.eval(session=sess) or sess.run(your_tensor) to get you feed tensor into the format of numpy.array and then feed it to your placeholder.</p>
</blockquote>
</li>
</ol>
<h2 id="用法"><a href="#用法" class="headerlink" title="用法"></a>用法</h2><h3 id="tf-app-flags"><a href="#tf-app-flags" class="headerlink" title="tf.app.flags"></a>tf.app.flags</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">flags.py</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">flags = tf.app.flags</span><br><span class="line">flags.DEFINE_string(<span class="string">'model'</span>, <span class="string">'mxx'</span>, <span class="string">'Type of model'</span>)</span><br><span class="line">flags.DEFINE_boolean(<span class="string">'gpu'</span>,<span class="string">'True'</span>, <span class="string">'use gpu?'</span>)</span><br><span class="line">FLAGS = flags.FLAGS</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(_)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> k,v <span class="keyword">in</span> FLAGS.flag_values_dict().items():</span><br><span class="line">        print(k, v)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    tf.app.run(main)</span><br></pre></td></tr></table></figure>
<p>传递参数的方法有两种，一种是命令行~$:python flags.py —model hhhh ，一种是pycharm中传递参数。</p>
<h3 id="tf-train-Saver"><a href="#tf-train-Saver" class="headerlink" title="tf.train.Saver()"></a>tf.train.Saver()</h3><h4 id="代码示例-可运行"><a href="#代码示例-可运行" class="headerlink" title="代码示例(可运行)"></a>代码示例(可运行)</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">graph = tf.Graph()</span><br><span class="line"><span class="keyword">with</span> graph.as_default():</span><br><span class="line">    W = tf.Variable([<span class="number">0.3</span>], dtype=tf.float32)</span><br><span class="line">    b = tf.Variable([<span class="number">-0.3</span>], dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># input and output</span></span><br><span class="line">    x = tf.placeholder(tf.float32)</span><br><span class="line">    y = tf.placeholder(tf.float32)</span><br><span class="line">    predicted_y = W*x+b</span><br><span class="line"></span><br><span class="line">    <span class="comment"># MSE loss</span></span><br><span class="line">    loss = tf.reduce_mean(tf.square(y - predicted_y))</span><br><span class="line">    <span class="comment"># optimizer</span></span><br><span class="line">    optimizer = tf.train.GradientDescentOptimizer(<span class="number">0.01</span>)</span><br><span class="line">    train_op = optimizer.minimize(loss)</span><br><span class="line"></span><br><span class="line">inputs = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]</span><br><span class="line">outputs = [<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session(graph=graph) <span class="keyword">as</span> sess:</span><br><span class="line">    saver = tf.train.Saver()</span><br><span class="line">    init_op = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5000</span>):</span><br><span class="line">        sess.run(train_op, feed_dict=&#123;x: inputs, y: outputs&#125;)</span><br><span class="line">    l_, W_, b_ = sess.run([loss, W, b], feed_dict=&#123;x: inputs, y: outputs&#125;)</span><br><span class="line">    print(<span class="string">"loss: "</span>, l_, <span class="string">"w: "</span>, W_, <span class="string">"b:"</span>, b_)</span><br><span class="line">    checkpoint = <span class="string">"./checkpoint/saver1.ckpt"</span></span><br><span class="line">    save_path = saver.save(sess, checkpoint)</span><br><span class="line">    print(<span class="string">"Model has been saved in %s."</span> % save_path)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session(graph=graph) <span class="keyword">as</span> sess:</span><br><span class="line">    saver = tf.train.Saver()</span><br><span class="line">    saver.restore(sess, checkpoint)</span><br><span class="line">    l_, W_, b_ = sess.run([loss, W, b], feed_dict=&#123;x: inputs, y: outputs&#125;)</span><br><span class="line">    print(<span class="string">"loss: "</span>, l_, <span class="string">"w: "</span>, W_, <span class="string">"b:"</span>, b_)</span><br><span class="line">    print(<span class="string">"Model has been restored."</span>)</span><br></pre></td></tr></table></figure>
<h3 id="tf-summary"><a href="#tf-summary" class="headerlink" title="tf.summary"></a>tf.summary</h3><h4 id="api"><a href="#api" class="headerlink" title="api"></a>api</h4><p>函数<br>tf.summary.scalar(name, tensor, collections=None, family=None)<br>定义一个summary标量</p>
<p>类<br>tf.summary.FileWriter(self, logdir,　graph=None, max_queue=10,flush_secs=120, graph_def=None, filename_suffix=None)<br>定义将数据写入文件的类</p>
<p>类内函数<br>tf.summary.FileWriter.add_summary(self, summary, global_step=None)<br>将summary类型变量转换为事件 </p>
<h4 id="代码示例-无法运行"><a href="#代码示例-无法运行" class="headerlink" title="代码示例(无法运行)"></a>代码示例(无法运行)</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">summary_loss = tf.summary.scalar(<span class="string">'loss'</span>, loss)</span><br><span class="line">summary_weights = tf.summary.scalar(<span class="string">'weights'</span>, weights)</span><br><span class="line">writer = tf.summary.FileWriter(<span class="string">"./summary/"</span>)</span><br><span class="line">sess = tf.Session()</span><br><span class="line">loss_, weights_ = sess.run([summary_loss, summary_weights], feed_dict=&#123;&#125;)</span><br><span class="line">writer.add_summary(loss_)</span><br><span class="line">writer.add_summary(weights_)</span><br><span class="line"><span class="comment"># 或者先把loss和weights merge 一下，然后再run</span></span><br><span class="line">merged = tf.summary.merge_all()</span><br><span class="line">merged_ = sess.rum([merged], feed_dict=&#123;&#125;)</span><br><span class="line">writer.add_summary(merged_, global_step)</span><br></pre></td></tr></table></figure>
<p>使用tensorboard —logdir ./summary/打开tensorboard</p>
<h3 id="tf-multinomial"><a href="#tf-multinomial" class="headerlink" title="tf.multinomial"></a>tf.multinomial</h3><p>多项分布，采样。<br>如下<a href="https://github.com/mxxhcm/myown_code/blob/master/tf/some_ops/tf_multinominal.py" target="_blank" rel="noopener">示例</a><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># tf.multinomial(logits, num_samples, seed=None, name=None)</span></span><br><span class="line"><span class="comment"># logits 是一个二维张量，指定概率，num_samples是采样个数</span></span><br><span class="line">sess = tf.Session()</span><br><span class="line">sample = tf.multinomial([[<span class="number">5.0</span>, <span class="number">5.0</span>, <span class="number">5.0</span>], [<span class="number">5.0</span>, <span class="number">4</span>, <span class="number">3</span>]], <span class="number">10</span>) <span class="comment"># 注意logits必须是float</span></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">  print(sess.run(sample))</span><br></pre></td></tr></table></figure></p>
<p>输出结果如下:</p>
<blockquote>
<p>[[2 1 2 1 0 2 1 1 1 0]<br> [1 0 0 1 0 1 0 1 0 0]]<br>[[2 2 0 2 2 0 2 0 1 2]<br> [1 0 0 2 0 1 0 1 1 0]]<br>[[0 0 0 2 0 0 1 2 0 1]<br> [0 0 0 1 0 1 0 0 0 0]]<br>[[2 1 0 1 1 1 0 0 2 0]<br> [1 0 0 2 0 0 0 0 0 1]]<br>[[1 0 1 0 0 1 2 2 0 0]<br> [1 0 0 0 0 1 1 1 2 0]]</p>
</blockquote>
<h3 id="tf-max"><a href="#tf-max" class="headerlink" title="tf.max"></a>tf.max</h3><h4 id="tf-maximum"><a href="#tf-maximum" class="headerlink" title="tf.maximum"></a>tf.maximum</h4><p>比较两个tensor，返回element-wise两个tensor的最大值。<br>如下<a href="https://github.com/mxxhcm/myown_code/blob/master/tf/some_ops/tf_maximum.py" target="_blank" rel="noopener">示例</a>：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line">a = tf.Variable([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">b = tf.Variable([<span class="number">2</span>, <span class="number">1</span>, <span class="number">4</span>])</span><br><span class="line"></span><br><span class="line">sess.run(tf.global_variables_initializer())</span><br><span class="line">print(<span class="string">"a: "</span>, sess.run(a))</span><br><span class="line">print(<span class="string">"b: "</span>, sess.run(b))</span><br><span class="line">c = tf.maximum(a, b)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"tf.maximum(a, b):\n  "</span>, sess.run(c))</span><br></pre></td></tr></table></figure></p>
<p>输出如下：</p>
<blockquote>
<p>a:  [1 2 3]<br>b:  [2 1 4]<br>tf.maximum(a, b):<br>   [2 2 4]</p>
</blockquote>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>1.<a href="https://github.com/tensorflow/tensorflow/issues/4842" target="_blank" rel="noopener">https://github.com/tensorflow/tensorflow/issues/4842</a><br>2.<a href="https://www.bilibili.com/read/cv681031/" target="_blank" rel="noopener">https://www.bilibili.com/read/cv681031/</a><br>3.<a href="https://cv-tricks.com/tensorflow-tutorial/save-restore-tensorflow-models-quick-complete-tutorial/" target="_blank" rel="noopener">https://cv-tricks.com/tensorflow-tutorial/save-restore-tensorflow-models-quick-complete-tutorial/</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://mxxhcm.github.io/2019/03/07/python-pip-使用国内源/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="马晓鑫爱马荟荟">
      <meta itemprop="description" content="记录硕士三年自己的积累">
      <meta itemprop="image" content="/images/favicon.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="mxxhcm's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/03/07/python-pip-使用国内源/" class="post-title-link" itemprop="http://mxxhcm.github.io/page/7/index.html">pip 使用国内源</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-03-07 09:56:54" itemprop="dateCreated datePublished" datetime="2019-03-07T09:56:54+08:00">2019-03-07</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-05-07 00:51:39" itemprop="dateModified" datetime="2019-05-07T00:51:39+08:00">2019-05-07</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/工具/" itemprop="url" rel="index"><span itemprop="name">工具</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="暂时使用国内pip源"><a href="#暂时使用国内pip源" class="headerlink" title="暂时使用国内pip源"></a>暂时使用国内pip源</h2><p>使用清华源<br>~#:pip install -i <a href="https://pypi.tuna.tsinghua.edu.cn/simple" target="_blank" rel="noopener">https://pypi.tuna.tsinghua.edu.cn/simple</a> package-name<br>使用阿里源<br>~#:pip install -i <a href="https://mirrors.aliyun.com/pypi/simple" target="_blank" rel="noopener">https://mirrors.aliyun.com/pypi/simple</a> package-name</p>
<h2 id="将国内pip源设为默认"><a href="#将国内pip源设为默认" class="headerlink" title="将国内pip源设为默认"></a>将国内pip源设为默认</h2><p>~#:pip install pip -U<br>~#:pip config set global.index-url <a href="https://pypi.tuna.tsinghua.edu.cn/simple" target="_blank" rel="noopener">https://pypi.tuna.tsinghua.edu.cn/simple</a></p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>1.<a href="https://mirrors.tuna.tsinghua.edu.cn/help/pypi/" target="_blank" rel="noopener">https://mirrors.tuna.tsinghua.edu.cn/help/pypi/</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/6/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/6/">6</a><span class="page-number current">7</span><a class="page-number" href="/page/8/">8</a><span class="space">&hellip;</span><a class="page-number" href="/page/10/">10</a><a class="extend next" rel="next" href="/page/8/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <div class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/favicon.jpg" alt="马晓鑫爱马荟荟">
            
              <p class="site-author-name" itemprop="name">马晓鑫爱马荟荟</p>
              <p class="site-description motion-element" itemprop="description">记录硕士三年自己的积累</p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">92</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  <a href="/categories/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">10</span>
                    <span class="site-state-item-name">分类</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  <a href="/tags/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">107</span>
                    <span class="site-state-item-name">标签</span>
                  </a>
                </div>
              
            </nav>
          

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="https://github.com/mxxhcm" title="GitHub &rarr; https://github.com/mxxhcm" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="mailto:mxxhcm@gmail.com" title="E-Mail &rarr; mailto:mxxhcm@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                </span>
              
            </div>
          

          

          
          

          
            
          
          

        </div>
      </div>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">马晓鑫爱马荟荟</span>

  

  
</div>


  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v3.8.0</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> v6.6.0</div>




        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  
    <script src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  


  


  <script src="/js/src/utils.js?v=6.6.0"></script>

  <script src="/js/src/motion.js?v=6.6.0"></script>



  
  


  <script src="/js/src/affix.js?v=6.6.0"></script>

  <script src="/js/src/schemes/pisces.js?v=6.6.0"></script>



  

  


  <script src="/js/src/bootstrap.js?v=6.6.0"></script>



  



  






<!-- LOCAL: You can save these files to your site and update links -->
    
        
        <link rel="stylesheet" href="https://aimingoo.github.io/gitmint/style/default.css">
        <script src="https://aimingoo.github.io/gitmint/dist/gitmint.browser.js"></script>
    
<!-- END LOCAL -->

    

    







  





  

  

  

  

  
  

  
  
    
      
    
      
    
      
    
      
        
      
    
      
    
      
    
      
    
      
    
      
    
      
    
  

  
    
      <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      },
      TeX: {equationNumbers: { autoNumber: "AMS" }}
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<script src="//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<style>
.MathJax_Display {
    overflow: auto hidden;
}
</style>

    
  


  
  

  

  

  

  

  

  

</body>
</html>

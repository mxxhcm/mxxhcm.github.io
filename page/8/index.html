<!DOCTYPE html>












  


<html class="theme-next pisces use-motion" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
































<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=6.6.0" rel="stylesheet" type="text/css">



  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.jpg?v=6.6.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.jpg?v=6.6.0">










<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '6.6.0',
    sidebar: {"position":"left","display":"always","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="记录硕士三年自己的积累">
<meta property="og:type" content="website">
<meta property="og:title" content="mxxhcm&#39;s blog">
<meta property="og:url" content="http://mxxhcm.github.io/page/8/index.html">
<meta property="og:site_name" content="mxxhcm&#39;s blog">
<meta property="og:description" content="记录硕士三年自己的积累">
<meta property="og:locale" content="zh-CN">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="mxxhcm&#39;s blog">
<meta name="twitter:description" content="记录硕士三年自己的积累">



  <link rel="alternate" href="/atom.xml" title="mxxhcm's blog" type="application/atom+xml">




  <link rel="canonical" href="http://mxxhcm.github.io/page/8/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>mxxhcm's blog</title>
  












  <noscript>
  <style>
    .use-motion .motion-element,
    .use-motion .brand,
    .use-motion .menu-item,
    .sidebar-inner,
    .use-motion .post-block,
    .use-motion .pagination,
    .use-motion .comments,
    .use-motion .post-header,
    .use-motion .post-body,
    .use-motion .collection-title { opacity: initial; }

    .use-motion .logo,
    .use-motion .site-title,
    .use-motion .site-subtitle {
      opacity: initial;
      top: initial;
    }

    .use-motion .logo-line-before i { left: initial; }
    .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">mxxhcm's blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
    
      
    

    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>标签</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">

    
    
    
      
    

    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>分类</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>归档</a>

  </li>

      
      
    </ul>
  

  

  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
            

          
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://mxxhcm.github.io/2019/03/13/CNN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="马晓鑫爱马荟荟">
      <meta itemprop="description" content="记录硕士三年自己的积累">
      <meta itemprop="image" content="/images/favicon.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="mxxhcm's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/03/13/CNN/" class="post-title-link" itemprop="http://mxxhcm.github.io/page/8/index.html">CNN</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-03-13 15:21:27" itemprop="dateCreated datePublished" datetime="2019-03-13T15:21:27+08:00">2019-03-13</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-05-07 00:22:27" itemprop="dateModified" datetime="2019-05-07T00:22:27+08:00">2019-05-07</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/机器学习/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="Alexnet-2012"><a href="#Alexnet-2012" class="headerlink" title="Alexnet(2012)"></a>Alexnet(2012)</h2><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>作者训练了一个深度卷积神经网络用来将Imagenet数据集中1000个类别中的120万张图片进行分类。整个网络结构包含五个卷积层，三个全连接层，以及一个1000-way的softmax层，整个网络共有6000万参数，65000个神经元。此外，作者提出了一些方法用来提高性能和减少训练的时间，并且介绍了一些防止过拟合的技巧。最后，在测试集上，它们跑出$37.5%$的top-1 error以及$17.0%$的top-5 error。</p>
<h3 id="存在的问题"><a href="#存在的问题" class="headerlink" title="存在的问题"></a>存在的问题</h3><p>1.之前的数据集太小，都是数以万计的，需要更大的数据集。</p>
<h3 id="创新"><a href="#创新" class="headerlink" title="创新"></a>创新</h3><h4 id="ReLU非线性激活函数"><a href="#ReLU非线性激活函数" class="headerlink" title="ReLU非线性激活函数"></a>ReLU非线性激活函数</h4><h5 id="作用"><a href="#作用" class="headerlink" title="作用"></a>作用</h5><p>作者说实验表明ReLU可以加速训练过程。</p>
<h5 id="saturating-nonlinearity"><a href="#saturating-nonlinearity" class="headerlink" title="saturating nonlinearity"></a>saturating nonlinearity</h5><p>一个饱和的激活函数会将输出挤压到一个区间内。</p>
<blockquote>
<p>A saturating activation function squeezes the input.</p>
</blockquote>
<p><strong>定义</strong><br>f是non-saturating 当且仅当$|lim_{z\rightarrow -\infty} f(z)| \rightarrow + \infty$或者$|lim_{z\rightarrow +\infty} f(z)| \rightarrow + \infty$<br>f是saturating 当且仅当f不是non-saturating<br><strong>例子</strong><br>ReLU就是non-saturating nonlinearity的激活函数，因为$f(x) = max(0, x)$，如下图所示。<br><img src="/2019/03/13/CNN/relu.png" alt="relu"><br>当$x$趋于无穷时，$f(x)$也趋于无穷。<br>sigmod和tanh是saturating nonlinearity激活函数，如下图所示。<br><img src="/2019/03/13/CNN/sigmod.png" alt="sigmo"><br><img src="/2019/03/13/CNN/tanh.png" alt="tanh"></p>
<h4 id="多块GPU并行"><a href="#多块GPU并行" class="headerlink" title="多块GPU并行"></a>多块GPU并行</h4><p>作者使用了两块GPU一块运行，每个GPU中的参数个数是一样的，在一些特定层中，两个GPU中的参数信息可以进行通信。</p>
<h4 id="Overlapping-Pooling"><a href="#Overlapping-Pooling" class="headerlink" title="Overlapping Pooling"></a>Overlapping Pooling</h4><p>就是Pooling kernel的size要比stride大。比如一个$12\times 12$的图片，用$5\times 5$的pooling kernel，步长为$3$，步长要比kernel核小，即$3$比$5$小。<br>为什么这能减小过拟合？</p>
<ul>
<li>可能是减小了Pooling过程中信息的丢失。</li>
</ul>
<blockquote>
<p>If the pooling regions do not overlap, the pooling regions are disjointed and if that is the case, more information is lost in each pooling layer. If some overlap is allowed the pooling regions overlap with some degree and less spatial information is lost in each layer.[4]</p>
</blockquote>
<h4 id="数据增强"><a href="#数据增强" class="headerlink" title="数据增强"></a>数据增强</h4><p>防止过拟合</p>
<h5 id="裁剪和翻转"><a href="#裁剪和翻转" class="headerlink" title="裁剪和翻转"></a>裁剪和翻转</h5><p>输入是$256\times 256 \times 3$的图像。<br>训练：对每张图片都提取多个$224\times 224$大小的patch，这样子总共就多产生了$(256-224)\times (256-224) = 1024$个样本，然后对每个patch做一个水平翻转，就有$1024\times 2 = 2048$个样本。<br>测试：通过对每张图片裁剪五个（四个角落加中间）$224\times 224$的patches，并且对它们做翻转，也就是有$10$个patches，网络对十个patch的softmax层输出做平均作为预测结果。</p>
<h5 id="在图片上调整RGB通道的密度"><a href="#在图片上调整RGB通道的密度" class="headerlink" title="在图片上调整RGB通道的密度"></a>在图片上调整RGB通道的密度</h5><p>使用PCA对RGB值做主成分分析。对于每张训练图片，加上主成分，其大小正比于特征值乘上一个均值为$0$，方差为$0.1$的高斯分布产生的随机变量。对于一张图片$x,y$点处的像素值$I_{xy}=[I_{xy}^R, I_{xy}^G,I_{xy}^B]^T$，加上$[\bold{p_1},\bold{p_2},\bold{p_3}][\alpha_1\lambda_1,\alpha_2\lambda_2,\alpha_3\lambda_3]$，其中$[\bold{p_1},\bold{p_2},\bold{p_3}]$是特征向量，$\lambda_i$是特征值，$\alpha_i$就是前面说的随机变量。</p>
<h4 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h4><p>通过学习鲁棒的特征防止过拟合。<br>在训练的时候，每个隐藏单元的输出有$p$的概率被设置为$0$，在该次训练中，如果这个神经元的输出被设置为$0$，它就对loss函数没有贡献，反向传播也不会被更新。对于一层有$N$个神经单元的全连接层，总共有$2^N$种神经元的组合结果，这就相当于训练了一系列共享参数的模型。<br>在测试的时候，所有隐藏单元的输出都不丢弃，但是会乘上$p$的概率，相当于对一系列集成模型取平均。具体可见<a href="https://mxxhcm.github.io/2019/03/23/神经网络-dropout/">dropout</a><br>在该模型中，作者在三层全连接层的前两层输出上加了dropout。</p>
<h4 id="局部响应归一化-Local-Response-Normalizaiton"><a href="#局部响应归一化-Local-Response-Normalizaiton" class="headerlink" title="局部响应归一化(Local Response Normalizaiton)"></a>局部响应归一化(Local Response Normalizaiton)</h4><p>事实上，后来发现这个东西没啥用。但是这里还是给出一个公式。</p>
<script type="math/tex; mode=display">b^i_{x,y} = \frac{a^i_{x,y}}{(k+\alpha \sum^{min(N-1,\frac{i+n}{2})}_{j=max(0,\frac{i-n}{2})}(a^j_{x,y})^2)^{\beta}}</script><p>其中$a^i_{x,y}$是在点$(x,y)$处使用kernel $i$之后，在经过ReLU激活函数。$k,n,\alpha,\beta$是超参数。</p>
<blockquote>
<p>It seems that these kinds of layers have a minimal impact and are not used any more. Basically, their role have been outplayed by other regularization techniques (such as dropout and batch normalization), better initializations and training methods.</p>
</blockquote>
<h3 id="整体架构"><a href="#整体架构" class="headerlink" title="整体架构"></a>整体架构</h3><h4 id="目标函数"><a href="#目标函数" class="headerlink" title="目标函数"></a>目标函数</h4><p>多峰logistic回归。</p>
<h4 id="并行框架"><a href="#并行框架" class="headerlink" title="并行框架"></a>并行框架</h4><p>下图是并行的架构，分为两层，上面一层用一个GPU，下面一层用一个GPU，它们只在第三个卷积层有交互。<br><img src="/2019/03/13/CNN/alexnet.png" alt="alexnet"></p>
<h4 id="简化框架"><a href="#简化框架" class="headerlink" title="简化框架"></a>简化框架</h4><p>下图是简化版的结构，不需要使用两个GPU。<br><img src="/2019/03/13/CNN/alexnet_simple.png" alt="alexnet_simple"></p>
<h4 id="数据流（简化框架）"><a href="#数据流（简化框架）" class="headerlink" title="数据流（简化框架）"></a>数据流（简化框架）</h4><p>输入是$224\times 224 \times 3$的图片，第一层是$96$个stride为$4$的$11\times 11\times 3$卷积核构成的卷积层，输出经过max pooling(步长为2，kernel size为3)输入到第二层；第二层有$256$个$5\times 5\times 96$个卷积核，输出经过max pooling(步长为2，kernel size为3)输入到第三层；第三层到第四层，第四层到第五层之间没有经过pooling和normalization)，第三层有384个$3\times 3\times 256$个卷积核，第四层有$384$个$3\times 3\times 384$个卷积核，第五层有$256$个$3\times 3\times 384$个卷积核。然后接了两个$2048$个神经元的全连接层和一个$1000$个神经元的全连接层。</p>
<h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><h4 id="Datasets"><a href="#Datasets" class="headerlink" title="Datasets"></a>Datasets</h4><p>ILSVRC-2010</p>
<h4 id="Baselines"><a href="#Baselines" class="headerlink" title="Baselines"></a>Baselines</h4><ul>
<li>Sparse coding</li>
<li>SIFT+FV</li>
<li>CNN</li>
</ul>
<h4 id="Metric"><a href="#Metric" class="headerlink" title="Metric"></a>Metric</h4><ul>
<li>top-1 error rate</li>
<li>top-5 error rate</li>
</ul>
<h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p>pytorch实现<br><a href="https://github.com/mxxhcm/myown_code/blob/master/CNN/alexnet.py" target="_blank" rel="noopener">https://github.com/mxxhcm/myown_code/blob/master/CNN/alexnet.py</a></p>
<h2 id="Vggnet-2013"><a href="#Vggnet-2013" class="headerlink" title="Vggnet(2013)"></a>Vggnet(2013)</h2><h3 id="概述-1"><a href="#概述-1" class="headerlink" title="概述"></a>概述</h3><h3 id="存在的问题-1"><a href="#存在的问题-1" class="headerlink" title="存在的问题"></a>存在的问题</h3><h3 id="方案"><a href="#方案" class="headerlink" title="方案"></a>方案</h3><h4 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h4><h4 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h4><h4 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h4><h2 id><a href="#" class="headerlink" title=" "></a> </h2><h3 id="概述-2"><a href="#概述-2" class="headerlink" title="概述"></a>概述</h3><h3 id="存在的问题-2"><a href="#存在的问题-2" class="headerlink" title="存在的问题"></a>存在的问题</h3><h3 id="方案-1"><a href="#方案-1" class="headerlink" title="方案"></a>方案</h3><h4 id="背景-1"><a href="#背景-1" class="headerlink" title="背景"></a>背景</h4><h4 id="算法-1"><a href="#算法-1" class="headerlink" title="算法"></a>算法</h4><h4 id="代码-2"><a href="#代码-2" class="headerlink" title="代码"></a>代码</h4><h1 id="-1"><a href="#-1" class="headerlink" title="#"></a>#</h1><h3 id="概述-3"><a href="#概述-3" class="headerlink" title="概述"></a>概述</h3><h3 id="存在的问题-3"><a href="#存在的问题-3" class="headerlink" title="存在的问题"></a>存在的问题</h3><h3 id="方案-2"><a href="#方案-2" class="headerlink" title="方案"></a>方案</h3><h4 id="背景-2"><a href="#背景-2" class="headerlink" title="背景"></a>背景</h4><h4 id="算法-2"><a href="#算法-2" class="headerlink" title="算法"></a>算法</h4><h4 id="代码-3"><a href="#代码-3" class="headerlink" title="代码"></a>代码</h4><h1 id="-2"><a href="#-2" class="headerlink" title="#"></a>#</h1><h3 id="概述-存在的问题"><a href="#概述-存在的问题" class="headerlink" title="概述 ### 存在的问题"></a>概述 ### 存在的问题</h3><h3 id="方案-3"><a href="#方案-3" class="headerlink" title="方案"></a>方案</h3><h4 id="背景-3"><a href="#背景-3" class="headerlink" title="背景"></a>背景</h4><h4 id="算法-3"><a href="#算法-3" class="headerlink" title="算法"></a>算法</h4><h4 id="代码-4"><a href="#代码-4" class="headerlink" title="代码"></a>代码</h4><h1 id="-3"><a href="#-3" class="headerlink" title="#"></a>#</h1><h3 id="概述-4"><a href="#概述-4" class="headerlink" title="概述"></a>概述</h3><h3 id="存在的问题-4"><a href="#存在的问题-4" class="headerlink" title="存在的问题"></a>存在的问题</h3><h3 id="方案-4"><a href="#方案-4" class="headerlink" title="方案"></a>方案</h3><h4 id="背景-4"><a href="#背景-4" class="headerlink" title="背景"></a>背景</h4><h4 id="算法-4"><a href="#算法-4" class="headerlink" title="算法"></a>算法</h4><h4 id="代码-5"><a href="#代码-5" class="headerlink" title="代码"></a>代码</h4><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>1.<a href="https://stats.stackexchange.com/a/174438" target="_blank" rel="noopener">https://stats.stackexchange.com/a/174438</a><br>2.<a href="https://www.zhihu.com/question/264163033/answer/277481519" target="_blank" rel="noopener">https://www.zhihu.com/question/264163033/answer/277481519</a><br>3.<a href="https://stats.stackexchange.com/questions/145768/importance-of-local-response-normalization-in-cnn" target="_blank" rel="noopener">https://stats.stackexchange.com/questions/145768/importance-of-local-response-normalization-in-cnn</a><br>4.<a href="https://stats.stackexchange.com/a/386304" target="_blank" rel="noopener">https://stats.stackexchange.com/a/386304</a><br>5.<a href="https://blog.csdn.net/luoyang224/article/details/78088582/" target="_blank" rel="noopener">https://blog.csdn.net/luoyang224/article/details/78088582/</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://mxxhcm.github.io/2019/03/13/pytorch-notes（不定期更新）/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="马晓鑫爱马荟荟">
      <meta itemprop="description" content="记录硕士三年自己的积累">
      <meta itemprop="image" content="/images/favicon.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="mxxhcm's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/03/13/pytorch-notes（不定期更新）/" class="post-title-link" itemprop="http://mxxhcm.github.io/page/8/index.html">pytorch踩坑（不定期更新）</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-03-13 15:10:22" itemprop="dateCreated datePublished" datetime="2019-03-13T15:10:22+08:00">2019-03-13</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-05-07 00:22:27" itemprop="dateModified" datetime="2019-05-07T00:22:27+08:00">2019-05-07</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/python/" itemprop="url" rel="index"><span itemprop="name">python</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="常见问题"><a href="#常见问题" class="headerlink" title="常见问题"></a>常见问题</h2><h3 id="RuntimeError-CUDNN-STATUS-ARCH-MISMATCH"><a href="#RuntimeError-CUDNN-STATUS-ARCH-MISMATCH" class="headerlink" title="RuntimeError: CUDNN_STATUS_ARCH_MISMATCH"></a>RuntimeError: CUDNN_STATUS_ARCH_MISMATCH</h3><p>CUDNN doesn’t support CUDA arch 2.1 cards.<br>CUDNN requires Compute Capability 3.0, at least.<br>意思是GPU的加速能力不够，CUDNN只支持CUDA Capability 3.0以上的GPU加速，实验室主机是GT620的显卡，2.1的加速能力。<br>GPU对应的capability: <a href="https://developer.nvidia.com/cuda-gpus" target="_blank" rel="noopener">https://developer.nvidia.com/cuda-gpus</a><br>所以，对于不能使用cudnn对cuda加速的显卡，我们可以设置cudnn加速为False，这个默认是为True的<br>torch.backends.cudnn.enabled=False<br>但是，由于显卡版本为2.1，太老了，没有二进制版本。所以，还是会报其他错误，因此，就别使用cpu进行加速啦。</p>
<h3 id="查看cuda版本"><a href="#查看cuda版本" class="headerlink" title="查看cuda版本"></a>查看cuda版本</h3><p>~#:nvcc —version</p>
<h2 id="神经网络参数初始化"><a href="#神经网络参数初始化" class="headerlink" title="神经网络参数初始化"></a>神经网络参数初始化</h2><h3 id="方法-1-Model-apply-fn"><a href="#方法-1-Model-apply-fn" class="headerlink" title="方法$1$.Model.apply(fn)"></a>方法$1$.Model.apply(fn)</h3><p><a href="https://github.com/mxxhcm/myown_code/blob/master/pytorch/tutorials/initialize/apply.py" target="_blank" rel="noopener">示例</a>如下<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_weights</span><span class="params">(m)</span>:</span></span><br><span class="line">  print(m)</span><br><span class="line">  <span class="keyword">if</span> type(m) == nn.Linear:</span><br><span class="line">    m.weight.data.fill_(<span class="number">1.0</span>)</span><br><span class="line">    print(m.weight)</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(nn.Linear(<span class="number">2</span>, <span class="number">2</span>), nn.Linear(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">net.apply(init_weights)</span><br></pre></td></tr></table></figure></p>
<p>输出结果如下：</p>
<blockquote>
<p>Linear(in_features=2, out_features=2, bias=True)<br>Parameter containing:<br>tensor([[1., 1.],<br>        [1., 1.]], requires_grad=True)<br>Linear(in_features=2, out_features=2, bias=True)<br>Parameter containing:<br>tensor([[1., 1.],<br>        [1., 1.]], requires_grad=True)<br>Sequential(<br>  (0): Linear(in_features=2, out_features=2, bias=True)<br>  (1): Linear(in_features=2, out_features=2, bias=True)<br>)<br>Linear(in_features=2, out_features=2, bias=True)<br>Linear(in_features=2, out_features=2, bias=True)</p>
</blockquote>
<p>其中最后两行为net对象调用self.children()函数返回的模块，就是模型中所有网络的参数。事实上，调用net.apply(fn)函数，会对self.children()中的所有模块应用fn函数，</p>
<h2 id="torch-multiprocessing"><a href="#torch-multiprocessing" class="headerlink" title="torch.multiprocessing"></a>torch.multiprocessing</h2><h3 id="join"><a href="#join" class="headerlink" title="join"></a>join</h3><p>等待调用join()方法的线程执行完毕，然后继续执行。<br>可参见github<a href="https://github.com/mxxhcm/myown_code/tree/master/pytorch/tutorials/multiprocess_torch/mnist_hogwild" target="_blank" rel="noopener">官方demo</a>。</p>
<h3 id="share-memory"><a href="#share-memory" class="headerlink" title="share_memory_()"></a>share_memory_()</h3><p>在多个线程之间共享参数，如下<a href="https://github.com/mxxhcm/myown_code/blob/master/pytorch/tutorials/multiprocess_torch/share_memory.py" target="_blank" rel="noopener">代码</a>所示。可以用来实现A3C。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.multiprocessing <span class="keyword">as</span> mp</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">proc</span><span class="params">(sec, x)</span>:</span></span><br><span class="line">   print(os.getpid(),<span class="string">"  "</span>, x)</span><br><span class="line">   time.sleep(sec)</span><br><span class="line">   print(os.getpid(), <span class="string">"  "</span>, x)</span><br><span class="line">   x += sec</span><br><span class="line">   print(str(os.getpid()) + <span class="string">"  over.  "</span>, x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">   num_processes = <span class="number">3</span></span><br><span class="line">   processes = []</span><br><span class="line">   x = torch.ones([<span class="number">3</span>,])</span><br><span class="line">   x.share_memory_()</span><br><span class="line">   <span class="keyword">for</span> rank <span class="keyword">in</span> range(num_processes):</span><br><span class="line">     p = mp.Process(target=proc, args=(rank + <span class="number">1</span>, x))</span><br><span class="line">     p.start() </span><br><span class="line">     processes.append(p)</span><br><span class="line">   <span class="keyword">for</span> p <span class="keyword">in</span> processes:</span><br><span class="line">     p.join()</span><br><span class="line">   print(x)</span><br></pre></td></tr></table></figure></p>
<p>输出结果如下所示：</p>
<blockquote>
<p>python share_memory.py<br>7739    tensor([1., 1., 1.])<br>7738    tensor([1., 1., 1.])<br>7737    tensor([1., 1., 1.])<br>7737    tensor([1., 1., 1.])<br>7737  over.   tensor([2., 2., 2.])<br>7738    tensor([2., 2., 2.])<br>7738  over.   tensor([4., 4., 4.])<br>7739    tensor([4., 4., 4.])<br>7739  over.   tensor([7., 7., 7.])<br>tensor([7., 7., 7.])</p>
</blockquote>
<p>我们可以发现$7739$这个线程中，传入的$x$还是和最开始的一样，但是在$7738$线程更新完$x$之后，$7739$使用的$x$就已经变成了更新后的$x$。所以，我猜测这里面应该是有一个对$x$的锁，保证$x$在同一时刻只能被一个线程访问。</p>
<h2 id="torch-distributions"><a href="#torch-distributions" class="headerlink" title="torch.distributions"></a>torch.distributions</h2><p>这个库和gym.space库很相似，都是提供一些分布，然后从中采样。<br>常见的有ExponentialFamily,Bernoulli,Binomial,Categorical,Exponential,Gamma,Independent,Laplace,Multinomial,MultivariateNormal。这里不做过程陈述，可以看<a href="http://localhost:4000/2019/04/12/gym%E4%BB%8B%E7%BB%8D/" target="_blank" rel="noopener">gym</a>中。</p>
<h3 id="Categorical"><a href="#Categorical" class="headerlink" title="Categorical"></a>Categorical</h3><p>对应tensorflow中的<a href="https://github.com/mxxhcm/myown_code/blob/master/tf/some_ops/tf_multinominal.py" target="_blank" rel="noopener">tf.multinomial</a>。<br>类原型：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CLASS torch.distributions.categorical.Categorical(probs=<span class="literal">None</span>, logits=<span class="literal">None</span>, validate_args=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure></p>
<p>参数probs只能是$1$维或者$2$维，而且必须是非负，有限非零和的，然后将其归一化到和为$1$。<br>这个类和torch.multinormal是一样的，从$\{0,\cdots, K-1\}$中按照probs的概率进行采样，$K$是probs.size(-1)，即是size()矩阵的最后一列，$2$维时把第$1$维当成了batch。</p>
<p>举一个简单的例子，<a href>代码</a>。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.distributions <span class="keyword">as</span> diss</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">torch.manual_seed(<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">m = diss.Categorical(torch.tensor([<span class="number">0.25</span>, <span class="number">0.25</span>, <span class="number">0.25</span>, <span class="number">0.25</span> ]))</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">    print(m.sample())</span><br><span class="line"></span><br><span class="line">m = diss.Categorical(torch.tensor([[<span class="number">0.5</span>, <span class="number">0.25</span>, <span class="number">0.25</span>], [<span class="number">0.25</span>, <span class="number">0.25</span>, <span class="number">0.5</span>]]))</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">    print(m.sample())</span><br></pre></td></tr></table></figure></p>
<p>输出结果如下：</p>
<blockquote>
<p>tensor(2)<br>tensor(1)<br>tensor(1)<br>tensor(1)<br>tensor(1)<br>tensor([2, 2])<br>tensor([1, 2])<br>tensor([0, 1])<br>tensor([0, 2])<br>tensor([0, 0])</p>
</blockquote>
<p>作为对比，gym.spaces.Discrete示例如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> gym <span class="keyword">import</span> spaces</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.Discrete</span></span><br><span class="line"><span class="comment"># 取值是&#123;0, 1, ..., n - 1&#125;</span></span><br><span class="line">dis = spaces.Discrete(<span class="number">5</span>)</span><br><span class="line">dis.seed(<span class="number">5</span>)</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">    print(dis.sample())</span><br></pre></td></tr></table></figure></p>
<p>输出结果是：</p>
<blockquote>
<p>3<br>0<br>1<br>0<br>4</p>
</blockquote>
<h2 id="torch"><a href="#torch" class="headerlink" title="torch"></a>torch</h2><h3 id="torch-cat"><a href="#torch-cat" class="headerlink" title="torch.cat"></a>torch.cat</h3><h4 id="函数原型"><a href="#函数原型" class="headerlink" title="函数原型"></a>函数原型</h4><p>将多个tensor在某一个维度上（默认是第0维）拼接到一起（除了拼接的维度上，其他维度的shape必须一定），最后返回一个tensor。<br>torch.cat(tensors, dim=0, out=None) → Tensor</p>
<blockquote>
<p>Concatenates the given sequence of seq tensors in the given dimension. All tensors must either have the same shape (except in the concatenating dimension) or be empty.</p>
</blockquote>
<h4 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h4><p>tensors (sequence of Tensors) – 任意类型相同python序列或者tensor<br>dim (int, optional) - 在第几个维度上进行拼接(只有在拼接的维度上可以不同，其余维度必须相同。<br>out (Tensor, optional) – 输出的tensor</p>
<h4 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x1 = torch.randn(<span class="number">3</span>, <span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line">x2 = torch.randn(<span class="number">3</span>, <span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line">x = torch.cat([x1, x2], <span class="number">1</span>)</span><br><span class="line">print(x.size())</span><br></pre></td></tr></table></figure>
<p>输出如下：</p>
<blockquote>
<p>torch.Size([3, 5, 4])</p>
</blockquote>
<h3 id="torch中图像-img-格式"><a href="#torch中图像-img-格式" class="headerlink" title="torch中图像(img)格式"></a>torch中图像(img)格式</h3><p>torch中图像的shape是(‘RGB’,width, height)，而numpy和matplotlib中都是(width, height, ‘RGB’)<br>matplotlib.pyplot.imshow()需要的参数是图像矩阵，如果矩阵中是整数，那么它的值需要在区间[0,255]之内，如果是浮点数，需要在[0,1]之间。</p>
<blockquote>
<p>Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).</p>
</blockquote>
<h2 id="torch-autograd-torch-autograd-Variable和torch-autograd-Function"><a href="#torch-autograd-torch-autograd-Variable和torch-autograd-Function" class="headerlink" title="torch.autograd(torch.autograd.Variable和torch.autograd.Function)"></a>torch.autograd(torch.autograd.Variable和torch.autograd.Function)</h2><h3 id="Variable-class-torch-autograd-Variable"><a href="#Variable-class-torch-autograd-Variable" class="headerlink" title="Variable(class torch.autograd.Variable)"></a>Variable(class torch.autograd.Variable)</h3><h4 id="声明一个tensor"><a href="#声明一个tensor" class="headerlink" title="声明一个tensor"></a>声明一个tensor</h4><p>torch.zeros,torch.ones,torch.rand,torch.Tensor<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">torch.manual_seed(<span class="number">5</span>)</span><br><span class="line">x = torch.empty(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">print(torch.empty(<span class="number">5</span>, <span class="number">3</span>)) <span class="comment"># construct a 5x3 matrix, uninitialized</span></span><br><span class="line"><span class="comment"># tensor([[4.6179e-38, 4.5845e-41, 4.6179e-38],</span></span><br><span class="line"><span class="comment">#         [4.5845e-41, 6.3010e-36, 6.3010e-36],</span></span><br><span class="line"><span class="comment">#         [2.5204e-35, 6.3010e-36, 1.0082e-34],</span></span><br><span class="line"><span class="comment">#         [6.3010e-36, 6.3010e-36, 6.6073e-30],</span></span><br><span class="line"><span class="comment">#         [6.3010e-36, 6.3010e-36, 6.3010e-36]])</span></span><br><span class="line"></span><br><span class="line">print(torch.rand(<span class="number">3</span>, <span class="number">4</span>))  <span class="comment"># construct a 4x3 matrix, uniform [0,1] </span></span><br><span class="line"><span class="comment"># tensor([[0.8303, 0.1261, 0.9075, 0.8199],</span></span><br><span class="line"><span class="comment">#         [0.9201, 0.1166, 0.1644, 0.7379],</span></span><br><span class="line"><span class="comment">#         [0.0333, 0.9942, 0.6064, 0.5646]])</span></span><br><span class="line"></span><br><span class="line">print(torch.randn(<span class="number">5</span>, <span class="number">3</span>)) <span class="comment"># construct a 5x3 matrix, normal distribution</span></span><br><span class="line"><span class="comment"># tensor([[-1.4017, -0.7626,  0.6312],</span></span><br><span class="line"><span class="comment">#         [-0.8991, -0.5578,  0.6907],</span></span><br><span class="line"><span class="comment">#         [ 0.2225, -0.6662,  0.6846],</span></span><br><span class="line"><span class="comment">#         [ 0.5740, -0.5829,  0.7679],</span></span><br><span class="line"><span class="comment">#         [ 0.5740, -0.5829,  0.7679],</span></span><br><span class="line"></span><br><span class="line">print(torch.randn(<span class="number">2</span>, <span class="number">3</span>).type())</span><br><span class="line"><span class="comment"># torch.FloatTensor</span></span><br><span class="line"></span><br><span class="line">print(torch.zeros(<span class="number">5</span>, <span class="number">3</span>)) <span class="comment"># construct a 5x3 matrix filled zeros</span></span><br><span class="line"><span class="comment"># tensor([[0., 0., 0.],</span></span><br><span class="line"><span class="comment">#         [0., 0., 0.],</span></span><br><span class="line"><span class="comment">#         [0., 0., 0.],</span></span><br><span class="line"><span class="comment">#         [0., 0., 0.],</span></span><br><span class="line"><span class="comment">#         [0., 0., 0.]])</span></span><br><span class="line"></span><br><span class="line">print(torch.ones(<span class="number">5</span>, <span class="number">3</span>)) <span class="comment"># construct a 5x3 matrix filled ones</span></span><br><span class="line"><span class="comment"># tensor([[1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 1., 1.]])</span></span><br><span class="line"></span><br><span class="line">print(torch.ones(<span class="number">5</span>, <span class="number">3</span>, dtype=torch.long)) <span class="comment"># construct a tensor with dtype=torch.long</span></span><br><span class="line"><span class="comment"># tensor([[1, 1, 1],</span></span><br><span class="line"><span class="comment">#         [1, 1, 1],</span></span><br><span class="line"><span class="comment">#         [1, 1, 1],</span></span><br><span class="line"><span class="comment">#         [1, 1, 1],</span></span><br><span class="line"><span class="comment">#         [1, 1, 1]])</span></span><br><span class="line"></span><br><span class="line">print(torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])) <span class="comment"># construct a tensor direct from data</span></span><br><span class="line"><span class="comment"># tensor([1, 2, 3])</span></span><br><span class="line"></span><br><span class="line">print(x.new_ones(<span class="number">5</span>,<span class="number">4</span>)) <span class="comment"># constuct a tensor has the same property as x</span></span><br><span class="line"><span class="comment"># tensor([[1., 1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 1., 1., 1.]])</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(torch.full([<span class="number">4</span>,<span class="number">3</span>],<span class="number">9</span>))  <span class="comment"># construct a tensor with a value </span></span><br><span class="line"><span class="comment"># tensor([[9., 9., 9.],</span></span><br><span class="line"><span class="comment">#         [9., 9., 9.],</span></span><br><span class="line"><span class="comment">#         [9., 9., 9.],</span></span><br><span class="line"><span class="comment">#         [9., 9., 9.]])</span></span><br><span class="line"></span><br><span class="line">print(x.new_ones(<span class="number">5</span>,<span class="number">4</span>,dtype=torch.int)) <span class="comment"># construct a tensor with the same property as x, and also can have the specified type.</span></span><br><span class="line"><span class="comment"># tensor([[1, 1, 1, 1],</span></span><br><span class="line"><span class="comment">#         [1, 1, 1, 1],</span></span><br><span class="line"><span class="comment">#         [1, 1, 1, 1],</span></span><br><span class="line"><span class="comment">#         [1, 1, 1, 1],</span></span><br><span class="line"><span class="comment">#         [1, 1, 1, 1]], dtype=torch.int32)</span></span><br><span class="line"></span><br><span class="line">print(torch.randn_like(x,dtype=torch.float)) <span class="comment"># construct a tensor with the same shape with x, </span></span><br><span class="line"><span class="comment"># tensor([[ 0.4699, -1.9540, -0.5587],</span></span><br><span class="line"><span class="comment">#         [ 0.4295, -2.2643, -0.2017],</span></span><br><span class="line"><span class="comment">#         [ 1.0677,  0.3246, -0.0684],</span></span><br><span class="line"><span class="comment">#         [-0.9959,  1.1563, -0.3992],</span></span><br><span class="line"><span class="comment">#         [ 1.2153, -0.8115, -0.8848]])</span></span><br><span class="line"></span><br><span class="line">print(torch.ones_like(x))</span><br><span class="line"><span class="comment"># tensor([[1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 1., 1.]])</span></span><br><span class="line"></span><br><span class="line">print(torch.zeros_like(x))</span><br><span class="line"><span class="comment"># tensor([[0., 0., 0.],</span></span><br><span class="line"><span class="comment">#         [0., 0., 0.],</span></span><br><span class="line"><span class="comment">#         [0., 0., 0.],</span></span><br><span class="line"><span class="comment">#         [0., 0., 0.],</span></span><br><span class="line"><span class="comment">#         [0., 0., 0.]])</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(torch.Tensor(<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line"><span class="comment"># tensor([[-3.8809e-21,  3.0948e-41,  2.3822e-44,  0.0000e+00],</span></span><br><span class="line"><span class="comment">#         [        nan,  7.2251e+28,  1.3733e-14,  1.8888e+31],</span></span><br><span class="line"><span class="comment">#         [ 4.9656e+28,  4.5439e+30,  7.1426e+22,  1.8759e+28]])</span></span><br><span class="line"></span><br><span class="line">print(torch.Tensor(<span class="number">3</span>,<span class="number">4</span>).uniform_(<span class="number">0</span>,<span class="number">1</span>))</span><br><span class="line"><span class="comment"># tensor([[0.8437, 0.1399, 0.2239, 0.3462],</span></span><br><span class="line"><span class="comment">#         [0.5668, 0.3059, 0.1890, 0.4087],</span></span><br><span class="line"><span class="comment">#         [0.2560, 0.5138, 0.1299, 0.3750]])</span></span><br><span class="line"></span><br><span class="line">print(torch.Tensor(<span class="number">3</span>,<span class="number">4</span>).normal_(<span class="number">0</span>,<span class="number">1</span>))</span><br><span class="line"><span class="comment"># tensor([[-0.5490, -0.0838, -0.1387, -0.5289],</span></span><br><span class="line"><span class="comment">#         [-0.4919, -0.4646, -0.0588,  1.2624],</span></span><br><span class="line"><span class="comment">#         [ 1.1935,  1.5696, -0.8977, -0.1139]])</span></span><br><span class="line"></span><br><span class="line">print(torch.Tensor(<span class="number">3</span>,<span class="number">4</span>).fill_(<span class="number">5</span>))</span><br><span class="line"><span class="comment"># tensor([[5., 5., 5., 5.],</span></span><br><span class="line"><span class="comment">#         [5., 5., 5., 5.],</span></span><br><span class="line"><span class="comment">#         [5., 5., 5., 5.]])</span></span><br><span class="line"></span><br><span class="line">print(torch.arange(<span class="number">1</span>, <span class="number">3</span>, <span class="number">0.4</span>))</span><br><span class="line"><span class="comment"># tensor([1.0000, 1.4000, 1.8000, 2.2000, 2.6000])</span></span><br></pre></td></tr></table></figure></p>
<h4 id="tensor的各种操作"><a href="#tensor的各种操作" class="headerlink" title="tensor的各种操作"></a>tensor的各种操作</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">a = torch.ones(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">b = torch.ones(<span class="number">2</span>,<span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<h5 id="加操作"><a href="#加操作" class="headerlink" title="加操作"></a>加操作</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">print(a+b)                <span class="comment">#方法1</span></span><br><span class="line">c = torch.add(a,b)    <span class="comment">#方法2</span></span><br><span class="line">torch.add(a,b,result)    <span class="comment">#方法3</span></span><br><span class="line">a.add(b)                    <span class="comment">#方法4,将a加上b，且a不变</span></span><br><span class="line">a.add_(b)                <span class="comment">#方法5,将a加上b并将其赋值给a</span></span><br></pre></td></tr></table></figure>
<h5 id="转置操作"><a href="#转置操作" class="headerlink" title="转置操作"></a>转置操作</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(a.t())               <span class="comment"># 打印出tensor a的转置</span></span><br><span class="line">print(a.t_())                 <span class="comment">#将tensor a 转置，并将其赋值给a</span></span><br></pre></td></tr></table></figure>
<h5 id="求最大行和列"><a href="#求最大行和列" class="headerlink" title="求最大行和列"></a>求最大行和列</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.max(tensor,dim)</span><br><span class="line">np.max(array,dim)</span><br></pre></td></tr></table></figure>
<h5 id="和relu功能比较类似。"><a href="#和relu功能比较类似。" class="headerlink" title="和relu功能比较类似。"></a>和relu功能比较类似。</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.clamp(tensor, min, max,out=<span class="literal">None</span>)</span><br><span class="line">np.maximun(x1, x2)  <span class="comment"># x1 and x2 must hava the same shape</span></span><br></pre></td></tr></table></figure>
<h4 id="tensor和numpy转化"><a href="#tensor和numpy转化" class="headerlink" title="tensor和numpy转化"></a>tensor和numpy转化</h4><h5 id="convert-tensor-to-numpy"><a href="#convert-tensor-to-numpy" class="headerlink" title="convert tensor to numpy"></a>convert tensor to numpy</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = torch.ones(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">b = a.numpy()</span><br></pre></td></tr></table></figure>
<h5 id="convert-numpy-to-tensor"><a href="#convert-numpy-to-tensor" class="headerlink" title="convert numpy to tensor"></a>convert numpy to tensor</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a =  numpy.ones(<span class="number">4</span>,<span class="number">3</span>)</span><br><span class="line">b = torch.from_numpy(a)</span><br></pre></td></tr></table></figure>
<h4 id="Variable和Tensor"><a href="#Variable和Tensor" class="headerlink" title="Variable和Tensor"></a>Variable和Tensor</h4><p>Variable<br>图1.Variable</p>
<h5 id="属性"><a href="#属性" class="headerlink" title="属性"></a>属性</h5><p>如图1,Variable wrap a Tensor,and it has six attributes,data,grad,requies_grad,volatile,is_leaf and grad_fn.We can acess the raw tensor through .data operation, we can accumualte gradients w.r.t this Variable into .grad,.Finally , creator attribute will tell us how the Variable were created,we can acess the creator attibute by .grad_fn,if the Variable was created by the user,then the grad_fn is None,else it will show us which Function created the Variable.<br>if the grad_fn is None,we call them graph leaves<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Variable.shape  <span class="comment">#查看Variable的size</span></span><br><span class="line">Variable.size()</span><br></pre></td></tr></table></figure></p>
<h5 id="parameters"><a href="#parameters" class="headerlink" title="parameters"></a>parameters</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.autograd.Variable(data,requires_grad=<span class="literal">False</span>,volatile=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p>requires_grad : indicate whether the backward() will ever need to be called</p>
<h5 id="backward"><a href="#backward" class="headerlink" title="backward"></a>backward</h5><p>backward(gradient=None,retain_graph=None,create_graph=None,retain_variables=None)<br>如果Variable是一个scalar output，我们不需要指定gradient，但是如果Variable不是一个scalar，而是有多个element，我们就需要根据output指定一下gradient，gradient的type可以是tensor也可以是Variable，里面的值为梯度的求值比例，例如<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = Variable(torch.Tensor([<span class="number">3</span>,<span class="number">6</span>,<span class="number">4</span>]),requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = Variable(torch.Tensor([<span class="number">5</span>,<span class="number">3</span>,<span class="number">6</span>]),requires_grad=<span class="literal">True</span>)</span><br><span class="line">z = x+y</span><br><span class="line">z.backward(gradient=torch.Tensor([<span class="number">0.1</span>,<span class="number">1</span>,<span class="number">10</span>]))</span><br></pre></td></tr></table></figure></p>
<p>这里[0.1,1,10]分别表示的是对正常梯度分别乘上$0.1,1,10$，然后将他们累积在leaves Variable上<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">detach()    <span class="comment">#</span></span><br><span class="line">detach_()</span><br><span class="line">register_hook()</span><br><span class="line">register_grad()</span><br></pre></td></tr></table></figure></p>
<h3 id="Function-class-torch-autograd-Funtion"><a href="#Function-class-torch-autograd-Funtion" class="headerlink" title="Function(class torch.autograd.Funtion)"></a>Function(class torch.autograd.Funtion)</h3><h4 id="用法"><a href="#用法" class="headerlink" title="用法"></a>用法</h4><p>Function一般只定义一个操作，并且它无法保存参数，一般适用于激活函数，pooling等，它需要定义三个方法，<strong>init</strong>(),forward(),backward()（这个需要自己定义怎么求导）<br>Model保存了参数，适合定义一层，如线性层(Linear layer),卷积层(conv layer),也适合定义一个网络。<br>和Model的区别，model只需要定义<strong>init()</strong>,foward()方法，backward()不需要我们定义，它可以由自动求导机制计算。</p>
<p>Function定义只是一个函数，forward和backward都只与这个Function的输入和输出有关</p>
<h4 id="functions"><a href="#functions" class="headerlink" title="functions"></a>functions</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyReLU</span><span class="params">(torch.autograd.Function)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    We can implement our own custom autograd Functions by subclassing</span></span><br><span class="line"><span class="string">    torch.autograd.Function and implementing the forward and backward passes</span></span><br><span class="line"><span class="string">    which operate on Tensors.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        In the forward pass we receive a Tensor containing the input and return a</span></span><br><span class="line"><span class="string">        Tensor containing the output. You can cache arbitrary Tensors for use in the</span></span><br><span class="line"><span class="string">        backward pass using the save_for_backward method.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.save_for_backward(input)</span><br><span class="line">        <span class="keyword">return</span> input.clamp(min=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(self, grad_output)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        In the backward pass we receive a Tensor containing the gradient of the loss</span></span><br><span class="line"><span class="string">        with respect to the output, and we need to compute the gradient of the loss</span></span><br><span class="line"><span class="string">        with respect to the input.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        input, = self.saved_tensors</span><br><span class="line">        grad_input = grad_output.clone()</span><br><span class="line">        grad_input[input &lt; <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">        <span class="keyword">return</span> grad_input</span><br><span class="line"></span><br><span class="line">dtype = torch.FloatTensor</span><br><span class="line"><span class="comment"># dtype = torch.cuda.FloatTensor # Uncomment this to run on GPU</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></span><br><span class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></span><br><span class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create random Tensors to hold input and outputs, and wrap them in Variables.</span></span><br><span class="line">x = Variable(torch.randn(N, D_in).type(dtype), requires_grad=<span class="literal">False</span>)</span><br><span class="line">y = Variable(torch.randn(N, D_out).type(dtype), requires_grad=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create random Tensors for weights, and wrap them in Variables.</span></span><br><span class="line">w1 = Variable(torch.randn(D_in, H).type(dtype), requires_grad=<span class="literal">True</span>)</span><br><span class="line">w2 = Variable(torch.randn(H, D_out).type(dtype), requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">1e-6</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</span><br><span class="line">    <span class="comment"># Construct an instance of our MyReLU class to use in our network</span></span><br><span class="line">    relu = MyReLU()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Forward pass: compute predicted y using operations on Variables; we compute</span></span><br><span class="line">    <span class="comment"># ReLU using our custom autograd operation.</span></span><br><span class="line">    y_pred = relu(x.mm(w1)).mm(w2)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute and print loss</span></span><br><span class="line">    loss = (y_pred - y).pow(<span class="number">2</span>).sum()</span><br><span class="line">    print(t, loss.data[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Use autograd to compute the backward pass.</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update weights using gradient descent</span></span><br><span class="line">    w1.data -= learning_rate * w1.grad.data</span><br><span class="line">    w2.data -= learning_rate * w2.grad.data</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Manually zero the gradients after updating weights</span></span><br><span class="line">    w1.grad.data.zero_()</span><br><span class="line">    w2.grad.data.zero_()</span><br></pre></td></tr></table></figure>
<h2 id="torch-nn"><a href="#torch-nn" class="headerlink" title="torch.nn"></a>torch.nn</h2><h3 id="Container"><a href="#Container" class="headerlink" title="Container"></a>Container</h3><h4 id="Module"><a href="#Module" class="headerlink" title="Module"></a>Module</h4><p>Module是所有模型的基类。<br>它有以下几个常用的函数和常见的属性。</p>
<h5 id="常见的函数"><a href="#常见的函数" class="headerlink" title="常见的函数"></a>常见的函数</h5><ul>
<li>Module.forward() # 前向传播</li>
<li>Module.modules()  # 返回module中所有的module，包含整个module，详情可见<a href="htts">module.modules()</a></li>
<li>Module.named_modules() # 同时返回module的名字</li>
<li>Module.children() # 返回module中所有的子module，不包含整个module，详情可见<a href="htts">module.modules()</a></li>
<li>Module.named_children() # 同时返回子module的名字</li>
<li>Module.parameters() # 返回模型的参数</li>
<li>Module.named_parameters() # 同时返回parameter的名字</li>
<li>Module.state_dict()  # 保存模型参数</li>
<li>Module.load_state_dict()  # 加载模型参数</li>
<li>Module.to() # </li>
<li>Module.cuda()</li>
<li>Module.cpu() </li>
<li>Module.apply(fn) # 对模型中的每一个module都调用fn函数</li>
<li>Module._apply()</li>
</ul>
<h5 id="常见的属性"><a href="#常见的属性" class="headerlink" title="常见的属性"></a>常见的属性</h5><ul>
<li>self._backend = thnn_backend</li>
<li>self._parameters = OrderedDict()</li>
<li>self._buffers = OrderedDict()</li>
<li>self._backward_hooks = OrderedDict()</li>
<li>self._forward_hooks = OrderedDict()</li>
<li>self._forward_pre_hooks = OrderedDict()</li>
<li>self._state_dict_hooks = OrderedDict()</li>
<li>self._load_state_dict_pre_hooks = OrderedDict()</li>
<li>self._modules = OrderedDict()</li>
<li>self.training = True</li>
</ul>
<h5 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h5><p>关于module,children_modules,parameters的<a href>代码</a></p>
<h4 id="Sequential"><a href="#Sequential" class="headerlink" title="Sequential"></a>Sequential</h4><h3 id="Convolution-Layers"><a href="#Convolution-Layers" class="headerlink" title="Convolution Layers"></a>Convolution Layers</h3><h4 id="torch-nn-Conv2d"><a href="#torch-nn-Conv2d" class="headerlink" title="torch.nn.Conv2d"></a>torch.nn.Conv2d</h4><h5 id="类声明"><a href="#类声明" class="headerlink" title="类声明"></a>类声明</h5><p>应用2维卷积到输入信号中。<br>torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)</p>
<blockquote>
<p>Applies a 2D convolution over an input signal composed of several input planes.</p>
</blockquote>
<h5 id="参数声明"><a href="#参数声明" class="headerlink" title="参数声明"></a>参数声明</h5><p>in_channels (int) – 输入图像的通道<br>out_channels (int) – 卷积产生的输出通道数（也就是有几个kernel）<br>kernel_size (int or tuple) – kernel的大小<br>stride (int or tuple, optional) – 卷积的步长，默认为$1$<br>padding (int or tuple, optional) – 向输入数据的各边添加Zero-padding的数量，默认为$0$<br>dilation (int or tuple, optional) – Spacing between kernel elements. Default: 1<br>groups (int, optional) – Number of blocked connections from input channels to output channels. Default: 1<br>bias (bool, optional) – If True, adds a learnable bias to the output</p>
<h5 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h5><h6 id="例子1"><a href="#例子1" class="headerlink" title="例子1"></a>例子1</h6><p>用$6$个$5\times 5$的filter处理维度为$32\times 32\times 1$的图像。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">model = torch.nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">input = torch.randn(<span class="number">16</span>, <span class="number">1</span>, <span class="number">32</span>, <span class="number">32</span>)</span><br><span class="line">output = model(input)</span><br><span class="line">print(output.size())</span><br></pre></td></tr></table></figure></p>
<p>输出是：</p>
<blockquote>
<p>torch.Size([16, 6, 28, 28])</p>
</blockquote>
<h6 id="例子2"><a href="#例子2" class="headerlink" title="例子2"></a>例子2</h6><p>stride和padding<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"></span><br><span class="line">inputs = Variable(torch.randn(<span class="number">64</span>,<span class="number">3</span>,<span class="number">32</span>,<span class="number">32</span>))</span><br><span class="line"></span><br><span class="line">m1 = nn.Conv2d(<span class="number">3</span>,<span class="number">16</span>,<span class="number">3</span>)</span><br><span class="line">print(m1)</span><br><span class="line">output1 = m1(inputs)</span><br><span class="line">print(output1.size())</span><br><span class="line"></span><br><span class="line">m2 = nn.Conv2d(<span class="number">3</span>, <span class="number">16</span>, <span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">print(m2)</span><br><span class="line">output2 = m2(inputs)</span><br><span class="line">print(output2.size())</span><br></pre></td></tr></table></figure></p>
<p>输出</p>
<blockquote>
<p>Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))<br>torch.Size([64, 16, 30, 30])<br>Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))<br>torch.Size([64, 16, 32, 32])</p>
</blockquote>
<h3 id="Pooling-Layers"><a href="#Pooling-Layers" class="headerlink" title="Pooling Layers"></a>Pooling Layers</h3><h4 id="MaxPool2dd"><a href="#MaxPool2dd" class="headerlink" title="MaxPool2dd"></a>MaxPool2dd</h4><p>MaxPool2d这个layer stride默认是和kernel_size相同的<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch import nn</span><br><span class="line">from torch.autograd import Variable</span><br><span class="line"></span><br><span class="line"># maxpool2d</span><br><span class="line"># class torch.nn.MaxPool2d(kernel_size,stride=None,padding=0,dilation=1,return_indices=False,ceil_mode=False)</span><br><span class="line"></span><br><span class="line">print(&quot;input:&quot;)</span><br><span class="line">input = Variable(torch.randn(30,20,32,32))</span><br><span class="line">print(input.size())</span><br><span class="line"></span><br><span class="line">m2 = nn.MaxPool2d(5)</span><br><span class="line">print(m2)</span><br><span class="line"></span><br><span class="line">for param in m2.parameters():</span><br><span class="line">  print(param)</span><br><span class="line"></span><br><span class="line">print(m2.state_dict().keys())</span><br><span class="line"></span><br><span class="line">output = m2(input)</span><br><span class="line">print(&quot;output:&quot;)</span><br><span class="line">print(output.size())</span><br></pre></td></tr></table></figure></p>
<p>输出</p>
<blockquote>
<p>input:<br>torch.Size([30, 20, 32, 32])<br>MaxPool2d (size=(5, 5), stride=(5, 5), dilation=(1, 1))<br>[]<br>output:<br>torch.Size([30, 20, 6, 6])</p>
</blockquote>
<h3 id="Padding-Layers"><a href="#Padding-Layers" class="headerlink" title="Padding Layers"></a>Padding Layers</h3><h3 id="Linear-layers"><a href="#Linear-layers" class="headerlink" title="Linear layers"></a>Linear layers</h3><h3 id="Dropout-layers"><a href="#Dropout-layers" class="headerlink" title="Dropout layers"></a>Dropout layers</h3><h4 id="Drop2D"><a href="#Drop2D" class="headerlink" title="Drop2D"></a>Drop2D</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line">m = nn.Dropout2d(<span class="number">0.3</span>)</span><br><span class="line">print(m)</span><br><span class="line">inputs = torch.randn(<span class="number">1</span>,<span class="number">28</span>,<span class="number">28</span>)</span><br><span class="line">outputs = m(inputs)</span><br><span class="line">print(outputs)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<blockquote>
<p>Dropout2d(p=0.3)<br>([[[ 0.8535,  1.0314,  2.7904,  1.2136,  2.7561, -2.0429,  0.0772,<br>     -1.9372, -0.0864, -1.4132, -0.1648,  0.2403,  0.5727,  0.8102,<br>      0.4544,  0.1414,  0.1547, -0.9266, -0.6033,  0.5813, -1.3541,<br>     -0.0536,  0.9574,  0.0554,  0.8368,  0.7633, -0.3377, -1.4293],<br>    [ 0.0000,  0.0000, -0.0000, -0.0000,  0.0000, -0.0000,  0.0000,<br>      0.0000,  0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000,<br>      0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000,<br>     -0.0000,  0.0000,  0.0000, -0.0000, -0.0000, -0.0000, -0.0000],<br>      …<br>    [ 0.6452, -0.6455,  0.2370,  0.1088, -0.5421, -0.5120, -2.2915,<br>      0.2061,  1.6384,  2.2276,  2.4022,  0.2033,  0.6984,  0.1254,<br>      1.1627,  1.0699, -2.1868,  1.1293, -0.7030,  0.0454, -1.5428,<br>      -2.4052, -0.3204, -1.5984,  0.1282,  0.2127, -2.3506, -2.2395]]])</p>
</blockquote>
<p>会发现输出的数组中有很多被置为$0$了。</p>
<h3 id="Loss-function"><a href="#Loss-function" class="headerlink" title="Loss function"></a>Loss function</h3><h2 id="torch-nn-functional"><a href="#torch-nn-functional" class="headerlink" title="torch.nn.functional"></a>torch.nn.functional</h2><h3 id="convoludion-functions"><a href="#convoludion-functions" class="headerlink" title="convoludion functions"></a>convoludion functions</h3><h4 id="conv2d"><a href="#conv2d" class="headerlink" title="conv2d"></a>conv2d</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torch.nn as nn</span><br><span class="line">import torch.nn.functional as F</span><br><span class="line">from torch.autograd import Variable</span><br><span class="line"></span><br><span class="line">inputs = Variable(torch.randn(64,3,32,32))</span><br><span class="line"></span><br><span class="line">filters1 = Variable(torch.randn(16,3,3,3))</span><br><span class="line">output1 = F.conv2d(inputs,filters1)</span><br><span class="line">print(output1.size())</span><br><span class="line"></span><br><span class="line">filters2 = Variable(torch.randn(16,3,3,3))</span><br><span class="line">output2 = F.conv2d(inputs,filters2,padding=1)</span><br><span class="line">print(output2.size())</span><br></pre></td></tr></table></figure>
<p>输出</p>
<blockquote>
<p>torch.Size([64, 16, 30, 30])<br>torch.Size([64, 16, 32, 32])</p>
</blockquote>
<h3 id="relu-functions"><a href="#relu-functions" class="headerlink" title="relu functions"></a>relu functions</h3><h3 id="pooling-functions"><a href="#pooling-functions" class="headerlink" title="pooling functions"></a>pooling functions</h3><h3 id="dropout-functions"><a href="#dropout-functions" class="headerlink" title="dropout functions"></a>dropout functions</h3><h4 id="例子-1"><a href="#例子-1" class="headerlink" title="例子"></a>例子</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"> </span><br><span class="line">x = torch.randn(<span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line">y = F.dropout(x, <span class="number">0.5</span>, <span class="literal">True</span>)</span><br><span class="line">y = F.dropout2d(x, <span class="number">0.5</span>)</span><br><span class="line"> </span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure>
<p>注意$9$中说的问题，不过可能已经被改正了，注意一些就是了。</p>
<h3 id="linear-functions"><a href="#linear-functions" class="headerlink" title="linear functions"></a>linear functions</h3><h3 id="loss-functions"><a href="#loss-functions" class="headerlink" title="loss functions"></a>loss functions</h3><h2 id="torch-optim"><a href="#torch-optim" class="headerlink" title="torch.optim"></a>torch.optim</h2><h3 id="基类class-Optimizer-object"><a href="#基类class-Optimizer-object" class="headerlink" title="基类class Optimizer(object)"></a>基类class Optimizer(object)</h3><p>Optimizer是所有optimizer的基类。<br>调用任何优化器都要先初始化Optimizer类，这里拿Adam优化器举例子。Adam optimizer的init函数如下所示：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Adam</span><span class="params">(Optimizer)</span>:</span></span><br><span class="line">    <span class="string">r"""Implements Adam algorithm.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    It has been proposed in `Adam: A Method for Stochastic Optimization`_.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        params (iterable): iterable of parameters to optimize or dicts defining</span></span><br><span class="line"><span class="string">            parameter groups</span></span><br><span class="line"><span class="string">        lr (float, optional): learning rate (default: 1e-3)</span></span><br><span class="line"><span class="string">        betas (Tuple[float, float], optional): coefficients used for computing</span></span><br><span class="line"><span class="string">            running averages of gradient and its square (default: (0.9, 0.999))</span></span><br><span class="line"><span class="string">        eps (float, optional): term added to the denominator to improve</span></span><br><span class="line"><span class="string">            numerical stability (default: 1e-8)</span></span><br><span class="line"><span class="string">        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)</span></span><br><span class="line"><span class="string">        amsgrad (boolean, optional): whether to use the AMSGrad variant of this</span></span><br><span class="line"><span class="string">            algorithm from the paper `On the Convergence of Adam and Beyond`_</span></span><br><span class="line"><span class="string">            (default: False)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    .. _Adam\: A Method for Stochastic Optimization:</span></span><br><span class="line"><span class="string">        https://arxiv.org/abs/1412.6980</span></span><br><span class="line"><span class="string">    .. _On the Convergence of Adam and Beyond:</span></span><br><span class="line"><span class="string">        https://openreview.net/forum?id=ryQu7f-RZ</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, params, lr=<span class="number">1e-3</span>, betas=<span class="params">(<span class="number">0.9</span>, <span class="number">0.999</span>)</span>, eps=<span class="number">1e-8</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 weight_decay=<span class="number">0</span>, amsgrad=False)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="number">0.0</span> &lt;= lr:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">"Invalid learning rate: &#123;&#125;"</span>.format(lr))</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="number">0.0</span> &lt;= eps:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">"Invalid epsilon value: &#123;&#125;"</span>.format(eps))</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="number">0.0</span> &lt;= betas[<span class="number">0</span>] &lt; <span class="number">1.0</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">"Invalid beta parameter at index 0: &#123;&#125;"</span>.format(betas[<span class="number">0</span>]))</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="number">0.0</span> &lt;= betas[<span class="number">1</span>] &lt; <span class="number">1.0</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">"Invalid beta parameter at index 1: &#123;&#125;"</span>.format(betas[<span class="number">1</span>]))</span><br><span class="line">        defaults = dict(lr=lr, betas=betas, eps=eps,</span><br><span class="line">                        weight_decay=weight_decay, amsgrad=amsgrad)</span><br><span class="line">        super(Adam, self).__init__(params, defaults)</span><br></pre></td></tr></table></figure></p>
<p>上述代码将学习率lr,beta,epsilon,weight_decay,amsgrad等封装在一个dict中，然后将其传给Optimizer的init函数，其代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Optimizer</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">r"""Base class for all optimizers.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    .. warning::</span></span><br><span class="line"><span class="string">        Parameters need to be specified as collections that have a deterministic</span></span><br><span class="line"><span class="string">        ordering that is consistent between runs. Examples of objects that don't</span></span><br><span class="line"><span class="string">        satisfy those properties are sets and iterators over values of dictionaries.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        params (iterable): an iterable of :class:`torch.Tensor` s or</span></span><br><span class="line"><span class="string">            :class:`dict` s. Specifies what Tensors should be optimized.</span></span><br><span class="line"><span class="string">        defaults: (dict): a dict containing default values of optimization</span></span><br><span class="line"><span class="string">            options (used when a parameter group doesn't specify them).</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, params, defaults)</span>:</span></span><br><span class="line">        self.defaults = defaults</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> isinstance(params, torch.Tensor):</span><br><span class="line">            <span class="keyword">raise</span> TypeError(<span class="string">"params argument given to the optimizer should be "</span></span><br><span class="line">                            <span class="string">"an iterable of Tensors or dicts, but got "</span> +</span><br><span class="line">                            torch.typename(params))</span><br><span class="line"></span><br><span class="line">        self.state = defaultdict(dict)</span><br><span class="line">        self.param_groups = []</span><br><span class="line"></span><br><span class="line">        param_groups = list(params)</span><br><span class="line">        <span class="keyword">if</span> len(param_groups) == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">"optimizer got an empty parameter list"</span>)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> isinstance(param_groups[<span class="number">0</span>], dict):</span><br><span class="line">            param_groups = [&#123;<span class="string">'params'</span>: param_groups&#125;]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> param_group <span class="keyword">in</span> param_groups:</span><br><span class="line">            self.add_param_group(param_group)</span><br></pre></td></tr></table></figure></p>
<p>从这里可以看出来，每个pytorch给出的optimizer至少有以下三个属性和四个函数：<br>属性：</p>
<ul>
<li>self.defaults # 字典类型，主要包含学习率等值</li>
<li>self.state # defaultdict(\<class 'dict'\>, {}) state存放的是</class></li>
<li>self.param_gropus # \<class 'list'\>:[]，prama_groups是一个字典类型的列表，用来存放parameters。</class></li>
</ul>
<p>函数：</p>
<ul>
<li>self.zero_grad()  # 将optimizer中参数的梯度置零</li>
<li>self.step()  # 将梯度应用在参数上</li>
<li>self.state_dict() # 返回optimizer的state,包括state和param_groups。</li>
<li>self.load_state_dict()  # 加载optimizer的state。</li>
<li>self.add_param_group()  # 将一个param group添加到param_groups。可以用在fine-tune上，只添加我们需要训练的层数，然后其他层不动。</li>
</ul>
<p>如果param已经是一个字典列表的话，就无需操作，否则就需要把param转化成一个字典param_groups。然后对param_groups中的每一个param_group调用add_param_group(param_group)函数将param_group字典和defaults字典拼接成一个新的param_group字典添加到self.param_groups中。</p>
<h2 id="torchvision"><a href="#torchvision" class="headerlink" title="torchvision"></a>torchvision</h2><h3 id="torchvision-datasets"><a href="#torchvision-datasets" class="headerlink" title="torchvision.datasets"></a>torchvision.datasets</h3><p>torchvision提供了很多数据集<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line">print(torchvision.datasets.__all__)</span><br></pre></td></tr></table></figure></p>
<blockquote>
<p>(‘LSUN’, ‘LSUNClass’, ‘ImageFolder’, ‘DatasetFolder’, ‘FakeData’, ‘CocoCaptions’,     ‘CocoDetection’, ‘CIFAR10’, ‘CIFAR100’, ‘EMNIST’, ‘FashionMNIST’, ‘MNIST’, ‘STL10’,     ‘SVHN’, ‘PhotoTour’, ‘SEMEION’, ‘Omniglot’)</p>
</blockquote>
<h3 id="CIFAR10"><a href="#CIFAR10" class="headerlink" title="CIFAR10"></a>CIFAR10</h3><h4 id="原型"><a href="#原型" class="headerlink" title="原型"></a>原型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">calss torchvision.datasets.CIFAR10(root, train=<span class="literal">True</span>, transform=<span class="literal">None</span>, target_transform=<span class="literal">None</span>, download=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<h4 id="参数-1"><a href="#参数-1" class="headerlink" title="参数"></a>参数</h4><p>root (string) – cifar-10-batches-py的存放目录或者download设置为True时将会存放的目录。<br>train (bool, optional) – 设置为True的时候, 从training set创建dataset, 否则从test set创建dataset.<br>transform (callable, optional) – 输入是一个 PIL image，返回一个transformed的版本。如，transforms.RandomCrop<br>target_transform (callable, optional) – A function/transform that takes in the target and transforms it.<br>download (bool, optional) – If true, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again.</p>
<h4 id="例子-2"><a href="#例子-2" class="headerlink" title="例子"></a>例子</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line">trainset = torchvision.datasets.CIFAR100(root=<span class="string">"./datasets"</span>, train=<span class="literal">True</span>, transform=    <span class="literal">None</span>, download=<span class="literal">True</span>)</span><br><span class="line">testset = torchvision.datasets.CIFAR100(root=<span class="string">"./datasets"</span>, train=<span class="literal">False</span>, transform=    <span class="literal">None</span>, download=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h3 id="torchvision-models"><a href="#torchvision-models" class="headerlink" title="torchvision.models"></a>torchvision.models</h3><p>模型</p>
<h3 id="torchvision-transforms"><a href="#torchvision-transforms" class="headerlink" title="torchvision.transforms"></a>torchvision.transforms</h3><p>transform</p>
<h3 id="torchvision-utils"><a href="#torchvision-utils" class="headerlink" title="torchvision.utils"></a>torchvision.utils</h3><p>一些工具包</p>
<h2 id="torch-utils-data"><a href="#torch-utils-data" class="headerlink" title="torch.utils.data"></a>torch.utils.data</h2><h3 id="Dataloader"><a href="#Dataloader" class="headerlink" title="Dataloader"></a>Dataloader</h3><h4 id="原型-1"><a href="#原型-1" class="headerlink" title="原型"></a>原型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">torch</span>.<span class="title">utils</span>.<span class="title">data</span>.<span class="title">DataLoader</span><span class="params">(dataset, batch_size=<span class="number">1</span>, shuffle=False, sampler=None, batch_sampler=None, num_workers=<span class="number">0</span>, collate_fn=&lt;function default_collate&gt;, pin_memory=False, drop_last=False, timeout=<span class="number">0</span>, worker_init_fn=None)</span></span></span><br></pre></td></tr></table></figure>
<h4 id="参数-2"><a href="#参数-2" class="headerlink" title="参数"></a>参数</h4><p>dataset (Dataset) – 从哪加载数据<br>batch_size (int, optional) – batch大小 (default: 1).<br>shuffle (bool, optional) – 每个epoch的数据是否打乱 (default: False).<br>sampler (Sampler, optional) – 定义采样策略。如果指定这个参数, shuffle必须是False.<br>batch_sampler (Sampler, optional) – like sampler, but returns a batch of indices at a time. Mutually exclusive with batch_size, shuffle, sampler, and drop_last.<br>num_workers (int, optional) – 多少个子进程用来进行数据加载。0代表使用主进程加载数据 (default: 0)<br>collate_fn (callable, optional) – merges a list of samples to form a mini-batch.<br>pin_memory (bool, optional) – If True, the data loader will copy tensors into CUDA pinned memory before returning them.<br>drop_last (bool, optional) – set to True to drop the last incomplete batch, if the dataset size is not divisible by the batch size. If False and the size of dataset is not divisible by the batch size, then the last batch will be smaller. (default: False)<br>timeout (numeric, optional) – if positive, the timeout value for collecting a batch from workers. Should always be non-negative. (default: 0)<br>worker_init_fn (callable, optional) – If not None, this will be called on each worker subprocess with the worker id (an int in [0, num_workers - 1]) as input, after seeding and before data loading. (default: None)</p>
<h4 id="例子-3"><a href="#例子-3" class="headerlink" title="例子"></a>例子</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line">dataset = torchvision.datasets.CIFAR100(root=<span class="string">'./data'</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=<span class="literal">None</span>)</span><br><span class="line">train_loader = torch.utils.data.DataLoader(dataset, bath_size=<span class="number">16</span>, shuffle=<span class="literal">False</span>, num_worker=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<h4 id="如何访问DataLoader返回值"><a href="#如何访问DataLoader返回值" class="headerlink" title="如何访问DataLoader返回值"></a>如何访问DataLoader返回值</h4><p>train_loader不是整数，所以不能用range，这里用enumerate()，i是<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i, data <span class="keyword">in</span> enumerate(train_loader):</span><br><span class="line">    images, labels = data</span><br></pre></td></tr></table></figure></p>
<h2 id="torch-distributed"><a href="#torch-distributed" class="headerlink" title="torch.distributed"></a>torch.distributed</h2><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>1.<a href="https://pytorch.org/docs/stable/torch.html" target="_blank" rel="noopener">https://pytorch.org/docs/stable/torch.html</a><br>2.<a href="https://pytorch.org/docs/stable/nn.html" target="_blank" rel="noopener">https://pytorch.org/docs/stable/nn.html</a><br>3.<a href="http://pytorch.org/tutorials/beginner/pytorch_with_examples.html" target="_blank" rel="noopener">http://pytorch.org/tutorials/beginner/pytorch_with_examples.html</a><br>4.<a href="https://discuss.pytorch.org/t/distributed-model-parallelism/10377" target="_blank" rel="noopener">https://discuss.pytorch.org/t/distributed-model-parallelism/10377</a><br>5.<a href="https://ptorch.com/news/40.html" target="_blank" rel="noopener">https://ptorch.com/news/40.html</a><br>6.<a href="https://discuss.pytorch.org/t/distributed-data-parallel-freezes-without-error-message/8009" target="_blank" rel="noopener">https://discuss.pytorch.org/t/distributed-data-parallel-freezes-without-error-message/8009</a><br>7.<a href="https://discuss.pytorch.org/t/runtimeerror-cudnn-status-arch-mismatch/3580" target="_blank" rel="noopener">https://discuss.pytorch.org/t/runtimeerror-cudnn-status-arch-mismatch/3580</a><br>8.<a href="https://discuss.pytorch.org/t/error-when-using-cudnn/577/7" target="_blank" rel="noopener">https://discuss.pytorch.org/t/error-when-using-cudnn/577/7</a><br>9.<a href="https://www.zhihu.com/question/67209417" target="_blank" rel="noopener">https://www.zhihu.com/question/67209417</a><br>10.<a href="https://pytorch.org/docs/stable/distributions.html#categorical" target="_blank" rel="noopener">https://pytorch.org/docs/stable/distributions.html#categorical</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://mxxhcm.github.io/2019/03/13/dict-values-object-does-not-support-indexing/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="马晓鑫爱马荟荟">
      <meta itemprop="description" content="记录硕士三年自己的积累">
      <meta itemprop="image" content="/images/favicon.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="mxxhcm's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/03/13/dict-values-object-does-not-support-indexing/" class="post-title-link" itemprop="http://mxxhcm.github.io/page/8/index.html">'dict_values' object does not support indexing</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-03-13 10:40:03" itemprop="dateCreated datePublished" datetime="2019-03-13T10:40:03+08:00">2019-03-13</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-05-07 00:22:27" itemprop="dateModified" datetime="2019-05-07T00:22:27+08:00">2019-05-07</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Error/" itemprop="url" rel="index"><span itemprop="name">Error</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="python3-dict"><a href="#python3-dict" class="headerlink" title="python3 dict"></a>python3 dict</h2><p>python3 中调用字典对象的一些函数，返回值是view objects。如果要转换为list的话，需要使用list()强制转换。<br>而python2的返回值直接就是list。</p>
<blockquote>
<p>The objects returned by dict.keys(), dict.values() and dict.items() are view objects. They provide a dynamic view on the dictionary’s entries, which means that when the dictionary changes, the view reflects these changes.</p>
</blockquote>
<h3 id="代码示例"><a href="#代码示例" class="headerlink" title="代码示例"></a>代码示例</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">m_dict = &#123;<span class="string">'a'</span>: <span class="number">10</span>, <span class="string">'b'</span>: <span class="number">20</span>&#125;</span><br><span class="line">values = m_dict.values()</span><br><span class="line">print(type(values))</span><br><span class="line">print(values)</span><br><span class="line">print(<span class="string">"\n"</span>)</span><br><span class="line"></span><br><span class="line">items = m_dict.items()</span><br><span class="line">print(type(items))</span><br><span class="line">print(items)</span><br><span class="line">print(<span class="string">"\n"</span>)</span><br><span class="line"></span><br><span class="line">keys = m_dict.keys()</span><br><span class="line">print(type(keys))</span><br><span class="line">print(keys)</span><br><span class="line">print(<span class="string">"\n"</span>)</span><br></pre></td></tr></table></figure>
<p>如果使用python3执行以上代码，输出结果如下所示：</p>
<blockquote>
<p>class ‘dict_values’<br>dict_values([10, 20])</p>
<p>class ‘dict_items’<br>dict_items([(‘a’, 10), (‘b’, 20)])</p>
<p>class ‘dict_keys’<br>dict_keys([‘a’, ‘b’])</p>
</blockquote>
<p>如果使用python2执行以上代码，输出结果如下所示：</p>
<blockquote>
<p>type ‘list’<br>[10, 20]                                                                                </p>
<p>type ‘list’<br>[(‘a’, 10), (‘b’, 20)]                                                                  </p>
<p>type ‘list’<br>[‘a’, ‘b’]</p>
</blockquote>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>1.<a href="https://www.cnblogs.com/timxgb/p/8905290.html" target="_blank" rel="noopener">https://www.cnblogs.com/timxgb/p/8905290.html</a><br>2.<a href="https://docs.python.org/3/library/stdtypes.html#dictionary-view-objects" target="_blank" rel="noopener">https://docs.python.org/3/library/stdtypes.html#dictionary-view-objects</a><br>3.<a href="https://stackoverflow.com/questions/43663206/typeerror-unsupported-operand-types-for-dict-values-and-int" target="_blank" rel="noopener">https://stackoverflow.com/questions/43663206/typeerror-unsupported-operand-types-for-dict-values-and-int</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://mxxhcm.github.io/2019/03/09/markdown帮助/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="马晓鑫爱马荟荟">
      <meta itemprop="description" content="记录硕士三年自己的积累">
      <meta itemprop="image" content="/images/favicon.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="mxxhcm's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/03/09/markdown帮助/" class="post-title-link" itemprop="http://mxxhcm.github.io/page/8/index.html">markdown帮助</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-03-09 19:53:32" itemprop="dateCreated datePublished" datetime="2019-03-09T19:53:32+08:00">2019-03-09</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-05-07 00:22:27" itemprop="dateModified" datetime="2019-05-07T00:22:27+08:00">2019-05-07</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/工具/" itemprop="url" rel="index"><span itemprop="name">工具</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h2><h3 id="代码引用"><a href="#代码引用" class="headerlink" title="代码引用"></a>代码引用</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br></pre></td></tr></table></figure>
<h3 id="文字引用"><a href="#文字引用" class="headerlink" title="文字引用"></a>文字引用</h3><blockquote>
<p>实际是人类进步的阶梯。　－－高尔基</p>
</blockquote>
<h2 id="表格"><a href="#表格" class="headerlink" title="表格"></a>表格</h2><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">name</th>
<th style="text-align:center">age</th>
<th style="text-align:center">gender</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Alice</td>
<td style="text-align:center">11</td>
<td style="text-align:center">female</td>
</tr>
<tr>
<td style="text-align:center">Bob</td>
<td style="text-align:center">82</td>
<td style="text-align:center">male</td>
</tr>
</tbody>
</table>
</div>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://mxxhcm.github.io/2019/03/07/tensorflow-踩坑（不定期更新）/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="马晓鑫爱马荟荟">
      <meta itemprop="description" content="记录硕士三年自己的积累">
      <meta itemprop="image" content="/images/favicon.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="mxxhcm's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/03/07/tensorflow-踩坑（不定期更新）/" class="post-title-link" itemprop="http://mxxhcm.github.io/page/8/index.html">tensorflow踩坑（不定期更新）</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-03-07 14:51:01" itemprop="dateCreated datePublished" datetime="2019-03-07T14:51:01+08:00">2019-03-07</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-05-08 17:41:45" itemprop="dateModified" datetime="2019-05-08T17:41:45+08:00">2019-05-08</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/python/" itemprop="url" rel="index"><span itemprop="name">python</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><ol>
<li>TypeError: The value of a feed cannot be a tf.Tensor object<br>Sess.run(train, feed_dict={x:images, y:labels}的输入不能是tensor，可以使用sess.run(tensor)得到numpy.array形式的数据再喂给feed_dict。<blockquote>
<p>Once you have launched a sess, you can use your_tensor.eval(session=sess) or sess.run(your_tensor) to get you feed tensor into the format of numpy.array and then feed it to your placeholder.</p>
</blockquote>
</li>
</ol>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>1.<a href="https://github.com/tensorflow/tensorflow/issues/4842" target="_blank" rel="noopener">https://github.com/tensorflow/tensorflow/issues/4842</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://mxxhcm.github.io/2019/03/07/python-pip-使用国内源/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="马晓鑫爱马荟荟">
      <meta itemprop="description" content="记录硕士三年自己的积累">
      <meta itemprop="image" content="/images/favicon.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="mxxhcm's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/03/07/python-pip-使用国内源/" class="post-title-link" itemprop="http://mxxhcm.github.io/page/8/index.html">pip 使用国内源</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-03-07 09:56:54" itemprop="dateCreated datePublished" datetime="2019-03-07T09:56:54+08:00">2019-03-07</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-05-07 00:51:39" itemprop="dateModified" datetime="2019-05-07T00:51:39+08:00">2019-05-07</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/工具/" itemprop="url" rel="index"><span itemprop="name">工具</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="暂时使用国内pip源"><a href="#暂时使用国内pip源" class="headerlink" title="暂时使用国内pip源"></a>暂时使用国内pip源</h2><p>使用清华源<br>~#:pip install -i <a href="https://pypi.tuna.tsinghua.edu.cn/simple" target="_blank" rel="noopener">https://pypi.tuna.tsinghua.edu.cn/simple</a> package-name<br>使用阿里源<br>~#:pip install -i <a href="https://mirrors.aliyun.com/pypi/simple" target="_blank" rel="noopener">https://mirrors.aliyun.com/pypi/simple</a> package-name</p>
<h2 id="将国内pip源设为默认"><a href="#将国内pip源设为默认" class="headerlink" title="将国内pip源设为默认"></a>将国内pip源设为默认</h2><p>~#:pip install pip -U<br>~#:pip config set global.index-url <a href="https://pypi.tuna.tsinghua.edu.cn/simple" target="_blank" rel="noopener">https://pypi.tuna.tsinghua.edu.cn/simple</a></p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>1.<a href="https://mirrors.tuna.tsinghua.edu.cn/help/pypi/" target="_blank" rel="noopener">https://mirrors.tuna.tsinghua.edu.cn/help/pypi/</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://mxxhcm.github.io/2019/03/04/lychee图床搭建/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="马晓鑫爱马荟荟">
      <meta itemprop="description" content="记录硕士三年自己的积累">
      <meta itemprop="image" content="/images/favicon.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="mxxhcm's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/03/04/lychee图床搭建/" class="post-title-link" itemprop="http://mxxhcm.github.io/page/8/index.html">lychee图床搭建</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-03-04 21:03:55" itemprop="dateCreated datePublished" datetime="2019-03-04T21:03:55+08:00">2019-03-04</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-05-07 00:22:27" itemprop="dateModified" datetime="2019-05-07T00:22:27+08:00">2019-05-07</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/工具/" itemprop="url" rel="index"><span itemprop="name">工具</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><h3 id="基本要求"><a href="#基本要求" class="headerlink" title="基本要求"></a>基本要求</h3><ol>
<li>web server (Apache, nginx, etc)</li>
<li>A MySQL database (MariaDB also works)</li>
<li>PHP 7.1 or later with the following extensions: session, exif, mbstring, gd, mysqli, json, zip, and optionally, imagick<br>~#:apt install nginx<br>~#:apt install mysql-server<br>~#:apt install php<h3 id="配置nginx"><a href="#配置nginx" class="headerlink" title="配置nginx"></a>配置nginx</h3>重新安装nginx出现问题，见参考文献2。<h3 id="配置mysql"><a href="#配置mysql" class="headerlink" title="配置mysql"></a>配置mysql</h3>~#:mysql -u root -p<br>默认密码是回车？？？<br>修改密码<br>~#:</li>
</ol>
<h3 id="配置php"><a href="#配置php" class="headerlink" title="配置php"></a>配置php</h3><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>1.<a href="https://juejin.im/post/5c1b869b6fb9a049ad770424" target="_blank" rel="noopener">https://juejin.im/post/5c1b869b6fb9a049ad770424</a><br>2.<a href="https://segmentfault.com/a/1190000014027697?utm_source=tag-newest" target="_blank" rel="noopener">https://segmentfault.com/a/1190000014027697?utm_source=tag-newest</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://mxxhcm.github.io/2019/03/04/查看python-package的安装位置/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="马晓鑫爱马荟荟">
      <meta itemprop="description" content="记录硕士三年自己的积累">
      <meta itemprop="image" content="/images/favicon.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="mxxhcm's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/03/04/查看python-package的安装位置/" class="post-title-link" itemprop="http://mxxhcm.github.io/page/8/index.html">查看python package的安装位置</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-03-04 14:52:16" itemprop="dateCreated datePublished" datetime="2019-03-04T14:52:16+08:00">2019-03-04</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-05-07 00:22:27" itemprop="dateModified" datetime="2019-05-07T00:22:27+08:00">2019-05-07</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/工具/" itemprop="url" rel="index"><span itemprop="name">工具</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>使用pip install package-name之后，不知道该包存在了哪个路径下。<br>可以再次使用pip install package-name，这时候就会给出该包存放在哪个路径下。</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ol>
<li><a href="https://blog.csdn.net/weixin_41712059/article/details/82940516" target="_blank" rel="noopener">https://blog.csdn.net/weixin_41712059/article/details/82940516</a></li>
</ol>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://mxxhcm.github.io/2019/03/04/linux-shadowsocks服务端以及客户端配置/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="马晓鑫爱马荟荟">
      <meta itemprop="description" content="记录硕士三年自己的积累">
      <meta itemprop="image" content="/images/favicon.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="mxxhcm's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/03/04/linux-shadowsocks服务端以及客户端配置/" class="post-title-link" itemprop="http://mxxhcm.github.io/page/8/index.html">shadowsocks服务端以及客户端配置</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-03-04 13:03:57" itemprop="dateCreated datePublished" datetime="2019-03-04T13:03:57+08:00">2019-03-04</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-05-08 10:39:19" itemprop="dateModified" datetime="2019-05-08T10:39:19+08:00">2019-05-08</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/工具/" itemprop="url" rel="index"><span itemprop="name">工具</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="1-服务器端配置"><a href="#1-服务器端配置" class="headerlink" title="1.服务器端配置"></a>1.服务器端配置</h2><p>首先需要有一个VPS账号，vultr,digitalocean,搬瓦工等等都行。</p>
<h3 id="1-1-启用BBR加速"><a href="#1-1-启用BBR加速" class="headerlink" title="1.1.启用BBR加速"></a>1.1.启用BBR加速</h3><p>~#:apt update<br>~#:apt upgrade<br>~#:echo “net.core.default_qdisc=fq” &gt;&gt; /etc/sysctl.conf<br>~#:echo “net.ipv4.tcp_congestion_control=bbr” &gt;&gt; /etc/sysctl.conf<br>~#:sysctl -p<br>上述命令就完成了BBR加速，执行以下命令验证：<br>~#:lsmod |grep bbr<br>看到输出包含tcp_bbr就说明已经成功了。</p>
<h3 id="1-2-搭建shadowsocks-server"><a href="#1-2-搭建shadowsocks-server" class="headerlink" title="1.2.搭建shadowsocks server"></a>1.2.搭建shadowsocks server</h3><h4 id="1-2-1-安装shadowsocks-server"><a href="#1-2-1-安装shadowsocks-server" class="headerlink" title="1.2.1.安装shadowsocks server"></a>1.2.1.安装shadowsocks server</h4><p>~#:apt install python-pip<br>~#:pip install shadowsocks<br>需要说一下的是，shadowsocks目前还不支持python3.5及以上版本，上次我把/usr/bin/python指向了python3.6，就是系统默认的python指向了python3.6，然后就gg了。一定要使用Python 2.6,2.7,3.3,3.4中的一个版本才能使用。。</p>
<h4 id="1-2-2-创建shadowsocks配置文件"><a href="#1-2-2-创建shadowsocks配置文件" class="headerlink" title="1.2.2.创建shadowsocks配置文件"></a>1.2.2.创建shadowsocks配置文件</h4><p>如果你的VPS支持ipv6的话，那么可以开多进程分别运行ipv4和ipv6的shadowsocks server。本地只有ipv4的话，可以用本地ipv4访问ipv6，从而访问byr等网站，但是六维空间对此做了屏蔽。如果本地有ipv6的话，还可以用本地的ipv6访问ipv6实现校园网不走ipv4流量。</p>
<h5 id="1-2-2-1-ipv4配置"><a href="#1-2-2-1-ipv4配置" class="headerlink" title="1.2.2.1.ipv4配置"></a>1.2.2.1.ipv4配置</h5><p>~#:vim /etc/shadowsocks_v4.json<br>配置文件如下<br>{<br>“server”:”0.0.0.0”,<br>“server_port”:8889,<br>“local_address”:”127.0.0.1”,<br>“local_port”:1080,<br>“password”:”你的密码”,<br>“timeout”:600,<br>“method”:”aes-256-cfb”<br>}</p>
<h5 id="1-2-2-2-ipv6配置"><a href="#1-2-2-2-ipv6配置" class="headerlink" title="1.2.2.2.ipv6配置"></a>1.2.2.2.ipv6配置</h5><p>~#:vim /etc/shadowsocks_v6.json<br>配置文件如下<br>{<br>“server”:”::”,<br>“server_port”:8888,<br>“local_address”:”127.0.0.1”,<br>“local_port”:1080,<br>“password”:”你的密码”,<br>“timeout”:600,<br>“method”:”aes-256-cfb”<br>}<br>注意这两个文件的server_port一定要不同，以及双引号必须是英文引号。</p>
<h5 id="1-2-2-3-手动运行shadowsocks-server"><a href="#1-2-2-3-手动运行shadowsocks-server" class="headerlink" title="1.2.2.3.手动运行shadowsocks server"></a>1.2.2.3.手动运行shadowsocks server</h5><p>~#:ssserver -c /etc/shadowsock_v4.json -d start —pid-file ss1.pid<br>~#:ssserver -c /etc/shadowsock_v6.json -d start —pid-file ss2.pid<br>注意这里要给两条命令分配不同的进程号。</p>
<h3 id="1-3-设置shadowsocks-server开机自启"><a href="#1-3-设置shadowsocks-server开机自启" class="headerlink" title="1.3.设置shadowsocks server开机自启"></a>1.3.设置shadowsocks server开机自启</h3><p>如果重启服务器的话，就需要重新手动执行上述命令，这里我们可以把它写成开机自启脚本。<br>~#:vim /etc/init.d/shadowsocks_v4<br>内容如下：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span>!/bin/sh</span><br><span class="line"><span class="meta">#</span>## BEGIN INIT INFO</span><br><span class="line"><span class="meta">#</span> Provides:          apache2</span><br><span class="line"><span class="meta">#</span> Required-Start:    $local_fs $remote_fs $network $syslog</span><br><span class="line"><span class="meta">#</span> Required-Stop:     $local_fs $remote_fs $network $syslog</span><br><span class="line"><span class="meta">#</span> Default-Start:     2 3 4 5</span><br><span class="line"><span class="meta">#</span> Default-Stop:      0 1 6</span><br><span class="line"><span class="meta">#</span> Short-Description: apache2 service</span><br><span class="line"><span class="meta">#</span> Description:       apache2 service daemon</span><br><span class="line"><span class="meta">#</span>## END INIT INFO</span><br><span class="line">start()&#123;</span><br><span class="line">  ssserver -c /etc/shadowsocks_v4.json -d start --pid-file ss2.pid</span><br><span class="line">&#125;</span><br><span class="line">stop()&#123;</span><br><span class="line">  ssserver -c /etc/shadowsocks_v4.json -d stop --pid-file ss2.pid</span><br><span class="line">&#125;</span><br><span class="line">case "$1" in</span><br><span class="line">start)</span><br><span class="line">  start</span><br><span class="line">  ;;</span><br><span class="line">stop)</span><br><span class="line">  stop</span><br><span class="line">  ;;</span><br><span class="line">restart)</span><br><span class="line">  stop</span><br><span class="line">  start</span><br><span class="line">  ;;</span><br><span class="line">*)</span><br><span class="line">  echo "Uasage: $0 &#123;start|reload|stop&#125;$"</span><br><span class="line">  exit 1</span><br><span class="line">  ;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure></p>
<p>~#:vim /etc/init.d/shadowsocks_v6<br>内容如下：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span>!/bin/sh</span><br><span class="line"><span class="meta">#</span>## BEGIN INIT INFO</span><br><span class="line"><span class="meta">#</span> Provides:          apache2</span><br><span class="line"><span class="meta">#</span> Required-Start:    $local_fs $remote_fs $network $syslog</span><br><span class="line"><span class="meta">#</span> Required-Stop:     $local_fs $remote_fs $network $syslog</span><br><span class="line"><span class="meta">#</span> Default-Start:     2 3 4 5</span><br><span class="line"><span class="meta">#</span> Default-Stop:      0 1 6</span><br><span class="line"><span class="meta">#</span> Short-Description: apache2 service</span><br><span class="line"><span class="meta">#</span> Description:       apache2 service daemon</span><br><span class="line"><span class="meta">#</span>## END INIT INFO</span><br><span class="line">start()&#123;</span><br><span class="line">  ssserver -c /etc/shadowsocks_v6.json -d start --pid-file ss1.pid</span><br><span class="line">&#125;</span><br><span class="line">stop()&#123;</span><br><span class="line">  ssserver -c /etc/shadowsocks_v6.json -d stop --pid-file ss1.pid</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line">case "$1" in</span><br><span class="line">start)</span><br><span class="line">  start</span><br><span class="line">  ;;</span><br><span class="line">stop)</span><br><span class="line">  stop</span><br><span class="line">  ;;</span><br><span class="line">restart)</span><br><span class="line">  stop</span><br><span class="line">  start</span><br><span class="line">  ;;</span><br><span class="line">*)</span><br><span class="line">  echo "Uasage: $0 &#123;start|reload|stop&#125;$"</span><br><span class="line">  exit 1</span><br><span class="line">  ;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure></p>
<p>然后执行下列命令即可：<br>~#:chmod a+x /etc/init.d/shadowsocks_v4<br>~#:chmod a+x /etc/init.d/shadowsocks_v6<br>~#:update-rc.d shadowsocks_v4 defaults<br>~#:update-rc.d shadowsocks_v6 defaults</p>
<p>至此，服务器端配置完成。</p>
<h2 id="2-客户端配置"><a href="#2-客户端配置" class="headerlink" title="2.客户端配置"></a>2.客户端配置</h2><h3 id="2-1-Windows客户端配置"><a href="#2-1-Windows客户端配置" class="headerlink" title="2.1.Windows客户端配置"></a>2.1.Windows客户端配置</h3><h4 id="2-1-1安装shadowsock客户端"><a href="#2-1-1安装shadowsock客户端" class="headerlink" title="2.1.1安装shadowsock客户端"></a>2.1.1安装shadowsock客户端</h4><p>到该网址 <a href="https://github.com/shadowsocks/shadowsocks-windows/releases" target="_blank" rel="noopener">https://github.com/shadowsocks/shadowsocks-windows/releases</a> 下载相应的windows客户端程序。<br>然后配置服务器即可～</p>
<h3 id="2-2-Linux客户端配置"><a href="#2-2-Linux客户端配置" class="headerlink" title="2.2.Linux客户端配置"></a>2.2.Linux客户端配置</h3><h4 id="2-2-1-安装shadowsocks程序"><a href="#2-2-1-安装shadowsocks程序" class="headerlink" title="2.2.1.安装shadowsocks程序"></a>2.2.1.安装shadowsocks程序</h4><p>~$:sudo pip install shadowsocks</p>
<h4 id="2-2-2-运行shadowsocks客户端程序"><a href="#2-2-2-运行shadowsocks客户端程序" class="headerlink" title="2.2.2.运行shadowsocks客户端程序"></a>2.2.2.运行shadowsocks客户端程序</h4><p>~$:sudo vim /etc/shadowsocks.json<br>填入以下配置文件<br>{<br>“server”:”填上自己的shadowsocks server ip地址”,<br>“server_port”:”8888”,//填上自己的shadowsocks server 端口”<br>“local_port”:1080,<br>“password”:”mxxhcm150929”,<br>“timeout”:600,<br>“method”:”aes-256-cfb”<br>}</p>
<p>接下来可以执行以下命令运行shadowsocks客户端：<br>~$:sudo sslocal -c /etc/shadowsocks.json<br>然后报错：</p>
<blockquote>
<p>INFO: loading config from /etc/shadowsocks.json<br>2019-03-04 14:37:49 INFO     loading libcrypto from libcrypto.so.1.1<br>Traceback (most recent call last):<br>  File “/usr/local/bin/sslocal”, line 11, in <module><br>    load_entry_point(‘shadowsocks==2.8.2’, ‘console_scripts’, ‘sslocal’)()<br>  File “/usr/local/lib/python2.7/dist-packages/shadowsocks/local.py”, line 39, in main<br>    config = shell.get_config(True)<br>  File “/usr/local/lib/python2.7/dist-packages/shadowsocks/shell.py”, line 262, in get_config<br>    check_config(config, is_local)<br>  File “/usr/local/lib/python2.7/dist-packages/shadowsocks/shell.py”, line 124, in check_config<br>    encrypt.try_cipher(config[‘password’], config[‘method’])<br>  File “/usr/local/lib/python2.7/dist-packages/shadowsocks/encrypt.py”, line 44, in try_cipher<br>    Encryptor(key, method)<br>  File “/usr/local/lib/python2.7/dist-packages/shadowsocks/encrypt.py”, line 83, in <strong>init</strong><br>    random_string(self._method_info[1]))<br>  File “/usr/local/lib/python2.7/dist-packages/shadowsocks/encrypt.py”, line 109, in get_cipher<br>    return m<a href="method, key, iv, op">2</a><br>  File “/usr/local/lib/python2.7/dist-packages/shadowsocks/crypto/openssl.py”, line 76, in <strong>init</strong><br>    load_openssl()<br>  File “/usr/local/lib/python2.7/dist-packages/shadowsocks/crypto/openssl.py”, line 52, in load_openssl<br>    libcrypto.EVP_CIPHER_CTX_cleanup.argtypes = (c_void_p,)<br>  File “/usr/lib/python2.7/ctypes/<strong>init</strong>.py”, line 379, in <strong>getattr</strong><br>    func = self.<strong>getitem</strong>(name)<br>  File “/usr/lib/python2.7/ctypes/<strong>init</strong>.py”, line 384, in <strong>getitem</strong><br>    func = self._FuncPtr((name_or_ordinal, self))<br>AttributeError: /usr/lib/x86_64-linux-gnu/libcrypto.so.1.1: undefined symbol: EVP_CIPHER_CTX_cleanup</module></p>
</blockquote>
<p>按照参考文献4的做法，是在openssl 1.1.0版本中放弃了EVP_CIPHER_CTX_cleanup函数</p>
<blockquote>
<p>EVP_CIPHER_CTX was made opaque in OpenSSL 1.1.0. As a result, EVP_CIPHER_CTX_reset() appeared and EVP_CIPHER_CTX_cleanup() disappeared.<br>EVP_CIPHER_CTX_init() remains as an alias for EVP_CIPHER_CTX_reset().</p>
</blockquote>
<p>将openssl库中的EVP_CIPHER_CTX_cleanup改为EVP_CIPHER_CTX_reset即可。<br>再次执行以下命令，查看shadowsocks安装位置<br>~#:pip install shadowsocks<br>Requirement already satisfied: shadowsocks in /usr/local/lib/python2.7/dist-packages<br>~#:cd /usr/local/lib/python2.7/dist-packages/shadowsocks<br>~#:vim crypto/openssl.py<br>搜索cleanup，将其替换为reset<br>具体位置在第52行libcrypto.EVP_CIPHER_CTX_cleanup.argtypes = (c_void_p,)和第111行libcrypto.EVP_CIPHER_CTX_cleanup(self._ctx) </p>
<h4 id="2-2-3-手动运行后台挂起"><a href="#2-2-3-手动运行后台挂起" class="headerlink" title="2.2.3 手动运行后台挂起"></a>2.2.3 手动运行后台挂起</h4><p>将所有的log重定向到~/.log/sslocal.log文件中<br>~$:mkdir ~/.log<br>~$:touch ~/.log/ss-local.log<br>~$:nohup sslocal -c /etc/shadowsocks_v6.json \&lt;/dev/null &amp;>&gt;~/.log/ss-local.log &amp;</p>
<h4 id="2-2-4-开机自启shadowsocks-client"><a href="#2-2-4-开机自启shadowsocks-client" class="headerlink" title="2.2.4.开机自启shadowsocks client"></a>2.2.4.开机自启shadowsocks client</h4><p>但是这样子的话，每次开机都要重新运行上述命令，太麻烦了。可以写个开机自启脚本。执行以下命令：<br>~$:sudo vim /etc/init.d/shadowsocks<br>内容为以下shell脚本</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span>!/bin/sh</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span>## BEGIN INIT INFO</span><br><span class="line"><span class="meta">#</span> Provides:          shadowsocks local</span><br><span class="line"><span class="meta">#</span> Required-Start:    $local_fs $remote_fs $network $syslog</span><br><span class="line"><span class="meta">#</span> Required-Stop:     $local_fs $remote_fs $network $syslog</span><br><span class="line"><span class="meta">#</span> Default-Start:     2 3 4 5</span><br><span class="line"><span class="meta">#</span> Default-Stop:      0 1 6</span><br><span class="line"><span class="meta">#</span> Short-Description: shadowsocks service</span><br><span class="line"><span class="meta">#</span> Description:       shadowsocks service daemon</span><br><span class="line"><span class="meta">#</span>## END INIT INFO</span><br><span class="line">start()&#123;</span><br><span class="line">　　  sslocal -c /etc/shadowsocks.json -d start</span><br><span class="line">&#125;</span><br><span class="line">stop()&#123;</span><br><span class="line">　　  sslocal -c /etc/shadowsocks.json -d stop</span><br><span class="line">&#125;</span><br><span class="line">case “$1” in</span><br><span class="line">start)</span><br><span class="line">　　　start</span><br><span class="line">　　　;;</span><br><span class="line">stop)</span><br><span class="line">　　　stop</span><br><span class="line">　　　;;</span><br><span class="line">reload)</span><br><span class="line">　　　stop</span><br><span class="line">　　　start</span><br><span class="line">　　　;;</span><br><span class="line">\*)</span><br><span class="line">　　　echo “Usage: $0 &#123;start|reload|stop&#125;”</span><br><span class="line">　　　exit 1</span><br><span class="line">　　　;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure>
<p>然后执行以下命令即可：<br>~$:sudo chomod a+x /etc/init.d/shadowsocks<br>~$:sudo update_rc.d shadowsocks defaults<br>上述命令执行完成以后，进行测试<br>~$:sudo service shadosowcks start</p>
<h4 id="2-2-4-配置代理"><a href="#2-2-4-配置代理" class="headerlink" title="2.2.4.配置代理"></a>2.2.4.配置代理</h4><p>上一步的目的是建立了shadowsocks服务的本地客户端，socks5流量会走该通道，但是浏览器的网页的流量是https的，我们需要配置相应的代理，将https流量转换为socks5流量，走ss客户端到达ss服务端。当然，也可以把其他各种流量，如tcp,udp等各种流量都转换为socks5流量，这个可以通过全局代理实现，也可以通过添加特定的代理规则实现。</p>
<h5 id="2-2-4-1-配置全局代理"><a href="#2-2-4-1-配置全局代理" class="headerlink" title="2.2.4.1.配置全局代理"></a>2.2.4.1.配置全局代理</h5><p>如下图所示，添加ubuntu socks5系统代理：</p>
<p>然后就可以成功上网了。</p>
<h5 id="2-2-4-2-使用SwitchyOmega配置chrome代理"><a href="#2-2-4-2-使用SwitchyOmega配置chrome代理" class="headerlink" title="2.2.4.2.使用SwitchyOmega配置chrome代理"></a>2.2.4.2.使用SwitchyOmega配置chrome代理</h5><p>首先到 <a href="https://github.com/FelisCatus/SwitchyOmega/releases" target="_blank" rel="noopener">https://github.com/FelisCatus/SwitchyOmega/releases</a> 下载SyitchyOmega.crx。然后在chrome的地址栏输入chrome://extensions，将刚才下载的插件拖进去。<br>然后在浏览器右上角就有了这个插件，接下来配置插件。如下图：<br><img src="https:" alt="mxx"><br>直接配置proxy，添加如图所示的规则，这样chrome打开的所有网站都是走代理的。</p>
<h5 id="2-2-4-3-使用privoxy让terminal走socks5"><a href="#2-2-4-3-使用privoxy让terminal走socks5" class="headerlink" title="2.2.4.3.使用privoxy让terminal走socks5"></a>2.2.4.3.使用privoxy让terminal走socks5</h5><p>~$:sudo apt install privoxy<br>~$:sudo vim /etc/privoxy/config<br>取消下列行的注释，或者添加相应条目<br>forward-socks5 / 127.0.0.1:1080 . # SOCKS5代理地址<br>listen-address 127.0.0.1:8118     # HTTP代理地址<br>forward 10.*.*.*/ .               # 内网地址不走代理<br>forward .abc.com/ .             # 指定域名不走代理<br>重启privoxy服务<br>~$:sudo service privoxy restart<br>在bashrc中添加如下环境变量<br>export http_proxy=”<a href="http://127.0.0.1:8118" target="_blank" rel="noopener">http://127.0.0.1:8118</a>“<br>export https_proxy=”<a href="http://127.0.0.1:8118" target="_blank" rel="noopener">http://127.0.0.1:8118</a>“</p>
<p>~$:source ~/.bashrc<br>~$:curl.gs</p>
<h2 id="3-参考文献"><a href="#3-参考文献" class="headerlink" title="3.参考文献"></a>3.参考文献</h2><ol>
<li><a href="http://godjose.com/2017/06/14/new-article/" target="_blank" rel="noopener">http://godjose.com/2017/06/14/new-article/</a></li>
<li><a href="https://www.polarxiong.com/archives/搭建ipv6-VPN-让ipv4上ipv6-下载速度提升到100M.html" target="_blank" rel="noopener">https://www.polarxiong.com/archives/搭建ipv6-VPN-让ipv4上ipv6-下载速度提升到100M.html</a></li>
<li><a href="https://blog.csdn.net/li1914309758/article/details/86510127" target="_blank" rel="noopener">https://blog.csdn.net/li1914309758/article/details/86510127</a></li>
<li><a href="https://blog.csdn.net/blackfrog_unique/article/details/60320737" target="_blank" rel="noopener">https://blog.csdn.net/blackfrog_unique/article/details/60320737</a></li>
<li><a href="https://blog.csdn.net/qq_31851531/article/details/78410146" target="_blank" rel="noopener">https://blog.csdn.net/qq_31851531/article/details/78410146</a></li>
</ol>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://mxxhcm.github.io/2019/03/02/DQN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="马晓鑫爱马荟荟">
      <meta itemprop="description" content="记录硕士三年自己的积累">
      <meta itemprop="image" content="/images/favicon.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="mxxhcm's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/03/02/DQN/" class="post-title-link" itemprop="http://mxxhcm.github.io/page/8/index.html">DQN</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-03-02 19:29:35" itemprop="dateCreated datePublished" datetime="2019-03-02T19:29:35+08:00">2019-03-02</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-05-07 00:22:27" itemprop="dateModified" datetime="2019-05-07T00:22:27+08:00">2019-05-07</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/强化学习/" itemprop="url" rel="index"><span itemprop="name">强化学习</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="Playing-Atari-with-Deep-Reinforcement-Learning"><a href="#Playing-Atari-with-Deep-Reinforcement-Learning" class="headerlink" title="Playing Atari with Deep Reinforcement Learning"></a>Playing Atari with Deep Reinforcement Learning</h2><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>本文首次成功的将深度学习模型应用到强化学习领域，直接从高维的感知输入中学习控制策略。作者提出用一个卷积神经网络来表示值函数，用Q-learning算法的变种来训练这个神经网络。神经网络的输入是原始的图片信息，输出是这个图片对应状态的估计值函数。<br>Atari 2600是一个强化学习的benchmark，共有2600个游戏，每个智能体会得到一个高维的图像输入(210 x 160 RGB视频60Hz)。本文的目标是设计出一个神经网络能够尽可能成功的学会更多的游戏，这个网络的输入只有视频信息，reward和terminal信号以及可能采取的action，和人类玩家得到的信息是一模一样的，当然是计算机能看得懂的信号。</p>
<h3 id="存在的问题"><a href="#存在的问题" class="headerlink" title="存在的问题"></a>存在的问题</h3><ol>
<li>目前绝大多数深度学习问题都需要大量有标记的训练数据。</li>
<li>强化学习需要从一个稀疏的，有噪音的，通常是time-delayed(延迟的)标量信号中学习。这个延迟存在于action和reward之间，而且可以达到几千个时间步那么远，和监督学习中输入和输入之间关系相比要复杂的多。</li>
<li>绝大多数深度学习算法假设样本之间都是独立的，然而强化学习的一个sequence(序列)通常是高度相关的。</li>
<li>当强化学习算法学习到一个新的行为时，数据服从的分布可能会改变，然而深度学习通常假设数据服从一个固定的分布。</li>
</ol>
<h3 id="方案"><a href="#方案" class="headerlink" title="方案"></a>方案</h3><h4 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h4><ol>
<li>智能体与环境进行交互，这里的环境是Atari模拟器。智能体不能观测到模拟器的内部状态，只能得到当前屏幕的一个数字化表示。注意目前智能体的得分取决于之间所有序列上的action和observation。一个action的feedback可能等到好几千个timesteps之后才能得到。</li>
<li>因为智能体只能得到当前屏幕上的图像，所以这个task可以认为是部分可观测的，因为仅仅从当前的屏幕图像$x_t$上是不能完全理解当前的状况的。所有的序列都认为在有限步骤内是会结束的。</li>
<li>智能体的目标是通过采取action和智能体交互最大化未来的reward。定义$t$时刻的回报return为$R_t = \sum^T_{t’=t}\gamma^{t’-t}r_{t’}$，其中$\gamma$是折扣因子，$T$是游戏终止的时间步。</li>
<li>定义最优的动作值函数$Q^{*}(s,a)$是遵循某个最优策略在状态$s$处采取动作$a$能获得的最大的期望回报，$Q^{*(s,a)} = max_{\pi}E[R_t|s_t=s,a_t=a,\pi]$。</li>
<li>最优的动作值函数遵循Bellman equation。如果在下个时间步的状态$s’$处，对于所有可能的$a’$，$Q^{*}(s’,a’)$的最优值是已知的（这里就是对于每一个$a’$，都会有一个最优的$Q(s’,a’)$，最优的策略就是选择最大化$r+Q^{*}(s’,a’)$的动作$a’$：<script type="math/tex; mode=display">Q^{*}(s,a) = E_{s\sim E}[r+ \gamma max_{a'} Q^{*}(s',a')|s,a]</script>强化学习的一个思路就是使用Bellman equation更新动作值函数，$Q_{i+1}(s,a) = E[r + \gamma Q_i(s’,a’)|s,a]$，当$i\rightarrow \infty$时，$Q_i \rightarrow Q^{*}$。</li>
<li>在实践中，上述的方法是不可行的，因为动作值函数是基于每一个序列进行评估的，并没有进行泛化，当有无穷多个序列的时候，这就是不可行的，这时候可以采用函数来估计动作值函数，$Q(s,a;\theta) \approx Q^{*}(s,a)$。一般来说，通常采用线性函数进行估计，当然可以采用非线性的函数，如神经网络等等。这里就采用神经网络，用$\theta$表示网络的参数，这里我们把这个网络叫做Q网络，Q网络可以通过最小化下列loss进行训练：<script type="math/tex; mode=display">L_i(\theta_i) = E_{s,a\sim \rho(\cdot)}\left[(y_i - Q(s,a;\theta_i))^2\right]</script>其中$y_i = E_{s’\sim E}[r+\gamma max_{a’}Q(s’,a’;\theta_{i-1})]$是第$i$次迭代的target值，其中$\rho(s,a)$是$(s,a)$服从的概率分布。</li>
<li>注意在优化$L_i(\theta_i)$时，上一次迭代的$\theta_{i-1}$是不变的，注意网络的target是取决于网络参数的，和监督学习作对比，监督学习的target和网络无关，在一开始时就是固定的。</li>
<li>对Loss函数进行求导，得到下列的gradient信息：<script type="math/tex; mode=display">\nabla_{\theta_i}L_i(\theta_i) = E_{s,a~\rho(\cdot),s'\sim E}\left[(r+\gamma max_{a'}Q(s',a';\theta_{i-1})-Q(s,a;\theta_i))\nabla_{\theta_i}Q(s,a;\theta_i)\right]</script>作者通过SGD优化loss函数而不是计算整个期望。如果权重是每隔几个timestep进行更新，并且用从分布$\rho$和环境$E$中采样得到的样本取代期望，就可以得到熟悉的Q-learning算法[2]。(这个具体为什么是这样，我也不清楚，可以看参考文献2)</li>
<li>这个算法是一个Model-Free的模型，因为它直接从环境$E$中采样，并没有显式的对环境进行建模。</li>
<li>它是一个off-policy算法，它评估策略的时候采用的是贪心策略，但是生成数据的策略和评估时是不同。生成数据的策略包含了对环境的exploration(探索)，而评估策略采用的是贪心策略，不包含探索。<blockquote>
<p>On-policy methods attempt to evaluate or improve the policy that is used to make decisions, whereas  off-policy methods evaluate or improve a policy different from that used to generate the data.</p>
</blockquote>
</li>
</ol>
<p>Sarsa和Q-learning的区别在于更新$Q(s,a)$时,$s’$处action采用的是原来的策略(behaviour policy（行为策略），$\epsilon$-贪心策略)还是现在新的策略(贪心策略)决定，这其实就是policy evaluation和value iteration的区别，policy evaluation使用动态规划算法更新$V(s)$，但是并没有改变行为策略，更新迭代用的数据都是利用之前的行为策略生成的。而值迭代是policy evaluation+policy improvement，每一步都用贪心策略选择出最大的$a$更新$V(s)$，评估用的策略（贪心策略）和行为策略（$\epsilon$-策略）是不同的。</p>
<h4 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h4><ol>
<li>DQN使用了experience replay，将多个episodes中的经验存储到同一个replay buffer中。在更新$Q$值的时候，从replay buffer中进行采样更新。当前时间步动作的选择采用的是$\epsilon$-greedy策略，保持探索。因为replay buffer中存放的有很久之前的experience，所以更新$Q$值的策略(replay buffer)和真实采取动作的策略($epsilon$-greedy)是不一样的，所以是off-policy的方法。采用experience replay的online算法[5]和标准的online算法相比有三个好处[4]，第一个是每一个experience可以多次用来更新参数，提高了数据训练效率；第二个是直接从连续的样本中进行学习是低效的，因为样本之间存在强关联性。第三个是on-policy的学习中，当前的参数决定下一次采样的样本，就可能使学习出来的结果发生偏移。</li>
<li>replay buffer中只存储最近N个experience。</li>
<li>原始图像是$210\times 160$的RGB图像，预处理首先将它变为灰度图，并进行下采样得到一个$110\times 84$的图像，然后从这个图像中截取一个$84\times 84$的图像。</li>
<li>作者使用预处理函数$\phi$处理连续四张的图像，而不是仅仅一张，然后将这个预处理后的结果输入$Q$函数。</li>
<li>预处理函数$\phi$是一个卷积神经网络，输入是$84\times 84\times 4$的图像矩阵，经过$16$个stride为$4$的$8\times 8$filter，经过relu激活函数，再经过$32$个stride为$2$的$4\times 4$filter，经过relu激活函数，最后接一个256个单元的全连接层。输出层的单元根据动作的个数决定。</li>
<li>$Q$函数的输入是预处理后的图像，也就是状态，输出是所有action的$Q$值，也就是说给定一个状态，这个网络能够计算所有的action的值。</li>
<li>DQN是不收敛的。</li>
</ol>
<h3 id="伪代码"><a href="#伪代码" class="headerlink" title="伪代码"></a>伪代码</h3><p>Algorithm 1 Deep Q-learning with Experience Replay<br>Initialize replay memory D to capacity N<br>Initialize action-value function Q with random weights<br>for episode = $1, M$ do<br>$\ \ \ \ \ \ \ \ $Initialize sequence $s_1 = {x_1}$ and preprocessed sequenced $\phi_1 = \phi(s_1)$<br>$\ \ \ \ \ \ \ \ $for $t = 1,T$ do<br>$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $With probability $\epsilon$ select a random action $a_t$<br>$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $otherwise select $a_t = max_a Q^{∗}(\phi(s_t), a; θ)$<br>$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $Execute action $a_t$ in emulator and observe reward $r_t$ and image $x_{t+1}$<br>$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $Set $s_{t+1} = s_t, a_t, x_{t+1}$ and preprocess $\phi_{t+1} = \phi(s_{t+1})$<br>$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $Store transition $(\phi_t, a_t, r_t, \phi_{t+1})$ in D<br>$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $Sample random minibatch of transitions $(\phi_j, a_j, r_j, \phi_{j+1})$ from D<br>$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $Set $y_j = \begin{cases}r_j&amp;\ \ \ \ for\ terminal\ \phi_{j+1}\\r_j+\gamma max_{a’}Q(\phi_{j+1},a’|\theta)&amp;\ \ \ \ for\ non-terminal\ \phi_{j+1}\end{cases}$<br>$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $Perform a gradient descent step on $(y_j − Q(\phi_j, a_j; θ))^2$<br>$\ \ \ \ \ \ \ \ $end for<br>end for</p>
<h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><h4 id="Datasets"><a href="#Datasets" class="headerlink" title="Datasets"></a>Datasets</h4><p>七个Atari 2600 games: B.Rider, Breakout, Enduro, Pong, Q bert, Seaquest, S.Invaders。<br>在六个游戏上DQN的表现超过了之前所有的方法，在三个游戏上DQN的表现超过了人类。</p>
<h4 id="Settings"><a href="#Settings" class="headerlink" title="Settings"></a>Settings</h4><ol>
<li>不同游戏的reward变化很大，这里把正的reward全部设置为$1$，把负的reward全部设置为$-1$，reward为$0$的保持不变。这样子在不同游戏中也可以统一学习率。</li>
<li>采用RMSProp优化算法，batchsize为$32$，behaviour policy采用的是$epsilon-greedy$，在前$100$万步内，$epsilon$从$1$变到$0.1$，接下来保持不变。</li>
<li>使用了跳帧技术，每隔$k$步，智能体才选择一个action，在中间的$k-1$步中，保持原来的action不变。这里选择了$k=4$，有的游戏设置的为$k=3$。</li>
</ol>
<h4 id="Metrics"><a href="#Metrics" class="headerlink" title="Metrics"></a>Metrics</h4><h5 id="average-reward"><a href="#average-reward" class="headerlink" title="average reward"></a>average reward</h5><p>第一个metric是在一个episode或者一次游戏内total reward的平均值。这个metric带有很大噪音，因为policy权值一个很小的改变可能就会对policy访问states的分布造成很大的影响。</p>
<h5 id="action-value-function"><a href="#action-value-function" class="headerlink" title="action value function"></a>action value function</h5><p>第二个metric是估计的action-value function，这里作者的做法是在训练开始前使用random policy收集一个固定的states set，然后track这个set中states最大预测$Q$值的平均。</p>
<h4 id="Baselines"><a href="#Baselines" class="headerlink" title="Baselines"></a>Baselines</h4><ol>
<li>Sarsa</li>
<li>Contingency</li>
<li>DQN</li>
<li>Human</li>
</ol>
<h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><p><a href="https://github.com/devsisters/DQN-tensorflow" target="_blank" rel="noopener">https://github.com/devsisters/DQN-tensorflow</a></p>
<h2 id="Nature-DQN"><a href="#Nature-DQN" class="headerlink" title="Nature DQN"></a>Nature DQN</h2><h3 id="非线性拟合函数不收敛的原因"><a href="#非线性拟合函数不收敛的原因" class="headerlink" title="非线性拟合函数不收敛的原因"></a>非线性拟合函数不收敛的原因</h3><ol>
<li>序列中状态的高度相关性。</li>
<li>$Q$值的一点更新就会对policy改变造成很大的影响，从而改变数据的分布。</li>
<li>待优化的$Q$值和target value(目标Q值)之间的关系，每次优化时的目标Q值都是固定上次的参数得来的，优化目标随着优化过程一直在变。<br>前两个问题是通过replay buffer解决的，第三个问题是Natura DQN中解决的，在初始版本中没有解决，解决方案是在固定时间步内，固定目标Q值的参数，更新待优化Q值的参数，然后每隔固定步数将待优化Q值的参数拷贝给目标Q值。<blockquote>
<p>This instability has several causes: the correlations present in the sequence of observations, the fact that small updates to Q may significantly change the policy and therefore change the data distribution, and the correlations between the action-values (Q) and the target values $r+\gamma max_{a’}Q(s’,a’)$.<br>We address these instabilities with a novel variant of Q-learning, which uses two key ideas. First, we used a biologically inspired mechanism termed experience replay that randomizes over the data, thereby removing correlations in the observation sequence and smoothing over changes in the data distribution. Second, we used an iterative update that adjusts the action-values (Q) towards target values that are only periodically updated, thereby reducing correlations with the target.</p>
</blockquote>
</li>
</ol>
<h3 id="Nature-DQN改进"><a href="#Nature-DQN改进" class="headerlink" title="Nature DQN改进"></a>Nature DQN改进</h3><ol>
<li>预处理的结构变了,CNN的层数增加了一层，</li>
<li>加了target network，</li>
<li>将error限制在$[-1,1]$之间。<blockquote>
<p>clip the error term from the update $r + \gamma max_{a’} Q(s’,a’;\theta_i^{-} - Q(s,a;\theta_i)$ to be between $-1$ and $1$. Because the absolute value loss function $|x|$ has a derivative of $-1$ for all negative values of $x$ and a derivative of $1$ for all positive values of $x$, clipping the squared error to be between $-1$ and $1$ corresponds to using an absolute value loss function for errors outside of the $(-1,1)$ interval. </p>
</blockquote>
</li>
</ol>
<h3 id="框架"><a href="#框架" class="headerlink" title="框架"></a>框架</h3><p>DNQ的框架如下所示<br><img src="/2019/03/02/DQN/nature_dqn.png" alt="ndqn"></p>
<h3 id="伪代码-1"><a href="#伪代码-1" class="headerlink" title="伪代码"></a>伪代码</h3><p>Algorithm 2 deep Q-learning with experience replay, target network<br>Initialize replay memory D to capacity N<br>Initialize action-value function Q with random weights $\theta$<br>Initialize target action-value function $\hat{Q}$ with weights $\theta^{-}=\theta$<br>for episode = $1, M$ do<br>$\ \ \ \ \ \ \ \ $Initialize sequence $s_1 = {x_1}$ and preprocessed sequenced $\phi_1 = \phi(s_1)$<br>$\ \ \ \ \ \ \ \ $for $t = 1,T$ do<br>$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $With probability $\epsilon$ select a random action $a_t$<br>$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $otherwise select $a_t = max_a Q^{∗}(\phi(s_t), a; θ)$<br>$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $Execute action $a_t$ in emulator and observe reward $r_t$ and image $x_{t+1}$<br>$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $Set $s_{t+1} = s_t, a_t, x_{t+1}$ and preprocess $\phi_{t+1} = \phi(s_{t+1})$<br>$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $Store transition $(\phi_t, a_t, r_t, \phi_{t+1})$ in D<br>$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $Sample random minibatch of transitions $(\phi_j, a_j, r_j, \phi_{j+1})$ from D<br>$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $Set $y_j = \begin{cases}r_j&amp;\ \ \ \ for\ terminal\ \phi_{j+1}\\r_j+\gamma max_{a’}Q(\phi_{j+1},a’|\theta^{-})&amp;\ \ \ \ for\ non-terminal\ \phi_{j+1}\end{cases}$<br>$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $Perform a gradient descent step on $(y_j − Q(\phi_j, a_j; θ))^2$ with respect to the network parameters $\theta$<br>$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $Every $C$ steps reset $\hat{Q} = Q$<br>$\ \ \ \ \ \ \ \ $end for<br>end for</p>
<h2 id="Double-DQN"><a href="#Double-DQN" class="headerlink" title="Double DQN"></a>Double DQN</h2><h2 id="Prioritized-DDQN"><a href="#Prioritized-DDQN" class="headerlink" title="Prioritized DDQN"></a>Prioritized DDQN</h2><h2 id="Dueling-DQN"><a href="#Dueling-DQN" class="headerlink" title="Dueling DQN"></a>Dueling DQN</h2><h2 id="Distributed-DQN"><a href="#Distributed-DQN" class="headerlink" title="Distributed DQN"></a>Distributed DQN</h2><h2 id="Noisy-DQN"><a href="#Noisy-DQN" class="headerlink" title="Noisy DQN"></a>Noisy DQN</h2><h2 id="Rainbow"><a href="#Rainbow" class="headerlink" title="Rainbow"></a>Rainbow</h2><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>1.<a href="https://blog.csdn.net/yangshaokangrushi/article/details/79774031" target="_blank" rel="noopener">https://blog.csdn.net/yangshaokangrushi/article/details/79774031</a><br>2.<a href="https://link.springer.com/article/10.1007%2FBF00992698" target="_blank" rel="noopener">https://link.springer.com/article/10.1007%2FBF00992698</a><br>3.<a href="https://www.jianshu.com/p/b92dac7a4225" target="_blank" rel="noopener">https://www.jianshu.com/p/b92dac7a4225</a><br>4.<a href="https://datascience.stackexchange.com/questions/20535/what-is-experience-replay-and-what-are-its-benefits/20542#20542" target="_blank" rel="noopener">https://datascience.stackexchange.com/questions/20535/what-is-experience-replay-and-what-are-its-benefits/20542#20542</a><br>5.<a href="https://stats.stackexchange.com/questions/897/online-vs-offline-learning" target="_blank" rel="noopener">https://stats.stackexchange.com/questions/897/online-vs-offline-learning</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/7/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/7/">7</a><span class="page-number current">8</span><a class="page-number" href="/page/9/">9</a><a class="page-number" href="/page/10/">10</a><a class="extend next" rel="next" href="/page/9/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <div class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/favicon.jpg" alt="马晓鑫爱马荟荟">
            
              <p class="site-author-name" itemprop="name">马晓鑫爱马荟荟</p>
              <p class="site-description motion-element" itemprop="description">记录硕士三年自己的积累</p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">98</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  <a href="/categories/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">11</span>
                    <span class="site-state-item-name">分类</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  <a href="/tags/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">107</span>
                    <span class="site-state-item-name">标签</span>
                  </a>
                </div>
              
            </nav>
          

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="https://github.com/mxxhcm" title="GitHub &rarr; https://github.com/mxxhcm" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="mailto:mxxhcm@gmail.com" title="E-Mail &rarr; mailto:mxxhcm@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                </span>
              
            </div>
          

          

          
          

          
            
          
          

        </div>
      </div>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">马晓鑫爱马荟荟</span>

  

  
</div>


  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v3.8.0</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> v6.6.0</div>




        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  
    <script src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  


  


  <script src="/js/src/utils.js?v=6.6.0"></script>

  <script src="/js/src/motion.js?v=6.6.0"></script>



  
  


  <script src="/js/src/affix.js?v=6.6.0"></script>

  <script src="/js/src/schemes/pisces.js?v=6.6.0"></script>



  

  


  <script src="/js/src/bootstrap.js?v=6.6.0"></script>



  



  






<!-- LOCAL: You can save these files to your site and update links -->
    
        
        <link rel="stylesheet" href="https://aimingoo.github.io/gitmint/style/default.css">
        <script src="https://aimingoo.github.io/gitmint/dist/gitmint.browser.js"></script>
    
<!-- END LOCAL -->

    

    







  





  

  

  

  

  
  

  
  
    
      
    
      
        
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
  

  
    
      <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      },
      TeX: {equationNumbers: { autoNumber: "AMS" }}
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<script src="//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<style>
.MathJax_Display {
    overflow: auto hidden;
}
</style>

    
  


  
  

  

  

  

  

  

  

</body>
</html>

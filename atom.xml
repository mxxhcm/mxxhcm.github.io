<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>mxxhcm&#39;s blog</title>
  <icon>https://www.gravatar.com/avatar/e8e79984d2e37363d60a84f0f1e8cf0e</icon>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://mxxhcm.github.io/"/>
  <updated>2019-10-21T14:53:18.789Z</updated>
  <id>http://mxxhcm.github.io/</id>
  
  <author>
    <name>马晓鑫爱马荟荟</name>
    <email>mxxhcm@gmail.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>classfication</title>
    <link href="http://mxxhcm.github.io/2018/10/21/classfication/"/>
    <id>http://mxxhcm.github.io/2018/10/21/classfication/</id>
    <published>2018-10-21T10:47:44.000Z</published>
    <updated>2019-10-21T14:53:18.789Z</updated>
    
    <content type="html"><![CDATA[<h2 id="classfication">Classfication</h2><h2 id="lda">LDA</h2><h2 id="logistic-regression">Logistic Regression</h2><h3 id="logistic-function">Logistic function</h3><p>$$ S(x) = \frac{1}{1+e^{x} }$$<br>如下图所示：<br><img src="/2018/10/21/classfication/logistic_function.png" alt="logistic_func"><br>它的取值在$[0,1]$之间。<br>logistic regression的目标函数是：<br>$$h(x) = \frac{1}{1+e<sup>{-\theta</sup>T x} 3}$$<br>其中$x$是输入，$\theta$是要求的参数。</p><h3 id="思路">思路</h3><p>Logistic regression利用logistic function进行分类，给出一个输入，经过参数$\theta$的变换，输出一个$[0,1]$之间的值，如果大于$0.5$，把它分为一类，小于$0.5$，分为另一类。这个$0.5$只是一个例子，可以根据不同的需求选择不同的值。<br>$\theta^T x$相当于给出了一个非线性的决策边界。</p><h3 id="cost-function">Cost function</h3><p>$$J(\theta) = -\log L(\theta) = -\sum_{i=1}^m (y(i)\log h(x^{(i)}) + (1-y<sup>{(i)})\log(1-h(x</sup>{(i)} )) )$$<br>给出两种方式推导logistic regression的cost function</p><h4 id="maximum-likelyhood-estimation">Maximum likelyhood estimation</h4><p>通过极大似然估计推导得到的，当是两个类别的分类时，即$0$或者$1$，有：<br>$$P(y=1|x,\theta) = h(x)$$<br>$$P(y=0|x,\theta) = 1- h(x)$$<br>服从二项分布，写成一个式子是：<br>$$P(y|x,\theta) = h(x)^y (1-h(x))^{1-y}$$<br>其中$y$取值只有$0$和$1$。<br>有了$y$的表达式，我们就可以使用最大似然估计进行求解了：<br>$$L(\theta) = \prod_{i=1}^m (h(x<sup>{(i)})</sup>{y(i)}(1-h(x^{(i)} ))<sup>{(1-y</sup>{(i)})}$$<br>似然函数要求最大化，即求使得$m$个observation出现概率最大的$\theta$，<br>损失函数是用来衡量损失的，令损失函数取负的对数似然，然后最小化loss也就是最大化似然函数了：<br>$$J(\theta) = -\log L(\theta) = -\sum_{i=1}^m (y(i)\log h(x^{(i)}) + (1-y<sup>{(i)})\log(1-h(x</sup>{(i)} )) )$$</p><h4 id="cross-entropy">Cross-entropy</h4><p>对于$k$类问题，写出交叉熵公式如下所示：<br>$$J(\theta) = -\frac{1}{n}\left[\sum_{i=1}^m \sum_k y_k^{(i)} \log h(x_k^{(i)} ) \right]$$<br>当$k=2$时：<br>$$J(\theta) = -\frac{1}{n}\left[\sum_{i=1}^m  y^{(i)} \log h(x^{(i)} ) + (1-y^{(i)}) \log (1-h(x^{(i)} ))\right]$$</p><h3 id="梯度下降">梯度下降</h3><p>$$J(\theta) = -\log L(\theta) = -\sum_{i=1}^m \left[y(i)\log h(x^{(i)}) + (1-y<sup>{(i)})\log(1-h(x</sup>{(i)} )) \right]$$</p><p>\begin{align*}<br>\nabla J &amp; =  -\sum_{i=1}^m \left[ y(i)\frac{1}{h(x^{(i)})}\nabla h(x^{(i)}) - (1-y<sup>{(i)})\frac{1}{\log(1-h(x</sup>{(i)} ))}\nabla\log(1-h(x^{(i)} ))\right]<br>&amp;=-\sum_{i=1}^m  (h(x^{(i)}) - y^{(i)}) x^{(i)}<br>\end{align*}</p><h2 id="参考文献">参考文献</h2><p>1.<a href="https://blog.csdn.net/jk123vip/article/details/80591619" target="_blank" rel="noopener">https://blog.csdn.net/jk123vip/article/details/80591619</a><br>2.<a href="https://zhuanlan.zhihu.com/p/28408516" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/28408516</a><br>3.<a href="https://www.cnblogs.com/pinard/p/6029432.html" target="_blank" rel="noopener">https://www.cnblogs.com/pinard/p/6029432.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;classfication&quot;&gt;Classfication&lt;/h2&gt;
&lt;h2 id=&quot;lda&quot;&gt;LDA&lt;/h2&gt;
&lt;h2 id=&quot;logistic-regression&quot;&gt;Logistic Regression&lt;/h2&gt;
&lt;h3 id=&quot;logistic-funct
      
    
    </summary>
    
      <category term="机器学习" scheme="http://mxxhcm.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://mxxhcm.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="classfication" scheme="http://mxxhcm.github.io/tags/classfication/"/>
    
      <category term="分类" scheme="http://mxxhcm.github.io/tags/%E5%88%86%E7%B1%BB/"/>
    
  </entry>
  
</feed>

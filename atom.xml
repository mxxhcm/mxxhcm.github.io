<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>mxxhcm&#39;s blog</title>
  <icon>https://www.gravatar.com/avatar/e8e79984d2e37363d60a84f0f1e8cf0e</icon>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://mxxhcm.github.io/"/>
  <updated>2019-10-21T11:49:41.769Z</updated>
  <id>http://mxxhcm.github.io/</id>
  
  <author>
    <name>马晓鑫爱马荟荟</name>
    <email>mxxhcm@gmail.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>python ptan</title>
    <link href="http://mxxhcm.github.io/2019/10/12/python-ptan/"/>
    <id>http://mxxhcm.github.io/2019/10/12/python-ptan/</id>
    <published>2019-10-12T12:36:40.000Z</published>
    <updated>2019-10-21T11:49:41.769Z</updated>
    
    <content type="html"><![CDATA[<h2 id="PyTorch-Agent-Net-library"><a href="#PyTorch-Agent-Net-library" class="headerlink" title="PyTorch Agent Net library"></a>PyTorch Agent Net library</h2><h1 id><a href="#" class="headerlink" title="#"></a>#</h1><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>1.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;PyTorch-Agent-Net-library&quot;&gt;&lt;a href=&quot;#PyTorch-Agent-Net-library&quot; class=&quot;headerlink&quot; title=&quot;PyTorch Agent Net library&quot;&gt;&lt;/a&gt;PyTorch Age
      
    
    </summary>
    
      <category term="python" scheme="http://mxxhcm.github.io/categories/python/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="pytorch" scheme="http://mxxhcm.github.io/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>python iteration-iterable and iterator</title>
    <link href="http://mxxhcm.github.io/2019/10/12/python-iteration/"/>
    <id>http://mxxhcm.github.io/2019/10/12/python-iteration/</id>
    <published>2019-10-12T07:51:26.000Z</published>
    <updated>2019-10-12T10:19:42.861Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Iteration"><a href="#Iteration" class="headerlink" title="Iteration"></a>Iteration</h2><p>Iteration并不是一个具体的东西，它是一个抽象的名词，指的是一个接一个的取某个对象的每一个项。包含隐式的，显式的loop，即while，do, for等，这叫iteration。</p><h2 id="Iterable和iterator"><a href="#Iterable和iterator" class="headerlink" title="Iterable和iterator"></a>Iterable和iterator</h2><p>而在python中，有iterator和iterable。<br>一个iterable object是实现了<strong>iter</strong>方法的object或者定义了<strong>getitem</strong>方法。一个iteratable object是一个可以得到iterator的object，但是它自己并不一定是iterator object。<br>而iterator是一个实现了<strong>next</strong>和<strong>iter</strong>方法的object。<br><strong>iterable object不一定是iterator，iterator一定是iterable object。</strong><br><strong>可以使用for循环的都是ieterable object，比如str，list，但是它们不是itertor，可以使用iter()方法得到iterator</strong><br><strong>可以next()的都是iterator</strong></p><h2 id="iter和-iter"><a href="#iter和-iter" class="headerlink" title="iter和__iter__"></a>iter和__iter__</h2><p>所有实现了<strong>iter</strong>方法的object，都是iterable object，可以通过iter()方法产生iterator object。<br>具体示例<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">from collections import Iterator</span><br><span class="line">from collections import Iterable</span><br><span class="line"></span><br><span class="line">class Fibs:</span><br><span class="line">    def __init__(self, a, b):</span><br><span class="line">        self.a = a</span><br><span class="line">        self.b = b</span><br><span class="line"></span><br><span class="line">    def __iter__(self):</span><br><span class="line">        a = self.a</span><br><span class="line">        b = self.b</span><br><span class="line">        while True:</span><br><span class="line">            yield a</span><br><span class="line">            a, b = b, a + b</span><br><span class="line"></span><br><span class="line">real_fibs = Fibs(0,1)</span><br><span class="line"></span><br><span class="line">print(&quot;real_fibs is iterator? &quot;, isinstance(real_fibs, Iterator))</span><br><span class="line">print(&quot;real_fibs is iterable? &quot;, isinstance(real_fibs, Iterable))</span><br><span class="line">print(&quot;iter(real_fibs) is iterator? &quot;, isinstance(iter(real_fibs), Iterator))</span><br><span class="line">print(&quot;iter(real_fibs) is iterable? &quot;, isinstance(iter(real_fibs), Iterable))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">for idx, i in enumerate(real_fibs):</span><br><span class="line">    print(i)</span><br><span class="line">    if idx &gt; 10:</span><br><span class="line">        break</span><br></pre></td></tr></table></figure></p><p>其中出现了yield关键字。yield关键字的作用是每次迭代执行到该行代码时，就返回一个值，并且记住相应的位置，在下次迭代时继续从该行位置开始执行。</p><h2 id="next和-next"><a href="#next和-next" class="headerlink" title="next和__next__"></a>next和__next__</h2><p>代码示例<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Iterator</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">A</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, max_v=<span class="number">5</span>)</span>:</span></span><br><span class="line">        self.max_v = max_v</span><br><span class="line">        self.v = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__next__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># if self.v &lt;= self.nax_v</span></span><br><span class="line">        self.v += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> self.v</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    a = A()</span><br><span class="line">    <span class="keyword">for</span> idx, v <span class="keyword">in</span> enumerate(a):</span><br><span class="line">        print(idx, v)</span><br><span class="line">        <span class="keyword">if</span> (idx &gt;= <span class="number">10</span>):</span><br><span class="line">            <span class="keyword">break</span></span><br></pre></td></tr></table></figure></p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>1.<a href="https://stackoverflow.com/questions/9884132/what-exactly-are-iterator-iterable-and-iteration" target="_blank" rel="noopener">https://stackoverflow.com/questions/9884132/what-exactly-are-iterator-iterable-and-iteration</a><br>2.<a href="https://stackoverflow.com/a/46411740/8939281" target="_blank" rel="noopener">https://stackoverflow.com/a/46411740/8939281</a><br>3.<a href="https://www.jianshu.com/p/f9b547874a14" target="_blank" rel="noopener">https://www.jianshu.com/p/f9b547874a14</a><br>4.<a href="https://www.jianshu.com/p/1b0686bc166d" target="_blank" rel="noopener">https://www.jianshu.com/p/1b0686bc166d</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Iteration&quot;&gt;&lt;a href=&quot;#Iteration&quot; class=&quot;headerlink&quot; title=&quot;Iteration&quot;&gt;&lt;/a&gt;Iteration&lt;/h2&gt;&lt;p&gt;Iteration并不是一个具体的东西，它是一个抽象的名词，指的是一个接一个的取某个
      
    
    </summary>
    
      <category term="python" scheme="http://mxxhcm.github.io/categories/python/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="iteration" scheme="http://mxxhcm.github.io/tags/iteration/"/>
    
      <category term="iterable" scheme="http://mxxhcm.github.io/tags/iterable/"/>
    
      <category term="iterator" scheme="http://mxxhcm.github.io/tags/iterator/"/>
    
  </entry>
  
  <entry>
    <title>python pickle</title>
    <link href="http://mxxhcm.github.io/2019/10/08/python-pickle/"/>
    <id>http://mxxhcm.github.io/2019/10/08/python-pickle/</id>
    <published>2019-10-08T09:56:48.000Z</published>
    <updated>2019-10-11T05:30:51.354Z</updated>
    
    <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>pickle是一个序列化模块，它能将python对象序列化转换成二进制串再反序列化成python对象。</p><h2 id="常用函数"><a href="#常用函数" class="headerlink" title="常用函数"></a>常用函数</h2><h3 id="pickle-dump"><a href="#pickle-dump" class="headerlink" title="pickle.dump()"></a>pickle.dump()</h3><h4 id="API"><a href="#API" class="headerlink" title="API"></a>API</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pickle.dump(obj, file, [,protocol])</span><br></pre></td></tr></table></figure><h4 id="作用"><a href="#作用" class="headerlink" title="作用"></a>作用</h4><p>将python对象obj以二进制字符串形式保存到文件file中，使用protocol。</p><h4 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"></span><br><span class="line">dictionary = &#123;<span class="string">"name"</span>: <span class="string">"mxx"</span>, <span class="string">"age"</span>: <span class="number">23</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">"test.txt"</span>, <span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    pickle.dump(dictionary, f)</span><br></pre></td></tr></table></figure><h3 id="pickle-load"><a href="#pickle-load" class="headerlink" title="pickle.load()"></a>pickle.load()</h3><h4 id="API-1"><a href="#API-1" class="headerlink" title="API"></a>API</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pickle.load(file)</span><br></pre></td></tr></table></figure><h4 id="作用-1"><a href="#作用-1" class="headerlink" title="作用"></a>作用</h4><p>从文件file中读取二进制字符串，将其反序列成python对象。</p><h4 id="示例-1"><a href="#示例-1" class="headerlink" title="示例"></a>示例</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">"test.txt"</span>, <span class="string">'rb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    b = pickle.load(f)</span><br><span class="line"></span><br><span class="line">print(b)</span><br><span class="line">print(type(b))</span><br></pre></td></tr></table></figure><h3 id="pickle-dumps"><a href="#pickle-dumps" class="headerlink" title="pickle.dumps()"></a>pickle.dumps()</h3><h4 id="API-2"><a href="#API-2" class="headerlink" title="API"></a>API</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pickle.dumps(obj, [,protocol])</span><br></pre></td></tr></table></figure><h4 id="作用-2"><a href="#作用-2" class="headerlink" title="作用"></a>作用</h4><p>将python对象obj转化成二进制字符串，返回一个字符串</p><h4 id="示例-2"><a href="#示例-2" class="headerlink" title="示例"></a>示例</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"></span><br><span class="line">dictionary = &#123;<span class="string">"name"</span>: <span class="string">"mxx"</span>, <span class="string">"age"</span>: <span class="number">23</span>&#125;</span><br><span class="line"></span><br><span class="line">s = pickle.dumps(dictionary)</span><br><span class="line">print(s)</span><br><span class="line">print(type(s))</span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line"><span class="comment"># b'\x80\x03&#125;q\x00(X\x04\x00\x00\x00nameq\x01X\x03\x00\x00\x00mxxq\x02X\x03\x00\x00\x00ageq\x03K\x17u.'</span></span><br><span class="line"><span class="comment"># &lt;class 'bytes'&gt;</span></span><br></pre></td></tr></table></figure><h3 id="pickle-loads"><a href="#pickle-loads" class="headerlink" title="pickle.loads()"></a>pickle.loads()</h3><h4 id="API-3"><a href="#API-3" class="headerlink" title="API"></a>API</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pickle.loads(string)</span><br></pre></td></tr></table></figure><h4 id="作用-3"><a href="#作用-3" class="headerlink" title="作用"></a>作用</h4><p>从二进制字符串中返回序列化前的python obj对象。</p><h4 id="示例-3"><a href="#示例-3" class="headerlink" title="示例"></a>示例</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"></span><br><span class="line">dictionary = &#123;<span class="string">"name"</span>: <span class="string">"mxx"</span>, <span class="string">"age"</span>: <span class="number">23</span>&#125;</span><br><span class="line"></span><br><span class="line">s = pickle.dumps(dictionary)</span><br><span class="line">b = pickle.loads(s)</span><br><span class="line">print(b)</span><br><span class="line">print(type(b))</span><br></pre></td></tr></table></figure><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>1.<a href="https://www.jianshu.com/p/cf91849064e3" target="_blank" rel="noopener">https://www.jianshu.com/p/cf91849064e3</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h2&gt;&lt;p&gt;pickle是一个序列化模块，它能将python对象序列化转换成二进制串再反序列化成python对象。&lt;/p&gt;
&lt;h2 id=&quot;常用函数&quot;&gt;
      
    
    </summary>
    
      <category term="python" scheme="http://mxxhcm.github.io/categories/python/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="pickle" scheme="http://mxxhcm.github.io/tags/pickle/"/>
    
  </entry>
  
  <entry>
    <title>python mpi4py</title>
    <link href="http://mxxhcm.github.io/2019/10/08/python-mpi4py/"/>
    <id>http://mxxhcm.github.io/2019/10/08/python-mpi4py/</id>
    <published>2019-10-08T09:25:46.000Z</published>
    <updated>2019-10-11T05:30:51.354Z</updated>
    
    <content type="html"><![CDATA[<h2 id="MPI"><a href="#MPI" class="headerlink" title="MPI"></a>MPI</h2><p>MPI全名是Message Passing Interface，它是一个标准，而不是一个实现，专门为进程间通信实现的。它的工作原理很简单，启动一组进程，在同一个通信域中的不同进程有不同的编号，可以给不同编号的进程分配不同的任务，最终实现整个任务。<br>MPI4PY就是python中MPI的实现。在python中有很多种方法实现多进程以及进程间通信，比如multiprocessing，但是multiprocessing进程间通信不够方便，mpi4py的效率更高一些。<br>mpi4py提供了点对点通信，点对面，面对点通信。点对点通信又包含阻塞和非阻塞等等，通信的内容包含python内置对象，也包含numpy数组等。</p><h2 id="mpi4py简单对象和方法介绍"><a href="#mpi4py简单对象和方法介绍" class="headerlink" title="mpi4py简单对象和方法介绍"></a>mpi4py简单对象和方法介绍</h2><p>MPI.COMM_WORLD是一个通信域，在这个通信域中有不同的进程，每个进程的编号以及进程的数量都可以通过这个通信域获得。具体看以下comm_world.py代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mpi4py <span class="keyword">import</span> MPI</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获得多进程通信域</span></span><br><span class="line">comm = MPI.COMM_WORLD</span><br><span class="line"><span class="comment"># 获得当前进程通信域中进程数量</span></span><br><span class="line">size = comm.Get_size()</span><br><span class="line"><span class="comment"># 获得当前进程在通信域中的编号</span></span><br><span class="line">rank = comm.Get_rank()</span><br></pre></td></tr></table></figure></p><blockquote><blockquote><blockquote><p>mpiexec -np 3 python comm_world.py</p></blockquote></blockquote></blockquote><h2 id="点对点通信"><a href="#点对点通信" class="headerlink" title="点对点通信"></a>点对点通信</h2><h3 id="阻塞通信"><a href="#阻塞通信" class="headerlink" title="阻塞通信"></a>阻塞通信</h3><h4 id="python对象"><a href="#python对象" class="headerlink" title="python对象"></a>python对象</h4><h5 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h5><p>comm.send(data, dest, tag)<br>comm.recv(source, tag)<br>send和recv都是阻塞方法，即调用这个方法之后，等到该函数调用结束之后再返回。dest是目的process编号，source是发送的process编号。data是要发送的数据，需要是python的内置对象，即可以pickle的对象。</p><h5 id="代码示例"><a href="#代码示例" class="headerlink" title="代码示例"></a>代码示例</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mpi4py <span class="keyword">import</span> MPI</span><br><span class="line"></span><br><span class="line">comm = MPI.COMM_WORLD</span><br><span class="line">rank = comm.Get_rank()</span><br><span class="line">size = comm.Get_size()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> rank == <span class="number">0</span>:</span><br><span class="line">    data = &#123;<span class="string">'name'</span>: <span class="string">"mxx"</span>, <span class="string">"age"</span>: <span class="number">23</span>&#125;</span><br><span class="line">    comm.send(data, dest=<span class="number">1</span>, tag=<span class="number">10</span>)</span><br><span class="line">    print(<span class="string">"data has sent."</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    data = comm.recv(source=<span class="number">0</span>, tag=<span class="number">10</span>)</span><br><span class="line">    print(<span class="string">"data has been receieved."</span>)</span><br></pre></td></tr></table></figure><h4 id="numpy数组"><a href="#numpy数组" class="headerlink" title="numpy数组"></a>numpy数组</h4><h5 id="简介-1"><a href="#简介-1" class="headerlink" title="简介"></a>简介</h5><p>comm.Send(data, dest, tag)<br>comm.Recv(source, tag)<br>Send和Recv都是阻塞方法，即调用这个方法之后，等到该函数调用结束之后再返回。dest是目的process编号，source是发送的process编号。data是要发送的数据，需要是numpy对象，和c语言的效率差不多。</p><h5 id="代码示例-1"><a href="#代码示例-1" class="headerlink" title="代码示例"></a>代码示例</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mpi4py <span class="keyword">import</span> MPI</span><br><span class="line"></span><br><span class="line">comm = MPI.COMM_WORLD</span><br><span class="line"></span><br><span class="line">rank = comm.Get_rank()</span><br><span class="line">size = comm.Get_size()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> rank == <span class="number">0</span>:</span><br><span class="line">    data = &#123;<span class="string">'name'</span>: <span class="string">"mxx"</span>, <span class="string">"age"</span>: <span class="number">23</span>&#125;</span><br><span class="line">    comm.isend(data, dest=<span class="number">1</span>, tag=<span class="number">10</span>)</span><br><span class="line">    print(<span class="string">"data has sent."</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    data = comm.irecv(source=<span class="number">0</span>, tag=<span class="number">10</span>)</span><br><span class="line">    print(<span class="string">"data has been receieved."</span>)</span><br></pre></td></tr></table></figure><h3 id="非阻塞通信"><a href="#非阻塞通信" class="headerlink" title="非阻塞通信"></a>非阻塞通信</h3><h4 id="简介-2"><a href="#简介-2" class="headerlink" title="简介"></a>简介</h4><p>comm.isend(data, dest, tag)<br>comm.irecv(source, tag)<br>isend和irecv都是非阻塞方法，即调用这个方法之后，调用该函数之后立即返回，无需等待它执行结束。dest是目的process编号，source是发送的process编号。data要是python对象，可以被pickle处理的。</p><h4 id="代码示例-2"><a href="#代码示例-2" class="headerlink" title="代码示例"></a>代码示例</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mpi4py <span class="keyword">import</span> MPI</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">comm = MPI.COMM_WORLD</span><br><span class="line"></span><br><span class="line">rank = comm.Get_rank()</span><br><span class="line">size = comm.Get_size()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> rank == <span class="number">0</span>:</span><br><span class="line">    data = np.ones((<span class="number">3</span>, <span class="number">4</span>), dtype=<span class="string">'i'</span>)</span><br><span class="line">    comm.Send([data, MPI.INT], dest=<span class="number">1</span>, tag=<span class="number">10</span>)</span><br><span class="line">    print(<span class="string">"data has sent."</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    data = np.empty((<span class="number">3</span>, <span class="number">4</span>), dtype=<span class="string">'i'</span>)</span><br><span class="line">    data = comm.Recv([data, MPI.INT], source=<span class="number">0</span>, tag=<span class="number">10</span>)</span><br><span class="line">    print(<span class="string">"data has been receieved."</span>)</span><br></pre></td></tr></table></figure><h2 id="组通信"><a href="#组通信" class="headerlink" title="组通信"></a>组通信</h2><h3 id="bcast"><a href="#bcast" class="headerlink" title="bcast"></a>bcast</h3><h4 id="简介-3"><a href="#简介-3" class="headerlink" title="简介"></a>简介</h4><p>将一个process中的数据发送给所有在通信池中的process。<br>comm.bcast(data, dest, tag)</p><h4 id="代码示例-3"><a href="#代码示例-3" class="headerlink" title="代码示例"></a>代码示例</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> mpi4py</span><br><span class="line"><span class="keyword">from</span> mpi4py <span class="keyword">import</span> MPI</span><br><span class="line"></span><br><span class="line">comm = MPI.COMM_WORLD</span><br><span class="line">size = comm.Get_size()</span><br><span class="line">rank = comm.Get_rank()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> rank == <span class="number">1</span>:</span><br><span class="line">    data = &#123;<span class="string">"name"</span>: <span class="string">"mxx"</span>, <span class="string">"age"</span>: <span class="number">23</span>&#125;</span><br><span class="line">    print(<span class="string">"data bcast to others"</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    data = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">data = comm.bcast(data, root=<span class="number">1</span>)</span><br><span class="line">print(<span class="string">"process &#123;&#125; has received data"</span>.format(rank))</span><br></pre></td></tr></table></figure><h3 id="scatter"><a href="#scatter" class="headerlink" title="scatter"></a>scatter</h3><h4 id="简介-4"><a href="#简介-4" class="headerlink" title="简介"></a>简介</h4><p>将一个process的数据拆分成n份，发送给所有在通信池中的process每个一份，和bcast的区别在于，bcast发送的数据对于每一个process都是一样的，而scatter是将一份数据拆分成n份分别发送给每个process。<br>comm.scatter(data, dest, tag)</p><h4 id="代码示例-4"><a href="#代码示例-4" class="headerlink" title="代码示例"></a>代码示例</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> mpi4py</span><br><span class="line"><span class="keyword">from</span> mpi4py <span class="keyword">import</span> MPI</span><br><span class="line"></span><br><span class="line">comm = MPI.COMM_WORLD</span><br><span class="line">size = comm.Get_size()</span><br><span class="line">rank = comm.Get_rank()</span><br><span class="line"></span><br><span class="line">recv_data = <span class="literal">None</span></span><br><span class="line"><span class="keyword">if</span> rank == <span class="number">1</span>:</span><br><span class="line">    send_data = range(size) </span><br><span class="line">    print(<span class="string">"data bcast to others"</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    send_data = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">recv_data = comm.scatter(send_data, root=<span class="number">1</span>)</span><br><span class="line">print(<span class="string">"process &#123;&#125; has received data &#123;&#125;"</span>.format(rank, recv_data))</span><br></pre></td></tr></table></figure><h3 id="gather"><a href="#gather" class="headerlink" title="gather"></a>gather</h3><h4 id="简介-5"><a href="#简介-5" class="headerlink" title="简介"></a>简介</h4><p>和comm.bcast相反，将每个process中的数据收集到一个process中。<br>comm.gather(data, dest, tag)</p><h4 id="代码示例-5"><a href="#代码示例-5" class="headerlink" title="代码示例"></a>代码示例</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> mpi4py</span><br><span class="line"><span class="keyword">from</span> mpi4py <span class="keyword">import</span> MPI</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">comm = MPI.COMM_WORLD</span><br><span class="line">size = comm.Get_size()</span><br><span class="line">rank = comm.Get_rank()</span><br><span class="line"></span><br><span class="line">send_data = rank</span><br><span class="line">print(<span class="string">"process &#123;&#125; send data &#123;&#125; to root."</span>.format(rank, send_data))</span><br><span class="line"></span><br><span class="line">recv_data = comm.gather(send_data, root=<span class="number">9</span>)</span><br><span class="line"><span class="keyword">if</span> rank == <span class="number">9</span>:</span><br><span class="line">    print(<span class="string">"process &#123;&#125; gather all data &#123;&#125; to others."</span>.format(rank, recv_data))</span><br></pre></td></tr></table></figure><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>1.<a href="https://zhuanlan.zhihu.com/p/25332041" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/25332041</a><br>2.<a href="https://www.jianshu.com/p/f497f3a5855f" target="_blank" rel="noopener">https://www.jianshu.com/p/f497f3a5855f</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;MPI&quot;&gt;&lt;a href=&quot;#MPI&quot; class=&quot;headerlink&quot; title=&quot;MPI&quot;&gt;&lt;/a&gt;MPI&lt;/h2&gt;&lt;p&gt;MPI全名是Message Passing Interface，它是一个标准，而不是一个实现，专门为进程间通信实现的。它的工作原理很
      
    
    </summary>
    
      <category term="python" scheme="http://mxxhcm.github.io/categories/python/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="mpi4py" scheme="http://mxxhcm.github.io/tags/mpi4py/"/>
    
      <category term="多进程" scheme="http://mxxhcm.github.io/tags/%E5%A4%9A%E8%BF%9B%E7%A8%8B/"/>
    
  </entry>
  
  <entry>
    <title>reinforcement learning why use baseline ?</title>
    <link href="http://mxxhcm.github.io/2019/10/04/reinforcement-learning-why-use-baseline/"/>
    <id>http://mxxhcm.github.io/2019/10/04/reinforcement-learning-why-use-baseline/</id>
    <published>2019-10-04T07:46:00.000Z</published>
    <updated>2019-10-04T07:46:00.448Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>reinforcement learning importance sampling</title>
    <link href="http://mxxhcm.github.io/2019/09/27/reinforcement-learning-importance-sampling/"/>
    <id>http://mxxhcm.github.io/2019/09/27/reinforcement-learning-importance-sampling/</id>
    <published>2019-09-27T15:41:36.000Z</published>
    <updated>2019-09-30T04:27:02.181Z</updated>
    
    <content type="html"><![CDATA[<h2 id="importance-sampling">Importance Sampling</h2><p>Importance sampling是使用一个分布近似估计另一个分布期望的方法，即通过分布$q$计算分布$p$下$f(x)$的期望。通过从$q$中采样而不是从$p$中采样近似：<br>$$\mathbb{E}_p\left[f(x)\right] = \mathbb{E}_q\left[ \frac{p(x)f(x)}{q(x)}\right] \tag{1}$$<br>使用采样分布$q$估计分布$p$下的期望：<br>$$\mathbb{E}_p\left[f(x)\right] \approx \frac{1}{n} \sum_{i=1}^n \frac{p(x_i)f(x_i)}{q(x_i)} x_i\sim q\tag{2}$$<br>上面的公式需要满足$p(x_i)$不为$0$时，$q(x_i)$也不为$0$。直接计算$\mathbb{E}_p\left[f(x)\right]$和$\mathbb{E}_q\left[f(x)\right]$，一般来说是不同的，通过importance ratio调整权重，就可以使用$q$分布估计$p$分布的期望了。举个例子：<br>$$f(1) = 2, f(2) = 3, f(3) = 4, otherwise 0 \tag{3}$$<br>概率分布$p$为：$p(x=1) = 0, p(x=2) = \frac{1}{3},p(x=3) = \frac{2}{3}$，概率分布$q$为：$q(x=1) = \frac{1}{3}, q(x=2) = \frac{1}{3}, q(x=3) = \frac{1}{3}$。计算期望，$\mathbb{E}_p\left[f(x)\right] = \frac{11}{3}$，$\mathbb{E}_q\left[f(x)\right] = 3$<br>使用importance ratio进行权重调整：<br>\begin{align*}<br>\mathbb{E}_p\left[f(x)\right] &amp; = \mathbb{E}_q\left[\frac{q(x)}{p(x)}f(x)\right] \\<br>&amp; = \mathbb{E}_q\left[\frac{p(x=1)}{q(x=1)}f(x=1) \right] + \mathbb{E}_q\left[\frac{p(x=2)}{q(x=2)}f(x=2) \right] + \mathbb{E}_q\left[\frac{p(x=3)}{q(x=3)}f(x=3) \right] \\<br>&amp; = \frac{1}{3}*0 + \frac{1}{3}\frac{\frac{1}{3}}{\frac{1}{3}}*3 + \frac{1}{3}\frac{\frac{2}{3}}{\frac{1}{3}}*4\\<br>&amp; =\frac{11}{3}\\<br>\end{align*}<br>可以看出来，我们使用分布$q$估计除了分布$p$的期望。通过使用一个简单分布$q$进行采样，可以计算出$p$的期望。在RL中，通常通过复用old policy的sample trajectory学习current policy。</p><h2 id="optimal-importance-sampling">Optimal Importance Sampling</h2><p>Importance sampling使用采样近似估计$\mathbb{E}_p\left[f(x)\right]\approx \frac{1}{N}\sum_i \frac{p(x_i)}{q(x_i)}f(x_i)$近似计算$\mathbb{E}_p\left[f(x)\right]$。随着样本数量$N$的增加，期望值越准确。但是这种方法的方差很大，为了减少方差，样本分布$q$应该满足：<br>$$q(x) \propto p(x)\vert f(x)\vert \tag{4}$$<br>简单来说，为了减少方差，我们需要采样return更大的点。</p><h2 id="normalized-importanct-sampling">Normalized importanct sampling</h2><p>上面介绍的方法叫做unnormalized importance sampling。可以使用下里面的公式将unnormalized importance sampling转换为normalized importance sampling。<br>$$p(x) = \frac{\hat{p}(x)}{Z}\tag{5}$$<br>许多ML方法属于贝叶斯网络或者马尔科夫随机场，对于贝叶斯网络中，$p$很容易计算。但是当$p$是马尔科夫随机场时，$\sum\hat{p}(x)$是很难计算的。<br>\begin{align*}<br>\mathbb{E}_p\left[f(x)\right] &amp; = \int f(x) p(x) dx\\<br>&amp; = \int f(x) \frac{\hat{p}(x)}{Z} \frac{q(x)}{q(x)} dx\\<br>&amp; = \frac{\int f(x) \hat{p}(x) \frac{q(x)}{q(x)}dx}{Z}\\<br>&amp; = \frac{\int f(x) \hat{p}(x) \frac{q(x)}{q(x)} dx}{\int \hat{p}(x) dx}\\<br>&amp; = \frac{\int f(x) \hat{p}(x) \frac{q(x)}{q(x)} dx}{\int \hat{p}(x)\frac{q(x)}{q(x)} dx}\\<br>&amp; = \frac{\int f(x) q(x)\frac{\hat{p}(x)}{q(x)} dx}{\int q(x)\frac{\hat{p}(x)}{q(x)} dx}\\<br>&amp; = \frac{\int f(x) r(x)q(x) dx}{\int r(x)q(x) dx}\qquad\qquad 记r(x) = \frac{\hat{p}(x)}{q(x)}\\<br>\end{align*}<br>接下来用采样样本的求和近似积分求期望：<br>\begin{align*}<br>\mathbb{E}_p\left[f(x)\right] &amp; = \frac{\int f(x) r(x)q(x) dx}{\int r(x)q(x) dx}\qquad\qquad 记r(x) = \frac{\hat{p}(x)}{q(x)}\\<br>&amp; \approx \frac{\sum_i f(x^i) r^i }{\sum r^i}\qquad\qquad 其中 r^i = \frac{\hat{p}(x^i ) }{q(x^i ) }\\<br>&amp; = \sum_i f(x^i) r^i  \frac{r^i}{\sum_i r^i}\\<br>\end{align*}<br>通过计算<br>这就避免了计算$Z$，这种方法叫做normalized importance sampling。但是需要付出一定代价，unnormalized importance sampling是无偏的，而normalized importance是有偏的但是方差更小。</p><h2 id="importance-sampling-in-rl">Importance sampling in RL</h2><p>我们可以使用importance sampling方法从old policy $\pi’$采样估计new policy $\pi$的值函数。计算一个action的returns的代价很高，但是如果新的action和老的action很接近，importance sampling可以帮助我们利用old calculation计算新的returns。举个例子，在MC方法中，无论何时更新$\theta$，都需要根据新的trajectories计算returns。<br>$$\nabla_{\theta}J(\theta) = \frac{1}{N}\sum_{i=1}^T \left(\sum_{t=1}^T \nabla_{\theta} \log \pi_{\theta}(a_{i,t}|s_{i,t})\right)\left(\prod_{t=1}^T R(s_{i,t},a_{i,t})\right) \tag{6}$$<br>一个trajectory可以有几百个steps，单个的更新是非常低效的。有了importance sampling之后，我们可以基于old samples计算新的return。然而，如果两个policy差的太远，accuracy会降低。因此周期性的同步policy是非常必要的。<br>使用importance sampling，重写policy gradient的等式：<br>$$\nabla_{\theta}J(\theta) = \mathbb{E}_{\tau\sim\bar{\pi}(\tau)}\left[\sum_{t=1}^T \nabla_{\theta} \log \pi_{\theta}(a_t|s_t)\left(\prod_{t’=1}^T \frac{\pi_{\theta}(a_{t’}|s_{t’})}{\hat{\pi}_{\theta}(a_{t’}|s_{t’})}\right)\left(\prod_{t’=t}^T R(s_{t’},a_{t’})\right)\right]\tag{7}$$<br>为了约束policy的变化，可以加入trust region约束条件，在这个region内，我们认为使用importance sampling得到的结果是可信的：<br>$$\max_{\theta} \hat{\mathbb{E}}_t\left[\frac{\pi_{\theta}(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t}\hat{A}_t\right]\tag{8}$$<br>$$s.t. \hat{\mathbb{E}}_t\left[\text{KL}\left[\pi_{\theta_{old}}(\cdot|s_t),\pi_{\theta}(\cdot|s_t)\right]\right]\tag{9}$$</p><h2 id="参考文献">参考文献</h2><p>1.<a href="https://medium.com/@jonathan_hui/rl-importance-sampling-ebfb28b4a8c6" target="_blank" rel="noopener">https://medium.com/@jonathan_hui/rl-importance-sampling-ebfb28b4a8c6</a><br>2.<a href="http://webee.technion.ac.il/people/shimkin/MC15/MC15lect4-ImportanceSampling.pdf" target="_blank" rel="noopener">http://webee.technion.ac.il/people/shimkin/MC15/MC15lect4-ImportanceSampling.pdf</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;importance-sampling&quot;&gt;Importance Sampling&lt;/h2&gt;
&lt;p&gt;Importance sampling是使用一个分布近似估计另一个分布期望的方法，即通过分布$q$计算分布$p$下$f(x)$的期望。通过从$q$中采样而不是从$p$
      
    
    </summary>
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="important sampling" scheme="http://mxxhcm.github.io/tags/important-sampling/"/>
    
      <category term="重要性采样" scheme="http://mxxhcm.github.io/tags/%E9%87%8D%E8%A6%81%E6%80%A7%E9%87%87%E6%A0%B7/"/>
    
  </entry>
  
  <entry>
    <title>reinforcement learning an introduction 第8章笔记</title>
    <link href="http://mxxhcm.github.io/2019/08/07/reinforcement-learning-an-introduction-%E7%AC%AC8%E7%AB%A0%E7%AC%94%E8%AE%B0/"/>
    <id>http://mxxhcm.github.io/2019/08/07/reinforcement-learning-an-introduction-第8章笔记/</id>
    <published>2019-08-07T08:32:52.000Z</published>
    <updated>2019-08-20T13:05:53.387Z</updated>
    
    <content type="html"><![CDATA[<h2 id="planning-and-learning-with-tabular-methods">Planning and Learning with Tabular Methods</h2><p>前面几章，介绍了MC算法，TD算法，再用$n$-step TD框架把它们统一了起来，此外，这些算法都属于model free的方法。这一章要介绍model based方法，model based方法主要用于planning，而model free的方法主要用于learning。</p><h2 id="models和planning">Models和Planning</h2><h3 id="model">Model</h3><h4 id="定义">定义</h4><p>Environment的model是agents用来预测environment会如何对agent的action做出响应的任何东西。给定一个state和一个action，model可以预测下一个state和action。如果model是stochastic的话，有多个可能的next states和rewards，每一个都有一定概率。Distribution models计算所有可能性以及相应概率，sample models根据概率进行采样，只计算采样的结果。</p><h4 id="用处">用处</h4><p>Model可以用来模仿或者仿真。给定一个start state和action，sample model产生一个transition，distribution model产生所有可能的transitions，并使用概率进行加权。给定start state和一个policy，sample model产生一个episode，distribution model产生所有可能的episodes以及相应的概率。Model被用来模拟environment或者产生simulated experience。</p><h4 id="示例">示例</h4><p>DP中使用的$p(s’, r|s,a)$就是distribution model，第五章中blackjack例子中使用的模型是sample model。Distribution model可以用来产生samples，但是sample model要比distribution models好获得。考虑投掷很多骰子的和，distribution models计算所有可能值和相应的概率，而sample model只计算根据概率产生的一个样本。</p><h3 id="planning">Planning</h3><h4 id="定义-v2">定义</h4><p>Planning的定义是给定model，不断的与environment交互生成或者改进poilcy的过程。有两种planning的方法，state space在state space中寻找一个optimal policy或者optimal path，这本书中介绍的方法都是这类。Plan-space planning在plans space中search。Plan-sapce 方法很难高效的应用到stochastic sequential decision problems，在这本书中不做过多介绍。<br>这一章要介绍的state-space planning方法具有相同的结构，这个结构在learing和planning中都有。基本的想法是：计算value funtions，通过应用simulated experience的update以及backup操作计算value functions。</p><h4 id="示例-v2">示例</h4><p>如DP方法，扫描整个state space，然后生成每一个state可能的transitions的distribution，用于计算update target，更新state’s estimated value。这一章介绍的其他方法，也满足这个结构，只不过计算target的方式顺序以及长度不同而已。</p><h4 id="planning和learning的关系">planning和learning的关系</h4><p>将planning方法表示成这种形式主要是为了强调planning方法和learning方法之间的联系。learning和planning的重点都是使用backup update op更新estimations of value functions。区别在于plannning使用了model生成的simulate experience，而learning使用environment生成的real experience。同时perfomance measure以及experience的灵活性也不同，但是由于相同的结构，许多learning的方法可以直接应用到planning上去，使用simulated experience代替real experience即可。<br>给出一个利用sample model和Q-learning结合起来的planning算法：</p><h3 id="learning算法示例">learning算法示例</h3><p>Random-sample one-step tabular Q-planning<br>Loop forever<br>$\qquad 1.$随机选择初始state $S\in \mathbb{S}, A\in \mathbb{A}$<br>$\qquad 2.$将$S,A$发送给sample model，得到next state和reward $S’, R$<br>$\qquad 3.$应用one-step Q-learning更新公式：<br>$\qquad\qquad Q(S,A) \leftarrow Q(S,A) + \alpha \left[R+\gamma mx_a Q(S’, a) - Q(S, A)\right]$</p><h2 id="dyna">Dyna</h2><h3 id="简介">简介</h3><p>Online的planning更新需要不断的与environment交互，从交互中获得的information可能会改变model，以及与environment的交互，所以可能model也需要不断的学习。这一节主要介绍Dyna-Q，将online planning需要的内容都整合了起来，Dyna算法中包含了planning, acting以及learning。<br>Planning中real experience至少有两个作用，一个是改进model，叫做model-learning，另一个是使用learning的方法直接改进value function和policy，叫做direct reinforcement learning。相应的关系如下图所示：<br><img src="/2019/08/07/reinforcement-learning-an-introduction-第8章笔记/model_learning.png" alt="model_learning"><br>如图所示，experience可以直接改进value function，也可以通过model间接改进value fucntion，叫做indirect reinforcement learning。Direct learning和indirect learning各有优势，indirect方法能够充分利用有限的experience，得到一个更好的policy；direct方法更简单，不会受到model bias的影响。</p><h3 id="dyna-q-简介">Dyna-$Q$简介</h3><p>Dyna-$Q$包含planning, acting, model-learning和direct RL。planning是random-sample one-step tabular Q-planning；direct RL就是one-step tabular Q-learning，model-learning是table-based并且假设environment是deterministic，对于每一个transition $S_t,A_t\rightarrow R_{t+1}, S_{t+1}$，model用表格的形式记录下$S_t,A_t$的prediction值是$R_{t+1}, S_{t+1}$。如下图是Dyna算法的整体框架图（Dyna-Q是一个示例）。<br><img src="/2019/08/07/reinforcement-learning-an-introduction-第8章笔记/dyna.png" alt="dyna_q"><br>左边使用real experience进行direct RL，右边是model-based的方法，从real experience中学习出model，然后利用model生成simulated experience。search control指的是从model生成的simulated experiences中选择指定starting state和actions的experience。最后，planning使用simulated experience更新value function。从概念上来讲，Dyna agents中planning, acting, model-learning以及direct RL几乎同时发生。但是在实现的时候，还是需要串行的进行。Dyna-$Q$中，计算量主要集中在planning上。具体的算法如下：</p><h3 id="tabular-dyna-q">Tabular Dyna-Q</h3><p>初始化$Q(s,a), Model(s,a), s\in \mathbb{S}, a\in \mathbb{A}$<br>Loop forever<br>$\qquad (a)S\leftarrow$ current state<br>$\qquad (b)A\leftarrow \epsilon$-greedy$(S,Q)$<br>$\qquad ( c )$采取action $A$，得到下一时刻的state $S’$和reward $R$<br>$\qquad (d)Q(S,A) \leftarrow Q(S,A) + \alpha\left[R+\gamma max_a Q(S’, a) -Q(S,A)\right]$<br>$\qquad (e)Model(S,A)\leftarrow R,S’$（假设deterministic environment)<br>$\qquad (f)$Loop repeat $n$ 次<br>$\qquad\qquad S\leftarrow$ 任意之前的观测状态<br>$\qquad\qquad A\leftarrow$ 在$S$处采取的任意action $A$<br>$\qquad\qquad R,S’\leftarrow Model(S,A)$<br>$\qquad\qquad Q(S,A) \leftarrow Q(S,A) + \alpha\left[R+\gamma max_a Q(S’, a) -Q(S,A)\right]$</p><p>其中$Model(s,a)$表示预测$(s,a)$的next state。(d)是direct RL，(e)是model-learning，(f)是planning。如果忽略了(e,f)，就是one-step tabular Q-learning。</p><h2 id="如果model出错">如果model出错</h2><p>前面给的例子很简单，model是不会错的。但是如果environment是stochastic的，或者samples很少的话，或者function approximation效果不好，或者environment刚刚改变，新的behaviour还没有被观测到，model就可能会出错。当model是错的话，就可能会产生suboptimal policy。在一些情况下，suboptimal会发现并且纠正model的error。当模型预测的结果比真实的结果好的时候就会发生这种情况。这里给出两个例子。一个environment变坏，一个environment变好。<br>第一个例子，有一个迷宫，如图所示。<br><img src="/2019/08/07/reinforcement-learning-an-introduction-第8章笔记/blocking_maze.png" alt="blocking mase"><br>刚开始，路在右边，1000步之后，右边的路被堵上了，左边有一条新的路。<br><img src="/2019/08/07/reinforcement-learning-an-introduction-第8章笔记/shortcut_maze.png" alt="blocking mase"><br>第二个例子，刚开始路在左边，3000步之后，右边有一条新的路，左边的路也被保留。<br>这又是一个exploration和exploitation问题。在planning中，exploration意味着尝试那些让model变得更好的actions，而exploitation意味着给定当前的model，选择optimal action。我们想要agents能够explore environment的变化，但是不影响performance。</p><h3 id="启发式搜索">启发式搜索</h3><p>Dyna-Q+ 使用了一个简单有效的heuristics，agent记录每一个state-action pair 在real environment中从上次使用到现在经历了多少个time steps，累计的时间步越多，说明这个pair改变的可能性越大，model不对的可能性越大。为了鼓励使用很久没有用的action，这里加了一个bonus reward，如果一个transition的reward是$r$，这个transition已经有$\tau$步没有试过，在planning进行update的时候，用一个新的reward $r + k\sqrt{\tau}$，$k$很小。不过新添加的bonus会有一定的cost，而且会对value function造成影响，但是在上面的两个例子中，这个cost比performance的提升要好。</p><h2 id="优先级">优先级</h2><p>在Dyna进行planning时，所有的state-action pair被选中的概率是一样的，显然是不合理的，planning的效率太低，可以有效的集中在某些state-action pair中。举个例子，在dyna_maze例子中，在第二个episode开始的时候，只有goal state前的那个state会产生正的reward，其余的都仍然是$0$，这里意味着很多updates都是无意义的。从一个value为$0$的state转移到另一个value为$0$的state，这个updates是没有意义的。只有那些在goal前面的state才会被更新，或者，那些value不为$0$的state的前面的state的value的更新才有意义。在planning过程中，有用的更新变多了，但是离effcient还差得多。在真实应用中，states可能相当大，这种没有重点的更新是非常低效的。。</p><h3 id="backward-focusing">backward focusing</h3><p>Dyna maze的例子给了我们一个提示，从goal state backward的进行更新。这里的goal state不是一个具体的goal state，指的是抽象的goal state。一般来说，包括goal state以及value改变的state。假设model的value都是正确的，如果agent发现了environment的一个变化以及相应state value的变化，首先更新这个state的predecessor states value，然后一直往前利用value改变的state进行更新就行了。这种想法叫做backward focusing。对于那些低效的state，不更新就是了。<br>Backward过程很快，会有很多state-action被更新，但并不是所有的pair是等价的。有的state value变化很大，有的变化很小。在stochastic环境中，对transiton probability变化的估计也会导致change大小的变化和紧急性的变化。所以，根据紧急性对不同的pair排一个优先级，然后根据这个优先级进行更新。用一个queue记录如果更新某个state-action pair的话，它的estimated value会变多少，根据这个值的大小排优先级。队头的元素取出来进行更新以后，计算它的predecessor pair变化大小。如果这个大小大于某个阈值，就使用新的优先级，把它加入队列，如果queue中有这个pair了，queue中保存大的优先级。完整的算法如下所示：</p><h3 id="deteriministic-environment下的优先级">deteriministic environment下的优先级</h3><p>初始化$Q(s,a), Model(s,a), \forall s, a$，置$PQueue$为空<br>Loop forever<br>$\qquad(a) S\leftarrow$ currnet state<br>$\qquad(b) A\leftarrow policy(S,Q)$<br>$\qquad( c )$采取action $A$；得到下一个reward $R$和state $S’$<br>$\qquad(d)Model(S,A) \leftarrow R, S’$<br>$\qquad(e) P\leftarrow |R+\gamma max_aQ(S’,a) - Q(S,A)|$<br>$\qquad$(f)如果$P\gt \theta$，将$S,A$以$P$的优先级插入$PQueue$<br>$\qquad$(g) Loop repeat $n$次，当$PQueue$非空的时候<br>$\qquad S, A\leftarrow fisrt(PQueue)$<br>$\qquad R,S’\leftarrow Model(S,A)$<br>$\qquad Q(S,A) \leftarrow Q(S,A) +\alpha \left[R+\gamma max_a Q(S’,a) -Q(S,A)\right]$<br>$\qquad$Loop for all 预计能到$S$的$\bar{S}, \bar{A} $<br>$\qquad\qquad \bar{R} \leftarrow $predicted reward for $\bar{S}, \bar{A}, S$<br>$\qquad\qquad P\leftarrow |\bar{R}+\gamma max_aQ(S,a) -Q(\bar{S}, \bar{A})$<br>$\qquad\qquad$如果$P\gt \theta$，将$\bar{S},\bar{A}$以$P$的优先级插入到$PQueue$</p><p>推广到stochastic environment上，用model记录state-action pair experience的次数，以及next state，计算出概率，然后进行expected update。Expected update会计算许多低概率transition上，浪费资源，所以可以使用sample update。<br>Sample update相对于expected update的好处，相当于将完整的backup分解成单个更小的transition。这个idea是从van Seijen和Sutton(2013)的一篇论文中得到的，叫做&quot;small backups&quot;，使用单个的transition进行更新，像sample update，但是使用的是这个transition的概率，而不是sampling的$1$。</p><h2 id="expected-updates和sample-updates">Expected updates和Sample updates</h2><p>这本书介绍了不同的value function updates，就one-step方法来说，主要有三个binary dimensions。第一个是估计state values还是action values；第二个是估计optimal policy还是random policy的value，对应四种组合:$q_{*}, v_{*}, q_{\pi}, v_{\pi}$；第三个是使用expected updates还是sample updates，也是这节的内容。总共有八种组合，其中七种对应具体的算法，如下图所示：<br><img src="/2019/08/07/reinforcement-learning-an-introduction-第8章笔记/seven_backup.png" alt="seven_bakcup"><br>这些算法都可以用来planning，之前介绍的Dyna-$Q$使用的是$q_{*}$ sample update，也可以用$q_{*}$ expected update，还可以用$q_{\pi}$ sample updates。</p><h3 id="简介和比较">简介和比较</h3><p>Expected update对于每一个$(s,a)$ pair，考虑所有可能的next state和next action $(s’,a’)$，需要distributions model进行精确计算；而sample update仅仅需要sample model，考虑一个next state，会有采样误差。所以expected upadte一定要比sample update好，但是需要的计算量也大。当环境是deteriministic的话，expected udpaet和sample update其实是一样的，只有在stochastic环境下才有区别。<br>假设每一个state有$b$个next state，expected upadte要比sample update的计算量大$b$倍。如果有足够的时间进行完全的expected update，进行一次完全的expected update一定比进行$b$次sample update好，因为虽然计算次数相等，但是sample update有sampling error；如果没有足够的时间的话，在计算次数小于$b$次的时候，sample update要比expected update好，sample update至少进行了一部分improvement，而sample update只有完全进行$b$次计算后才会得到正确的value function。而当state-action pairs很多的情况下，进行完全的expected update是不可能的，sample update是一个可行的方案。<br>给定一个state-action pair，到底是进行一些（小于$b$次）expected update好呢，还是进行$b$次sample updates好呢？如下图所示，展示了expected update和sample update在不同的$b$下，estimation error关于计算次数的函数。<br><img src="/2019/08/07/reinforcement-learning-an-introduction-第8章笔记/fig_8_7.png" alt="fig_8_7"><br>在这个例子中，所有$b$个后继状态出现的可能性相等，开始的error为$1$。假设所有的next state value都是正确的，expected update完成之后，error从$1$变成了$0$。而sample updates根据$\sqrt{\frac{b-1}{bt}}$减少error。对于$b$很大的值来说，进行$b$的很小比例次数的更新，error就会下降很快。</p><h3 id="sample-updates的好处">Sample updates的好处</h3><p>上图中，sample update的结果可能要比实际结果差一些。在实际问题中，后继状态的value会被它们自身更新。他们的estimate value会更精确，从后继状态进行backup也会更精确。</p><h3 id="结论">结论</h3><p>在stochastic环境下，如果每个state处可能的next state数量非常多，并且有很多个states，那么sample update可能要比expected update好。</p><h2 id="trajectory-sampling">Trajectory Sampling</h2><p>这里比较两种distributing updates。<br>DP进行update的时候，每一次扫描整个state spaces，很多state其实是没有用的，没有focus，所有的states地位一样。原则上，只要确保收敛，任何distributed方法都行，然而在实际上常用的是exhaustive sweep。<br>第二种是根据一些distributions从state space或者state action space中进行采样。Dyna-$Q$使用均匀分布采样，和exhaustive sweep问题一样，没有focus。使用on-policy distribution是一种很不错的想法，根据当前的policy不断的与model交互，产生一个trajectory。Sample state transitions以及reward是model给出来的，sample action是当前的policy给出来的。这种方法叫做generating experience和update trajectory sampling。</p><h3 id="原因">原因</h3><p>如果on-policy的distribution是已知的，可以根据这个distribution对所有的states进行加权，但是这和exhaustive sweeps的计算量差不多。或者从distribution中采样state-action paris，这比simulating trajectories好在哪里呢？事实上我们不知道distribution，当policy改变，计算distribution的计算量和policy evaluation相等。</p><h3 id="分析">分析</h3><p>使用on-policy distribution可以去掉很多我们不感兴趣的内容，或者一遍又一遍的进行无用的重复更新。通过一个小例子分析它的效果。使用one-step expected updates，公式如下：<br>$$Q(s,a) \leftarrow \sum_{s’,r} \hat{p}(s’,r|s,a)\left[r+\gamma max_{a’}Q(s’,a’)\right]$$<br>使用均匀分布的时候，对所有的state-action pair进行in place的expected update更新；使用on-policy的时候，使用当前$\epsilon$-greedy policy（$\epsilon=0.1$）直接生成episodes，对episodes中出现的state-action pair进行expected update。也就是说一个是随机选的，一个是on-policy中出现的，选择的方式不一样，但是都是进行expected update。<br>简单介绍一下environment。每个state都有两个action，等可能性的到达$b$个next states，$b$对于所有state-action pair都是一样的，所有的transition都有$0.1$的概率到达terminal state。Expected reward服从均值为$0$,方差为$1$的高斯分布。<br>在planning过程中的任何一步都可以停止，根据当前估计的action value，计算greedy policy $\hat{\pi}$下start state的value $v_{\hat{\pi}}(s_0)$，也就是说告诉我们使用greedy重新开始一个新的episodes，agent的表现会怎么样。（假设model是正确的）。<br><img src="/2019/08/07/reinforcement-learning-an-introduction-第8章笔记/fig_8_8.png" alt="fig_8_8"><br>上半部分是进行了$200$次sample任务，$1000$个states，$b$分别为$1,3,10$的结果。在图中根据on-policy 采样更新的方法，一开始很快，时间一长，就慢下来了。当$b$越小的时候，效果越好，越快。另一个实验中表明，随着states数量的增加，on-policy distribution采样的效果也在变好。<br>使用on-policy distribtion进行采样，能够帮助我们关注start state的后继状态。如果states很多，并且$b$很小，这个效果很好并且会持续很久。在长时间的计算中，使用on-policy distribution采样可能会有副作用，因为一直在更新那些value已经正确的states。对它们进行采样没用了，还不如采样一些其他的states。这就是为什么在长时间的运行中使用均匀分布进行采样的效果更好。</p><h2 id="real-time-dp">Real-time DP</h2><p>Real time DP是DP的on-policy trajectory sampling版本。RLDP和上一节介绍的on-policy expected update的区别在update的方式不同，上一届实际上使用的是state-action pair，而这一节DP用的是state，它的更新公式是：<br>$$v_{k+1}(s) = \max_a \sum_{s’,r}p(s’,r|s,a) \left[r+\gamma v_k(s’)\right]$$<br>一个是更新action value function，一个是更新state valut function。</p><h3 id="irrelevant-states">irrelevant states</h3><p>如果trajectories可以仅仅从指定的states开始，我们感兴趣的仅仅是给定policy下states的value，on-policy trajectory sampling可以完全跳过给定policy下不能到达的states。这些states跟prediction问题无关。对于control问题，目标是找到optimal policy而不是evaluating一个给定的policy，可能存在无论从哪个start states开始都无法到达的states，所以就没有必要给出这些无关states的action。我们需要的是一个optimal partial policy，对于relevant states，给出optimal policy，而对于irrelevant states，给出任意的actions。<br>找到这样一个policy，按道理来说需要访问所有的state-action pairs无数次，包括那些无关状态。<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.137.1955&amp;rep=rep1&amp;type=pdf" target="_blank" rel="noopener">Korf</a>证明了在满足以下条件的时候，能够在很少访问relevant states甚至不访问的时候，找到这样一个policy。</p><ol><li>goal state的初始value为$0$</li><li>存在至少一个policy保证从任何start state都能到达goal state</li><li>所有到达的非goal states的reward是严格为负的</li><li>所有states的初始value要大于等于optimal values（可以设置为$0$，一定比负值大）</li></ol><p>如果满足这些条件，RTDP一定会收敛到relevant states的optimal policy。</p><h2 id="决策时进行planning">决策时进行planning</h2><h3 id="backgroud-planning">backgroud planning</h3><p>进行planning至少有两种方法。第一种是已经介绍的DP和Dyna这些算法，使用planning基于从model得到的simulated experience不断的改进policy和value function。通过比较某一个state处不同state-action pairs value值的大小选择action。在action被选择之前，planning更新所有的$Q$值。这里planning的结果被很多个states用来选择action。这种planning叫做background planning。</p><h3 id="decision-time-planning">decision-time planning</h3><p>第二种方法是使用planning输出单个state的action，遇到一个新的state $S_t$，输出是单个的action $A_t$，然后再下一个时间步根据$S_{t+1}$继续计算$A_{t+1}$。最简单的一个例子是当只有states value可以使用的时候，通过比较model预测每一个action能够到达的后继state的value（也就是使用after state value）选择相应的action，。这种方法叫做decision-time planning。<br>事实上，decision-time planning和background planning的流程是一样的，都是使用simulated experience到backup values再到policy。只不过decision-time planning只是对当前可访问的单个state和action的value和policy进行planning。在许多decision-time planning的过程中，用于选择当前state对应的action时，使用到的value和policy用过以后都被丢弃了，这并不会造成太大的计算损失，因为在大部分任务中很多states在短时间内都不会被再次访问到。</p><h3 id="应用场景">应用场景</h3><p>decision-time planning适用于不需要快速实时相应的场景，比如各种下棋。如果需要快速响应的，那么最好使用backgroud planning计算一个policy可以快速应用到新的states。</p><h2 id="启发式搜索-v2">启发式搜索</h2><p>AI中经典的state-space方法都是decision-time planning方法，统称为heuristic search。在启发式搜索中，会使用树进行搜索，近似的value function应用到叶子节点，进行backup到根节点（当前state），相应的backup diagram和optimal的expected updates类似。根据计算的backed-up值，选择相应的action之后，丢弃这些值。<br>传统的启发式搜索并不保存backup value，它更像是多步的greedy policy，目的是更好的选择actions。如果我们有一个perfect model以及imperfect action value function，搜索的越深结果越好。如果search沿着所有可能的路一直到episode结束，那么imperfect value function的结果被抵消了，选出来的action也是optimal。如果搜索的深度$k$足够深，$\gamma^k$接近于$0$,那么找到的action也是近乎于最优的。当然，搜索的深度越深，需要的计算资源就越多。<br>启发式搜索的的updates集中在current state，它的search tree集中在接下来可能的states和actions，这也是它的结果为什么很好的原因。在某些特殊的情况下，我们可以将具体的启发式搜索算法构建出一个tree，自底向上的执行one-step update，如下图所示：<br><img src="/2019/08/07/reinforcement-learning-an-introduction-第8章笔记/heuristic_search_tree.png" alt="heuristic_search_tree"><br>如果update是有序的，并且使用tabular representation，整个updates可以看成深搜，所有的state-space搜索可以看成很多个one-step updates的组合。我们得出了一个结论，搜索深度越深，性能越好的原因不是multistep updates的使用，因为它实际上使用的是多个one-step update。真正的原因是更新都集中在current state downstream的states和actions上，所有的计算都集中在candidate actions相关的states和actions上。</p><h2 id="rollout算法">Rollout算法</h2><h3 id="什么是rollout算法">什么是Rollout算法</h3><p>Rollout算法是将Monte Carlo Control应用到从current state开始的simulate trajectories上的decision-time planning算法。Rollout算法根据给定的policy，这个policy叫做rolloutpolicy，从当前state可能采取的所有action开始生成很多simulated trajectories，对得到的returns进行平均估计aciton values。当action value估计的足够精确的时候，选择最大的那个action执行。<br>和MC Control的区别在于，MC Control的目的是估计整个action value function $q_{\pi}$或者$q_{*}$，而Rollout算法的目的是对于每一个current state，在一个给定policy下估计每一个可能的action的value。Rollout是decision-time planning算法，计算完相应的estimate action value之后，就丢弃它们。</p><h3 id="rollout算法做了什么">Rollout算法做了什么</h3><p>Rollout算法和policy iteration差不多。在policy improvement theorem理论中，如果在一个state处采取新的action，它的value要比原来的value高，那么就说这个新的policy要比老的policy好。Rollout算法在每个current state处，估计不同的state action value，然后选择最好的，其实就相当于one-step的policy iteration，或者更像on-step的asynchronous value iteration。<br>也就是说，rollout算法的目的是改进rollout policy，而不是寻找最优的policy。Rollout算法非常有效，但是它的效果也取决于rollout policy，roloout policy越好，最后算法生成的policy就越好。</p><h3 id="如何选择好的rollout-policy">如何选择好的rollout policy</h3><p>更好的rollout policy也就需要更多的资源，因为是decision-time算法，时间约束一定要满足，rollout算法的计算时间取决于每一个decision需要选择的action数量，sample trajectories的长度，rollout policy做决策的事件，以及足够的sample trajectories的数量。接下来给出几种方法去权衡这些影响因素：<br>第一个方法，MC trials都是独立的，所以可以使用多个分开的处理器运行多个trials。第二个方法是在simulated trajectories结束之前截断，通过一个分类评估函数对truncated returns进行修正。第三个方法是剪枝，剪掉那些不可能是最优的actions，或者那些和当前最优结果没啥差别的acitons。</p><h3 id="rollout算法和learning算法的关系">rollout算法和learning算法的关系</h3><p>Rollout算法并不是learning算法，因为它没有保存values和polices。但是rollout算法具有rl很多好的特征。作为MC Control的应用，他们使用sample trajectories，避免了DP的exhausstive sweeps，同时不需要使用distributin models，使用sample models。最后，rollout算法还使用了policy improvement property，即选择当前estimate action values最大的action。</p><h2 id="monte-carlo-tree-search">Monte Carlo Tree Search</h2><p>MCTS是decision-time planning算法，实际上，MCTS是一个rollout算法，它在上一节介绍的rollout算法上，加上了acucumulating value estimates的均值。MCTS是AlphaGO的基础算法。<br>每到达一个新的state，MCTS根据state选择action，到达新的state，再选择action，持续下去。在大多数情况下，simulated trajectories使用rollout policy生成actions。当model和rollout policy都不需要大量计算的时候，可以在短时间内生成大量simulated trajectories。只保留在接下来的几步内最后可能访问到的state-action pairs的子集，形成一棵树，如下图所示。任意simulated trajectory都会经过这棵树，并且从叶子节点退出。在tree的外边，使用rollout policy选择actions，在tree的内部使用一个新的policy，称为tree policy，平衡exploration和exploitation。Tree policy可以使用$\epsilon$-greedy算法。<br><img src="/2019/08/07/reinforcement-learning-an-introduction-第8章笔记/mcts.png" alt="mcts"><br>MCTS总共有四个部分，一直在迭代进行，第一步是Selection，根据tree policy生成一个episode的前半部分；第二步是expansion，从选中的叶子节点处的上一个节点探索其他没有探索过的节点；第三步是simulation，从选中节点，或者第二步中增加的节点处，使用rollout policy生成一个episode的后半部分；第四步是backup，从第一二三步得到的episode进行backup。这四步一直迭代下去，等到资源耗尽，或者没有time的时候，就退出，然后根据生成的tree中的信息选择相应的aciton，比如可以选择root node处action value最大的action，也可以选择最经常访问的action。<br>MCTS是一种rollout算法，所以它拥有online，incremental，sample-based和policy improvement等优点。同时，它保存了tree边上的estimate action value，并且使用sample update进行更新。优势是让trivals的初始部分集中在之前simulated的high-return trajectories的公共部分。然后不断的expanding这个tree，高效增长相关的action value table，通过这样子，MCTS避免了全局近似value funciton，同时又能够利用过去的experience指定exploraion。</p><h2 id="参考文献">参考文献</h2><p>1.《reinforcement learning an introduction》第二版</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;planning-and-learning-with-tabular-methods&quot;&gt;Planning and Learning with Tabular Methods&lt;/h2&gt;
&lt;p&gt;前面几章，介绍了MC算法，TD算法，再用$n$-step TD框架把它们统
      
    
    </summary>
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="model based" scheme="http://mxxhcm.github.io/tags/model-based/"/>
    
  </entry>
  
  <entry>
    <title>python selenium</title>
    <link href="http://mxxhcm.github.io/2019/08/06/python-selenium%E5%AE%89%E8%A3%85/"/>
    <id>http://mxxhcm.github.io/2019/08/06/python-selenium安装/</id>
    <published>2019-08-06T03:24:09.000Z</published>
    <updated>2019-08-06T07:18:18.592Z</updated>
    
    <content type="html"><![CDATA[<h2 id="ubuntu-安装chrome-driver"><a href="#ubuntu-安装chrome-driver" class="headerlink" title="ubuntu 安装chrome driver"></a>ubuntu 安装chrome driver</h2><ol><li>下载chrome driver<br><a href="http://chromedriver.chromium.org/downloads" target="_blank" rel="noopener">http://chromedriver.chromium.org/downloads</a><br>根据自己的操作系统和chrome下载相应的chrome driver</li><li>解压<br>将解压后的chromedriver放置在PATH环境变量中的任意目录即可，我是放置在了/usr/local/bin</li></ol><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>1.<a href="https://www.jianshu.com/p/dd848e40c7ad" target="_blank" rel="noopener">https://www.jianshu.com/p/dd848e40c7ad</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;ubuntu-安装chrome-driver&quot;&gt;&lt;a href=&quot;#ubuntu-安装chrome-driver&quot; class=&quot;headerlink&quot; title=&quot;ubuntu 安装chrome driver&quot;&gt;&lt;/a&gt;ubuntu 安装chrome driv
      
    
    </summary>
    
      <category term="python" scheme="http://mxxhcm.github.io/categories/python/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="selenium" scheme="http://mxxhcm.github.io/tags/selenium/"/>
    
  </entry>
  
  <entry>
    <title>python-spider</title>
    <link href="http://mxxhcm.github.io/2019/08/06/python-spider/"/>
    <id>http://mxxhcm.github.io/2019/08/06/python-spider/</id>
    <published>2019-08-06T02:53:56.000Z</published>
    <updated>2019-08-09T08:37:45.484Z</updated>
    
    <content type="html"><![CDATA[<h2 id="requests"><a href="#requests" class="headerlink" title="requests"></a>requests</h2><p>获取网页<br>response = requests.post(url, headers)<br>返回的response包含有网页返回的内容。<br>response.text以文字方式访问。</p><h2 id="beautiful-soup"><a href="#beautiful-soup" class="headerlink" title="beautiful soup"></a>beautiful soup</h2><p>soup.find_all()</p><h2 id="selenium"><a href="#selenium" class="headerlink" title="selenium"></a>selenium</h2><ol><li><p>xpath查找<br>查找class为”wos-style-checkbox”，type为”checkbox”的任意elements。可以把*号换成input，就变成查找满足上述条件的input elemetns。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">driver.find_elements_by_xpath(<span class="string">"//*[@type='checkbox'][@class='wos-style-checkbox']"</span>)</span><br></pre></td></tr></table></figure></li><li><p>访问元素的内容<br>element.text</p></li><li><p>复选框选中与取消选中</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">checkbox = driver.find_element_by_id(<span class="string">"checkbox"</span>)</span><br><span class="line"><span class="keyword">if</span> checkbox.is_selected():  <span class="comment">#如果被选中</span></span><br><span class="line">    checkbox.click()    <span class="comment"># 再点击一次，就变成了取消选中</span></span><br><span class="line"><span class="keyword">else</span>:   <span class="comment"># 如果没有被选中</span></span><br><span class="line">    checkbox.click()    <span class="comment"># 再点击一次，就变成了选中</span></span><br></pre></td></tr></table></figure></li><li><p>提交表单</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">keywords = driver.find_element_by_id(<span class="string">"input"</span>)</span><br><span class="line"><span class="comment"># 清空表单默认值</span></span><br><span class="line">keywords.clear()</span><br><span class="line"><span class="comment"># 提交表单</span></span><br><span class="line">keywords.send_keys(values)</span><br></pre></td></tr></table></figure></li><li><p>输完表单内容需要回车<br>直接在表单内容中添加\n即可。</p></li><li><p>下拉框<br>使用Select对象</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">from selenium.webdriver.support.ui import Select</span><br><span class="line">from selenium import webdriver</span><br><span class="line">s = Select(driver.find_element_by_id(&quot;databases&quot;))</span><br><span class="line"></span><br><span class="line">s.select_by_value(&quot;value&quot;)</span><br><span class="line">s.select_by_index(index)</span><br><span class="line">s.select_by_visible_text(&quot;visible_text&quot;)</span><br></pre></td></tr></table></figure></li><li><p>模拟鼠标操作</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">from selenium.webdriver.common.action_chains import ActionChains</span><br><span class="line"></span><br><span class="line"># 定位element</span><br><span class="line">arrow = driver.find_element_by_id(&quot;next&quot;)</span><br><span class="line"># 单击</span><br><span class="line">ActionChains(driver).click(arrow).perform()</span><br><span class="line"># 双击</span><br><span class="line">ActionChains(driver).double_click(arrow).perform()</span><br></pre></td></tr></table></figure></li><li><p>display:none和visible: hidden<br>display:none表示完全不可见，不占据页面空间，而visible: hidden仅仅隐藏了element的显示效果，仍然占据页面空间，并且是可以被定位到，但是无法被访问（如selenium的click,clear以及send_keys等，会报错ElementNotVisibleException’: Message: Element is not currently visible and so may not be interacted with）。</p></li></ol><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>1.<a href="https://www.w3schools.com/xml/xml_xpath.asp" target="_blank" rel="noopener">https://www.w3schools.com/xml/xml_xpath.asp</a><br>2.<a href="https://devhints.io/xpath" target="_blank" rel="noopener">https://devhints.io/xpath</a><br>3.<a href="https://zhuanlan.zhihu.com/p/31604356" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/31604356</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;requests&quot;&gt;&lt;a href=&quot;#requests&quot; class=&quot;headerlink&quot; title=&quot;requests&quot;&gt;&lt;/a&gt;requests&lt;/h2&gt;&lt;p&gt;获取网页&lt;br&gt;response = requests.post(url, headers)
      
    
    </summary>
    
      <category term="python" scheme="http://mxxhcm.github.io/categories/python/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="spider" scheme="http://mxxhcm.github.io/tags/spider/"/>
    
  </entry>
  
  <entry>
    <title>reinforcement learning an introduction 第7章笔记</title>
    <link href="http://mxxhcm.github.io/2019/07/30/reinforcement-learning-an-introduction-%E7%AC%AC7%E7%AB%A0%E7%AC%94%E8%AE%B0/"/>
    <id>http://mxxhcm.github.io/2019/07/30/reinforcement-learning-an-introduction-第7章笔记/</id>
    <published>2019-07-30T01:59:50.000Z</published>
    <updated>2019-10-06T13:59:10.417Z</updated>
    
    <content type="html"><![CDATA[<h2 id="n-step-bootstrapping">n-step Bootstrapping</h2><p>这章要介绍的n-step TD方法，将MC和one-step TD用一个统一的框架整合了起来。在一个任务中，两种方法可以无缝切换。n-step方法生成了一系列方法，MC在一头，one-step在另一头。最好的方法往往不是MC也不是one-step TD。One-step TD每隔一个timestep进行bootstrapping，在许多需要及时更新变化的问题，这种方法很有用，但是一般情况下，经历了长时间的稳定变化之后，bootstrap的效果会更好。N-step TD就是将one-step TD方法中time interval的one改成了n。N-step方法的idea和eligibility traces很像，eligibility traces同时使用多个不同的time intervals进行bootstarp。</p><h2 id="n-step-td-prediction">n-step TD Prediction</h2><p>对于使用采样进行policy evaluation的方法来说，不断使用policy $\pi$生成sample episodes，然后估计$v_{\pi}$。MC方法用的是每一个episode中每个state的return进行更新，即无穷步reward之和。TD方法使用每一个state执行完某一个action之后的下一个reward加上下一个state的估计值进行更新。N-step方法使用的是每一个state接下来n步的reward之和加上第n步之后的state的估计值。相应的backup diagram如下所示：<br><img src="/2019/07/30/reinforcement-learning-an-introduction-第7章笔记/n_step_backup_diagram.png" alt="n_step_diagram"></p><p>n-step方法还是属于TD方法，因为n-step的更新还是基于时间维度上不同estimate的估计进行的。</p><blockquote><p>n-step updates are still TD methods because they still chagne an erlier estimate based on how it differs from a later estimate. Now the later estimate is not one step later, but n steps later。</p></blockquote><p>正式的，假设$S_t$的estimated value是基于state-reward sequence $S_t, R_{t+1}, S_{t+1}, R_{t+2}, \cdots, R_T,S_T$得到的。<br>MC方法更新基于completer return：<br>$$G_T = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots + \gamma^{T-t-1}R_T \tag{1}$$<br>one-step TD更新基于one-step return：<br>$$G_{t:t+1} = R_{t+1}+ \gamma V_t(S_{t+1}) \tag{2}$$<br>two-step TD更新基于two-step return：<br>$$G_{t:t+2} = R_{t+1}+ \gamma V_t(S_{t+1}) + \gamma^2 V_{t+1}(S_{t+2}) \tag{3}$$<br>n-step TD更新基于n-step return：<br>$$G_{t:t+n} = R_{t+1}+ \gamma V_t(S_{t+1}) + \gamma^2 V_{t+1}(S_{t+2}) +\cdots + \gamma^{n-1} R_{t+n} + \gamma^n V_{t+n-1}(S_{t+n}), n\ge1, 0\le t\le T-n \tag{4}$$<br>所有的n-step方法都可以看成使用n steps的rewards之和加上$V_{t+n-1}(S_{t+n})$近似其余项的rewards近似return。如果$t+n \ge T$时，$T$步以后的reward以及estimated value当做$0$，相当于定义$t+n \ge T$时，$G_{t:t+1} = G_t$。<br>当$n &gt; 1$时，只有在$t+n$时刻之后，$R_{t+n},S_{t+n}$也是已知的，所以不能使用real time的算法。使用$t+n-1$时刻的$V$近似估计$V_{t+n-1}(S_{t+n})$，将n-step return当做n-step TD的target，得到的更新公式如下：<br>$$V_{t+n}(S_t) = V_{t+n-1} (S_t) + \gamma(G_{t:t+n} - V_{t+n-1}(S_{t}))\tag{5}$$<br>当更新$V_{t+n}(S_t)$时，所有其他states的$V$不变，用公式来表示是$V_{t+n}(s) = V_{t+n-1}(s), \forall s\neq S_t$。在每个episode的前$n-1$步中，不进行任何更新，为了弥补这几步，在每个episode结束以后，即到达terminal state之后，仍然继续进行更新，直到下一个episode开始之前，依然进行updates。完整的n-step TD算法如下：</p><h3 id="n-step-td-prediction伪代码">n-step TD prediction伪代码</h3><p>n-step TD估计$V\approx v_{\pi}$<br>输入：一个policy $\pi$<br>算法参数：step size $\alpha \in (0, 1]$，正整数$n$<br>随机初始化$V(s), \forall s\in S$<br>Loop for each episode<br>$\qquad$初始化 $S_0 \neq terminal$<br>$\qquad$$T\leftarrow \infty$<br>$\qquad$Loop for $t=0, 1, 2, \cdots:$<br>$\qquad$ IF $t\lt T$, THEN<br>$\qquad\qquad$ 根据$\pi(\cdot|S_t)$执行action<br>$\qquad\qquad$ 接收并记录$R_{t+1}, S_{t+1}$<br>$\qquad\qquad$ 如果$S_{t+1}$是terminal ，更新$T\leftarrow t+1$<br>$\qquad$ END IF<br>$\qquad$ $\tau \leftarrow t - n + 1, \tau$是当前更新的时间<br>$\qquad$ If $\tau \ge 0$, then<br>$\qquad\qquad$ $G\leftarrow \sum_{i=\tau+1}^{min(\tau+n, T)} \gamma^{i-\tau -1} R_i$<br>$\qquad\qquad$ 如果$\tau+n \lt T$，那么$G\leftarrow G+ \gamma^n V(S_{\tau+n})$<br>$\qquad\qquad$ $V(S_{\tau}) \leftarrow V(S_{\tau}) +\alpha [G-V(S_{\tau})]$<br>$\qquad$ End if<br>Until $\tau = T-1$<br>n-step return有一个重要的属性叫做error reduction property，在最坏的情况下，n-step returns的期望也是一个比$V_{t+n-1}$更好的估计：<br>$$max_{s}|\mathbb{E}_{\pi}\left[G_{t:t+n}|S_t = s\right]- v_{\pi}(s)| \le \gamma^n max_s | V_{t+n-1}(s)-v_{\pi}(s)| \tag{6}$$<br>所以所有的n-step TD方法都可以收敛到真实值，MC和one-step TD是其中的一种特殊情况。</p><h2 id="n-step-sarsa">n-step Sarsa</h2><p>介绍完了prediction算法，接下来要介绍的就是control算法了。因为TD方法是model-free的，所以，还是要估计action value function。上一章介绍了one-step Sarsa，这一章自然介绍一下n-step Sarsa。n-step Sarsa的backup图如下所示：<br><img src="/2019/07/30/reinforcement-learning-an-introduction-第7章笔记/n_step_sarsa.png" alt="n_step_sarsa"><br>就是将n-step TD的state换成state-action就行了。定义action value的n-step returns如下：<br>$$G_{t:t+n} = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+1} + \cdots + \gamma^{n-1} R_{t+n} + \gamma^n Q_{t+n-1}(S_{t+n},A_{t+n}), n\ge 1, 0\le t\le T-n\tag{7}$$<br>如果$t+n\ge T$，那么$G_{t:t+n} = G_t$。<br>完整的$n$-step Sarsa如下所示：</p><h3 id="n-step-sarsa算法伪代码">n-step Sarsa算法伪代码</h3><p>n-step Sarsa算法，估计$Q\approx q_{*}$<br>随机初始化$Q(s,a),\forall s\in S, a\in A$<br>初始化$\pi$是相对于$Q$的$\epsilon$-greedy policy，或者是一个给定的不变policy<br>算法参数：step size $\alpha \in (0,1], \epsilon \gt 0$，一个正整数$n$<br>Loop for each episode<br>$\qquad$初始化$S_0\neq$ terminal<br>$\qquad$ 选择action $A_0= \pi(\cdot| S_0)$<br>$\qquad$ $T\leftarrow \infty$<br>$\qquad$ Loop for $t=0,1,2,\cdots$<br>$\qquad\qquad$ If $t\lt T$,then:<br>$\qquad\qquad\qquad$ 采取action $A_t$，<br>$\qquad\qquad\qquad$ 接收rewared $R_{t+1}$以及下一个state $S_{t+1}$<br>$\qquad\qquad$ 如果$S_{t+1}$是terminal，那么<br>$\qquad\qquad$ $T\leftarrow t+1$<br>$\qquad\qquad$ 否则选择$A_{t+1} = \pi(\cdot|S_{t+1})$<br>$\qquad\qquad$End if<br>$\qquad\qquad$ $\tau \leftarrow t-n+1$<br>$\qquad\qquad$ If $\tau \ge 0$<br>$\qquad\qquad\qquad$ $G\leftarrow \sum_{i=\tau +1}^{min(\tau+n, T)} \gamma^{i-\tau -1} R_i$<br>$\qquad\qquad\qquad$ If $\tau+n \le T$,then<br>$\qquad\qquad\qquad$ $G\leftarrow G+\gamma^n Q(S_{\tau+n}, A_{\tau+n})$<br>$\qquad\qquad\qquad$ $Q(S_{\tau}, Q_{\tau}) \leftarrow Q(S_{\tau}, Q_{\tau}) + \alpha \left[G-Q(S_{\tau}, Q_{\tau})\right]$<br>$\qquad\qquad$End if<br>$\qquad$ Until $\tau =T-1$</p><h3 id="n-step-expected-sarsa">n-step Expected Sarsa</h3><p>n-step Expected Sarsa的n-step return定义为：<br>$$G_{t:t+n} = R_{t+1} +\gamma R_{t+1} + \cdots +\gamma^{n-1} R_{t+n} + \gamma^n \bar{V}_{t+n-1}(S_{t+n}) \tag{8}$$<br>其中$\bar{V}_t(s) = \sum_{a}\pi(a|s) Q_t(s,a), \forall s\in S$<br>如果$s$是terminal，它的期望是$0$。</p><h2 id="n-step-off-policy-learning">n-step Off-policy Learning</h2><p>Off-policy算法使用behaviour policy采样的内容得到target policy的value function，但是需要使用他们在不同policy下采取某个action的relavtive probability。在$n$-step方法中，我们感兴趣的只有对应的$n$个actions，所以最简单的off-policy $n$-step TD算法，$t$时刻的更新可以使用importance sampling ratio $\rho_{t:t+n-1}$：<br>$$V_{t+n}(S_t) = V_{t+n-1} S_t + \alpha \rho_{t:t+n-1}\left[G_{t:t+n} - V_{t+n-1}(S_t)\right], 0\le t\le T \tag{9}$$<br>$\rho_{t:t+n-}$计算的是从$A_t$到$A_{t+n-1}$这$n$个action在behaviour policy和target policy下的relative probability，计算公式如下：<br>$$\rho_{t:h} = \prod_{k=t}^{min(h, T-1)} \frac{\pi(A_k|S_k)}{b(A_k|S_k)} \tag{10}$$<br>如果$\pi(A_k|S_k) = 0$，那么对应的$n$-step return对应的权重就应该是$0$，如果policy $pi$下选中某个action的概率很大，那么对应的return就应该比$b$下的权重大一些。因为在$b$下出现的概率下，很少出现，所以权重就该大一些啊。如果是on-policy的话，importance sampling ratio一直是$1$，所以公式$9$可以将on-policy和off-policy的$n$-step TD概括起来。同样的，$n$-step的Sarsa的更新公式也可以换成：<br>$$Q_{t+n}(S_t,A_t) = Q_{t+n-1}(S_t,A_t) + \alpha \rho_{t+1:t+n}\left[G_{t:t+n} - Q_{t+n-1} (S_t,A_t)\right], 0\le t\le T \tag{11}$$<br>公式$11$中的importance sampling ratio要比公式$9$中计算的晚一步。这是因为我们是在更新一个state-action pair，我们并不关心有多大的概率选中这个action，我们现在已经选中了它，importance sampling只是用于后续actions的选择。这个解释也让我理解了为什么Q-learning和Sarsa为什么没有使用importance sampling。完整的伪代码如下。</p><h3 id="off-policy-n-step-sarsa-估计-q">Off-policy $n$-step Sarsa 估计$Q$</h3><p>输入：任意的behaviour policy $b$, $b(a|s)\gt 0, \forall s\in S, a\in A$<br>随机初始化$Q(s,a), \forall s\in S, a\in A$<br>初始化$\pi$是相对于$Q$的greedy policy<br>算法参数：步长$\alpha \in (0,1]$，正整数$n$<br>Loop for each episode<br>$\qquad$初始化$S_0\neq terminal$<br>$\qquad$选择并存储$A_0 \sim b(\cdot|S_0)$<br>$\qquad T\leftarrow \infty$<br>$\qquad$Loop for $t=0,1,2,\cdots$<br>$\qquad\qquad$IF $t\lt T$,then<br>$\qquad\qquad\qquad$执行action $A_t$，接收$R_{t+1}, S_{t+1}$<br>$\qquad\qquad\qquad$如果$S_{t+1}$是terminal，那么$T\leftarrow t+1$<br>$\qquad\qquad\qquad$否则选择并记录$A_{t+1} \sim b(\cdot| S_{t+1})$<br>$\qquad\qquad$END IF<br>$\qquad\qquad \tau \leftarrow t-n +1$  (加$1$表示下标是从$0$开始的)<br>$\qquad\qquad$ IF $\tau \ge 0$<br>$\qquad\qquad\qquad \rho \leftarrow \prod_{i=\tau+1}^{min(\tau+n,T)} \frac{\pi(A_i|S_i)}{b(A_i|S_i)}$ （计算$\rho_{\tau+1:\tau+n}$，这里是不是写成了Expected Sarsa公式）<br>$\qquad\qquad\qquad G\leftarrow \sum_{i=\tau+1}^{min(\tau+n, T)}\gamma^{i-\tau -1}R_i$ （计算$n$个reward, $R_{\tau+1}+\cdots+R_{\tau+n}$）<br>$\qquad\qquad\qquad$如果$\tau+n \lt T$，$G\leftarrow G + \gamma^n Q(S_{\tau+n},A_{\tau+n})$ （因为没有$Q(S_T,A_T)$）<br>$\qquad\qquad\qquad Q(S_{\tau}, A_{\tau}) \leftarrow Q(S_{\tau}, A_{\tau})+\alpha \rho \left[G-Q(S_{\tau}, A_{\tau})\right]$（计算$Q(S_{\tau+n},A_{\tau+n})$）<br>$\qquad\qquad\qquad$确保$\pi$是相对于$Q$的greedy policy<br>$\qquad\qquad$ END IF<br>$\qquad$Until $\tau = T-1$<br>上面介绍的算法是$n$-step的Sarsa，计算importance ratio使用的$\rho_{t+1:t+n}$，$n$-step的Expected Sarsa计算importance ratio使用的是$\rho_{t+1:t+n-1}$，因为在估计$\bar{V}_{t+n-1}$时候，考虑到了last state中所有的actions。</p><h2 id="n-step-tree-backup算法">$n$-step Tree Backup算法</h2><p>这一章介绍的是不适用importance sampling的off-policy算法，叫做tree-backup algorithm。如下图所示，是一个$3$-step的tree-backup diaqgram。<br>沿着中间这条路走，有三个sample states和rewards以及两个sample actions。在旁边的是没有被选中的actions。对于这些没有选中的actions，因为没有采样，所以使用bootstrap计算target value。因为它的backup diagram有点像tree，所以就叫做tree backup upadte。或者更精确的说，backup update使用的是所用tree的叶子结点action value的估计值进行计算的。非叶子节点的action对应的是采样的action，不参与更新，所有叶子节点对于target的贡献都有一个权重正比于它在target policy下发生的概率。在$S_{t+1}$处的除了$A_{t+1}$之外所有action权重是$\pi(a|S_{t+1})$，$A_{t+1}$一点贡献没有；在$S_{t+2}$处所有没有被选中的action的权重是$\pi(A_{t+1}|S_{t+1})\pi(a’|S_{t+2})$；在$S_{t+3}$处所有没有被选中的action的权重是$\pi(A_{t+1}|S_{t+1})\pi(A_{t+2}|S_{t+2})\pi(a’’|S_{t+3})$<br>one-step Tree backup 算法其实就是Expected Sarsa：<br>$$G_{t:t+1} = R_{t+1} + \gamma \sum_a\pi(a|S_{t+1})Q(a, S_{t+1}), t\lt T-1 \tag{12}$$<br>two-step Tree backup算法return计算公式如下：<br>\begin{align*}<br>G_{t:t+2} &amp;= R_{t+1} + \gamma \sum_{a\neq A_{t+1}}\pi(a|S_{t+1})Q(a, S_{t+1}), t\lt T-1$ + \gamma\pi(A_{t+1}|S_{t+1})(R_{t+2} + \sum_{a} \pi(a|S_{t+2})Q(a, S_{t+2})\\<br>&amp;=R_{t+1} + \gamma \sum_{a\neq A_{t+1}}\pi(a|S_{t+1})Q(a, S_{t+1}), t\lt T-1$ + \gamma\pi(A_{t+1}|S_{t+1})G_{t+1:t+2}, t \lt T-2<br>\end{align*}<br>下面的形式给出了tree-backup的递归形式如下：<br>$$G_{t:t+n} = R_{t+1} + \gamma \sum_{a\neq A_{t+1}}\pi(a|S_{t+1})Q(a,S_{t+1}) + \gamma\pi(A_{t+1}|S_{t+1}) G_{t+1:t+n}, n\ge 2, t\lt T-1, \tag{13}$$<br>当$n=1$时除了$G_{T-1:t+n} = R_T$，其他的和式子$12$一样。使用这个新的target代替$n$-step Sarsa中的target就可以得到$n$-step tree backup 算法。<br>完整的算法如下所示：</p><h3 id="n-step-tree-backup-算法">$n$-step Tree Backup 算法</h3><p>随机初始化$Q(s,a),\forall s\in S, a\in A$<br>初始化$\pi$是相对于$Q$的$\epsilon$-greedy policy，或者是一个给定的不变policy<br>算法参数：step size $\alpha \in (0,1], \epsilon \gt 0$，一个正整数$n$<br>Loop for each episode<br>$\qquad$初始化$S_0\neq$ terminal<br>$\qquad$ 根据$S_0$随机选择action $A_0$<br>$\qquad$ $T\leftarrow \infty$<br>$\qquad$ Loop for $t=0,1,2,\cdots$<br>$\qquad\qquad$ If $t\lt T$,then:<br>$\qquad\qquad\qquad$ 采取action $A_t$，<br>$\qquad\qquad\qquad$ 接收rewared $R_{t+1}$以及下一个state $S_{t+1}$<br>$\qquad\qquad$ 如果$S_{t+1}$是terminal，那么<br>$\qquad\qquad$ $T\leftarrow t+1$<br>$\qquad\qquad$ 根据$S_{t+1}$随机选择action $A_{t+1}$<br>$\qquad\qquad$End IF<br>$\qquad\qquad$ $\tau \leftarrow t-n+1$<br>$\qquad\qquad\qquad$ IF $\tau+n\ge T$ then<br>$\qquad\qquad\qquad\qquad G\leftarrow R_T$<br>$\qquad\qquad\qquad$ ELSE<br>$\qquad\qquad\qquad\qquad G\leftarrow R_{t+1}+\gamma \sum_a\pi(a|S_{t+1})Q(a, S_{t+1})$<br>$\qquad\qquad\qquad$ END IF<br>$\qquad\qquad\qquad$Loop for $k = min(t, T-1)$ down $\tau+1$<br>$\qquad\qquad\qquad G\leftarrow R_k+\gamma^n\sum_{a\neq A_k}\pi(a|S_k)Q(a, S_k) + \gamma \pi(A_k|S_k) G$<br>$\qquad\qquad\qquad$ $Q(S_{\tau}, Q_{\tau}) \leftarrow Q(S_{\tau}, Q_{\tau}) + \alpha \left[G-Q(S_{\tau}, Q_{\tau})\right]$<br>$\qquad\qquad$End if<br>$\qquad$ Until $\tau =T-1$</p><h2 id="n-step-q-sigma">$n$-step $Q(\sigma)$</h2><p>现在已经讲了$n$-step Sarsa，$n$-step Expected Sarsa，$n$-step Tree Backup，$4$-step的一个backup diagram如下所示。它们其实有很多共同的特性，可以用一个框架把它们统一起来。<br>具体的算法就不细说了。</p><h2 id="参考文献">参考文献</h2><p>1.《reinforcement learning an introduction》第二版<br>2.<a href="https://stats.stackexchange.com/questions/335396/why-dont-we-use-importance-sampling-for-one-step-q-learning" target="_blank" rel="noopener">https://stats.stackexchange.com/questions/335396/why-dont-we-use-importance-sampling-for-one-step-q-learning</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;n-step-bootstrapping&quot;&gt;n-step Bootstrapping&lt;/h2&gt;
&lt;p&gt;这章要介绍的n-step TD方法，将MC和one-step TD用一个统一的框架整合了起来。在一个任务中，两种方法可以无缝切换。n-step方法生成了一系列方法
      
    
    </summary>
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="n-steps" scheme="http://mxxhcm.github.io/tags/n-steps/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow distributed tensorflow</title>
    <link href="http://mxxhcm.github.io/2019/07/13/tensorflow-distributed-tensorflow/"/>
    <id>http://mxxhcm.github.io/2019/07/13/tensorflow-distributed-tensorflow/</id>
    <published>2019-07-13T13:11:06.000Z</published>
    <updated>2019-07-13T14:01:56.649Z</updated>
    
    <content type="html"><![CDATA[<h2 id="distributed-tensorflow">Distributed Tensorflow</h2><p>这篇文章主要介绍如何创建cluster of tensorflow serves，并且将一个computation graph分发到这个cluster上。</p><h2 id="cluster和server">Cluster和Server</h2><h3 id="简介">简介</h3><p>tensorflow中，一个cluster是一系列参与tensorflow graph distriuted execution的tasks。每个task和一个tensorflow server相关联，这个server可能包含一个&quot;master&quot;用来创建session，或者&quot;worker&quot;用来执行图中的op。一个cluster可以被分成更多jobs，每一个job包含一个或者更多个tasks。<br>为了创建一个cluster，需要给每一个task指定一个server。每一个task都运行在不同的devices上。在每一个task上，做以下事情：</p><ol><li>创建一个tf.train.ClusterSpec描述这个cluster的所有tasks，这对于所有的task都是一样的。</li><li>创建一个tf.train.Server，需要的参数是tr.train.ClusterSpec，识别local task使用job name和task index。</li></ol><h3 id="创建一个tf-train-clusterspec">创建一个tf.train.ClusterSpec</h3><p>下面的表格中给出了两个cluster的示例。传入的参数是一个dictionary，key是job的name，value是该job的所有可用devices。第二列对应的task的scope name。</p><table><thead><tr><th>tf.train.ClusterSpec construction</th><th>Available tasks</th></tr></thead><tbody><tr><td>tf.train.ClusterSpec({“local”: [“localhost:2222”, “localhost:2223”]})</td><td>/job:local/task:0/job:local/task:1</td></tr><tr><td>tf.train.ClusterSpec({ “worker”: [ “<a href="http://worker0.example.com:2222" target="_blank" rel="noopener">worker0.example.com:2222</a>”, “<a href="http://worker1.example.com:2222" target="_blank" rel="noopener">worker1.example.com:2222</a>”, “<a href="http://worker2.example.com:2222" target="_blank" rel="noopener">worker2.example.com:2222</a>”], “ps”: [“<a href="http://ps0.example.com:2222" target="_blank" rel="noopener">ps0.example.com:2222</a>”, “<a href="http://ps1.example.com:2222" target="_blank" rel="noopener">ps1.example.com:2222</a>”]})</td><td>/job:worker/task:0    /job:worker/task:1  /job:worker/task:2  /job:ps/task:0  /job:ps/task:1</td></tr></tbody></table><h3 id="为每一个task创建一个tf-train-server-instance">为每一个task创建一个tf.train.Server instance</h3><p>tf.train.Server对象包含local devices的集合，和其他tasks的connections</p><h2 id="参考文献">参考文献</h2><p>1.<a href="https://github.com/tensorflow/examples/blob/master/community/en/docs/deploy/distributed.md" target="_blank" rel="noopener">https://github.com/tensorflow/examples/blob/master/community/en/docs/deploy/distributed.md</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;distributed-tensorflow&quot;&gt;Distributed Tensorflow&lt;/h2&gt;
&lt;p&gt;这篇文章主要介绍如何创建cluster of tensorflow serves，并且将一个computation graph分发到这个cluster上。
      
    
    </summary>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/categories/tensorflow/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/tags/tensorflow/"/>
    
      <category term="distributed tensorflow" scheme="http://mxxhcm.github.io/tags/distributed-tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tensosrflow Coordinator</title>
    <link href="http://mxxhcm.github.io/2019/07/13/tensorflow-Coordinator/"/>
    <id>http://mxxhcm.github.io/2019/07/13/tensorflow-Coordinator/</id>
    <published>2019-07-13T12:17:13.000Z</published>
    <updated>2019-07-13T12:29:31.073Z</updated>
    
    <content type="html"><![CDATA[<h2 id="coordinator">Coordinator</h2><h3 id="简介">简介</h3><p>这个类用来协调多个thread同时工作，同时停止。常用的方法有：</p><ul><li>should_stop()：如果满足thread停止条件的话，返回True</li><li>request_stop()：请求thread停止，调用该方法后，should_stop()返回True</li><li>join(<list of threads>)：等待所有的threads停止。</list></li></ul><h3 id="理解">理解</h3><p>其实我觉得这个类的作用好像没有那么大，或者我没有用到这种场景。反正就是满足thread的终止条件时，调用request_stop()函数，让should_stop()返回True。</p><h2 id="代码示例">代码示例</h2><p><a href="https://github.com/mxxhcm/code/blob/master/tf/ops/tf_train_Coordinator.py" target="_blank" rel="noopener">代码地址</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"></span><br><span class="line">n = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add</span><span class="params">(index)</span>:</span></span><br><span class="line">    <span class="keyword">global</span> n</span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">not</span> coord.should_stop():</span><br><span class="line">    <span class="comment">#while True:</span></span><br><span class="line">        <span class="keyword">if</span> n &gt; <span class="number">10</span>:</span><br><span class="line">            print(index, <span class="string">" done"</span>)</span><br><span class="line">            coord.request_stop()</span><br><span class="line">            <span class="comment">#break</span></span><br><span class="line">        print(<span class="string">"before A, thread "</span>, index, n)</span><br><span class="line">        n += <span class="number">1</span></span><br><span class="line">        print(<span class="string">"after A, thread "</span>, index, n)</span><br><span class="line">        time.sleep(random.random())</span><br><span class="line">        print(<span class="string">"before B, thread "</span>, index, n)</span><br><span class="line">        n += <span class="number">1</span></span><br><span class="line">        print(<span class="string">"after B, thread "</span>, index, n)</span><br><span class="line">    </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">        coord = tf.train.Coordinator()</span><br><span class="line"></span><br><span class="line">        jobs = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">            jobs.append(threading.Thread(target=add, args=(i,)))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> jobs:</span><br><span class="line">            j.start()</span><br><span class="line"></span><br><span class="line">        coord.join(jobs)</span><br><span class="line">        print(<span class="string">"Hello World!"</span>)</span><br></pre></td></tr></table></figure><h2 id="参考文献">参考文献</h2><p>1.<a href="https://wiki.jikexueyuan.com/project/tensorflow-zh/how_tos/threading_and_queues.html" target="_blank" rel="noopener">https://wiki.jikexueyuan.com/project/tensorflow-zh/how_tos/threading_and_queues.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;coordinator&quot;&gt;Coordinator&lt;/h2&gt;
&lt;h3 id=&quot;简介&quot;&gt;简介&lt;/h3&gt;
&lt;p&gt;这个类用来协调多个thread同时工作，同时停止。常用的方法有：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;should_stop()：如果满足thread停止条件的话，返回
      
    
    </summary>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/categories/tensorflow/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/tags/tensorflow/"/>
    
      <category term="Coortinator" scheme="http://mxxhcm.github.io/tags/Coortinator/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow one_hot</title>
    <link href="http://mxxhcm.github.io/2019/07/13/tensorflow-one-hot/"/>
    <id>http://mxxhcm.github.io/2019/07/13/tensorflow-one-hot/</id>
    <published>2019-07-13T04:08:55.000Z</published>
    <updated>2019-10-11T05:30:51.354Z</updated>
    
    <content type="html"><![CDATA[<h2 id="tf-one-hot">tf.one_hot</h2><h3 id="一句话介绍">一句话介绍</h3><p>返回一个one-hot tensor</p><h3 id="api">API</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">tf.one_hot(</span><br><span class="line">    indices,    <span class="comment"># 每一个one-hot向量不为0的维度</span></span><br><span class="line">    depth,  <span class="comment"># 指定每一个one-hot向量的维度</span></span><br><span class="line">    on_value=<span class="literal">None</span>,  <span class="comment"># indices上取该值</span></span><br><span class="line">    off_value=<span class="literal">None</span>,     <span class="comment">#其他地方取该值</span></span><br><span class="line">    axis=<span class="literal">None</span>,</span><br><span class="line">    dtype=<span class="literal">None</span>,</span><br><span class="line">    name=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="代码示例">代码示例</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">indices = [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line">depth = <span class="number">5</span></span><br><span class="line"></span><br><span class="line">result = tf.one_hot(indices, depth)</span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line">print(sess.run(result))</span><br><span class="line"><span class="comment"># [[0. 1. 0. 0. 0.]</span></span><br><span class="line"><span class="comment">#  [0. 1. 0. 0. 0.]</span></span><br><span class="line"><span class="comment">#  [0. 1. 0. 0. 0.]]</span></span><br></pre></td></tr></table></figure><h2 id="参考文献">参考文献</h2><p>1.<a href="https://www.tensorflow.org/api_docs/python/tf/one_hot" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/one_hot</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;tf-one-hot&quot;&gt;tf.one_hot&lt;/h2&gt;
&lt;h3 id=&quot;一句话介绍&quot;&gt;一句话介绍&lt;/h3&gt;
&lt;p&gt;返回一个one-hot tensor&lt;/p&gt;
&lt;h3 id=&quot;api&quot;&gt;API&lt;/h3&gt;
&lt;figure class=&quot;highlight pytho
      
    
    </summary>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/categories/tensorflow/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/tags/tensorflow/"/>
    
      <category term="one-hot" scheme="http://mxxhcm.github.io/tags/one-hot/"/>
    
  </entry>
  
  <entry>
    <title>python slice</title>
    <link href="http://mxxhcm.github.io/2019/07/13/python-slice/"/>
    <id>http://mxxhcm.github.io/2019/07/13/python-slice/</id>
    <published>2019-07-13T02:30:16.000Z</published>
    <updated>2019-07-13T02:43:00.402Z</updated>
    
    <content type="html"><![CDATA[<h2 id="python-slice-index"><a href="#python-slice-index" class="headerlink" title="python slice index"></a>python slice index</h2><p> +—-+—-+—-+—-+—-+—-+<br> | P | y | t | h | o | n |<br> +—-+—-+—-+—-+—-+—-+<br> 0   1   2   3   4   5   6<br>-6  -5  -4  -3  -2  -1</p><h2 id="start和stop"><a href="#start和stop" class="headerlink" title="start和stop"></a>start和stop</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a[start:stop]   # 从start到stop-1的所有items</span><br><span class="line">a[start:]   # 从start到array结尾的所有items</span><br><span class="line">a[:stop]   # 从开始到stop-1的所有items</span><br><span class="line">a[:]   # 整个array的copy</span><br></pre></td></tr></table></figure><h2 id="step"><a href="#step" class="headerlink" title="step"></a>step</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a[start:stop:step]   # 从start，每次加上step，不超过stop，step默认为1</span><br></pre></td></tr></table></figure><h2 id="负的start和stop"><a href="#负的start和stop" class="headerlink" title="负的start和stop"></a>负的start和stop</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a[-1]   # array的最后一个item</span><br><span class="line">a[-2:]   # array的最后两个items</span><br><span class="line">a[:-2]   # 从开始到倒数第三个的所有items</span><br></pre></td></tr></table></figure><h2 id="负的step"><a href="#负的step" class="headerlink" title="负的step"></a>负的step</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a[::-1]   # 所有元素，逆序</span><br><span class="line">a[1::-1]    # 前两个元素，逆序</span><br><span class="line">a[:-3:-1]   # 后两个元素，逆序</span><br><span class="line">a[-3::-1]   # 除了最后两个元素，逆序</span><br></pre></td></tr></table></figure><p>这里加一些自己的理解，其实就是倒着数而已，包含第一个:前面的，不包含两个:之间的。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>1.<a href="https://stackoverflow.com/a/509297/8939281" target="_blank" rel="noopener">https://stackoverflow.com/a/509297/8939281</a><br>2.<a href="https://stackoverflow.com/a/509295/8939281" target="_blank" rel="noopener">https://stackoverflow.com/a/509295/8939281</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;python-slice-index&quot;&gt;&lt;a href=&quot;#python-slice-index&quot; class=&quot;headerlink&quot; title=&quot;python slice index&quot;&gt;&lt;/a&gt;python slice index&lt;/h2&gt;&lt;p&gt; +—-+—
      
    
    </summary>
    
      <category term="python" scheme="http://mxxhcm.github.io/categories/python/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="slice" scheme="http://mxxhcm.github.io/tags/slice/"/>
    
  </entry>
  
  <entry>
    <title>python 指定长度list初始化</title>
    <link href="http://mxxhcm.github.io/2019/06/18/python-%E6%8C%87%E5%AE%9A%E9%95%BF%E5%BA%A6list%E5%88%9D%E5%A7%8B%E5%8C%96/"/>
    <id>http://mxxhcm.github.io/2019/06/18/python-指定长度list初始化/</id>
    <published>2019-06-18T07:26:32.000Z</published>
    <updated>2019-06-18T07:35:43.725Z</updated>
    
    <content type="html"><![CDATA[<h2 id="指定长度list初始化"><a href="#指定长度list初始化" class="headerlink" title="指定长度list初始化"></a>指定长度list初始化</h2><p>想要找到初始化指定长度list最快的方法。<br>方法一：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">length = <span class="number">10</span></span><br><span class="line">array = [[]] * length</span><br></pre></td></tr></table></figure></p><p>方法二<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">length = <span class="number">10</span></span><br><span class="line">array = [[] <span class="keyword">for</span> _ <span class="keyword">in</span> range(length)]</span><br></pre></td></tr></table></figure></p><p>事实上，只有第二种方法是对的。第一种方法中，arrary中的10个[]都指向了同一个对象。。</p><h2 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">length = <span class="number">10</span></span><br><span class="line">v1 = [[]]*length</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(length):</span><br><span class="line">    v1[i].append(i)</span><br><span class="line">print(v1)</span><br><span class="line"><span class="comment"># [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]]</span></span><br><span class="line"></span><br><span class="line">v2 = [[] <span class="keyword">for</span> _ <span class="keyword">in</span> range(length)]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(length):</span><br><span class="line">    v2[i].append(i)</span><br><span class="line"></span><br><span class="line">print(v2)</span><br><span class="line"><span class="comment"># [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9]]</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;指定长度list初始化&quot;&gt;&lt;a href=&quot;#指定长度list初始化&quot; class=&quot;headerlink&quot; title=&quot;指定长度list初始化&quot;&gt;&lt;/a&gt;指定长度list初始化&lt;/h2&gt;&lt;p&gt;想要找到初始化指定长度list最快的方法。&lt;br&gt;方法一：&lt;br&gt;&lt;
      
    
    </summary>
    
      <category term="python" scheme="http://mxxhcm.github.io/categories/python/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="list" scheme="http://mxxhcm.github.io/tags/list/"/>
    
  </entry>
  
  <entry>
    <title>python json</title>
    <link href="http://mxxhcm.github.io/2019/06/06/python-json/"/>
    <id>http://mxxhcm.github.io/2019/06/06/python-json/</id>
    <published>2019-06-06T07:58:02.000Z</published>
    <updated>2019-06-06T08:16:11.539Z</updated>
    
    <content type="html"><![CDATA[<h2 id="什么是json"><a href="#什么是json" class="headerlink" title="什么是json"></a>什么是json</h2><p>是一种文件格式<br>json object和python的字典差不多。<br>json在python中可以以字符串形式读入。</p><h2 id="python读取json"><a href="#python读取json" class="headerlink" title="python读取json"></a>python读取json</h2><h3 id="json-loads"><a href="#json-loads" class="headerlink" title="json.loads()"></a>json.loads()</h3><p>json.loads()从内存中读取。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import json</span><br><span class="line"></span><br><span class="line">person = &apos;&#123;&quot;name&quot;: &quot;Bob&quot;, &quot;languages&quot;: [&quot;English&quot;, &quot;Fench&quot;]&#125;&apos;</span><br><span class="line">person_dict = json.loads(person)</span><br><span class="line"></span><br><span class="line"># Output: &#123;&apos;name&apos;: &apos;Bob&apos;, &apos;languages&apos;: [&apos;English&apos;, &apos;Fench&apos;]&#125;</span><br><span class="line">print( person_dict)</span><br><span class="line"></span><br><span class="line"># Output: [&apos;English&apos;, &apos;French&apos;]</span><br><span class="line">print(person_dict[&apos;languages&apos;])</span><br></pre></td></tr></table></figure></p><h3 id="json-load"><a href="#json-load" class="headerlink" title="json.load()"></a>json.load()</h3><p>json.load()从文件对象中读取。<br>假设有名为person.json的json文件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">&quot;name&quot;: &quot;Bob&quot;, </span><br><span class="line">&quot;languages&quot;: [&quot;English&quot;, &quot;Fench&quot;]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>直接从文件对象中读取<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'person.json'</span>) <span class="keyword">as</span> f:</span><br><span class="line">  data = json.load(f)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output: &#123;'name': 'Bob', 'languages': ['English', 'Fench']&#125;</span></span><br><span class="line">print(data)</span><br></pre></td></tr></table></figure></p><h2 id="python写json文件"><a href="#python写json文件" class="headerlink" title="python写json文件"></a>python写json文件</h2><h3 id="json-dumps"><a href="#json-dumps" class="headerlink" title="json.dumps()"></a>json.dumps()</h3><p>json.dumps()将字典转化为JSON字符串<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line">person_dict = &#123;<span class="string">'name'</span>: <span class="string">'Bob'</span>,</span><br><span class="line"><span class="string">'age'</span>: <span class="number">12</span>,</span><br><span class="line"><span class="string">'children'</span>: <span class="literal">None</span></span><br><span class="line">&#125;</span><br><span class="line">person_json = json.dumps(person_dict)</span><br><span class="line"></span><br><span class="line">print(person_json)</span><br></pre></td></tr></table></figure></p><h3 id="json-dump"><a href="#json-dump" class="headerlink" title="json.dump()"></a>json.dump()</h3><p>json.dump()直接将字典写入文件对象<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line">person_dict = &#123;<span class="string">"name"</span>: <span class="string">"Bob"</span>,</span><br><span class="line"><span class="string">"languages"</span>: [<span class="string">"English"</span>, <span class="string">"Fench"</span>],</span><br><span class="line"><span class="string">"married"</span>: <span class="literal">True</span>,</span><br><span class="line"><span class="string">"age"</span>: <span class="number">32</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'person.txt'</span>, <span class="string">'w'</span>) <span class="keyword">as</span> json_file:</span><br><span class="line">  json.dump(person_dict, json_file)</span><br></pre></td></tr></table></figure></p><h2 id="pretty-JSON"><a href="#pretty-JSON" class="headerlink" title="pretty JSON"></a>pretty JSON</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line">person_string = <span class="string">'&#123;"name": "Bob", "languages": "English", "numbers": [2, 1.6, null]&#125;'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Getting dictionary</span></span><br><span class="line">person_dict = json.loads(person_string)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Pretty Printing JSON string back</span></span><br><span class="line">print(json.dumps(person_dict, indent = <span class="number">4</span>, sort_keys=<span class="literal">True</span>))</span><br></pre></td></tr></table></figure><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>1.<a href="https://www.programiz.com/python-programming/json" target="_blank" rel="noopener">https://www.programiz.com/python-programming/json</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;什么是json&quot;&gt;&lt;a href=&quot;#什么是json&quot; class=&quot;headerlink&quot; title=&quot;什么是json&quot;&gt;&lt;/a&gt;什么是json&lt;/h2&gt;&lt;p&gt;是一种文件格式&lt;br&gt;json object和python的字典差不多。&lt;br&gt;json在pytho
      
    
    </summary>
    
      <category term="python" scheme="http://mxxhcm.github.io/categories/python/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="json" scheme="http://mxxhcm.github.io/tags/json/"/>
    
  </entry>
  
  <entry>
    <title>reinforcement learning an introduction 第6章笔记</title>
    <link href="http://mxxhcm.github.io/2019/06/02/reinforcement-learning-an-introduction-%E7%AC%AC6%E7%AB%A0%E7%AC%94%E8%AE%B0/"/>
    <id>http://mxxhcm.github.io/2019/06/02/reinforcement-learning-an-introduction-第6章笔记/</id>
    <published>2019-06-02T06:31:49.000Z</published>
    <updated>2019-10-07T02:53:38.308Z</updated>
    
    <content type="html"><![CDATA[<h2 id="td-learning">TD Learning</h2><p>TD方法是DP和MC方法的结合，像MC一样，TD可以不需要model直接从experience中学习，像DP一样，TD是bootstrap的方法。<br>本章的结构和之前一样，首先研究policy evaluation或者叫prediction问题，即给定一个policy $\pi$，估计$v_{\pi}$；然后研究control问题。DP,TD,MC方法都是使用GPI方法解control问题，不同点在于prediction问题的解法。<br>为什么叫TD？<br>因为TD更新是基于不同时间上两个estimate的估计进行的。</p><h2 id="td-prediction">TD prediction</h2><p>TD和MC都是利用采样获得的experience求解prediction问题。给定policy $\pi$下的一个experience，TD和MC方法使用该experience中出现的non-terminal state $S_t$估计$v_{\pi}$的$V$。他们的不同之处在于MC需要等到整个experience的return知道以后，把这个return当做$V(S_t)$的target，every visit MC方法的更新规则如下：<br>$$V(S_t) = V(S_t) + \alpha \left[G_t - V(S_t)\right]\tag{1}$$<br>其中$G_t$是从时刻$t$到这个episode结束的return，$\alpha$是一个常数的步长，这个方法叫做$constant-\alpha$ MC。MC方法必须等到一个episode结束，才能进行更新，因为只有这个时候$G_t$才知道。为了更方便的训练，就有了TD方法。TD方法做的改进是使用$t+1$时刻state $V(S_{t+1})$的估计值和reward $R_{t+1}$的和作为target：<br>$$V(S_t) = V(S_t) + \alpha \left[R_{t+1}+\gamma V(S_{t+1}) - V(S_t)\right]\tag{2}$$<br>如果V在变的话，是不是应该是下面的公式？？<br>$$V_{t+1}(S_t) = V_t(S_t) + \alpha \left[R_{t+1}+\gamma V_t(S_{t+1}) - V_t(S_t)\right]$$<br>即只要有了到$S_{t+1}$的transition并且接收到了reward $R_{t+1}$就可以进行上述更新。MC方法的target是$G_t$，而TD方法的target是$\gamma V(S_{t+1} + R_{t+1})$，这种TD方法叫做$TD-0$或者$one\ step\ TD$，它是$TD(\lambda)$和$n-step\ TD$的一种特殊情况。</p><h3 id="算法">算法</h3><p>下面是$TD(0)$的完整算法：<br>算法1 Tabular TD(0) for $V$<br>输入： 待评估的policy $\pi$<br>算法参数：步长$\alpha \in (0,1]$<br>初始化： $V(s), \forall s\in S^{+}，V(terminal) = 0$<br><strong>Loop</strong> for each episode<br>$\qquad$初始化$S$<br>$\qquad A\leftarrow \pi(a|S)$<br>$\qquad$<strong>Loop</strong> for each step of episode<br>$\qquad\qquad$执行action $A$，得到$S’$和$R$<br>$\qquad\qquad V(S) = V(S) + \alpha \left[R + \gamma V(S’) - V(S)\right]$<br>$\qquad\qquad$$S\leftarrow S’$<br>$\qquad$<strong>Until</strong> $S$ 是terminal state</p><p>$TD(0)$是bootstrap方法，因为它基于其他state的估计value进行更新。从第三章中我们知道：<br>\begin{align*}<br>v_{\pi}(s) &amp; = \mathbb{E}_{\pi}\left[G_t\right]\tag{3}\\<br>&amp; = \mathbb{E}_{\pi}\left[R_{t+1}+\gamma G_{t+1}| S_t = s\right]\tag{4}\\<br>&amp; = \mathbb{E}_{\pi}\left[R_{t+1}+\gamma v_{\pi}(S_{t+1})|S_t = s\right]\tag{5}\\<br>\end{align*}<br>MC使用式子$(3)$的estimate作为target，而DP使用式子$(5)$的estimate作为target。MC方法用一个sample的return代替式子$(3)$中真实的未知expected return $G_t$；DP是用$V(S_{t+1})$作为$v_{\pi}(S_{t+1})$的一个估计，因为$v_{\pi}(S_{t+1})$的真实值是不知道的。TD结合了MC的采样以及DP的bootstrap，它对式子$(4)$的tranisition进行sample，同时使用$v_{\pi}$的估计值$V$进行计算。<br><img src="/2019/06/02/reinforcement-learning-an-introduction-第6章笔记/backup_td.png" alt="backup_TD"><br>TD的backup图如图所示。TD和MC updates被称为sample updates，因为这两个算法的更新都牵涉到采样一个sample successor state，使用这个state的value和它后继的这条路上的reward计算一个backed-up value，然后根据这个值更新该state的value。sample updates和DP之类的expected updates的不同在于，sample updates使用一个sample successor进行更新，expected updates使用所有可能的successors distribution进行更新。<br>$R_{t+1} + \gamma V(S_{t+1}) - V(S_t)$可以看成一种error，衡量了$S_t$当前的estimated value $V(S_t)$和一个更好的estimated value之间的差异$R_{t+1} +\gamma V(S_{t+1})$，我们把它叫做$TD-error$，用$\delta_t$表示。$\delta_t$是$t$时刻的$TD-error$，在$t+1$时刻可用，用公式表示是：<br>$$\delta_t = R_{t+1} + \gamma V(S_{t+1}) - V(S_t) \tag{6}$$<br>如果$V$在一个episode中改变的话，那么上述公式是不是应该写成：<br>$$\delta_t = R_{t+1} + \gamma V_t(S_{t+1}) - V_t(S_t)$$<br>应该在$t$时刻，计算的TD error是用来更新$t+1$时刻的value的。如果$V$在一个episdoe中不变的话，就像MC方法一样，那么MC error可以写成TD errors的和。<br>\begin{align*}<br>G_t - V(S_t) &amp; = R_{t+1} + \gamma G_{t+1} - V(S_t) + \gamma V(S_{t+1}) - \gamma V(S_{t+1})\\<br>&amp; = R_{t+1} + \gamma V(S_{t+1}) - V(S_t) + \gamma G_{t+1} - \gamma V(S_{t+1})\\<br>&amp; = \delta_t + \gamma G_{t+1} - \gamma V(S_{t+1})\\<br>&amp; = \delta_t + \gamma(G_{t+1} - V(S_{t+1}))\\<br>&amp; = \delta_t + \gamma\delta_{t+1} + \gamma^2(G_{t+2} - V(S_{t+2}))\\<br>&amp; = \delta_t + \gamma\delta_{t+1} + \gamma^2\delta_{t+2} + \cdots + \gamma^{T-t-1}\delta_{T-1} + \gamma^{T-t}(G_T-V(S_T))\\<br>&amp; = \delta_t + \gamma\delta_{t+1} + \gamma^2\delta_{t+2} + \cdots + \gamma^{T-t-1}\delta_{T-1} + \gamma^{T-t}(0-0)\\<br>&amp; = \sum_{k=t}^{T-1} \gamma^{k-t}\delta_k \tag{7}\\<br>\end{align*}<br>如果$V$在一个episode中改变了的话，像$TD(0)$一样，这个公式就不精确成立了，如果$\alpha$足够小的话，还是近似成立的。<br>$$\delta_t = R_{t+1} + \gamma V_t(S_{t+1}) - V_t(S_t)$$<br>\begin{align*}<br>V_{t+1}(S_t) &amp;= V_t(S_t) + \alpha \left[R_{t+1}+\gamma V_t(S_{t+1}) - V_t(S_t)\right]\\<br>&amp;= V_t(S_t) + \alpha \delta_t<br>\end{align*}</p><p>\begin{align*}<br>G_t - V_t(S_t) &amp; = R_{t+1} + \gamma G_{t+1} - V_t(S_t) + \gamma V_{t+1}(S_{t}) - \gamma V_{t+1}(S_{t})\\<br>&amp; = R_{t+1} + \gamma V_{t+1}(S_{t}) - V_t(S_t) + \gamma G_{t+1}- \gamma V_{t+1}(S_{t})\\<br>&amp; = R_{t+1} + \gamma (V_t(S_t) + \alpha \delta_t) - V_t(S_t) + \gamma G_{t+1}- \gamma V_{t+1}(S_{t})\\<br>&amp; = R_{t+1} + \gamma V_t(S_t) - V_t(S_t) + \gamma \alpha \delta_t + \gamma G_{t+1}- \gamma V_{t+1}(S_{t})\\<br>\end{align*}<br>然而上面是错误的，因为$\delta_t$需要的是$V_t(S_{t+1})$<br>\begin{align*}<br>G_t - V_t(S_t) &amp; = R_{t+1} + \gamma G_{t+1} - V_t(S_t) + \gamma V_{t+1}(S_{t+1}) - \gamma V_{t+1}(S_{t+1})\\<br>&amp; = R_{t+1} + \gamma V_{t+1}(S_{t+1}) - V_t(S_t) + \gamma G_{t+1}- \gamma V_{t+1}(S_{t+1})\\<br>\end{align*}<br>\begin{align*}<br>\delta_t &amp;= R_{t+1} + \gamma V_t(S_{t+1}) - V_t(S_t)\\<br>\delta_{t+1} &amp;= R_{t+2} + \gamma V_{t+1}(S_{t+2}) - V_{t+1}(S_{t+1})\\<br>\delta_{t+2} &amp;= R_{t+3} + \gamma V_{t+2}(S_{t+3}) - V_{t+2}(S_{t+2})\\<br>\delta_{t+3} &amp;= R_{t+4} + \gamma V_{t+3}(S_{t+4}) - V_{t+3}(S_{t+3})\\<br>\end{align*}</p><p>\begin{align*}<br>&amp;\delta_t+\delta_{t+1}+\delta_{t+2}+\delta_{t+3}\\<br>= &amp;R_{t+1} + \gamma V_t(S_{t+1}) - V_t(S_t)\\<br>+&amp;R_{t+2} + \gamma V_{t+1}(S_{t+2}) - V_{t+1}(S_{t+1})\\<br>+&amp;R_{t+3} + \gamma V_{t+2}(S_{t+3}) - V_{t+2}(S_{t+2})\\<br>+&amp;R_{t+4} + \gamma V_{t+3}(S_{t+4}) - V_{t+3}(S_{t+3})\\<br>\end{align*}<br>OK。。。还是没有算出来。。</p><h3 id="td例子">TD例子</h3><p>TD的一个例子。每天下班的时候，你会估计需要多久能到家。你回家的事件和星期，天气等相关。在周五的晚上6点，下班之后，你估计需要30分钟到家。到车旁边是$6:05$，而且天快下雨了。下雨的时候会有些堵车，所以估计从现在开始大概还需要$35$分钟才能到家。十五分钟后，下了高速，这个时候你估计总共的时间是$35$分钟（包括到达车里的$5$分钟）。然后就遇到了堵车，真正到达家里的街道是$6:40$，三分钟后到家了。</p><table><thead><tr><th style="text-align:center">State</th><th style="text-align:center">Elapsed Time</th><th style="text-align:center">Predicted Time to Go</th><th style="text-align:center">Predicted Total Time</th></tr></thead><tbody><tr><td style="text-align:center">leaveing office</td><td style="text-align:center">0</td><td style="text-align:center">30</td><td style="text-align:center">30</td></tr><tr><td style="text-align:center">reach car</td><td style="text-align:center">5</td><td style="text-align:center">35</td><td style="text-align:center">40</td></tr><tr><td style="text-align:center">下高速</td><td style="text-align:center">20</td><td style="text-align:center">15</td><td style="text-align:center">35</td></tr><tr><td style="text-align:center">堵车</td><td style="text-align:center">30</td><td style="text-align:center">10</td><td style="text-align:center">40</td></tr><tr><td style="text-align:center">门口的街道</td><td style="text-align:center">40</td><td style="text-align:center">3</td><td style="text-align:center">43</td></tr><tr><td style="text-align:center">到家</td><td style="text-align:center">43</td><td style="text-align:center">0</td><td style="text-align:center">43</td></tr></tbody></table><p>rewards是每一个journey leg的elapsed times，这里我们研究的是evaluation问题，所以可以直接使用elapsed time，如果是control问题，要在elapsed times前加负号。state value是expected time。上面的第一列数值是reward，第二列是当前state的value估计值。<br>如果使用$\alpha = 1$的TD和MC方法。对于MC方法，对于$S_t$的所有state，都有：<br>\begin{align*}<br>V(S_t) &amp;= V(S_t) + (G_t - V(S_t))\\<br>&amp; = G_t \\<br>&amp; = 43<br>\end{align*}<br>对于TD方法，让$\gamma=1$，有：<br>\begin{align*}<br>V(S_t) &amp;= V_t(S_t) + \alpha (R_{t+1} +  \gamma V_t(S_{t+1}) - V(S_t))\\<br>&amp;= R_{t+1} + V_t(S_{t+1})<br>\end{align*}</p><h3 id="td-prediction的好处">TD Prediction的好处</h3><p>TD是bootstrap方法，相对于MC和DP来说，TD的好处有以下几个：</p><ol><li>相对于DP，不需要environment, reward model以及next-state probability distribution。</li><li>相对于MC，TD是online，incremental的。MC需要等到一个episode结束，而TD只需要等一个时间步（本节介绍的TD0）。</li><li>TD在table-base case可以为证明收敛，而general linear function不一定收敛。</li></ol><p>但是具体TD好还是MC好，目前还没有明确的数学上的理论证明。而实践上表明，TD往往要比constant $\alpha$ MC算法收敛的快。</p><h2 id="td-0-的优势">TD(0)的优势</h2><p>如果我们只有很少的experience的话，比如有$10$个episodes，或者有$100$个timesteps。这种情况下，我们会重复的使用这些这些experience进行训练直到算法收敛。具体方法是，给定一个approximate value function $V$，在每一个只要不是terminal state的时间$t$处，计算MC和TD增量，最后使用所有增量之和只更新value function一次。举个例子好了，假如我们有三个episdoes，<br>A,B,C<br>B,A<br>A,A<br>更新的方法是，$V(A) = V(A) + \alpha(G_1 - V(A) + G_2 - V(A) + G_{31} - V(A) + G_{32} -V(A))$<br>这种方法叫做batch updating，因为只有在一个batch完全处理完之后才进行更新，其实这个和DP挺像的，只不过DP直接利用的是environment dynamic，而我们使用的是样本。<br>在batch updating中，TD(0)一定会收敛到一个与$\alpha$无关的结果，只要$\alpha$足够下即可，同理batch constant $\alpha$ MC算法同样条件下也会收敛到一个确定的结果，只不过和batch TD结果不同而已。Normal updating的方法并没有朝着整个batch increments的方法移动，但是大概方向差不多。其实就是一个把整个batch的所有experience的increment加起来一起更新，一个是每一个experience更新一次，就这么点区别。<br>具体来说，batch TD和batch MC哪个更好一些呢？这就牵扯到他们的原理了。Batch MC的目标是最小化training set上的mse，而batch TD的目标是寻找Markov process的最大似然估计。一般来说，maximum likeliood estimate是进行参数估计的。在这里的话，TD使用mle从已有episodes中生成markov process模型的参数：从$i$到$j$的transition probatility是观测到从$i$到$j$的transition所占的百分比，对应的expected reward是观测到的rewards的均值。给出了这个model之后，如果这个模型是exactly correct的话，那么我们就可以准确的计算出value function的estimate，这个成为certainty-equivalence estimate，因为它相当于假设markov process的model是一致的，而不是approximated，一般来说，batch TD(0)收敛到cetainty-equivalence estimate。<br>从而，我们可以简单的解释以下为什么batch TD比batch MC收敛的快。因为batch TD计算的是真实的cetainty-equivalence estimate。同样的，对于non batch的TD和MC来说，虽然TD没有使用cetainty-equivalence，但是它们大概在向那个方向移动。<br>尽管cetrinty-equivalence是最优解，但是，但是，但是，cost太大了，如果有$n$个states，计算mle需要$n^2$的空间，计算value function时候，需要$n^3$的计算步数。当states太多的话，实际上并不可行，还是老老实实的使用TD把，只会用不超过$n$的空间。。</p><h2 id="td具体算法介绍">TD具体算法介绍</h2><h3 id="sarsa">Sarsa</h3><h4 id="介绍">介绍</h4><p>Sarsa是一个on-policy的 TD control算法。按照GPI的思路来，先进行policy evaluation，在进行policy improvement。首先解prediction问题，按照以下action value的$TD(0)$公式估计当前policy $\pi$下，所有action和state的$q$值$q_{\pi}(s,a)$：<br>$$Q(S_t,A_T) \leftarrow Q(S_t,A_t) + \alpha \left[R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) -Q(S_t,A_t)\right] \tag{8}$$<br>当$S_{t+1} = 0$时，$Q(S_{t+1}, A_{t+1})=0$，相应的backup diagram如下图所示。<br><img src="/2019/06/02/reinforcement-learning-an-introduction-第6章笔记/ff.png" alt="f"><br>第二步解control问题，在on-policy的算法中，不断的估计behaviour policy $\pi$的$q_{\pi}$，同时改变$\pi$朝着$q_{\pi}$更大的方向移动。Sarsa算法中，behaviour policy和target policy是一样的，在不断的改变。完整的算法如下：<br><em><em>Sarsa算法(on-policy control) 估计$Q\approx q_</em>$</em>*<br>对于所有$s\in S^{+}, a\in A(s)$，随机初始化$Q(s,a)$，$Q(terminal, \cdot) = 0$<br>Loop for each episode<br>$\qquad$ 获得初始状态$S$<br>$\qquad$ 使用policy（如$\epsilon$-greedy算法）根据state $S$选择当前动作$A$<br>$\qquad$ Loop for each step of episode<br>$\qquad\qquad$ 采取action，得到R和S’<br>$\qquad\qquad$ 使用policy（和上面的policy一样）根据S’选择A’<br>$\qquad\qquad Q(S,A) \leftarrow Q(S,A) + \alpha \left[R+ \gamma Q(S’,A’) - Q(S,A)\right]$<br>$\qquad\qquad S\leftarrow S’, A\leftarrow A’$<br>$\qquad$ until $S$是terminal</p><h4 id="示例">示例</h4><h3 id="q-learning">Q-learning</h3><p>$$Q(S_t,A_T) \leftarrow Q(S_t,A_t) + \alpha \left[R_{t+1} + \gamma max Q(S_{t+1}, A_{t+1}) -Q(S_t,A_t)\right]\tag{9}$$<br>这一节介绍的是off-policy的TD contrl算法，Q-learning。对于off-policy算法来说，behaviour policy用来选择action，target policy是要评估的算法。在Q-learning算法中，直接学习的就是target policy的optimal action value function $q_{*}$，和behaviour policy无关。完整的Q-learning算法如下：<br><strong>Q-learning算法(off-policy control) 估计$\pi \approx \pi_{*}$</strong><br>对于所有$s\in S^{+}, a\in A(s)$，随机初始化$Q(s,a)$，$Q(terminal, \cdot) = 0$<br>Loop for each episode<br>$\qquad$ 获得初始状态$S$<br>$\qquad$ Loop for each step of episode<br>$\qquad\qquad$ 使用behaviour policy（如$\epsilon$-greedy算法）根据state $S$选择当前动作$A$<br>$\qquad\qquad$ 执行action $A$，得到$R$和$S’$<br>$\qquad\qquad Q(S,A) \leftarrow Q(S,A) + \alpha \left[R+ \gamma max Q(S’,A’) - Q(S,A)\right]$<br>$\qquad\qquad S\leftarrow S’$<br>$\qquad$ until $S$是terminal</p><h3 id="expected-sarsa">Expected Sarsa</h3><p>Q-learning对所有next state-action pairs取了max操作。如果不是取max，而是取期望呢？<br>\begin{align*}<br>Q(S_t,A_T) &amp; \leftarrow Q(S_t,A_t) + \alpha \left[R_{t+1} + \gamma \mathbb{E}_{\pi}\left[ Q(S_{t+1}, A_{t+1})| S_{t+1} \right] -Q(S_t,A_t)\right]\\<br>&amp;\leftarrow Q(S_t,A_t) + \alpha \left[R_{t+1} + \gamma \sum_a\pi(a|S_{t+1})Q -Q(S_t,A_t)\right]\tag{10}<br>\end{align*}<br>其他的和Q-learning保持一致。给定next state $S_{t+1}$，算法在expectation上和sarsa移动的方向一样，所以被称为expected sarsa。这个算法可以是on-policy，但是通常它是是off-policy的。比如，on-policy的话，policy使用$\epsilon$ greedy算法，off-policy的话，behaviour policy使用stochastic policy，而target policy使用greedy算法，这其实就是Q-learning算法了。所以，Expected Sarsa实际上是对Q-learning的一个归纳，同时又有对Sarsa的改进。<br>Q-learning和Expected Sarsa的backup diagram如下所示：<br><img src="/2019/06/02/reinforcement-learning-an-introduction-第6章笔记/q_learning_and_expected_Sarsa_backup_diagram.png" alt="q_learning_and_expected_Sarsa_backup_diagram"></p><h3 id="sarsa-vs-q-learning-vs-expected-sarsa">Sarsa vs Q-learning vs Expected Sarsa</h3><p>Sarsa是on-policy的，behaviour policy和target policy一直在变（$\epsilon$在变），但是behaviour policy和target policy一直都是一样的。<br>Q-learning是off-policy的，target policy和behaviour policy一直都不变（可能$\epsilon$会变，但是这个不是Q-learning的重点），behaviour policy保证exploration，target policy是greedy算法。<br>Q-learning是off-policy算法，那么又为什么one-step Q-learning不需要importance sampling？Importance sampling的作用是为了使用policy $b$下观察到的rewards估计policy $\pi$下的expected rewards。尽管在Q-learning中，behaviour policy和target policy不同，behaviour policy仅仅用来采样$s_t, a_t, R_{t+1}$，在更新$Q$值时，使用target policy(epsilon policy)生成的实际上是$a’ = \max_a Q(s_{t+1}, a)$。Target policy和behaviour policy不同的实际上是$a_t$，但是在更新$Q$值时，用的也是$a_t$。也就是说使用behaviour policy选择的是$a_t$，接下来使用target policy选择执行$a_t$后的新action $a_{t+1}$。<br>Q(0)和Expected Sarsa(0)都没有使用importance sampling，因为在$Q(s,a)$中，action $a$已经被选择了，用哪个policy选择的是无关紧要的，TD error可以使用$Q(s’,*)$上的boostrap进行计算，而不需要behaviour policy。</p><h2 id="maximization-bias和double-learning">Maximization Bias和Double Learning</h2><p>目前介绍的所有control算法，都涉及到target polices的maximization操作。Q-learning中有greedy target policy，Sarsa的policy通常是$\epsilon$ greedy，也会牵扯到maximization。Max操作会引入一个问题，加入某一个state，它的许多action对应的$q(s,a)=0$，然后它的估计值$Q(s,a)$是不确定的，可能比$0$大，可能比$0$小，还可能就是$0$。如果使用max $Q(s,a)$的话，得到的值一定是大于等于$0$的，显然有一个positive bias，这就叫做maximization bias。</p><h3 id="maximization-bias例子">Maximization Bias例子</h3><p>给出如下的一个例子：<br><img src="/2019/06/02/reinforcement-learning-an-introduction-第6章笔记/example_6_7.png" alt="example_6_7"><br>这个MDP有四个state，A,B,C,D，C和D是terminal state，A总是start state，并且有left和right两个action，right action转换到C，reward是0,left action转换到B，reward是$0$，B有很多个actions，都是转换到$D$，但是rewards是不同，reward服从一个均值为$-0.5$，方差为$1.0$的正态分布。所以reward的期望是负的，$-0.5$。这就意味着在大量实验中，reward的均值往往是小于$0$的。<br>基于这个假设，在A处总是选择left action是很蠢的，但是因为其中有一些reward是positive，如果使用max操作的话，整个policy会倾向于选择left action，这就造成了在一些episodes中，reward是正的，但是如果在long run中，reward的期望就是负的。</p><h3 id="maximizaiton-bias出现的直观解释">Maximizaiton Bias出现的直观解释</h3><p>那么为什么会出现这种问题呢？<br>用$X1$和$X2$表示reward的两组样本数据。如下所示：<br><img src="/2019/06/02/reinforcement-learning-an-introduction-第6章笔记/maximization_bias.png" alt="maximization_bias"><br>在$X1$这组样本中，样本均值是$-0.43$，X2样本均值是$-0.36$。在增量式计算样本均值$\mu$时，得到的最大样本均值的期望是$0.09$，而实际上计算出来的期望的最大值$\mathbb{E}(X)$是$-0.36$。要使用$\mathbb{E} \left[max\ (\mu)\right]$估计$max\ \mathbb{E}(X)$，显然它们的差距有点大，$max(\mu)$是$max E(X)$的有偏估计。也就是说使用$max Q(s’,a’)$更新$Q(s,a)$时，$Q(s,a)$并没有朝着它的期望$-0.5$移动。估计这只是一个直观的解释，严格的证明可以从论文中找。</p><h3 id="如何解决maximization-bias问题">如何解决Maximization Bias问题</h3><p>那么怎么解决这个问题呢，就是同时学习两个$Q$函数$Q_1, Q_2$，这两个$Q$函数的地位是一样的，每次随机选择一个选择action，然后更新另一个。证明的话，Van Hasselt证明了$\mathbb{E}(Q_2(s’,a*)\le max\ Q_1(s’,a*)$，也就是说$Q_1(s,a)$不再使用它自己的max value进行更新了。<br>下面是$Q$-learning和Double $Q$-learning在训练过程中在A处选择left的统计：<br><img src="/2019/06/02/reinforcement-learning-an-introduction-第6章笔记/q_learning_vs_double_q_learning.png" alt="q_learning_vs_double_q_learning"><br>可以看出来，Double $Q$-learning要比$Q$-learning收敛的快和好。<br>当然，Sarsa和Expected Sarsa也有maximization bias问题，然后有对应的double版本，Double Sarsa和Double Expected Sarsa。</p><h2 id="afterstates">Afterstates</h2><p>之前介绍了state value function和action value function。这里介绍一个afterstate value function，afterstate value function就是在某个state采取了某个action之后再进行评估，一开始我想这步就是action value function。事实上不是的，action value function估计的是$Q(s,a)$，重点是state和action这些pair，对于afterstate value来说，可能有很多个state和action都能到同一个next state，这个时候它们的作用是一样的，因为我们估计的是next state的value。<br>象棋就是一个这样的例子。。这里只是介绍一下，还有很多各种各样特殊的形式，它们可以用来解决各种各样的特殊问题。具体可以自己多了解一下。</p><h2 id="总结">总结</h2><p>这一章主要介绍了最简单的一种TD方法，one-step，tabular以及model-free。接下来的两章会介绍一些n-step的TD方法，可以和MC方法联系起来，以及包含一个模型的方法，和DP联系起来。在第二部分的时候，会将tabular的TD扩展到function approximation的形式，和deep learning以及artificial neural networks联系起来。</p><h2 id="参考文献">参考文献</h2><p>1.《reinforcement learning an introduction》第二版<br>2.<a href="https://stats.stackexchange.com/a/297892" target="_blank" rel="noopener">https://stats.stackexchange.com/a/297892</a><br>3.<a href="https://towardsdatascience.com/double-q-learning-the-easy-way-a924c4085ec3" target="_blank" rel="noopener">https://towardsdatascience.com/double-q-learning-the-easy-way-a924c4085ec3</a><br>4.<a href="https://stats.stackexchange.com/a/347090/254953" target="_blank" rel="noopener">https://stats.stackexchange.com/a/347090/254953</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;td-learning&quot;&gt;TD Learning&lt;/h2&gt;
&lt;p&gt;TD方法是DP和MC方法的结合，像MC一样，TD可以不需要model直接从experience中学习，像DP一样，TD是bootstrap的方法。&lt;br&gt;
本章的结构和之前一样，首先研究policy
      
    
    </summary>
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>reinforcment learning terms</title>
    <link href="http://mxxhcm.github.io/2019/05/29/reinforcement-learning-terms/"/>
    <id>http://mxxhcm.github.io/2019/05/29/reinforcement-learning-terms/</id>
    <published>2019-05-29T09:59:28.000Z</published>
    <updated>2019-09-28T03:55:24.123Z</updated>
    
    <content type="html"><![CDATA[<h2 id="术语定义">术语定义</h2><p>更多介绍可以点击查看<a href="https://mxxhcm.github.io/2018/12/21/reinforcement-learning-an-introduction-%E7%AC%AC3%E7%AB%A0%E7%AC%94%E8%AE%B0/">reinforcement learning an introduction 第三章</a></p><h3 id="状态集合">状态集合</h3><p>$\mathcal{S}$是有限states set，包含所有state的可能取值</p><h3 id="动作集合">动作集合</h3><p>$\mathcal{A}$是有限actions set，包含所有action的可能取值</p><h3 id="转换概率矩阵或者状态转换函数">转换概率矩阵或者状态转换函数</h3><p>$P:\mathcal{S}\times \mathcal{A}\times \mathcal{S} \rightarrow \mathbb{R}$是transition probability distribution，或者写成$p(s_{t+1}|s_t,a_t)$</p><h3 id="奖励函数">奖励函数</h3><p>$R:\mathcal{S}\times \mathcal{A}\rightarrow \mathbb{R}$是reward function</p><h3 id="折扣因子">折扣因子</h3><p>$\gamma \in (0, 1)$</p><h3 id="初始状态分布">初始状态分布</h3><p>$\rho_0$是初始状态$s_0$服从的distribution，$s_0\sim \rho_0$</p><h3 id="带折扣因子的mdp">带折扣因子的MDP</h3><p>定义为tuple $\left(\mathcal{S},\mathcal{A},P,R,\rho_0, \gamma\right)$</p><h3 id="随机策略">随机策略</h3><p>选择action，stochastic policy表示为：$\pi_\theta: \mathcal{S}\rightarrow P(\mathcal{A})$，其中$P(\mathcal{A})$是选择$\mathcal{A}$中每个action的概率，$\theta$表示policy的参数，$\pi_\theta(a_t|s_t)$是在$s_t$处取action $a_t$的概率</p><h3 id="accumulated-reward">Accumulated Reward</h3><h4 id="期望折扣回报">期望折扣回报</h4><p>定义<br>$$G_t =\mathbb{E} \left[\sum_{k=t}^{\infty} \gamma^{k-t} R_{k+1}\right] \tag{1}$$<br>为expected discounted returns，表示从$t$时刻开始的expected discounted return；</p><h4 id="状态值函数">状态值函数</h4><p>state value function的定义是从$t$时刻的$s_t$开始的累计期望折扣奖励：<br>$$V^{\pi} (s_t) = \mathbb{E}_{a_{t}, s_{t+1},\cdots\sim \pi}\left[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \right] \tag{2}$$<br>或者有时候也定义成从$t=0$开始的expected return：<br>$$V^{\pi} (s_0) = \mathbb{E}_{\pi}\left[G_0|S_0=s_0;\pi\right]=\mathbb{E}_{\pi}\left[\sum_{t=0}^{\infty} \gamma^t R_{t+1}|S_0=s_0;\pi \right] \tag{3}$$</p><h4 id="动作值函数">动作值函数</h4><p>action value function定义为从$t$时刻的$s_t, a_t$开始的累计期望折扣奖励：<br>$$Q^{\pi} (s_t, a_t) = \mathbb{E}_{s_{t+1}, a_{t+1},\cdots\sim\pi}\left[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \right] \tag{4}$$<br>或者有时候也定义为从$t=0$开始的return的期望：<br>$$Q^{\pi} (s_0, a_0) = \mathbb{E}_{\pi}\left[G_0|S_0=s_0,A_0=a_0;\pi\right]=\mathbb{E}_{\pi}\left[\sum_{t=0}^{\infty} \gamma^t R_{t+1}|S_0=s_0,A_0=a_0;\pi \right] \tag{5}$$</p><h4 id="优势函数">优势函数</h4><p>$$A^{\pi} (s,a) = Q^{\pi}(s,a) -V^{\pi} (s) \tag{6}$$<br>其中$a_t\sim \pi(a_t|s_t), s_{t+1}\sim P(s_{t+1}|s_t, a_t)$。$V^{\pi} (s)$可以看成状态$s$下所有$Q(s,a)$的期望，而$A^{\pi} (s,a)$可以看成当前的单个$Q(s,a)$是否要比$Q(s,a)$的期望要好，如果为正，说明这个$Q$比$Q$的期望要好，否则就不好。<br>优势函数的期望是$0$：<br>$$\mathbb{E}_{\pi}\left[A^{\pi}(s,a)\right] = \mathbb{E}_{\pi}\left[Q^{\pi}(s,a) - V^{\pi}(s)\right] = \mathbb{E}_{\pi}\left[Q^{\pi}(s,a)\right] -  \mathbb{E}_{\pi}\left[V^{\pi}(s)\right] = V^{\pi}(s) - V^{\pi}(s) = 0$$</p><h4 id="目标函数">目标函数</h4><p>Agents的目标是找到一个policy，最大化从state $s_0$开始的expected return：$J(\pi)=\mathbb{E}_{\pi} \left[G_0|\pi\right]$，或者写成：<br>$$\eta(\pi)= \mathbb{E}_{s_0, a_0, \cdots\sim \pi}\left[\sum_{t=0}^{\infty} \gamma^t R_{t+1}\right] \tag{7}$$<br>表示$t=0$时policy $\pi$的expected discounted return，其中$s_0\sim\rho_0(s_0)$, $a_t\sim\pi(a_t|s_t)$, $s_{t+1}\sim P(s_{t+1}|s_t,a_t)$。</p><h4 id="station-distribution">station distribution</h4><p>用$p(s_0\rightarrow s,t,\pi)$表示从$s_0$经过$t$个timesteps到$s$的概率，则<a href="https://zhuanlan.zhihu.com/p/60257706" target="_blank" rel="noopener">policy $\pi$下$s$服从的概率分布为</a>：<br>$$\rho^{\pi} (s) = \int_S \sum_{t=0}^{\infty} \gamma^{t} \rho_0(s_0)p(s_0\rightarrow s, t,\pi)ds_0 \tag{8}$$<br>其中$\rho_0(s_0)$是初始状态$s_0$服从的概率分布，当指定$s_0$时，可以看成$\rho_0(s_0) = 1$。$\rho^{\pi} (s)$可以理解为：<br>$$\rho^{\pi} (s) = P(s_0 = s) +\gamma P(s_1=s) + \gamma^2 P(s_2 = s)+\cdots \tag{9}$$<br>表示policy $\pi$下state $s$出现的概率。在每一个timestep $t$处，$s_t=s$都有一定概率发生的，也就是式子$9$。</p><h3 id="average-reward">Average Reward</h3><h4 id="目标函数-v2">目标函数</h4><p>定义average reward $\eta$为在state distribution $\rho^\pi $和policy $\pi_\theta$上的期望：<br>\begin{align*}<br>\eta(\pi) &amp;= \int_S \rho^{\pi} (s) \int_A \pi(s,a) R^{\pi}(s,a)dads\\<br>&amp;= \mathbb{E}_{s\sim \rho^{\pi} , a\sim \pi}\left[R^{\pi}(s,a)\right] \tag{10}\\<br>\end{align*}<br>其中$R(s,a) = \mathbb{E}\left[ r_{t+1}|s_t=s, a_t=a\right]$，是state action pair $(s,a)$的immediate reward的期望值。</p><h4 id="动作值函数-v2">动作值函数</h4><p>根据average reward，给出一种新的state-action value的定义方式：<br>$$Q^{\pi} (s,a) = \sum_{t=0}^{\infty} \mathbb{E}\left[R_t - \eta(\pi)|s_0=s,a_0=a,\pi\right], \forall s\in S, a\in A \tag{11}$$</p><h4 id="状态值函数-v2">状态值函数</h4><p>Value function定义还和原来一样，形式没有变，但是因为$Q$计算方法变了，所以$V$的值也变了：<br>$$V^{\pi} (s) = \mathbb{E}_{\pi(a’;s)}\left[Q^{\pi}(s,a’)\right] \tag{12}$$</p><h4 id="stationary-distribution">stationary distribution</h4><p>对于average reward来说，它的stationary distribution和accumulated reward有一些不同：<br>$$\rho^{\pi}(s) = \lim_{t\rightarrow \infty}Pr\left[s_t=s|s_0, \pi\right] \tag{13}$$<br>表示的是当MDP稳定之后，state $s$出现的概率。</p><h3 id="accumulated-reward和average-reward">Accumulated Reward和Average Reward</h3><p>这两种方式，accumulated reward需要加上折扣因子，而average reward不需要。我们常见的都是accumulated reward这种方式的动作值函数以及状态值函数。</p><h2 id="分类方式">分类方式</h2><h3 id="online-vs-offline">online vs offline</h3><p>online方法中训练数据一直在不断增加，基本上强化学习都是online的，而监督学习是offline的。</p><h3 id="on-policy-vs-off-policy">on-policy vs off-policy</h3><p>behaviour policy是采样的policy。<br>target policy是要evaluation的policy。<br>behaviour policy和target policy是不是相同的，相同的就是on-policy，不同的就是off-policy，带有replay buffer的都是off-policy的方法。</p><h2 id="bootstrap">bootstrap</h2><p>当前value的计算是否基于其他value的估计值。<br>常见的bootstrap算法有DP，TD-gamma<br>MC算法不是bootstrap算法。</p><h2 id="value-based-vs-policy-gradient-vs-actor-critic">value-based vs policy gradient vs actor-critic</h2><h3 id="value-based">value-based</h3><p>values-based方法主要有policy iteration和value iteration。policy iteration又分为policy evaluation和policy improvement。<br>给出一个任务，如果可以使用value-based。随机初始化一个policy，然后可以计算这个policy的value function，这就叫做policy evaluation，然后根据这个value function，可以对policy进行改进，这叫做policy improvement，可以证明policy一定会更好。policy evaluation和policy improvement交替迭代，在线性case下，收敛性是可以证明的，在non-linear情况下，就不一定了。<br>policy iteraion中，policy evaluation每一次都要进行收敛后才进行policy improvemetn，如果policy evalution只进行一次，然后就进行一次policy improvemetn的话，也就是policy evalution的粒度变小后，就是value iteration。</p><h3 id="policy-gradient">policy gradient</h3><p>value-based方法只适用于discrete action space，对于contionous action space的话，就无能为力了。这个时候就有了policy gradient，给出一个state，policy gradient给出一个policy直接计算出相应的action，然后给出一个衡量action好坏的指标，直接对policy的参数求导，最后收敛之后就求解出一个使用与contionous的policy</p><h3 id="actor-critic">actor-critic</h3><p>如果policy gradient的metrics选择使用value function，一般是aciton value function的话，我们把这个value function叫做critic，然后把policy叫做actor。通过value funciton Q对policy的参数求导进行优化。<br>critic跟policy没有关系，而critic指导actor的训练，通过链式法则实现。critic对a求偏导，a对actor的参数求偏导。</p><h2 id="参考文献">参考文献</h2><p>1.<a href="https://stats.stackexchange.com/questions/897/online-vs-offline-learning" target="_blank" rel="noopener">https://stats.stackexchange.com/questions/897/online-vs-offline-learning</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;术语定义&quot;&gt;术语定义&lt;/h2&gt;
&lt;p&gt;更多介绍可以点击查看&lt;a href=&quot;https://mxxhcm.github.io/2018/12/21/reinforcement-learning-an-introduction-%E7%AC%AC3%E7%AB%A0
      
    
    </summary>
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="policy gradient" scheme="http://mxxhcm.github.io/tags/policy-gradient/"/>
    
      <category term="on-policy" scheme="http://mxxhcm.github.io/tags/on-policy/"/>
    
      <category term="online" scheme="http://mxxhcm.github.io/tags/online/"/>
    
      <category term="offline" scheme="http://mxxhcm.github.io/tags/offline/"/>
    
      <category term="off-policy" scheme="http://mxxhcm.github.io/tags/off-policy/"/>
    
      <category term="bootstrap" scheme="http://mxxhcm.github.io/tags/bootstrap/"/>
    
      <category term="model-free" scheme="http://mxxhcm.github.io/tags/model-free/"/>
    
      <category term="model-based" scheme="http://mxxhcm.github.io/tags/model-based/"/>
    
      <category term="value based" scheme="http://mxxhcm.github.io/tags/value-based/"/>
    
      <category term="actor critic" scheme="http://mxxhcm.github.io/tags/actor-critic/"/>
    
      <category term="state space" scheme="http://mxxhcm.github.io/tags/state-space/"/>
    
      <category term="action space" scheme="http://mxxhcm.github.io/tags/action-space/"/>
    
      <category term="reward" scheme="http://mxxhcm.github.io/tags/reward/"/>
    
      <category term="return" scheme="http://mxxhcm.github.io/tags/return/"/>
    
      <category term="advantage" scheme="http://mxxhcm.github.io/tags/advantage/"/>
    
      <category term="state value" scheme="http://mxxhcm.github.io/tags/state-value/"/>
    
      <category term="action value" scheme="http://mxxhcm.github.io/tags/action-value/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow contrib vs layers vs nn</title>
    <link href="http://mxxhcm.github.io/2019/05/18/tensorflow-nn-layers-contrib/"/>
    <id>http://mxxhcm.github.io/2019/05/18/tensorflow-nn-layers-contrib/</id>
    <published>2019-05-18T07:58:59.000Z</published>
    <updated>2019-07-25T12:39:48.109Z</updated>
    
    <content type="html"><![CDATA[<h2 id="tf-contrib">tf.contrib</h2><p>根据tensorflow官网的说法，tf.contrib模块中包含了易修改的测试代码，</p><blockquote><p>contrib module containing volatile or experimental code.</p></blockquote><p>当其中的某一个模块完成的时候，就会从contrib模块中移除。为了保持对历史版本的兼容性，可能这几个模块会存在同一个函数的不同实现。</p><h2 id="tf-nn-tf-layers和tf-contrib">tf.nn,tf.layers和tf.contrib</h2><p>tf.nn中是low-level的op<br>tf.layers是high-level的op<br>而tf.contrib中的是非正式版本的实现，在后续版本中可能会被弃用。</p><h2 id="tf-nn-conv2d-vs-tf-layers-conv2d">tf.nn.conv2d vs tf.layers.conv2d</h2><h3 id="api">API</h3><h4 id="tf-layer-conv2d">tf.layer.conv2d</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">tf.layers.conv2d(</span><br><span class="line">    inputs, </span><br><span class="line">    filters, </span><br><span class="line">    kernel_size, </span><br><span class="line">    strides=(<span class="number">1</span>, <span class="number">1</span>), </span><br><span class="line">    padding=<span class="string">'valid'</span>, </span><br><span class="line">    data_format=<span class="string">'channels_last'</span>, </span><br><span class="line">    dilation_rate=(<span class="number">1</span>, <span class="number">1</span>), </span><br><span class="line">    activation=<span class="literal">None</span>, </span><br><span class="line">    use_bias=<span class="literal">True</span>, </span><br><span class="line">    kernel_initializer=<span class="literal">None</span>, </span><br><span class="line">    bias_initializer=tf.zeros_initializer(), </span><br><span class="line">    kernel_regularizer=<span class="literal">None</span>, </span><br><span class="line">    bias_regularizer=<span class="literal">None</span>, </span><br><span class="line">    activity_regularizer=<span class="literal">None</span>, </span><br><span class="line">    trainable=<span class="literal">True</span>, </span><br><span class="line">    name=<span class="literal">None</span>, </span><br><span class="line">    reuse=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h4 id="tf-nn-conv2d">tf.nn.conv2d</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.conv2d(</span><br><span class="line">    input, </span><br><span class="line">    filter, </span><br><span class="line">    strides, </span><br><span class="line">    padding, </span><br><span class="line">    use_cudnn_on_gpu=<span class="literal">None</span>, </span><br><span class="line">    data_format=<span class="literal">None</span>, </span><br><span class="line">    name=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="nn-conv2d-vs-layers-conv2d">nn.conv2d vs layers.conv2d</h3><p>tf.nn.conv2d需要手动创建filter的tensor，传入filter的参数[kernel_height, kernel_width, in_channels, num_filters]。<br>tf.layer.conv2d需要传入filter的维度即可。</p><p>对于tf.nn.conv2d，<br>filter:和input的type一样，是一个4D的tensor，shape为[filter_height, filter_width, in_channels, out_channels]<br>对于tf.layers.conv2d，<br>filters:是整数，是需要多少个filters。</p><p>可以使用tf.nn.conv2d来加载一个pretrained model，使用tf.layers.conv2d从头开始训练一个model。</p><h3 id="用法">用法</h3><h4 id="tf-layers-conv2d">tf.layers.conv2d</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Convolution Layer with 32 filters and a kernel size of 5</span></span><br><span class="line">conv1 = tf.layers.conv2d(x, <span class="number">32</span>, <span class="number">5</span>, activation=tf.nn.relu) </span><br><span class="line"><span class="comment"># Max Pooling (down-sampling) with strides of 2 and kernel size of 2</span></span><br><span class="line">conv1 = tf.layers.max_pooling2d(conv1, <span class="number">2</span>, <span class="number">2</span>)</span><br></pre></td></tr></table></figure><h4 id="tf-nn-conv2d-v2">tf.nn.conv2d</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">strides = <span class="number">1</span></span><br><span class="line"><span class="comment"># Weights matrix looks like: [kernel_size(=5), kernel_size(=5), input_channels (=3), filters (= 32)]</span></span><br><span class="line"><span class="comment"># Similarly bias = looks like [filters (=32)]</span></span><br><span class="line">out = tf.nn.conv2d(input, weights, padding=<span class="string">"SAME"</span>, strides = [<span class="number">1</span>, strides, strides, <span class="number">1</span>])</span><br><span class="line">out = tf.nn.bias_add(out, bias)</span><br><span class="line">out = tf.nn.relu(out)</span><br></pre></td></tr></table></figure><h2 id="参考文献">参考文献</h2><p>1.<a href="https://www.tensorflow.org/api_docs/python/tf/contrib" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/contrib</a><br>2.<a href="https://stackoverflow.com/questions/48001759/what-is-right-batch-normalization-function-in-tensorflow" target="_blank" rel="noopener">https://stackoverflow.com/questions/48001759/what-is-right-batch-normalization-function-in-tensorflow</a><br>3.<a href="https://stackoverflow.com/a/48003210" target="_blank" rel="noopener">https://stackoverflow.com/a/48003210</a><br>4.<a href="https://stackoverflow.com/questions/42785026/tf-nn-conv2d-vs-tf-layers-conv2d" target="_blank" rel="noopener">https://stackoverflow.com/questions/42785026/tf-nn-conv2d-vs-tf-layers-conv2d</a><br>5.<a href="https://stackoverflow.com/a/53683545" target="_blank" rel="noopener">https://stackoverflow.com/a/53683545</a><br>6.<a href="https://stackoverflow.com/a/45308609" target="_blank" rel="noopener">https://stackoverflow.com/a/45308609</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;tf-contrib&quot;&gt;tf.contrib&lt;/h2&gt;
&lt;p&gt;根据tensorflow官网的说法，tf.contrib模块中包含了易修改的测试代码，&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;contrib module containing volatile or
      
    
    </summary>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/categories/tensorflow/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow rnn</title>
    <link href="http://mxxhcm.github.io/2019/05/18/tensorflow-rnn/"/>
    <id>http://mxxhcm.github.io/2019/05/18/tensorflow-rnn/</id>
    <published>2019-05-18T07:55:34.000Z</published>
    <updated>2019-05-19T11:40:49.946Z</updated>
    
    <content type="html"><![CDATA[<h2 id="常见cell和函数">常见Cell和函数</h2><ul><li>tf.nn.rnn_cell.BasicRNNCell: 最基本的RNN cell.</li><li>tf.nn.rnn_cell.LSTMCell: LSTM cell</li><li>tf.nn.rnn_cell.LSTMStateTuple: tupled LSTM cell</li><li>tf.nn.rnn_cell.MultiRNNCell: 多层Cell</li><li>tf.nn.rnn_cell.DropoutCellWrapper: 给Cell加上dropout</li><li>tf.nn.dynamic_rnn: 动态rnn</li><li>tf.nn.static_rnn: 静态rnn</li></ul><h2 id="basicrnncell">BasicRNNCell</h2><h3 id="api">API</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">__init__(</span><br><span class="line">    num_units,</span><br><span class="line">    activation=<span class="literal">None</span>,</span><br><span class="line">    reuse=<span class="literal">None</span>,</span><br><span class="line">    name=<span class="literal">None</span>,</span><br><span class="line">    dtype=<span class="literal">None</span>,</span><br><span class="line">    **kwargs</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="示例">示例</h3><p><a href>完整代码地址</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">   myrnn = rnn.BasicRNNCell(rnn_size,activation=tf.nn.relu)</span><br><span class="line">   zero_state = myrnn.zero_state(batch_size, dtype=tf.float32)</span><br><span class="line">   outputs, states = rnn.static_rnn(myrnn, x, initial_state=zero_state, dtype=tf.float32)</span><br><span class="line"><span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure><h3 id="其他">其他</h3><p>TF 2.0将会弃用，等价于tf.keras.layers.SimpleRNNCell()</p><h2 id="lstmcell">LSTMCell</h2><h3 id="api-v2">API</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">__init__(</span><br><span class="line">    num_units, <span class="comment"># 隐藏层的大小</span></span><br><span class="line">    use_peepholes=<span class="literal">False</span>, <span class="comment"># </span></span><br><span class="line">    cell_clip=<span class="literal">None</span>,</span><br><span class="line">    initializer=<span class="literal">None</span>, <span class="comment"># 权重的初始化构造器</span></span><br><span class="line">    num_proj=<span class="literal">None</span>,</span><br><span class="line">    proj_clip=<span class="literal">None</span>,</span><br><span class="line">    num_unit_shards=<span class="literal">None</span>,</span><br><span class="line">    num_proj_shards=<span class="literal">None</span>,</span><br><span class="line">    forget_bias=<span class="number">1.0</span>,</span><br><span class="line">    state_is_tuple=<span class="literal">True</span>, <span class="comment"># c_state和m_state的元组</span></span><br><span class="line">    activation=<span class="literal">None</span>,</span><br><span class="line">    reuse=<span class="literal">None</span>,</span><br><span class="line">    name=<span class="literal">None</span>,</span><br><span class="line">    dtype=<span class="literal">None</span>,</span><br><span class="line">    **kwargs</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="示例-v2">示例</h3><p><a href>完整代码地址</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">lstm = rnn.BasicLSTMCell(lstm_size, forget_bias=<span class="number">1</span>, state_is_tuple=<span class="literal">True</span>)</span><br><span class="line">   zero_state = lstm.zero_state(batch_size, dtype=tf.float32)</span><br><span class="line">   outputs, states = rnn.static_rnn(lstm, x, initial_state=zero_state, dtype=tf.float32)</span><br><span class="line"><span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure><h3 id="其他-v2">其他</h3><p>TF 2.0将会弃用，等价于tf.keras.layers.LSTMCell</p><h2 id="lstmstatetuple">LSTMStateTuple</h2><p>和LSTMCell一样，只不过state用的是tuple。</p><h3 id="其他-v3">其他</h3><p>TF 2.0将会弃用，等价于tf.keras.layers.LSTMCell</p><h2 id="multirnncell">MultiRNNCell</h2><p>这个类可以实现多层RNN。</p><h3 id="api-v3">API</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">__init__(</span><br><span class="line">    cells,</span><br><span class="line">    state_is_tuple=<span class="literal">True</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="示例-v3">示例</h3><h4 id="代码1">代码1</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">num_units = [<span class="number">128</span>, <span class="number">64</span>]</span><br><span class="line">cells = [BasicLSTMCell(num_units=n) <span class="keyword">for</span> n <span class="keyword">in</span> num_units]</span><br><span class="line">stacked_rnn_cell = MultiRNNCell(cells)</span><br><span class="line">outputs, state = tf.nn.dynamic_rnn(cell=stacked_rnn_cell,</span><br><span class="line">                                   inputs=data,</span><br><span class="line">                                   dtype=tf.float32)</span><br></pre></td></tr></table></figure><h4 id="代码2">代码2</h4><p><a href>完整代码地址</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">   lstm_cell = rnn.BasicLSTMCell(lstm_size, forget_bias=<span class="number">1</span>, state_is_tuple=<span class="literal">True</span>)</span><br><span class="line">   cell = rnn.MultiRNNCell([lstm_cell]*layers, state_is_tuple=<span class="literal">True</span>)</span><br><span class="line">   state = cell.zero_state(batch_size, dtype=tf.float32)</span><br><span class="line">   outputs = []</span><br><span class="line">   <span class="keyword">with</span> tf.variable_scope(<span class="string">"Multi_Layer_RNN"</span>, reuse=reuse):</span><br><span class="line">       <span class="keyword">for</span> time_step <span class="keyword">in</span> range(time_steps):</span><br><span class="line">           <span class="keyword">if</span> time_step &gt; <span class="number">0</span>:</span><br><span class="line">               tf.get_variable_scope().reuse_variables()</span><br><span class="line">           </span><br><span class="line">           cell_outputs, state = cell(x[time_step], state)</span><br><span class="line">           outputs.append(cell_outputs)</span><br><span class="line"><span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure><h3 id="其他-v4">其他</h3><p>TF 2.0将会弃用，等价于tf.keras.layers.StackedRNNCells</p><h2 id="dropoutcellwrapper">DropoutCellWrapper</h2><h3 id="api-v4">API</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">__init__(</span><br><span class="line">    cell, <span class="comment"># </span></span><br><span class="line">    input_keep_prob=<span class="number">1.0</span>,</span><br><span class="line">    output_keep_prob=<span class="number">1.0</span>,</span><br><span class="line">    state_keep_prob=<span class="number">1.0</span>,</span><br><span class="line">    variational_recurrent=<span class="literal">False</span>,</span><br><span class="line">    input_size=<span class="literal">None</span>,</span><br><span class="line">    dtype=<span class="literal">None</span>,</span><br><span class="line">    seed=<span class="literal">None</span>,</span><br><span class="line">    dropout_state_filter_visitor=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="示例-v4">示例</h3><p><a href>完整代码地址</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">   lstm_cell = rnn.BasicLSTMCell(lstm_size, forget_bias=<span class="number">1</span>, state_is_tuple=<span class="literal">True</span>)</span><br><span class="line">   lstm_cell = rnn.DropoutWrapper(lstm_cell, output_keep_prob=<span class="number">0.9</span>)</span><br><span class="line">   cell = rnn.MultiRNNCell([lstm_cell]*layers, state_is_tuple=<span class="literal">True</span>)</span><br><span class="line">   state = cell.zero_state(batch_size, dtype=tf.float32)</span><br><span class="line">   outputs = []</span><br><span class="line">   <span class="keyword">with</span> tf.variable_scope(<span class="string">"Multi_Layer_RNN"</span>):</span><br><span class="line">       <span class="keyword">for</span> time_step <span class="keyword">in</span> range(time_steps):</span><br><span class="line">           <span class="keyword">if</span> time_step &gt; <span class="number">0</span>:</span><br><span class="line">               tf.get_variable_scope().reuse_variables()</span><br><span class="line">           cell_outputs, state = cell(x[time_step], state)</span><br><span class="line">           outputs.append(cell_outputs)</span><br><span class="line"><span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure><h3 id="其他-v5">其他</h3><h2 id="static-rnn">static_rnn</h2><h3 id="api-v5">API</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.static_rnn(</span><br><span class="line">    cell, <span class="comment"># RNNCell的具体对象</span></span><br><span class="line">    inputs, <span class="comment"># 输入，长度为T的输入列表，列表中每一个Tensor的shape都是[batch_size, input_size]</span></span><br><span class="line">    initial_state=<span class="literal">None</span>, <span class="comment"># rnn的初始状态，如果cell.state_size是整数，它的shape需要是[batch_size, cell.state_size]，如果cell.state_size是元组，那么终究会是一个tensors的元组，[batch_size, s] for s in cell.state_size</span></span><br><span class="line">    dtype=<span class="literal">None</span>, <span class="comment"># </span></span><br><span class="line">    sequence_length=<span class="literal">None</span>, <span class="comment"># </span></span><br><span class="line">    scope=<span class="literal">None</span></span><br><span class="line">)</span><br><span class="line"><span class="comment"># 最简单形式的RNN，就是该API的参数都是用默认值，给定cell和inputs，相当于做了以下操作：</span></span><br><span class="line"><span class="comment">#    state = cell.zero_state(...)</span></span><br><span class="line"><span class="comment">#    outputs = []</span></span><br><span class="line"><span class="comment">#    for input_ in inputs:</span></span><br><span class="line"><span class="comment">#      output, state = cell(input_, state)</span></span><br><span class="line"><span class="comment">#      outputs.append(output)</span></span><br><span class="line"><span class="comment">#    return (outputs, state)</span></span><br></pre></td></tr></table></figure><h3 id="示例-v5">示例</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">myrnn = tf.nn.rnn_cell.BasicRNNCell(rnn_size,activation=tf.nn.relu)</span><br><span class="line">   zero_state = myrnn.zero_state(batch_size, dtype=tf.float32)</span><br><span class="line">   outputs, states = tf.nn.static_rnn(myrnn, x, initial_state=zero_state, dtype=tf.float32)</span><br></pre></td></tr></table></figure><h2 id="dynamic-rnn">dynamic rnn</h2><h3 id="api-v6">API</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.dynamic_rnn(</span><br><span class="line">    cell, <span class="comment"># RNNCell的具体对象</span></span><br><span class="line">    inputs, <span class="comment"># RNN的输入,time_major = False, [batch_size, max_time, ...],time_major=True, [max_time, batch_size, ...]</span></span><br><span class="line">    sequence_length=<span class="literal">None</span>, <span class="comment"># </span></span><br><span class="line">    initial_state=<span class="literal">None</span>, <span class="comment"># rnn的初始状态，如果cell.state_size是整数，它的shape需要是[batch_size, cell.state_size]，如果cell.state_size是元组，那么就会是一个tensors的元组，[batch_size, s] for s in cell.state_size</span></span><br><span class="line">    dtype=<span class="literal">None</span>,</span><br><span class="line">    parallel_iterations=<span class="literal">None</span>,</span><br><span class="line">    swap_memory=<span class="literal">False</span>, <span class="comment">#</span></span><br><span class="line">    time_major=<span class="literal">False</span>, <span class="comment"># 如果为True,如果为False，对应不同的inputs </span></span><br><span class="line">    scope=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="示例-v6">示例</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 例子1.创建一个BasicRNNCell</span></span><br><span class="line">rnn_cell = tf.nn.rnn_cell.BasicRNNCell(hidden_size)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义初始化状态</span></span><br><span class="line">initial_state = rnn_cell.zero_state(batch_size, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 'outputs' shape [batch_size, max_time, cell_state_size]</span></span><br><span class="line"><span class="comment"># 'state' shape [batch_size, cell_state_size]</span></span><br><span class="line">outputs, state = tf.nn.dynamic_rnn(rnn_cell, input_data,</span><br><span class="line">                                   initial_state=initial_state,</span><br><span class="line">                                   dtype=tf.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 例子2.创建两个LSTMCells</span></span><br><span class="line">rnn_layers = [tf.nn.rnn_cell.LSTMCell(size) <span class="keyword">for</span> size <span class="keyword">in</span> [<span class="number">128</span>, <span class="number">256</span>]]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个多层RNNCelss。</span></span><br><span class="line">multi_rnn_cell = tf.nn.rnn_cell.MultiRNNCell(rnn_layers)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 'outputs' is a tensor of shape [batch_size, max_time, 256]</span></span><br><span class="line"><span class="comment"># 'state' is a N-tuple where N is the number of LSTMCells containing a</span></span><br><span class="line"><span class="comment"># tf.contrib.rnn.LSTMStateTuple for each cell</span></span><br><span class="line">outputs, state = tf.nn.dynamic_rnn(cell=multi_rnn_cell,</span><br><span class="line">                                   inputs=data,</span><br><span class="line">                                   dtype=tf.float32)</span><br></pre></td></tr></table></figure><h2 id="static-rnn-vs-dynamic-rnn">static_rnn vs dynamic_rnn</h2><h3 id="tf-keras-layers-rnn-cell">tf.keras.layers.RNN(cell)</h3><p>在tensorflow 2.0中，上述两个API都会被弃用，使用新的keras.layers.RNN(cell)</p><h2 id="tf-nn-rnn-cell">tf.nn.rnn_cell</h2><p>该模块提供了许多RNN cell类和rnn函数。</p><h3 id="类">类</h3><ul><li>class BasicRNNCell: 最基本的RNN cell.</li><li>class BasicLSTMCell: 弃用了，使用tf.nn.rnn_cell.LSTMCell代替，就是下面那个</li><li>class LSTMCell: LSTM cell</li><li>class LSTMStateTuple: tupled LSTM cell</li><li>class GRUCell: GRU cell (引用文献 <a href="http://arxiv.org/abs/1406.1078" target="_blank" rel="noopener">http://arxiv.org/abs/1406.1078</a>).</li><li>class RNNCell: 表示一个RNN cell的抽象对象</li><li>class MultiRNNCell: 由很多个简单cells顺序组合成的RNN cell</li><li>class DeviceWrapper: 保证一个RNNCell在一个特定的device运行的op.</li><li>class DropoutWrapper: 添加droput到给定cell的的inputs和outputs的op.</li><li>class ResidualWrapper: 确保cell的输入被添加到输出的RNNCell warpper。</li></ul><h3 id="函数">函数</h3><ul><li>static_rnn(…) # 未来将被弃用，和tf.contrib.rnn.static_rnn是一样的。</li><li>dynamic_rnn(…) # 未来将被弃用</li><li>static_bidirectional_rnn(…) # 未来将被弃用</li><li>bidirectional_dynamic_rnn(…) # 未来将被弃用</li><li>raw_rnn(…)</li></ul><h2 id="tf-contrib-rnn">tf.contrib.rnn</h2><p>该模块提供了RNN和Attention RNN的类和函数op。</p><h3 id="类-v2">类</h3><ul><li>class RNNCell: # 抽象类，所有Cell都要继承该类。所有的Warpper都要直接继承该Cell。</li><li>class LayerRNNCell: # 所有的下列定义的Cell都要使用继承该Cell，该Cell继承RNNCell，所以所有下列Cell都间接继承RNNCell。</li><li>class BasicRNNCell:</li><li>class BasicLSTMCell: # 将被弃用，使用下面的LSTMCell。</li><li>class LSTMCell:</li><li>class LSTMStateTuple:</li><li>class GRUCell:</li><li>class MultiRNNCell:</li><li>class ConvLSTMCell:</li><li>class GLSTMCell:</li><li>class Conv1DLSTMCell:</li><li>class Conv2DLSTMCell:</li><li>class Conv3DLSTMCell:</li><li>class BidirectionalGridLSTMCell:</li><li>class AttentionCellWrapper:</li><li>class CompiledWrapper:</li><li>class CoupledInputForgetGateLSTMCell:</li><li>class DeviceWrapper:</li><li>class DropoutWrapper:</li><li>class EmbeddingWrapper:</li><li>class FusedRNNCell:</li><li>class FusedRNNCellAdaptor:</li><li>class GRUBlockCell:</li><li>class GRUBlockCellV2:</li><li>class GridLSTMCell:</li><li>class HighwayWrapper:</li><li>class IndRNNCell:</li><li>class IndyGRUCell:</li><li>class IndyLSTMCell:</li><li>class InputProjectionWrapper:</li><li>class IntersectionRNNCell:</li><li>class LSTMBlockCell:</li><li>class LSTMBlockFusedCell:</li><li>class LSTMBlockWrapper:</li><li>class LayerNormBasicLSTMCell:</li><li>class NASCell:</li><li>class OutputProjectionWrapper:</li><li>class PhasedLSTMCell:</li><li>class ResidualWrapper:</li><li>class SRUCell:</li><li>class TimeFreqLSTMCell:</li><li>class TimeReversedFusedRNN:</li><li>class UGRNNCell:</li></ul><h3 id="函数-v2">函数</h3><ul><li>static_rnn(…) # 将被弃用，和tf.nn.static_rnn是一样的</li><li>static_bidirectional_rnn(…) # 将被弃用</li><li>best_effort_input_batch_size(…)</li><li>stack_bidirectional_dynamic_rnn(…)</li><li>stack_bidirectional_rnn(…)</li><li>static_state_saving_rnn(…)</li><li>transpose_batch_time(…)</li></ul><h2 id="tf-contrib-rnn-vs-tf-nn-rnn-cell">tf.contrib.rnn vs tf.nn.rnn_cell</h2><p>事实上，这两个模块中都定义了许多RNN cell，contrib定义的是测试性的代码，而nn.rnn_cell是contrib中经过测试后的代码。<br>contrib中的代码会经常修改，而nn中的代码比较稳定。<br>contrib中的cell类型比较多，而nn中的比较少。<br>contrib和nn中有重复的cell，基本上nn中有的contrib中都有。</p><h2 id="参考文献">参考文献</h2><p>1.<a href="https://www.tensorflow.org/api_docs/python/tf/nn/rnn_cell/RNNCell" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/nn/rnn_cell/RNNCell</a><br>2.<a href="https://www.tensorflow.org/api_docs/python/tf/nn/rnn_cell/BasicRNNCell" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/nn/rnn_cell/BasicRNNCell</a><br>3.<a href="https://www.tensorflow.org/api_docs/python/tf/nn/rnn_cell/LSTMCell" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/nn/rnn_cell/LSTMCell</a><br>4.<a href="https://www.tensorflow.org/api_docs/python/tf/nn/rnn_cell/MultiRNNCell" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/nn/rnn_cell/MultiRNNCell</a><br>5.<a href="https://www.tensorflow.org/api_docs/python/tf/nn/rnn_cell/LSTMStateTuple" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/nn/rnn_cell/LSTMStateTuple</a><br>6.<a href="https://www.tensorflow.org/api_docs/python/tf/nn/rnn_cell/DropoutWrapper" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/nn/rnn_cell/DropoutWrapper</a><br>7.<a href="https://www.tensorflow.org/api_docs/python/tf/nn/static_rnn" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/nn/static_rnn</a><br>8.<a href="https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn</a><br>9.<a href="https://www.tensorflow.org/api_docs/python/tf/contrib/rnn" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/contrib/rnn</a><br>10.<a href="https://www.tensorflow.org/api_docs/python/tf/nn/rnn_cell" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/nn/rnn_cell</a><br>11.<a href="https://www.cnblogs.com/wuzhitj/p/6297992.html" target="_blank" rel="noopener">https://www.cnblogs.com/wuzhitj/p/6297992.html</a><br>12.<a href="https://stackoverflow.com/questions/48001759/what-is-right-batch-normalization-function-in-tensorflow" target="_blank" rel="noopener">https://stackoverflow.com/questions/48001759/what-is-right-batch-normalization-function-in-tensorflow</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;常见cell和函数&quot;&gt;常见Cell和函数&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;tf.nn.rnn_cell.BasicRNNCell: 最基本的RNN cell.&lt;/li&gt;
&lt;li&gt;tf.nn.rnn_cell.LSTMCell: LSTM cell&lt;/li&gt;
&lt;li&gt;t
      
    
    </summary>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/categories/tensorflow/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow layers</title>
    <link href="http://mxxhcm.github.io/2019/05/18/tensorflow-layers-module/"/>
    <id>http://mxxhcm.github.io/2019/05/18/tensorflow-layers-module/</id>
    <published>2019-05-18T07:37:50.000Z</published>
    <updated>2019-05-19T08:43:15.232Z</updated>
    
    <content type="html"><![CDATA[<h2 id="tf-layers">tf.layers</h2><p>这个模块定义在tf.contrib.layers中。主要是构建神经网络，正则化和summaries等op。它包括1个模块，19个类，以及一系列函数。</p><h2 id="模块">模块</h2><h3 id="experimental-module">experimental module</h3><p>tf.layers.experimental的公开的API</p><h2 id="类">类</h2><h3 id="class-conv2d">class Conv2D</h3><p>二维卷积类。</p><h4 id="api">API</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">__init__(</span><br><span class="line">    filters, <span class="comment"># 卷积核的数量</span></span><br><span class="line">    kernel_size, <span class="comment"># 卷积核的大小</span></span><br><span class="line">    strides=(<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">    padding=<span class="string">'valid'</span>,</span><br><span class="line">    data_format=<span class="string">'channels_last'</span>, <span class="comment"># string, "channels_last", "channels_first"</span></span><br><span class="line">    dilation_rate=(<span class="number">1</span>, <span class="number">1</span>), <span class="comment">#</span></span><br><span class="line">    activation=<span class="literal">None</span>, <span class="comment"># 激活函数</span></span><br><span class="line">    use_bias=<span class="literal">True</span>,</span><br><span class="line">    kernel_initializer=<span class="literal">None</span>, <span class="comment"># 卷积核的构造器</span></span><br><span class="line">    bias_initializer=tf.zeros_initializer(), <span class="comment"># bias的构造器</span></span><br><span class="line">    kernel_regularizer=<span class="literal">None</span>, <span class="comment">#  卷积核的正则化</span></span><br><span class="line">    bias_regularizer=<span class="literal">None</span>,</span><br><span class="line">    activity_regularizer=<span class="literal">None</span>,</span><br><span class="line">    kernel_constraint=<span class="literal">None</span>,</span><br><span class="line">    bias_constraint=<span class="literal">None</span>,</span><br><span class="line">    trainable=<span class="literal">True</span>, <span class="comment"># 如果为True的话，将变量添加到TRANABLE_VARIABELS collection中</span></span><br><span class="line">    name=<span class="literal">None</span>,</span><br><span class="line">    **kwargs</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h4 id="示例">示例</h4><h4 id="其他">其他</h4><h3 id="所有类">所有类</h3><ul><li>class AveragePooling1D</li><li>class AveragePooling2D</li><li>class AveragePooling3D</li><li>class BatchNormalization</li><li>class Conv1D</li><li>class Conv2D</li><li>class Conv2DTranspose</li><li>class Conv3D</li><li>class Conv3DTranspose</li><li>class Dense</li><li>class Dropout</li><li>class Flatten</li><li>class InputSpec</li><li>class Layer</li><li>class MaxPooling1D</li><li>class MaxPooling2D</li><li>class MaxPooling3D</li><li>class SeparableConv1D</li><li>class SeparableConv2D</li></ul><h2 id="函数">函数</h2><h3 id="conv2d">conv2d</h3><h4 id="api-v2">API</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">tf.layers.conv2d(</span><br><span class="line">    inputs, <span class="comment"># 输入</span></span><br><span class="line">    filters, <span class="comment">#  一个整数,输出的维度，就是有几个卷积核</span></span><br><span class="line">    kernel_size,</span><br><span class="line">    strides=(<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">    padding=<span class="string">'valid'</span>,</span><br><span class="line">    data_format=<span class="string">'channels_last'</span>,</span><br><span class="line">    dilation_rate=(<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">    activation=<span class="literal">None</span>,</span><br><span class="line">    use_bias=<span class="literal">True</span>,</span><br><span class="line">    kernel_initializer=<span class="literal">None</span>,</span><br><span class="line">    bias_initializer=tf.zeros_initializer(),</span><br><span class="line">    kernel_regularizer=<span class="literal">None</span>,</span><br><span class="line">    bias_regularizer=<span class="literal">None</span>,</span><br><span class="line">    activity_regularizer=<span class="literal">None</span>,</span><br><span class="line">    kernel_constraint=<span class="literal">None</span>,</span><br><span class="line">    bias_constraint=<span class="literal">None</span>,</span><br><span class="line">    trainable=<span class="literal">True</span>,</span><br><span class="line">    name=<span class="literal">None</span>,</span><br><span class="line">    reuse=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h4 id="示例-v2">示例</h4><h4 id="其他-v2">其他</h4><h3 id="所有函数">所有函数</h3><p>需要注意的是，下列所有函数在以后版本都将被弃用。</p><ul><li>average_pooling1d(…)</li><li>average_pooling2d(…)</li><li>average_pooling3d(…)</li><li>batch_normalization(…)</li><li>conv1d(…)</li><li>conv2d(…)</li><li>conv2d_transpose(…)</li><li>conv3d(…)</li><li>conv3d_transpose(…)</li><li>dense(…)</li><li>dropout(…)</li><li>flatten(…)</li><li>max_pooling1d(…)</li><li>max_pooling2d(…)</li><li>max_pooling3d(…)</li><li>separable_conv1d(…)</li><li>separable_conv2d(…)</li></ul><h2 id="tf-layers-conv2d-vs-tf-layers-conv2d">tf.layers.conv2d vs tf.layers.Conv2d</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">tf.layers.Conv2d.__init__(</span><br><span class="line">    filters,</span><br><span class="line">    kernel_size,</span><br><span class="line">    strides=(<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">    padding=<span class="string">'valid'</span>,</span><br><span class="line">    data_format=<span class="string">'channels_last'</span>,</span><br><span class="line">    dilation_rate=(<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">    activation=<span class="literal">None</span>,</span><br><span class="line">    use_bias=<span class="literal">True</span>,</span><br><span class="line">    kernel_initializer=<span class="literal">None</span>,</span><br><span class="line">    bias_initializer=tf.zeros_initializer(),</span><br><span class="line">    kernel_regularizer=<span class="literal">None</span>,</span><br><span class="line">    bias_regularizer=<span class="literal">None</span>,</span><br><span class="line">    activity_regularizer=<span class="literal">None</span>,</span><br><span class="line">    kernel_constraint=<span class="literal">None</span>,</span><br><span class="line">    bias_constraint=<span class="literal">None</span>,</span><br><span class="line">    trainable=<span class="literal">True</span>,</span><br><span class="line">    name=<span class="literal">None</span>,</span><br><span class="line">    **kwargs</span><br><span class="line">)</span><br><span class="line">tf.layers.conv2d(</span><br><span class="line">    inputs,</span><br><span class="line">    filters,</span><br><span class="line">    kernel_size,</span><br><span class="line">    strides=(<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">    padding=<span class="string">'valid'</span>,</span><br><span class="line">    data_format=<span class="string">'channels_last'</span>,</span><br><span class="line">    dilation_rate=(<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">    activation=<span class="literal">None</span>,</span><br><span class="line">    use_bias=<span class="literal">True</span>,</span><br><span class="line">    kernel_initializer=<span class="literal">None</span>,</span><br><span class="line">    bias_initializer=tf.zeros_initializer(),</span><br><span class="line">    kernel_regularizer=<span class="literal">None</span>,</span><br><span class="line">    bias_regularizer=<span class="literal">None</span>,</span><br><span class="line">    activity_regularizer=<span class="literal">None</span>,</span><br><span class="line">    kernel_constraint=<span class="literal">None</span>,</span><br><span class="line">    bias_constraint=<span class="literal">None</span>,</span><br><span class="line">    trainable=<span class="literal">True</span>,</span><br><span class="line">    name=<span class="literal">None</span>,</span><br><span class="line">    reuse=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>conv2d是函数；Conv2d是类。<br>conv2d运行的时候需要传入卷积核参数，输入；Conv2d在构造的时候需要实例化卷积核参数，实例化后，可以使用不用的输入得到不同的输出。<br>调用conv2d就相当于调用Conv2d对象的apply(inputs)函数。</p><h2 id="参考文献">参考文献</h2><p>1.<a href="https://www.tensorflow.org/api_docs/python/tf/layers" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/layers</a><br>4.<a href="https://www.tensorflow.org/api_docs/python/tf/layers/Conv2D" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/layers/Conv2D</a><br>5.<a href="https://www.tensorflow.org/api_docs/python/tf/layers/conv2d" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/layers/conv2d</a><br>6.<a href="https://stackoverflow.com/questions/52011509/what-is-difference-between-tf-layers-conv2d-and-tf-layers-conv2d/52035621" target="_blank" rel="noopener">https://stackoverflow.com/questions/52011509/what-is-difference-between-tf-layers-conv2d-and-tf-layers-conv2d/52035621</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;tf-layers&quot;&gt;tf.layers&lt;/h2&gt;
&lt;p&gt;这个模块定义在tf.contrib.layers中。主要是构建神经网络，正则化和summaries等op。它包括1个模块，19个类，以及一系列函数。&lt;/p&gt;
&lt;h2 id=&quot;模块&quot;&gt;模块&lt;/h2&gt;
&lt;h3 
      
    
    </summary>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/categories/tensorflow/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow nn module</title>
    <link href="http://mxxhcm.github.io/2019/05/18/tensorflow-nn-module/"/>
    <id>http://mxxhcm.github.io/2019/05/18/tensorflow-nn-module/</id>
    <published>2019-05-18T07:25:34.000Z</published>
    <updated>2019-05-19T02:02:44.284Z</updated>
    
    <content type="html"><![CDATA[<h2 id="tf-nn">tf.nn</h2><p>提供神经网络op。包含构建RNN cell的rnn_cell模块和一些函数。</p><h2 id="tf-nn-rnn-cell">tf.nn.rnn_cell</h2><p>rnn_cell 用于构建RNN cells<br>包括以下几个类：</p><ul><li>class BasicLSTMCell: 弃用了，使用tf.nn.rnn_cell.LSTMCell代替。</li><li>class BasicRNNCell: 最基本的RNN cell.</li><li>class DeviceWrapper: 保证一个RNNCell在一个特定的device运行的op.</li><li>class DropoutWrapper: 添加droput到给定cell的的inputs和outputs的op.</li><li>class GRUCell: GRU cell (引用文献 <a href="http://arxiv.org/abs/1406.1078" target="_blank" rel="noopener">http://arxiv.org/abs/1406.1078</a>).</li><li>class LSTMCell: LSTM cell</li><li>class LSTMStateTuple: tupled LSTM cell</li><li>class MultiRNNCell: 由很多个简单cells顺序组合成的RNN cell</li><li>class RNNCell: 表示一个RNN cell的抽象对象</li><li>class ResidualWrapper: 确保cell的输入被添加到输出的RNNCell warpper。</li></ul><h2 id="函数">函数</h2><h3 id="conv2d">conv2d(…)</h3><p>给定一个4d输入和filter，计算2d卷积。</p><h4 id="api">API</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.conv2d(</span><br><span class="line">    input, <span class="comment"># 输入，[batch, in_height, in_width, in_channels]</span></span><br><span class="line">    filter, <span class="comment"># 4d tensor, [filter_height, filter_width, in_channels, out_channles]</span></span><br><span class="line">    strides, <span class="comment"># 长度为4的1d tensor。</span></span><br><span class="line">    padding, <span class="comment"># string, 可选"SAME"或者"VALID"</span></span><br><span class="line">    use_cudnn_on_gpu=<span class="literal">True</span>, <span class="comment">#</span></span><br><span class="line">    data_format=<span class="string">'NHWC'</span>, <span class="comment">#</span></span><br><span class="line">    dilations=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], <span class="comment">#</span></span><br><span class="line">    name=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h4 id="示例">示例</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv2d</span><span class="params">(inputs, output_dim, kernel_size, stride, initializer, activation_fn,</span></span></span><br><span class="line"><span class="function"><span class="params">           padding=<span class="string">'VALID'</span>, data_format=<span class="string">'NHWC'</span>, name=<span class="string">"conv2d"</span>, reuse=False)</span>:</span></span><br><span class="line">    kernel_shape = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(name, reuse=reuse):</span><br><span class="line">        <span class="keyword">if</span> data_format == <span class="string">'NCHW'</span>:</span><br><span class="line">            stride = [<span class="number">1</span>, <span class="number">1</span>, stride[<span class="number">0</span>], stride[<span class="number">1</span>]]</span><br><span class="line">            kernel_shape = [kernel_size[<span class="number">0</span>], kernel_size[<span class="number">1</span>], inputs.get_shape()[<span class="number">1</span>], output_dim]</span><br><span class="line">        <span class="keyword">elif</span> data_format == <span class="string">'NHWC'</span>:</span><br><span class="line">            stride = [<span class="number">1</span>, stride[<span class="number">0</span>], stride[<span class="number">1</span>], <span class="number">1</span>]</span><br><span class="line">            kernel_shape = [kernel_size[<span class="number">0</span>], kernel_size[<span class="number">1</span>], inputs.get_shape()[<span class="number">-1</span>], output_dim ]</span><br><span class="line"></span><br><span class="line">        w = tf.get_variable(<span class="string">'w'</span>, kernel_shape, tf.float32, initializer=initializer)</span><br><span class="line">        conv = tf.nn.conv2d(inputs, w, stride, padding, data_format=data_format)</span><br><span class="line"></span><br><span class="line">        b = tf.get_variable(<span class="string">'b'</span>, [output_dim], tf.float32, initializer=tf.constant_initializer(<span class="number">0.0</span>))</span><br><span class="line">        out = tf.nn.bias_add(conv, b, data_format=data_format)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> activation_fn <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        out = activation_fn(out)</span><br><span class="line">    <span class="keyword">return</span> out, w, b</span><br></pre></td></tr></table></figure><h3 id="convolution">convolution</h3><h4 id="api-v2">API</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.convolution(</span><br><span class="line">    input, <span class="comment"># 输入</span></span><br><span class="line">    filter, <span class="comment"># 卷积核</span></span><br><span class="line">    padding, <span class="comment"># string, 可选"SAME"或者"VALID"</span></span><br><span class="line">    strides=<span class="literal">None</span>, <span class="comment"># 步长</span></span><br><span class="line">    dilation_rate=<span class="literal">None</span>,</span><br><span class="line">    name=<span class="literal">None</span>,</span><br><span class="line">    data_format=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h4 id="和tf-nn-conv2d对比">和tf.nn.conv2d对比</h4><p>tf.nn.conv2d是2d卷积<br>tf.nn.convolution是nd卷积</p><h3 id="conv2d-transpose">conv2d_transpose</h3><p>反卷积</p><h4 id="api-v3">API</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.conv2d_transpose(</span><br><span class="line">    value, <span class="comment"># 输入，4d tensor，[batch, in_channels, height, width] for NCHW,或者[batch,height, width, in_channels] for NHWC</span></span><br><span class="line">    filter, <span class="comment"># 4d卷积核，shape是[height, width, output_channels, in_channels]</span></span><br><span class="line">    output_shape, <span class="comment"># 表示反卷积输出的shape一维tensor</span></span><br><span class="line">    strides, <span class="comment"># 步长</span></span><br><span class="line">    padding=<span class="string">'SAME'</span>,</span><br><span class="line">    data_format=<span class="string">'NHWC'</span>,</span><br><span class="line">    name=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h4 id="示例-v2">示例</h4><h3 id="max-pool">max_pool</h3><p>实现max pooling</p><h4 id="api-v4">API</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.max_pool(</span><br><span class="line">    value, <span class="comment"># 输入，4d tensor</span></span><br><span class="line">    ksize, <span class="comment"># 4个整数的list或者tuple，max pooling的kernel size</span></span><br><span class="line">    strides, <span class="comment"># 4个整数的list或者tuple</span></span><br><span class="line">    padding, <span class="comment"># string, 可选"VALID"或者"VALID"</span></span><br><span class="line">    data_format=<span class="string">'NHWC'</span>, <span class="comment"># string,可选"NHWC", "NCHW", NCHW_VECT_C"</span></span><br><span class="line">    name=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="几个常用的函数">几个常用的函数</h3><ul><li>bias_add(…)</li><li>raw_rnn(…)</li><li>static_rnn(…) # 未来将被弃用</li><li>dynamic_rnn(…) # 未来将被弃用</li><li>static_bidirectional_rnn(…) # 未来将被弃用</li><li>bidirectional_dynamic_rnn(…) # 未来将被弃用</li><li>dropout(…)</li><li>leaky_relu(…)</li><li>l2_loss(…)</li><li>log_softmax(…) # 参数弃用</li><li>softmax(…) # 参数弃用</li><li>softmax_cross_entropy_with_logits(…)# 未来将被弃用</li><li>softmax_cross_entropy_with_logits_v2(…) # 参数弃用</li><li>sparse_softmax_cross_entropy_with_logits(…)</li></ul><h4 id="全部函数">全部函数</h4><ul><li>all_candidate_sampler(…)</li><li>atrous_conv2d(…)</li><li>atrous_conv2d_transpose(…)</li><li>avg_pool(…)</li><li>avg_pool3d(…)</li><li>batch_norm_with_global_normalization(…)</li><li>batch_normalization(…)</li><li>bias_add(…)</li><li>bidirectional_dynamic_rnn(…)</li><li>collapse_repeated(…)</li><li>compute_accidental_hits(…)</li><li>conv1d(…)</li><li>conv2d(…)</li><li>conv2d_backprop_filter(…)</li><li>conv2d_backprop_input(…)</li><li>conv2d_transpose(…)</li><li>conv3d(…)</li><li>conv3d_backprop_filter(…)</li><li>conv3d_backprop_filter_v2(…)</li><li>conv3d_transpose(…)</li><li>convolution(…) - crelu(…)</li><li>ctc_beam_search_decoder(…)</li><li>ctc_beam_search_decoder_v2(…)</li><li>ctc_greedy_decoder(…)</li><li>ctc_loss(…)</li><li>ctc_loss_v2(…)</li><li>ctc_unique_labels(…)</li><li>depth_to_space(…)</li><li>depthwise_conv2d(…)</li><li>depthwise_conv2d_backprop_filter(…)</li><li>depthwise_conv2d_backprop_input(…)</li><li>depthwise_conv2d_native(…)</li><li>depthwise_conv2d_native_backprop_filter(…)</li><li>depthwise_conv2d_native_backprop_input(…)</li><li>dilation2d(…)</li><li>dropout(…)</li><li>dynamic_rnn(…)</li><li>elu(…)</li><li>embedding_lookup(…)</li><li>embedding_lookup_sparse(…)</li><li>erosion2d(…)</li><li>fixed_unigram_candidate_sampler(…)</li><li>fractional_avg_pool(…)</li><li>fractional_max_pool(…)</li><li>fused_batch_norm(…)</li><li>in_top_k(…)</li><li>l2_loss(…)</li><li>l2_normalize(…)</li><li>leaky_relu(…)</li><li>learned_unigram_candidate_sampler(…)</li><li>local_response_normalization(…)</li><li>log_poisson_loss(…)</li><li>log_softmax(…)</li><li>log_uniform_candidate_sampler(…)</li><li>lrn(…)</li><li>max_pool(…)</li><li>max_pool3d(…)</li><li>max_pool_with_argmax(…)</li><li>moments(…)</li><li>nce_loss(…)</li><li>normalize_moments(…)</li><li>pool(…)</li><li>quantized_avg_pool(…)</li><li>quantized_conv2d(…)</li><li>quantized_max_pool(…)</li><li>quantized_relu_x(…)</li><li>raw_rnn(…)</li><li>relu(…)</li><li>relu6(…)</li><li>relu_layer(…)</li><li>safe_embedding_lookup_sparse(…)</li><li>sampled_softmax_loss(…)</li><li>selu(…)</li><li>separable_conv2d(…)</li><li>sigmoid(…)</li><li>sigmoid_cross_entropy_with_logits(…)</li><li>softmax(…)</li><li>softmax_cross_entropy_with_logits(…)</li><li>softmax_cross_entropy_with_logits_v2(…)</li><li>softplus(…)</li><li>softsign(…)</li><li>space_to_batch(…)</li><li>space_to_depth(…)</li><li>sparse_softmax_cross_entropy_with_logits(…)</li><li>static_bidirectional_rnn(…)</li><li>static_rnn(…)</li><li>static_state_saving_rnn(…)</li><li>sufficient_statistics(…)</li><li>tanh(…)</li><li>top_k(…)</li><li>uniform_candidate_sampler(…)</li><li>weighted_cross_entropy_with_logits(…)</li><li>weighted_moments(…)</li><li>with_space_to_batch(…)</li><li>xw_plus_b(…)</li><li>zero_fraction(…)</li></ul><h2 id="参考文献">参考文献</h2><p>1.<a href="https://www.tensorflow.org/api_docs/python/tf/nn" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/nn</a><br>2.<a href="https://www.tensorflow.org/api_docs/python/tf/nn/rnn_cell" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/nn/rnn_cell</a><br>3.<a href="https://www.tensorflow.org/api_docs/python/tf/nn/conv2d" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/nn/conv2d</a><br>4.<a href="https://stackoverflow.com/questions/38601452/what-is-tf-nn-max-pools-ksize-parameter-used-for" target="_blank" rel="noopener">https://stackoverflow.com/questions/38601452/what-is-tf-nn-max-pools-ksize-parameter-used-for</a><br>5.<a href="https://www.tensorflow.org/api_docs/python/tf/nn/convolution" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/nn/convolution</a><br>6.<a href="https://stackoverflow.com/questions/47775244/difference-between-tf-nn-convolution-and-tf-nn-conv2d" target="_blank" rel="noopener">https://stackoverflow.com/questions/47775244/difference-between-tf-nn-convolution-and-tf-nn-conv2d</a><br>7.<a href="https://www.tensorflow.org/api_docs/python/tf/nn/conv2d_transpose" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/nn/conv2d_transpose</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;tf-nn&quot;&gt;tf.nn&lt;/h2&gt;
&lt;p&gt;提供神经网络op。包含构建RNN cell的rnn_cell模块和一些函数。&lt;/p&gt;
&lt;h2 id=&quot;tf-nn-rnn-cell&quot;&gt;tf.nn.rnn_cell&lt;/h2&gt;
&lt;p&gt;rnn_cell 用于构建RNN cell
      
    
    </summary>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/categories/tensorflow/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow softmax</title>
    <link href="http://mxxhcm.github.io/2019/05/16/tensorflow-softmax/"/>
    <id>http://mxxhcm.github.io/2019/05/16/tensorflow-softmax/</id>
    <published>2019-05-16T01:04:48.000Z</published>
    <updated>2019-09-28T13:25:45.387Z</updated>
    
    <content type="html"><![CDATA[<h2 id="各种softmax">各种softmax</h2><ul><li>tf.nn.softmax。</li><li>tf.nn.log_softmax。</li><li>tf.nn.softmax_cross_entropy_with_logits_v2中label是用稀疏的（one-hot）表示的。</li><li>tf.nn.sparse_softmax_cross_entropy_with_logits中label是非稀疏的。</li></ul><h2 id="对比">对比</h2><p>tf.nn.softmax()<br>tf.nn.log_softmax()<br>tf.nn.softmax_cross_entropy_with_logits_v2()<br>tf.nn.sparse_cross_entropy_with_logits()</p><h2 id="logits">logits</h2><p>什么是logits</p><h3 id="数学上">数学上</h3><p>假设一个事件发生的概率为 p，那么该事件的logits为$\text{logit}§ = \log\frac{p}{1-p}$.</p><h3 id="machine-learning中">Machine Learning中</h3><p>深度学习中的logits和数学上的logits没有太大联系。logits在机器学习中前向传播的输出，是未归一化的概率，总和不为$1$。将logits的输出输入softmax函数之后可以得到归一化的概率。</p><h2 id="tf-nn-softmax">tf.nn.softmax</h2><h3 id="api">API</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.softmax(</span><br><span class="line">logits,</span><br><span class="line">axis=<span class="literal">None</span>,</span><br><span class="line">name=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="功能">功能</h3><p>上面函数实现了如下的功能：<br>softmax = tf.exp(logits) / tf.reduce_sum(tf.exp(logits), axis)<br>就是将输入的logits经过softmax做归一化。</p><h3 id="示例">示例</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">logits = [<span class="number">1.0</span>, <span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">2.0</span>, <span class="number">2.0</span>, <span class="number">2.0</span>]</span><br><span class="line">res_op = tf.nn.softmax(logits)</span><br><span class="line">sess = tf.Session()</span><br><span class="line">result = sess.run(res_op)</span><br><span class="line">print(result)</span><br><span class="line">print(sum(result))</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line"><span class="comment"># [0.07768121 0.07768121 0.21115941 0.21115941 0.21115941 0.21115941]</span></span><br><span class="line"><span class="comment"># 1.0000000447034836</span></span><br><span class="line"><span class="comment"># 因为有指数运算，所以就不是整数</span></span><br></pre></td></tr></table></figure><h2 id="tf-nn-log-softmax">tf.nn.log_softmax</h2><h3 id="api-v2">API</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.log_softmax(</span><br><span class="line">logits,</span><br><span class="line">axis=<span class="literal">None</span>,</span><br><span class="line">name=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="功能-v2">功能</h3><p>该函数实现了如下功能。<br>logsoftmax = logits - log(reduce_sum(exp(logits), axis))</p><h3 id="示例-v2">示例</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">logits = [<span class="number">1.0</span>, <span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">2.0</span>, <span class="number">2.0</span>, <span class="number">2.0</span>]</span><br><span class="line">res_op = tf.nn.log_softmax(logits)</span><br><span class="line">sess = tf.Session()</span><br><span class="line">result = sess.run(res_op)</span><br><span class="line">print(result)</span><br><span class="line">print(sum(result))</span><br><span class="line"></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line"><span class="comment"># [-2.555142  -2.555142  -1.5551419 -1.5551419 -1.5551419 -1.5551419]</span></span><br><span class="line"><span class="comment"># -11.330851554870605</span></span><br></pre></td></tr></table></figure><h2 id="tf-nn-softmax-cross-entropy-with-logits-v2">tf.nn.softmax_cross_entropy_with_logits_v2</h2><h3 id="api-v3">API</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.softmax_cross_entropy_with_logits_v2(</span><br><span class="line">    labels, <span class="comment"># shape是[batch_size, num_calsses]，每一个labels[i]都应该是一个有效的probability distribution</span></span><br><span class="line">    logits, <span class="comment"># 没有normalized的log probabilities</span></span><br><span class="line">    axis=<span class="literal">None</span>,</span><br><span class="line">    name=<span class="literal">None</span></span><br><span class="line">    dim=<span class="number">-1</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="功能-v3">功能</h3><p>计算logits经过softmax之后和labels之间的交叉熵</p><h2 id="tf-sparse-softmax-cross-entropy-with-logits">tf.sparse_softmax_cross_entropy_with_logits</h2><h3 id="api-v4">API</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.sparse_softmax_cross_entropy_with_logits(</span><br><span class="line">    _sentinel=<span class="literal">None</span>,  <span class="comment"># pylint: disable=invalid-name</span></span><br><span class="line">    labels=<span class="literal">None</span>,    <span class="comment"># shape是[d_0, d_1, ..., d_&#123;r-1&#125;]其中r是labels的秩，type是int32或int64，每一个entry都应该在[0, num_classes)之间</span></span><br><span class="line">    logits=<span class="literal">None</span>,    <span class="comment"># logits 是[d_0, d_1, ..., d_&#123;r-1&#125;, num_classes]，是float类型的，可以看成unnormalized log probabilities</span></span><br><span class="line">    name=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="功能-v4">功能</h3><p>计算logits和labels之间的稀疏softmax交叉熵</p><h2 id="参考文献">参考文献</h2><p>1.<a href="http://landcareweb.com/questions/789/shi-yao-shi-logits-softmaxhe-softmax-cross-entropy-with-logits" target="_blank" rel="noopener">http://landcareweb.com/questions/789/shi-yao-shi-logits-softmaxhe-softmax-cross-entropy-with-logits</a><br>2.<a href="https://stackoverflow.com/questions/41455101/what-is-the-meaning-of-the-word-logits-in-tensorflow" target="_blank" rel="noopener">https://stackoverflow.com/questions/41455101/what-is-the-meaning-of-the-word-logits-in-tensorflow</a><br>3.<a href="https://stackoverflow.com/a/43577384" target="_blank" rel="noopener">https://stackoverflow.com/a/43577384</a><br>4.<a href="https://stackoverflow.com/a/47852892" target="_blank" rel="noopener">https://stackoverflow.com/a/47852892</a><br>5.<a href="https://www.tensorflow.org/tutorials/estimators/cnn" target="_blank" rel="noopener">https://www.tensorflow.org/tutorials/estimators/cnn</a><br>6.<a href="https://www.zhihu.com/question/60751553" target="_blank" rel="noopener">https://www.zhihu.com/question/60751553</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;各种softmax&quot;&gt;各种softmax&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;tf.nn.softmax。&lt;/li&gt;
&lt;li&gt;tf.nn.log_softmax。&lt;/li&gt;
&lt;li&gt;tf.nn.softmax_cross_entropy_with_logits_v2中la
      
    
    </summary>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/categories/tensorflow/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow collection</title>
    <link href="http://mxxhcm.github.io/2019/05/13/tensorflow-collection/"/>
    <id>http://mxxhcm.github.io/2019/05/13/tensorflow-collection/</id>
    <published>2019-05-13T02:28:29.000Z</published>
    <updated>2019-09-28T08:41:47.549Z</updated>
    
    <content type="html"><![CDATA[<h2 id="tf-collection">tf.collection</h2><p>Tensorflow用graph collection来管理不同类型的对象。tf.GraphKeys中定义了默认的collection，tf通过调用各种各样的collection操作graph中的变量。比如tf.Optimizer只优化tf.GraphKeys.TRAINABLE_VARIABLES collection中的变量。常见的collection如下，它们其实都是字符串：</p><ul><li>GLOBAL_VARIABLES: 所有的Variable对象在创建的时候自动加入该colllection，且在分布式环境中共享（model variables是它的子集）。一般来说，TRAINABLE_VARIABLES包含在MODEL_VARIABLES中，MODEL_VARIABLES包含在GLOBAL_VARIABLES中。也就是说TRAINABLE_VARIABLES$\le$MODEL_VARIABLES$\le$GLOBAL_VARIABLES。一般tf.train.Saver()对应的是GLOBAL_VARIABLES的变量。</li><li>LOCAL_VARIABLES: 它是GLOBAL_VARIABLES不同的是在本机器上的Variable子集。使用tf.contrib.framework.local_variable将变量添加到这个collection.</li><li>MODEL_VARIABLES: 模型变量，在构建模型中，所有用于前向传播的Variable都将添加到这里。使用 tf.contrib.framework.model_variable向这个collection添加变量。</li><li>TRAINALBEL_VARIABLES: 所有用于反向传播的Variable，可以被optimizer训练，进行参数更新的变量。tf.Variable对象同样会自动加入这个collection。</li><li>SUMMARIES: graph创建的所有summary Tensor都会记录在这里面。</li><li>QUEUE_RUNNERS:</li><li>MOVING_AVERAGE_VARIABLES: 保持Movering average的变量子集。</li><li>REGULARIZATION_LOSSES: 创建graph的regularization loss。</li></ul><p>这里主要介绍三类collection，一种是GLOBAL_VARIABLES，一种是SUMMARIES，一种是自定义的collections。</p><p>下面的一些collection也被定义了，但是并不会自动添加</p><blockquote><p>The following standard keys are defined, but their collections are not automatically populated as many of the others are:</p></blockquote><ul><li>WEIGHTS</li><li>BIASES</li><li>ACTIVATIONS</li></ul><h2 id="global-variable-collection">GLOBAL_Variable collection</h2><p>tf.Variable()对象在生成时会被默认添加到tf.GraphKeys中的GLOBAL_VARIABLES和TRAINABLE_VARIABLES collection中。</p><h3 id="代码示例">代码示例</h3><p><a href="https://github.com/mxxhcm/code/blob/master/tf/ops/tf_global_trainable_variables_collections.py" target="_blank" rel="noopener">代码地址</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">a = tf.Variable([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">b = tf.get_variable(<span class="string">"bbb"</span>, shape=[<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">tf.constant([<span class="number">3</span>])</span><br><span class="line">c = tf.ones([<span class="number">3</span>])</span><br><span class="line">d = tf.random_uniform([<span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">e = tf.log(c)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看GLOBAL_VARIABLES collection中的变量</span></span><br><span class="line">global_variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)</span><br><span class="line"><span class="keyword">for</span> var <span class="keyword">in</span> global_variables:</span><br><span class="line">   print(var)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看TRAINABLE_VARIABLES collection中的变量</span></span><br><span class="line">trainable_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)</span><br><span class="line"><span class="keyword">for</span> var <span class="keyword">in</span> global_variables:</span><br><span class="line">   print(var)</span><br></pre></td></tr></table></figure><h2 id="summary-collection">Summary collection</h2><p>Summary op产生的变量会被添加到tf.GraphKeys.SUMMARIES collection中。<br><a href="https://mxxhcm.github.io/2019/05/08/tensorflow-summary/">点击查看关于tf.summary的详细介绍</a></p><h3 id="代码示例-v2">代码示例</h3><p><a href="https://github.com/mxxhcm/code/blob/master/tf/ops/tf_summary_collection.py" target="_blank" rel="noopener">代码地址</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成一个图</span></span><br><span class="line">graph = tf.Graph()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> graph.as_default():</span><br><span class="line">    <span class="comment"># 指定模型参数</span></span><br><span class="line">    w = tf.Variable([<span class="number">0.3</span>], name=<span class="string">"w"</span>, dtype=tf.float32)</span><br><span class="line">    b = tf.Variable([<span class="number">0.2</span>], name=<span class="string">"b"</span>, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 输入数据placeholder</span></span><br><span class="line">    x = tf.placeholder(tf.float32, name=<span class="string">"inputs"</span>)</span><br><span class="line">    y = tf.placeholder(tf.float32, name=<span class="string">"outputs"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 前向传播</span></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">'linear_model'</span>):</span><br><span class="line">        linear = w * x + b</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算loss</span></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">'cal_loss'</span>):</span><br><span class="line">        loss = tf.reduce_mean(input_tensor=tf.square(y - linear), name=<span class="string">'loss'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义summary saclar op</span></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">'add_summary'</span>):</span><br><span class="line">        summary_loss = tf.summary.scalar(<span class="string">'MSE'</span>, loss)</span><br><span class="line">        summary_b = tf.summary.scalar(<span class="string">'b'</span>, b[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义优化器</span></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">'train_model'</span>):</span><br><span class="line">        optimizer = tf.train.GradientDescentOptimizer(<span class="number">0.01</span>)</span><br><span class="line">        train = optimizer.minimize(loss)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session(graph=graph) <span class="keyword">as</span> sess:</span><br><span class="line">inputs = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]</span><br><span class="line">outputs = [<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]</span><br><span class="line">    <span class="comment"># 定义写入文件类</span></span><br><span class="line">    writer = tf.summary.FileWriter(<span class="string">"./summary/"</span>, graph)</span><br><span class="line">    <span class="comment"># 获取所有的summary op，不用一个一个去单独run</span></span><br><span class="line">    merged = tf.summary.merge_all()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化</span></span><br><span class="line">    init_op = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5000</span>):</span><br><span class="line"><span class="comment"># 运行summary op merged</span></span><br><span class="line">        _, summ = sess.run([train, merged], feed_dict=&#123;x: inputs, y: outputs&#125;)</span><br><span class="line"><span class="comment"># 将summary op返回的变量转化为事件，写入文件</span></span><br><span class="line">        writer.add_summary(summ, global_step=i)</span><br><span class="line"></span><br><span class="line">    w_, b_, l_ = sess.run([w, b, loss], feed_dict=&#123;x: inputs, y: outputs&#125;)</span><br><span class="line">    print(<span class="string">"w: "</span>, w_, <span class="string">"b: "</span>, b_, <span class="string">"loss: "</span>, l_)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 查看SUMMARIES collection</span></span><br><span class="line">    <span class="keyword">for</span> var <span class="keyword">in</span> tf.get_collection(tf.GraphKeys.SUMMARIES):</span><br><span class="line">        print(var)</span><br></pre></td></tr></table></figure><h2 id="自定义collection">自定义collection</h2><p>通过tf.add_collection()和tf.get_collection()可以添加和访问custom collection。</p><h3 id="示例代码">示例代码</h3><p><a href="https://github.com/mxxhcm/code/blob/master/tf/ops/tf_custom_collection.py" target="_blank" rel="noopener">代码地址</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义第1个loss</span></span><br><span class="line">x1 = tf.constant(<span class="number">1.0</span>)</span><br><span class="line">l1 = tf.nn.l2_loss(x1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义第2个loss</span></span><br><span class="line">x2 = tf.constant([<span class="number">2.5</span>, <span class="number">-0.3</span>])</span><br><span class="line">l2 = tf.nn.l2_loss(x2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将loss添加到losses collection中</span></span><br><span class="line">tf.add_to_collection(<span class="string">"losses"</span>, l1)</span><br><span class="line">tf.add_to_collection(<span class="string">"losses"</span>, l2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看losses collection中的内容</span></span><br><span class="line">losses = tf.get_collection(<span class="string">'losses'</span>)</span><br><span class="line"><span class="keyword">for</span> var <span class="keyword">in</span> tf.get_collection(<span class="string">'losses'</span>):</span><br><span class="line">    print(var)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 建立session运行</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init)</span><br><span class="line">    losses_val = sess.run(losses)</span><br><span class="line">    print(losses_val)</span><br></pre></td></tr></table></figure><h2 id="疑问">疑问</h2><p>collection是和graph绑定在一起的，那么如果定义了很多个图，如何获得非默认图的tf.GraphKeys中定义的collection？？</p><h2 id="参考文献">参考文献</h2><p>1.<a href="https://blog.csdn.net/shenxiaolu1984/article/details/52815641" target="_blank" rel="noopener">https://blog.csdn.net/shenxiaolu1984/article/details/52815641</a><br>2.<a href="https://blog.csdn.net/hustqb/article/details/80398934" target="_blank" rel="noopener">https://blog.csdn.net/hustqb/article/details/80398934</a><br>3.<a href="https://www.tensorflow.org/api_docs/python/tf/GraphKeys?hl=zh_cn" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/GraphKeys?hl=zh_cn</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;tf-collection&quot;&gt;tf.collection&lt;/h2&gt;
&lt;p&gt;Tensorflow用graph collection来管理不同类型的对象。tf.GraphKeys中定义了默认的collection，tf通过调用各种各样的collection操作grap
      
    
    </summary>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/categories/tensorflow/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow graph和session</title>
    <link href="http://mxxhcm.github.io/2019/05/12/tensorflow-graph%E5%92%8Csession/"/>
    <id>http://mxxhcm.github.io/2019/05/12/tensorflow-graph和session/</id>
    <published>2019-05-12T13:45:04.000Z</published>
    <updated>2019-07-18T12:21:17.513Z</updated>
    
    <content type="html"><![CDATA[<h2 id="tf-graph和tf-session">tf.Graph和tf.Session</h2><p>Graph和Session之间的区别和联系。</p><ul><li>Graph定义了如何进行计算，但是并没有进行计算，graph不会hold任何值，它仅仅定义code中指定的各种operation</li><li>Session用来执行graph或者graph的一部分。它会分配资源（一个机器或者多个机器），并且会保存中间结果和variables的值。在不同session的执行过程也是分开的。</li></ul><h2 id="tf-graph">tf.Graph</h2><p>tf.Graph包含两类信息：</p><ul><li>Node和Edge，用来表示各个op如何进行组合。</li><li>collections。使用tf.add_to_collection和tf.get_collection对collection进行操作。一个常见的例子是创建tf.Variable的时候，默认会将它加入到&quot;global variables&quot;和&quot;trainable variables&quot; collection中。<br>当调用tf.train.Saver和tf.train.Optimizer的时候，它会使用这些collection中的变量作为默认参数。<br>常见的定义在tf.GraphKeys上的collection:<br>VARIABLES, TRAINABLE_VARIABLES, MOVING_AVERAGE_VARIABLES, LOCAL_VARIABLES, MODEL_VARIABLE,SUMMARIES.<br><a href="https://mxxhcm.github.io/2019/05/13/tensorflow-collection/">关于collections的详细介绍可点击这里</a></li></ul><h2 id="构建tf-graph">构建tf.Graph</h2><p>调用tensorflow API就会构建新的tf.Operation和tf.Tensor，并将他们添加到tf.Graph实例中去。</p><ul><li>调用 tf.constant(42.0) 创建单个 tf.Operation，该操作可以生成值 42.0，将该值添加到默认图中，并返回表示常量值的 tf.Tensor。</li><li>调用 tf.matmul(x, y) 可创建单个 tf.Operation，该操作会将 tf.Tensor 对象 x 和 y 的值相乘，将其添加到默认图中，并返回表示乘法运算结果的 tf.Tensor。</li><li>执行 v = tf.Variable(0) 可向图添加一个 tf.Operation，该操作可以存储一个可写入的张量值，该值在多个 tf.Session.run 调用之间保持恒定。tf.Variable 对象会封装此操作，并可以像张量一样使用，即读取已存储值的当前值。tf.Variable 对象也具有 assign 和 assign_add 等方法，这些方法可创建 tf.Operation 对象，这些对象在执行时将更新已存储的值。（请参阅变量了解关于变量的更多信息。）</li><li>调用 tf.train.Optimizer.minimize 可将操作和张量添加到计算梯度的默认图中，并返回一个 tf.Operation，该操作在运行时会将这些梯度应用到一组变量上。</li></ul><h2 id="获得默认图">获得默认图</h2><p>用 tf.get_default_graph，它会返回一个 tf.Graph 对象：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Print all of the operations in the default graph.</span></span><br><span class="line">g = tf.get_default_graph()</span><br></pre></td></tr></table></figure><h2 id="清空默认图">清空默认图</h2><p>tf.reset_default_graph()</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 清空当前session的默认图</span></span><br><span class="line">tf.reset_default_graph()</span><br></pre></td></tr></table></figure><h2 id="命名空间">命名空间</h2><p>tf.Graph 对象会定义一个命名空间（为其包含的 tf.Operation 对象）。TensorFlow 会自动为图中的每个指令选择一个唯一名称，也可以指定描述性名称，让程序阅读和调试起来更轻松。TensorFlow API 提供两种方法来指定op名称：</p><ul><li>如果API会创建新的op或返回新的 tf.Tensor，就可选 name 参数。例如，tf.constant(42.0, name=“answer”) 会创建一个新的 tf.Operation（名为 “answer”）并返回一个 tf.Tensor（名为 “answer:0”）。如果默认图已包含名为 “answer” 的操作，则 TensorFlow 会在名称上附加 “_1”、&quot;_2&quot; 等字符，以便让名称具有唯一性。</li><li>借助 tf.name_scope 函数，可以向在特定上下文中创建的所有op添加name_scope。当前name_scope是一个用 “/” 分隔的名称列表，其中包含所有活跃的 tf.name_scope 上下文管理器名称。如果某个name_scope已在当前上下文中被占用，TensorFlow 将在该作用域上附加 “_1”、&quot;_2&quot; 等字符。例如：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">c_0 = tf.constant(<span class="number">0</span>, name=<span class="string">"c"</span>)  <span class="comment"># =&gt; operation named "c"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Already-used names will be "uniquified".</span></span><br><span class="line">c_1 = tf.constant(<span class="number">2</span>, name=<span class="string">"c"</span>)  <span class="comment"># =&gt; operation named "c_1"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Name scopes add a prefix to all operations created in the same context.</span></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">"outer"</span>):</span><br><span class="line">  c_2 = tf.constant(<span class="number">2</span>, name=<span class="string">"c"</span>)  <span class="comment"># =&gt; operation named "outer/c"</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Name scopes nest like paths in a hierarchical file system.</span></span><br><span class="line">  <span class="keyword">with</span> tf.name_scope(<span class="string">"inner"</span>):</span><br><span class="line">    c_3 = tf.constant(<span class="number">3</span>, name=<span class="string">"c"</span>)  <span class="comment"># =&gt; operation named "outer/inner/c"</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Exiting a name scope context will return to the previous prefix.</span></span><br><span class="line">  c_4 = tf.constant(<span class="number">4</span>, name=<span class="string">"c"</span>)  <span class="comment"># =&gt; operation named "outer/c_1"</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Already-used name scopes will be "uniquified".</span></span><br><span class="line">  <span class="keyword">with</span> tf.name_scope(<span class="string">"inner"</span>):</span><br><span class="line">    c_5 = tf.constant(<span class="number">5</span>, name=<span class="string">"c"</span>)  <span class="comment"># =&gt; operation named "outer/inner_1/c"</span></span><br></pre></td></tr></table></figure><p>请注意，tf.Tensor 对象以输出张量的op明确命名。张量名称的形式为 “&lt;OP_NAME&gt;:&lt;i&gt;”，其中：</p><ul><li>“&lt;OP_NAME&gt;” 是生成该张量的操作的名称。</li><li>“&lt;i&gt;” 是一个整数，表示该张量在该op的输出中的索引。</li></ul><h2 id="获得图中的op">获得图中的op</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">c_0 = tf.constant(<span class="number">0</span>, name=<span class="string">"c"</span>)  <span class="comment"># =&gt; operation named "c"</span></span><br><span class="line"><span class="comment"># Already-used names will be "uniquified".  c_1 = tf.constant(2, name="c")  # =&gt; operation named "c_1"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Name scopes add a prefix to all operations created in the same context.</span></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">"outer"</span>):</span><br><span class="line">  c_2 = tf.constant(<span class="number">2</span>, name=<span class="string">"c"</span>)  <span class="comment"># =&gt; operation named "outer/c"</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Name scopes nest like paths in a hierarchical file system.</span></span><br><span class="line">  <span class="keyword">with</span> tf.name_scope(<span class="string">"inner"</span>):</span><br><span class="line">    c_3 = tf.constant(<span class="number">3</span>, name=<span class="string">"c"</span>)  <span class="comment"># =&gt; operation named "outer/inner/c"</span></span><br><span class="line"></span><br><span class="line">g = tf.get_default_graph()</span><br><span class="line">print(g.get_operations())</span><br><span class="line"><span class="comment"># [&lt;tf.Operation 'c' type=Const&gt;, &lt;tf.Operation 'c_1' type=Const&gt;, &lt;tf.Operation 'outer/c' type=Const&gt;, &lt;tf.Operation 'outer/inner/c' type=Const&gt;]</span></span><br></pre></td></tr></table></figure><h2 id="类张量对象">类张量对象</h2><p>许多 TensorFlow op都会接受一个或多个 tf.Tensor 对象作为参数。例如，tf.matmul 接受两个 tf.Tensor 对象，tf.add_n 接受一个具有 n 个 tf.Tensor 对象的列表。为了方便起见，这些函数将接受类张量对象来取代 tf.Tensor，并将它明确转换为 tf.Tensor（通过 tf.convert_to_tensor 方法）。类张量对象包括以下类型的元素：</p><ul><li>tf.Tensor</li><li>tf.Variable</li><li>numpy.ndarray</li><li>list（以及类似于张量的对象的列表）</li><li>标量 Python 类型：bool、float、int、str</li></ul><p><strong>注意</strong> 默认情况下，每次使用同一个类张量对象时，TensorFlow 将创建新的 tf.Tensor。如果类张量对象很大（例如包含一组训练样本的 numpy.ndarray），且多次使用该对象，则可能会耗尽内存。要避免出现此问题，请在类张量对象上手动调用 tf.convert_to_tensor 一次，并使用返回的 tf.Tensor。</p><h2 id="tf-session">tf.Session</h2><h3 id="api">API</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tf.Session.init(</span><br><span class="line">target, <span class="comment"># 可选参数，指定设备。</span></span><br><span class="line">graph, <span class="comment">#可选参数，默认情况下，新的session绑定到默认graph</span></span><br><span class="line">confi <span class="comment"># 可选参数，常见的一个选择为gpu_options.allow_growth。将此参数设置为 True 可更改 GPU 内存分配器，使该分配器逐渐增加分配的内存量，而不是在启动时分配掉大多数内存。</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="创建session">创建session</h3><h4 id="默认session">默认session</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create a default in-process session.</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  <span class="comment"># ...</span></span><br></pre></td></tr></table></figure><h4 id="none"></h4><h3 id="执行op">执行op</h3><p>tf.Session.run 方法是运行 tf.Operation 或评估 tf.Tensor 的主要机制。传入一个或多个 tf.Operation 或 tf.Tensor 对象到 tf.Session.run，TensorFlow 将执行计算结果所需的操作。<br>tf.Session.run 需要指定一组 fetch，这些 fetch 可确定返回值，并且可能是 tf.Operation、tf.Tensor 或类张量类型，例如 tf.Variable。这些 fetch 决定了必须执行哪些子图（属于整体 tf.Graph）以生成结果：该子图包含 fetch 列表中指定的所有op，以及其输出用于计算 fetch 值的所有操作。例如，以下代码段说明了 tf.Session.run 的不同参数如何导致执行不同的子图：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">x = tf.constant([[<span class="number">37.0</span>, <span class="number">-23.0</span>], [<span class="number">1.0</span>, <span class="number">4.0</span>]])</span><br><span class="line">w = tf.Variable(tf.random_uniform([<span class="number">2</span>, <span class="number">2</span>]))</span><br><span class="line">y = tf.matmul(x, w)</span><br><span class="line">output = tf.nn.softmax(y)</span><br><span class="line">init_op = w.initializer</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  <span class="comment"># 初始化w</span></span><br><span class="line">  sess.run(init_op)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Evaluate `output`. `sess.run(output)` will return a NumPy array containing</span></span><br><span class="line">  <span class="comment"># the result of the computation.</span></span><br><span class="line">  <span class="comment"># 计算output</span></span><br><span class="line">  print(sess.run(output))</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Evaluate `y` and `output`. Note that `y` will only be computed once, and its</span></span><br><span class="line">  <span class="comment"># result used both to return `y_val` and as an input to the `tf.nn.softmax()`</span></span><br><span class="line">  <span class="comment"># op. Both `y_val` and `output_val` will be NumPy arrays.</span></span><br><span class="line">  <span class="comment"># 计算y和output</span></span><br><span class="line">  y_val, output_val = sess.run([y, output])</span><br></pre></td></tr></table></figure><p>tf.Session.run 也可以接受 feed dict，该字典是从 tf.Tensor 对象（通常是 tf.placeholder 张量），在执行时会替换这些张量的值（通常是 Python 标量、列表或 NumPy 数组）的映射。例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Define a placeholder that expects a vector of three floating-point values,</span></span><br><span class="line"><span class="comment"># and a computation that depends on it.</span></span><br><span class="line">x = tf.placeholder(tf.float32, shape=[<span class="number">3</span>])</span><br><span class="line">y = tf.square(x)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  <span class="comment"># Feeding a value changes the result that is returned when you evaluate `y`.</span></span><br><span class="line">  print(sess.run(y, &#123;x: [<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>]&#125;))  <span class="comment"># =&gt; "[1.0, 4.0, 9.0]"</span></span><br><span class="line">  print(sess.run(y, &#123;x: [<span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">5.0</span>]&#125;))  <span class="comment"># =&gt; "[0.0, 0.0, 25.0]"</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Raises &lt;a href="../api_docs/python/tf/errors/InvalidArgumentError"&gt;&lt;code&gt;tf.errors.InvalidArgumentError&lt;/code&gt;&lt;/a&gt;, because you must feed a value for</span></span><br><span class="line">  <span class="comment"># a `tf.placeholder()` when evaluating a tensor that depends on it.</span></span><br><span class="line">  sess.run(y)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Raises `ValueError`, because the shape of `37.0` does not match the shape</span></span><br><span class="line">  <span class="comment"># of placeholder `x`.</span></span><br><span class="line">  sess.run(y, &#123;x: <span class="number">37.0</span>&#125;)</span><br></pre></td></tr></table></figure><p>tf.Session.run 也接受可选的 options 参数（允许指定与调用有关的选项）和可选的 run_metadata 参数（允许收集与执行有关的元数据）。例如，可以同时使用这些选项来收集与执行有关的跟踪信息：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">y = tf.matmul([[<span class="number">37.0</span>, <span class="number">-23.0</span>], [<span class="number">1.0</span>, <span class="number">4.0</span>]], tf.random_uniform([<span class="number">2</span>, <span class="number">2</span>]))</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  <span class="comment"># Define options for the `sess.run()` call.</span></span><br><span class="line">  options = tf.RunOptions()</span><br><span class="line">  options.output_partition_graphs = <span class="literal">True</span></span><br><span class="line">  options.trace_level = tf.RunOptions.FULL_TRACE</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Define a container for the returned metadata.</span></span><br><span class="line">  metadata = tf.RunMetadata()</span><br><span class="line"></span><br><span class="line">  sess.run(y, options=options, run_metadata=metadata)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Print the subgraphs that executed on each device.</span></span><br><span class="line">  print(metadata.partition_graphs)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Print the timings of each operation that executed.</span></span><br><span class="line">  print(metadata.step_stats)</span><br></pre></td></tr></table></figure><h2 id="不同session的结果">不同session的结果</h2><p><a href>代码地址</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">graph = tf.Graph()</span><br><span class="line"></span><br><span class="line">with graph.as_default():</span><br><span class="line">    variable = tf.Variable(10, name=&quot;foo&quot;)</span><br><span class="line">    initialize = tf.global_variables_initializer()</span><br><span class="line">    assign = variable.assign(12)</span><br><span class="line"></span><br><span class="line">with tf.Session(graph=graph) as sess:</span><br><span class="line">    sess.run(initialize)</span><br><span class="line">    sess.run(assign)</span><br><span class="line">    print(sess.run(variable))</span><br><span class="line"></span><br><span class="line">with tf.Session(graph=graph) as sess:</span><br><span class="line">    print(sess.run(variable))</span><br></pre></td></tr></table></figure><h2 id="访问当前sess的图">访问当前sess的图。</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sess = tf.Session()</span><br><span class="line">sess.graph</span><br></pre></td></tr></table></figure><h2 id="可视化图">可视化图</h2><p>使用图可视化工具。最简单的方法是传递tf.Graph到tf.summary.FileWriter中。如下示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Build your graph.</span></span><br><span class="line">x = tf.constant([[<span class="number">37.0</span>, <span class="number">-23.0</span>], [<span class="number">1.0</span>, <span class="number">4.0</span>]])</span><br><span class="line">w = tf.Variable(tf.random_uniform([<span class="number">2</span>, <span class="number">2</span>]))</span><br><span class="line">y = tf.matmul(x, w)</span><br><span class="line"><span class="comment"># ...</span></span><br><span class="line">loss = ...</span><br><span class="line">train_op = tf.train.AdagradOptimizer(<span class="number">0.01</span>).minimize(loss)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  <span class="comment"># `sess.graph` provides access to the graph used in a &lt;a href="../api_docs/python/tf/Session"&gt;&lt;code&gt;tf.Session&lt;/code&gt;&lt;/a&gt;.</span></span><br><span class="line">  writer = tf.summary.FileWriter(<span class="string">"/tmp/log/..."</span>, sess.graph)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Perform your computation...</span></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">    sess.run(train_op)</span><br><span class="line">    <span class="comment"># ...</span></span><br><span class="line"></span><br><span class="line">  writer.close()</span><br></pre></td></tr></table></figure><p>然后可以在 tensorboard 中打开日志并转到“图”标签，查看图结构的概要可视化图表。</p><h2 id="创建多个图">创建多个图</h2><p>TensorFlow 提供了一个“默认图”，此图明确传递给同一上下文中的所有 API 函数。TensorFlow 提供了操作默认图的方法，在更高级的用例中，这些方法可能有用。</p><ul><li>tf.Graph 会定义 tf.Operation 对象的命名空间：单个图中的每个操作必须具有唯一名称。如果请求的名称已被占用，TensorFlow 将在操作名称上附加 “_1”、&quot;_2&quot; 等字符，以便确保名称的唯一性。通过使用多个明确创建的图，可以更有效地控制为每个op指定什么样的名称。</li><li>默认图会存储与添加的每个 tf.Operation 和 tf.Tensor 有关的信息。如果程序创建了大量未连接的子图，更有效的做法是使用另一个 tf.Graph 构建每个子图，以便回收不相关的状态。</li></ul><h3 id="创建两个图">创建两个图</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">g_1 = tf.Graph()</span><br><span class="line"><span class="keyword">with</span> g_1.as_default():</span><br><span class="line">  <span class="comment"># Operations created in this scope will be added to `g_1`.</span></span><br><span class="line">  c = tf.constant(<span class="string">"Node in g_1"</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Sessions created in this scope will run operations from `g_1`.</span></span><br><span class="line">  sess_1 = tf.Session()</span><br><span class="line"></span><br><span class="line">g_2 = tf.Graph()</span><br><span class="line"><span class="keyword">with</span> g_2.as_default():</span><br><span class="line">  <span class="comment"># Operations created in this scope will be added to `g_2`.</span></span><br><span class="line">  d = tf.constant(<span class="string">"Node in g_2"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Alternatively, you can pass a graph when constructing a &lt;a href="../api_docs/python/tf/Session"&gt;&lt;code&gt;tf.Session&lt;/code&gt;&lt;/a&gt;:</span></span><br><span class="line"><span class="comment"># `sess_2` will run operations from `g_2`.</span></span><br><span class="line">sess_2 = tf.Session(graph=g_2)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> c.graph <span class="keyword">is</span> g_1</span><br><span class="line"><span class="keyword">assert</span> sess_1.graph <span class="keyword">is</span> g_1</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> d.graph <span class="keyword">is</span> g_2</span><br><span class="line"><span class="keyword">assert</span> sess_2.graph <span class="keyword">is</span> g_2</span><br></pre></td></tr></table></figure><h2 id="参考文献">参考文献</h2><p>1.<a href="https://www.tensorflow.org/guide/graphs?hl=zh_cn" target="_blank" rel="noopener">https://www.tensorflow.org/guide/graphs?hl=zh_cn</a><br>2.<a href="https://blog.csdn.net/shenxiaolu1984/article/details/52815641" target="_blank" rel="noopener">https://blog.csdn.net/shenxiaolu1984/article/details/52815641</a><br>3.<a href="https://danijar.com/what-is-a-tensorflow-session/" target="_blank" rel="noopener">https://danijar.com/what-is-a-tensorflow-session/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;tf-graph和tf-session&quot;&gt;tf.Graph和tf.Session&lt;/h2&gt;
&lt;p&gt;Graph和Session之间的区别和联系。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Graph定义了如何进行计算，但是并没有进行计算，graph不会hold任何值，它仅仅定义co
      
    
    </summary>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/categories/tensorflow/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow Varaible</title>
    <link href="http://mxxhcm.github.io/2019/05/12/tensorflow-Varaible/"/>
    <id>http://mxxhcm.github.io/2019/05/12/tensorflow-Varaible/</id>
    <published>2019-05-12T12:41:34.000Z</published>
    <updated>2019-07-09T12:27:12.102Z</updated>
    
    <content type="html"><![CDATA[<h2 id="创建variable">创建Variable</h2><p>Tensorflow有两种方式创建Variable：tf.Variable()和tf.get_variable()，这两种方式获得的都是tensorflow.python.ops.variables.Variable类型的对象，但是他们的输入参数还有些不一样。</p><table><thead><tr><th style="text-align:center"></th><th style="text-align:center">tf.Variable()</th><th style="text-align:center">tf.get_variable()</th></tr></thead><tbody><tr><td style="text-align:center">name</td><td style="text-align:center">不需要，已存在的变量名，会在后面加上递增的数值用来区分</td><td style="text-align:center">必须，已存在的会报错</td></tr><tr><td style="text-align:center">shape</td><td style="text-align:center">不需要，或者说已经包含在初值中了</td><td style="text-align:center">需要</td></tr><tr><td style="text-align:center">初值</td><td style="text-align:center">必须</td><td style="text-align:center">不需要</td></tr><tr><td style="text-align:center">复用</td><td style="text-align:center">不可以</td><td style="text-align:center">可以</td></tr></tbody></table><p>两种方法事实上都可以指定name和初值。而tf.Variable()的初值中已经包含了shape，所以不需要再显示传入shape了。这里的需要和不需要指的是必要不必要，如果没有传入需要的参数，就会报错，不需要的参数则不会影响。</p><h2 id="tf-variable">tf.Variable()</h2><h3 id="一句话介绍">一句话介绍</h3><p>创建一个类操作全局变量。在TensorFlow内部，tf.Variable会存储持久性张量，允许各种op读取和修改它的值。这些修改在多个Session之间是可见的，因此对于一个tf.Variable，多个工作器可以看到相同的值。</p><h3 id="和tf-tensor对比">和tf.Tensor对比</h3><p>tf.Variable 表示可通过对其运行op来改变其值的张量。与 tf.Tensor对象不同，tf.Variable 存在于单个session.run调用的上下文之外。tf.Tensor的值是不可以改变的，tf.Tensor没有assign函数。</p><h3 id="api">API</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">tf.Variable.__init__(</span><br><span class="line">initial_value=<span class="literal">None</span>,  <span class="comment"># 指定变量的初值</span></span><br><span class="line">trainable=<span class="literal">True</span>,  <span class="comment"># 是否在BP时训练该参数</span></span><br><span class="line">collections=<span class="literal">None</span>, <span class="comment"># 指定变量的collection</span></span><br><span class="line">validate_shape=<span class="literal">True</span>, </span><br><span class="line">caching_device=<span class="literal">None</span>, </span><br><span class="line">name=<span class="literal">None</span>,  <span class="comment"># 指定变量的名字</span></span><br><span class="line">...</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="代码示例">代码示例</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor1 = tf.Variable([[<span class="number">1</span>,<span class="number">2</span>], [<span class="number">3</span>,<span class="number">5</span>]])</span><br><span class="line">tensor2 = tf.Variable(tf.constant([[<span class="number">1</span>,<span class="number">2</span>], [<span class="number">3</span>,<span class="number">5</span>]]))</span><br><span class="line">sess.run(tf.global_variables_initializer())</span><br><span class="line">sess.run(tensor1)</span><br><span class="line">sess.run(tensor2)</span><br></pre></td></tr></table></figure><h3 id="初始化">初始化</h3><p>tf.Variable()生成的变量必须初始化，tf.constant()可以不用初始化。</p><ul><li>使用全局初始化<br>sess.run(tf.global_variables_initializer())</li><li>使用checkpoint</li><li>使用tf.assign赋值</li></ul><h2 id="tf-get-variable">tf.get_variable()</h2><h3 id="一句话介绍-v2">一句话介绍</h3><p>获取一个已经存在的变量或者创建一个新的变量。主要目的，变量复用。</p><h3 id="api-v2">API</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">tf.get_variable(</span><br><span class="line">    name, <span class="comment"># 指定变量的名字，必选项</span></span><br><span class="line">    shape=<span class="literal">None</span>, <span class="comment"># 指定变量的shape，可选项</span></span><br><span class="line">    dtype=<span class="literal">None</span>, <span class="comment"># 指定变量类型</span></span><br><span class="line">    initializer=<span class="literal">None</span>, <span class="comment"># 指定变量初始化器</span></span><br><span class="line">    regularizer=<span class="literal">None</span>,</span><br><span class="line">    trainable=<span class="literal">None</span>,</span><br><span class="line">    collections=<span class="literal">None</span>,</span><br><span class="line">    caching_device=<span class="literal">None</span>,</span><br><span class="line">    partitioner=<span class="literal">None</span>,</span><br><span class="line">    validate_shape=<span class="literal">True</span>,</span><br><span class="line">    use_resource=<span class="literal">None</span>,</span><br><span class="line">    custom_getter=<span class="literal">None</span>,</span><br><span class="line">    constraint=<span class="literal">None</span>,</span><br><span class="line">    synchronization=tf.VariableSynchronization.AUTO,</span><br><span class="line">    aggregation=tf.VariableAggregation.NONE</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="代码示例-v2">代码示例</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"model"</span>) <span class="keyword">as</span> scope:</span><br><span class="line">  output1 = my_image_filter(input1)</span><br><span class="line">  scope.reuse_variables()</span><br><span class="line">  output2 = my_image_filter(input2)</span><br></pre></td></tr></table></figure><h2 id="variable和collection">Variable和collection</h2><p><a href="https://mxxhcm.github.io/2019/05/13/tensorflow-collection/">点击查看关于collecion的详细介绍</a><br>默认情况下，每个tf.Variable()都会添加到以下两个collection中：</p><ul><li>tf.GraphKeys.GLOBAL_VARIABLES - 可以在多台设备间共享的变量，</li><li>tf.GraphKeys.TRAINABLE_VARIABLES - TensorFlow 将计算其梯度的变量。</li></ul><p>如果不希望变量是可训练的，可以在创建时指定其collection为 tf.GraphKeys.LOCAL_VARIABLES collection中。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">my_local = tf.get_variable(<span class="string">"my_local"</span>, shape=(), collections=[tf.GraphKeys.LOCAL_VARIABLES])</span><br></pre></td></tr></table></figure><p>或者可以指定 trainable=False：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">my_non_trainable = tf.get_variable(<span class="string">"my_non_trainable"</span>,</span><br><span class="line">                                   shape=(),</span><br><span class="line">                                   trainable=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><h3 id="获取collection">获取collection</h3><p>要检索放在某个collection中的所有变量的列表，可以使用：</p><h4 id="代码示例-v3">代码示例</h4><p><a href="https://github.com/mxxhcm/code/tree/master/tf/ops/tf_Variable_collection.py" target="_blank" rel="noopener">代码地址</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">a = tf.Variable([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">b = tf.get_variable(<span class="string">"bbb"</span>, shape=[<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">tf.constant([<span class="number">3</span>])</span><br><span class="line">c = tf.ones([<span class="number">3</span>])</span><br><span class="line">d = tf.random_uniform([<span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">print(tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES))</span><br><span class="line"><span class="comment"># [&lt;tf.Variable 'Variable:0' shape=(3,) dtype=int32_ref&gt;, &lt;tf.Variable 'bbb:0' shape=(2, 3) dtype=float32_ref&gt;]</span></span><br><span class="line"><span class="comment"># 可以看出来，只有tf.Variable()和tf.get_variable()产生的变量会加入到这个图中</span></span><br></pre></td></tr></table></figure><h3 id="自定义collection">自定义collection</h3><h4 id="添加自定义collection">添加自定义collection</h4><p>可以使用自定义的collection。collection名称可为任何字符串，且无需显式创建。创建对象（包括Variable和其他）后调用 tf.add_to_collection将其添加到相应collection中。以下代码将 my_local 变量添加到名为 my_collection_name 的collection中：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.add_to_collection(<span class="string">"my_collection_name"</span>, my_local)</span><br></pre></td></tr></table></figure><h2 id="初始化变量">初始化变量</h2><h3 id="初始化所有变量">初始化所有变量</h3><p>调用 tf.global_variables_initializer()在训练开始前一次性初始化所有可训练变量。此函数会返回一个op，负责初始化 tf.GraphKeys.GLOBAL_VARIABLES collection中的所有变量。运行此op会初始化所有变量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sess.run(tf.global_variables_initializer())</span><br></pre></td></tr></table></figure><h3 id="初始化单个变量">初始化单个变量</h3><p>运行变量的初始化器op。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sess.run(my_variable.initializer)</span><br></pre></td></tr></table></figure><h3 id="查询未初始化变量">查询未初始化变量</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(sess.run(tf.report_uninitialized_variables()))</span><br></pre></td></tr></table></figure><h2 id="共享变量">共享变量</h2><p>TensorFlow 支持两种共享变量的方式：</p><ul><li>显式传递 tf.Variable 对象。</li><li>将 tf.Variable 对象隐式封装在 tf.variable_scope 对象内。</li></ul><h3 id="variable-scope">variable_scope</h3><h4 id="代码示例1">代码示例1</h4><p>使用variable_scope区分weights和biases。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_relu</span><span class="params">(input, kernel_shape, bias_shape)</span>:</span></span><br><span class="line">    <span class="comment"># Create variable named "weights".</span></span><br><span class="line">    weights = tf.get_variable(<span class="string">"weights"</span>, kernel_shape,</span><br><span class="line">        initializer=tf.random_normal_initializer())</span><br><span class="line">    <span class="comment"># Create variable named "biases".</span></span><br><span class="line">    biases = tf.get_variable(<span class="string">"biases"</span>, bias_shape,</span><br><span class="line">        initializer=tf.constant_initializer(<span class="number">0.0</span>))</span><br><span class="line">    conv = tf.nn.conv2d(input, weights,</span><br><span class="line">        strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line">    <span class="keyword">return</span> tf.nn.relu(conv + biases)</span><br></pre></td></tr></table></figure><h4 id="代码示例2">代码示例2</h4><p>使用variable_scope声明不同作用域</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_image_filter</span><span class="params">(input_images)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"conv1"</span>):</span><br><span class="line">        <span class="comment"># Variables created here will be named "conv1/weights", "conv1/biases".</span></span><br><span class="line">        relu1 = conv_relu(input_images, [<span class="number">5</span>, <span class="number">5</span>, <span class="number">32</span>, <span class="number">32</span>], [<span class="number">32</span>])</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"conv2"</span>):</span><br><span class="line">        <span class="comment"># Variables created here will be named "conv2/weights", "conv2/biases".</span></span><br><span class="line">        <span class="keyword">return</span> conv_relu(relu1, [<span class="number">5</span>, <span class="number">5</span>, <span class="number">32</span>, <span class="number">32</span>], [<span class="number">32</span>])</span><br></pre></td></tr></table></figure><h3 id="共享方式1">共享方式1</h3><p>设置reuse=True</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"model"</span>):</span><br><span class="line">  output1 = my_image_filter(input1)</span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"model"</span>, reuse=<span class="literal">True</span>):</span><br><span class="line">  output2 = my_image_filter(input2)</span><br></pre></td></tr></table></figure><h3 id="共享方式2">共享方式2</h3><p>调用scope.reuse_variables触发重用</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"model"</span>) <span class="keyword">as</span> scope:</span><br><span class="line">  output1 = my_image_filter(input1)</span><br><span class="line">  scope.reuse_variables()</span><br><span class="line">  output2 = my_image_filter(input2)</span><br></pre></td></tr></table></figure><h2 id="参考文献">参考文献</h2><p>1.<a href="https://blog.csdn.net/MrR1ght/article/details/81228087" target="_blank" rel="noopener">https://blog.csdn.net/MrR1ght/article/details/81228087</a><br>2.<a href="https://www.tensorflow.org/guide/variables?hl=zh_cn" target="_blank" rel="noopener">https://www.tensorflow.org/guide/variables?hl=zh_cn</a><br>3.<a href="https://www.tensorflow.org/api_docs/python/tf/get_variable?hl=zh_cn" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/get_variable?hl=zh_cn</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;创建variable&quot;&gt;创建Variable&lt;/h2&gt;
&lt;p&gt;Tensorflow有两种方式创建Variable：tf.Variable()和tf.get_variable()，这两种方式获得的都是tensorflow.python.ops.variables.V
      
    
    </summary>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/categories/tensorflow/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow list of placeholder</title>
    <link href="http://mxxhcm.github.io/2019/05/12/tensorflow-list-of-placeholder/"/>
    <id>http://mxxhcm.github.io/2019/05/12/tensorflow-list-of-placeholder/</id>
    <published>2019-05-12T07:55:49.000Z</published>
    <updated>2019-05-12T12:31:11.231Z</updated>
    
    <content type="html"><![CDATA[<h2 id="list-of-placeholder">list of placeholder</h2><h3 id="目的">目的</h3><p>计算图中定义了一个placeholder list，如何使用feed_dict传入值。</p><h3 id="代码示例">代码示例</h3><p><a href="https://github.com/mxxhcm/code/blob/master/tf/ops/tf_placeholder_list.py" target="_blank" rel="noopener">代码地址</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个长度为n的placeholder list</span></span><br><span class="line">n = <span class="number">4</span></span><br><span class="line">ph_list = [tf.placeholder(tf.float32, [<span class="literal">None</span>, <span class="number">10</span>]) <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">4</span>)]</span><br><span class="line"><span class="comment"># 对这个ph list的操作</span></span><br><span class="line">result = tf.Variable(<span class="number">0.0</span>)</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> ph_list:</span><br><span class="line">    result = tf.add(result, x)</span><br><span class="line">hhhh = tf.log(result)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    sess = tf.Session()</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 生成数据</span></span><br><span class="line">    inputs = []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(n):</span><br><span class="line">        x = np.random.rand(<span class="number">16</span>, <span class="number">10</span>)</span><br><span class="line">        inputs.append(x)</span><br><span class="line">    <span class="comment"># 声明一个字典，存放placeholder和value键值对</span></span><br><span class="line">    feed_dictionary = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> k,v <span class="keyword">in</span> zip(ph_list, inputs):</span><br><span class="line">       feed_dictionary[k] = v</span><br><span class="line">    <span class="comment"># feed 数据</span></span><br><span class="line">    print(sess.run(hhhh, feed_dict=feed_dictionary).shape)</span><br></pre></td></tr></table></figure><h2 id="参考文献">参考文献</h2><p>1.<a href="https://stackoverflow.com/questions/51128427/how-to-feed-list-of-values-to-a-placeholder-list-in-tensorflow" target="_blank" rel="noopener">https://stackoverflow.com/questions/51128427/how-to-feed-list-of-values-to-a-placeholder-list-in-tensorflow</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;list-of-placeholder&quot;&gt;list of placeholder&lt;/h2&gt;
&lt;h3 id=&quot;目的&quot;&gt;目的&lt;/h3&gt;
&lt;p&gt;计算图中定义了一个placeholder list，如何使用feed_dict传入值。&lt;/p&gt;
&lt;h3 id=&quot;代码示例&quot;&gt;代
      
    
    </summary>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/categories/tensorflow/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow gather</title>
    <link href="http://mxxhcm.github.io/2019/05/11/tensorflow-gather/"/>
    <id>http://mxxhcm.github.io/2019/05/11/tensorflow-gather/</id>
    <published>2019-05-11T13:03:00.000Z</published>
    <updated>2019-05-12T12:35:59.200Z</updated>
    
    <content type="html"><![CDATA[<h2 id="tf-gather-nd">tf.gather_nd</h2><h3 id="一句话介绍">一句话介绍</h3><p>按照索引将输入tensor的某些维度拼凑成一个新的tenosr</p><h3 id="api">API</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tf.gather_nd(</span><br><span class="line">    params, <span class="comment"># 输入参数</span></span><br><span class="line">    indices, <span class="comment"># 索引</span></span><br><span class="line">    name=<span class="literal">None</span> <span class="comment">#</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>indices是一个K维的整形tensor。<br>indices的最后一维至多和params的rank一样大，如果indices.shape==params.rank，那么对应的是elements，如果indices.shape $\lt$ params.rank，那么对应的是slices。输出的tensor shape是：<br>indices.shape[:-1] + params.shape[indices.shape[-1]:]<br>原文如下：</p><blockquote><p>The last dimension of indices corresponds to elements (if indices.shape[-1] == params.rank) or slices (if indices.shape[-1] &lt; params.rank) along dimension indices.shape[-1] of params. The output tensor has shape<br>indices.shape[:-1] + params.shape[indices.shape[-1]:]</p></blockquote><p>如果indices是两维的，那么就相当于用第二维的indices去访问params，然后indices的第一维度相当于把第二维的tensor放入一个列表。<br>indices是高维（大于两维）的话，反正就是找最后一维的维度，然后到params中找对应的数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">    indices = [[[<span class="number">1</span>]], [[<span class="number">0</span>]]]</span><br><span class="line">    params = [[[<span class="string">'a0'</span>, <span class="string">'b0'</span>], [<span class="string">'c0'</span>, <span class="string">'d0'</span>]],</span><br><span class="line">              [[<span class="string">'a1'</span>, <span class="string">'b1'</span>], [<span class="string">'c1'</span>, <span class="string">'d1'</span>]]]</span><br><span class="line">    output = [[[[<span class="string">'a1'</span>, <span class="string">'b1'</span>], [<span class="string">'c1'</span>, <span class="string">'d1'</span>]]],</span><br><span class="line">              [[[<span class="string">'a0'</span>, <span class="string">'b0'</span>], [<span class="string">'c0'</span>, <span class="string">'d0'</span>]]]]</span><br><span class="line"><span class="comment"># 直接看indices的最后一维，然后到params中找，比如[1]，找params[1]=[['a1', 'b1'], ['c1', 'd1']]],params[0]=[['a0', 'b0'], ['c0', 'd0']]。然后在组成output，shape怎么确定？我的理解是，直接用params[1]的结果去替换indices中的[1]，也就是[[params[1]]]</span></span><br><span class="line"></span><br><span class="line">    indices = [[[<span class="number">0</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">0</span>]], [[<span class="number">0</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">1</span>]]]</span><br><span class="line">    params = [[[<span class="string">'a0'</span>, <span class="string">'b0'</span>], [<span class="string">'c0'</span>, <span class="string">'d0'</span>]],</span><br><span class="line">              [[<span class="string">'a1'</span>, <span class="string">'b1'</span>], [<span class="string">'c1'</span>, <span class="string">'d1'</span>]]]</span><br><span class="line">    output = [[[<span class="string">'c0'</span>, <span class="string">'d0'</span>], [<span class="string">'a1'</span>, <span class="string">'b1'</span>]],</span><br><span class="line">              [[<span class="string">'a0'</span>, <span class="string">'b0'</span>], [<span class="string">'c1'</span>, <span class="string">'d1'</span>]]]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    indices = [[[<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>]], [[<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>]]]</span><br><span class="line">    params = [[[<span class="string">'a0'</span>, <span class="string">'b0'</span>], [<span class="string">'c0'</span>, <span class="string">'d0'</span>]],</span><br><span class="line">              [[<span class="string">'a1'</span>, <span class="string">'b1'</span>], [<span class="string">'c1'</span>, <span class="string">'d1'</span>]]]</span><br><span class="line">    output = [[<span class="string">'b0'</span>, <span class="string">'b1'</span>], [<span class="string">'d0'</span>, <span class="string">'c1'</span>]]</span><br></pre></td></tr></table></figure><h3 id="代码示例1">代码示例1</h3><p><a href>代码地址</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line"></span><br><span class="line">data = np.array([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>],</span><br><span class="line">          [<span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>],</span><br><span class="line">          [<span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>, <span class="number">16</span>, <span class="number">17</span>],</span><br><span class="line">          [<span class="number">18</span>, <span class="number">19</span>, <span class="number">20</span>, <span class="number">21</span>, <span class="number">22</span>, <span class="number">23</span>],</span><br><span class="line">          [<span class="number">24</span>, <span class="number">25</span>, <span class="number">26</span>, <span class="number">27</span>, <span class="number">28</span>, <span class="number">29</span>]])</span><br><span class="line">data = np.reshape(np.arange(<span class="number">30</span>), [<span class="number">5</span>, <span class="number">6</span>])</span><br><span class="line">x = tf.constant(data)</span><br><span class="line">print(sess.run(x))</span><br><span class="line"><span class="comment"># [[ 0  1  2  3  4  5]</span></span><br><span class="line"><span class="comment">#  [ 6  7  8  9 10 11]</span></span><br><span class="line"><span class="comment">#  [12 13 14 15 16 17]</span></span><br><span class="line"><span class="comment">#  [18 19 20 21 22 23]</span></span><br><span class="line"><span class="comment">#  [24 25 26 27 28 29]]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Collecting elements from a tensor of rank 2</span></span><br><span class="line">result = tf.gather_nd(x, [<span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line">print(sess.run(result))</span><br><span class="line"><span class="comment"># indices.shape=(2,), indices.shape[:-1]=(), indices.shape[-1]=2, params.shape=(5,6), params.shape[indices.shape[-1]:]=(), outputs.shape=()+() = () </span></span><br><span class="line"><span class="comment"># 8 </span></span><br><span class="line">result = tf.gather_nd(x, [[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">2</span>,<span class="number">3</span>]])</span><br><span class="line">print(sess.run(result))</span><br><span class="line"><span class="comment"># indices.shape=(2,2), indices.shape[:-1]=(2,), indices.shape[-1]=2, params.shape=(5,6), params.shape[indices.shape[-1]:]=(), outputs.shape=(2,)+() = (2,) </span></span><br><span class="line"><span class="comment"># [8, 15]</span></span><br><span class="line"><span class="comment"># Collecting rows from a tensor of rank 2</span></span><br><span class="line">result = tf.gather_nd(x, [[<span class="number">1</span>],[<span class="number">2</span>]])</span><br><span class="line">print(sess.run(result))</span><br><span class="line"><span class="comment"># indices.shape=(2, 1), indices.shape[:-1]=(2,), indices.shape[-1]=1, params.shape=(5,6), params.shape[indices.shape[-1]:]=(6,), outputs.shape=(2,)+(6,) = (2,6,) </span></span><br><span class="line"><span class="comment"># [[ 6  7  8  9 10 11]</span></span><br><span class="line"><span class="comment">#  [12 13 14 15 16 17]]</span></span><br></pre></td></tr></table></figure><h3 id="代码示例2">代码示例2</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line"></span><br><span class="line">data = np.array([[[<span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">          [<span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">          [<span class="number">4</span>, <span class="number">5</span>]],</span><br><span class="line">         [[<span class="number">6</span>, <span class="number">7</span>],</span><br><span class="line">          [<span class="number">8</span>, <span class="number">9</span>],</span><br><span class="line">          [<span class="number">10</span>,<span class="number">11</span>]]])</span><br><span class="line">data = np.reshape(np.arange(<span class="number">12</span>), [<span class="number">2</span>, <span class="number">3</span>, <span class="number">2</span>])</span><br><span class="line">x = tf.constant(data)</span><br><span class="line">print(sess.run(x))</span><br><span class="line"><span class="comment">#[[[ 0  1]</span></span><br><span class="line"><span class="comment">#  [ 2  3]</span></span><br><span class="line"><span class="comment">#  [ 4  5]]</span></span><br><span class="line"><span class="comment"># [[ 6  7]</span></span><br><span class="line"><span class="comment">#  [ 8  9]</span></span><br><span class="line"><span class="comment">#  [10 11]]]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Collecting elements from a tensor of rank 3</span></span><br><span class="line">result = tf.gather_nd(x, [[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>]])</span><br><span class="line">print(sess.run(result))</span><br><span class="line"><span class="comment"># indices.shape=(2, 3), indices.shape[:-1]=(2,), indices.shape[-1]=3, params.shape=(2, 3, 2), params.shape[indices.shape[-1]:]=(), outputs.shape=(2,)+() = (2,) </span></span><br><span class="line"><span class="comment"># [0 11]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Collecting batched rows from a tensor of rank 3</span></span><br><span class="line">result = tf.gather_nd(x, [[[<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">1</span>]], [[<span class="number">1</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">1</span>]]])</span><br><span class="line">print(sess.run(result))</span><br><span class="line"><span class="comment"># indices.shape=(2, 2, 2), indices.shape[:-1]=(2, 2, ), indices.shape[-1]=2, params.shape=(2, 3, 2), params.shape[indices.shape[-1]:]=(2,), outputs.shape=(2, 2)+(2, ) = (2, 2, 2) </span></span><br><span class="line"><span class="comment"># [[[0 1]</span></span><br><span class="line"><span class="comment">#  [2 3]]</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># [[6 7]</span></span><br><span class="line"><span class="comment">#  [8 9]]]</span></span><br><span class="line"></span><br><span class="line">result = tf.gather_nd(x, [[<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">1</span>]])</span><br><span class="line">print(sess.run(result))</span><br><span class="line"><span class="comment"># indices.shape=(4, 2), indices.shape[:-1]=(4,), indices.shape[-1]=2, params.shape=(2, 3, 2), params.shape[indices.shape[-1]:]=(2,), outputs.shape=(4,)+(2,) = (4, 2) </span></span><br><span class="line"><span class="comment"># [[0 1]</span></span><br><span class="line"><span class="comment">#  [2 3]</span></span><br><span class="line"><span class="comment">#  [6 7]</span></span><br><span class="line"><span class="comment">#  [8 9]]</span></span><br></pre></td></tr></table></figure><h2 id="参考文献">参考文献</h2><p>1.<a href="https://www.tensorflow.org/api_docs/python/tf/gather_nd" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/gather_nd</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;tf-gather-nd&quot;&gt;tf.gather_nd&lt;/h2&gt;
&lt;h3 id=&quot;一句话介绍&quot;&gt;一句话介绍&lt;/h3&gt;
&lt;p&gt;按照索引将输入tensor的某些维度拼凑成一个新的tenosr&lt;/p&gt;
&lt;h3 id=&quot;api&quot;&gt;API&lt;/h3&gt;
&lt;figure class
      
    
    </summary>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/categories/tensorflow/"/>
    
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow cond</title>
    <link href="http://mxxhcm.github.io/2019/05/10/tensorflow-cond/"/>
    <id>http://mxxhcm.github.io/2019/05/10/tensorflow-cond/</id>
    <published>2019-05-10T09:01:14.000Z</published>
    <updated>2019-05-12T12:35:04.910Z</updated>
    
    <content type="html"><![CDATA[<h2 id="tf-cond">tf.cond</h2><h3 id="一句话介绍">一句话介绍</h3><p>和if语句的功能和很像，如果条件为真，返回一个函数，如果条件为假，返回另一个函数。</p><h3 id="api">API</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">tf.cond(</span><br><span class="line">    pred, <span class="comment"># 条件</span></span><br><span class="line">    true_fn=<span class="literal">None</span>, <span class="comment"># 如果条件为真，执行该函数</span></span><br><span class="line">    false_fn=<span class="literal">None</span>, <span class="comment"># 如果条件为假，执行该函数</span></span><br><span class="line">    strict=<span class="literal">False</span>,</span><br><span class="line">    name=<span class="literal">None</span>,</span><br><span class="line">    fn1=<span class="literal">None</span>,</span><br><span class="line">    fn2=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>最后返回的是true_fn或者false_fn返回的还是tf.Tensor类型的变量。</p><h3 id="代码示例1">代码示例1</h3><p><a href>代码地址</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x = tf.placeholder(tf.int32, [<span class="number">10</span>])</span><br><span class="line">y = tf.constant([<span class="number">10</span>, <span class="number">3.2</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># for i in range(10):</span></span><br><span class="line"><span class="comment">#     if tf.equal(x[i], 0):</span></span><br><span class="line"><span class="comment">#         y = tf.add(y, 1)</span></span><br><span class="line"><span class="comment">#     else:</span></span><br><span class="line"><span class="comment">#         y = tf.add(y, 10)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 上面的代码起到了和下面代码相同的作用，但是上面的代码在tensorflow中会报错，不能运行，因为x[i]==0返回的不是python的bool类型，而是bool类型的tf.Tensor。</span></span><br><span class="line"><span class="comment"># TypeError: Using a tf.Tensor as a Python bool is not allowed.</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    y = tf.cond(tf.equal(x[i], <span class="number">0</span>), <span class="keyword">lambda</span>: tf.add(y, <span class="number">1</span>), <span class="keyword">lambda</span>: tf.add(y, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">result = tf.log(y)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">   inputs = np.array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>])</span><br><span class="line">   print(sess.run(result, feed_dict=&#123;x: inputs&#125;))</span><br></pre></td></tr></table></figure><h3 id="代码示例2">代码示例2</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">myfunc</span><span class="params">(x)</span>:</span></span><br><span class="line">   <span class="keyword">if</span> (x &gt; <span class="number">0</span>):</span><br><span class="line">      <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">   <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    x = tf.constant(<span class="number">4</span>)</span><br><span class="line">    <span class="comment"># print(myfunc(x))</span></span><br><span class="line">    <span class="comment"># raise TypeError("Using a `tf.Tensor` as a Python `bool` is not allowed. "</span></span><br><span class="line">    <span class="comment"># TypeError: Using a `tf.Tensor` as a Python `bool` is not allowed. Use `if t is not None:` instead of `if t:` to test if a tensor is defined, and use TensorFlow ops such as tf.cond to execute subgraphs conditioned on the value of a tensor.</span></span><br><span class="line">    result = tf.cond(tf.greater(x, <span class="number">0</span>), <span class="keyword">lambda</span>: <span class="number">1</span>, <span class="keyword">lambda</span>: <span class="number">0</span>)</span><br><span class="line">    print(type(result))</span><br><span class="line">    print(result.eval())</span><br></pre></td></tr></table></figure><p>上述代码中定义了一个函数，实现判断某个值是否大于0。但是这个函数是错误的，因为$x\gt 0$返回一个bool类型的tf.Tensor不能用作if的判断条件，所以需要使用tf.cond语句。</p><h3 id="代码示例3">代码示例3</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Example 3</span></span><br><span class="line">x = tf.constant(<span class="number">4</span>)</span><br><span class="line">y = tf.constant(<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(x) </span><br><span class="line">    print(y) </span><br><span class="line">    <span class="keyword">if</span> x == y:</span><br><span class="line">      print(<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      print(<span class="literal">False</span>)</span><br><span class="line">    result = tf.equal(x, y)</span><br><span class="line">    print(result.eval())</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">f1</span><span class="params">()</span>:</span> </span><br><span class="line">      print(<span class="string">"f1 declare"</span>)</span><br><span class="line">      <span class="keyword">return</span> [<span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">f2</span><span class="params">()</span>:</span></span><br><span class="line">      print(<span class="string">"f2 declare"</span>)</span><br><span class="line">      <span class="keyword">return</span> [<span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">    res = tf.cond(tf.equal(x, y), f1, f2)</span><br><span class="line">    print(res)</span><br></pre></td></tr></table></figure><h2 id="参考文献">参考文献</h2><p>1.<a href="https://www.tensorflow.org/api_docs/python/tf/cond" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/cond</a><br>2.<a href="https://stackoverflow.com/questions/48571521/tensorflow-error-using-a-tf-tensor-as-a-python-bool-is-not-allowed" target="_blank" rel="noopener">https://stackoverflow.com/questions/48571521/tensorflow-error-using-a-tf-tensor-as-a-python-bool-is-not-allowed</a><br>3.<a href="https://blog.csdn.net/Cerisier/article/details/79819248" target="_blank" rel="noopener">https://blog.csdn.net/Cerisier/article/details/79819248</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;tf-cond&quot;&gt;tf.cond&lt;/h2&gt;
&lt;h3 id=&quot;一句话介绍&quot;&gt;一句话介绍&lt;/h3&gt;
&lt;p&gt;和if语句的功能和很像，如果条件为真，返回一个函数，如果条件为假，返回另一个函数。&lt;/p&gt;
&lt;h3 id=&quot;api&quot;&gt;API&lt;/h3&gt;
&lt;figure class
      
    
    </summary>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/categories/tensorflow/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>python cv2.imresize图像缩放</title>
    <link href="http://mxxhcm.github.io/2019/05/09/python-cv2-imresize/"/>
    <id>http://mxxhcm.github.io/2019/05/09/python-cv2-imresize/</id>
    <published>2019-05-09T13:37:30.000Z</published>
    <updated>2019-05-10T11:37:24.676Z</updated>
    
    <content type="html"><![CDATA[<h2 id="cv2-resize"><a href="#cv2-resize" class="headerlink" title="cv2.resize"></a>cv2.resize</h2><p>cv2是python的opencv包，实现的功能是对一个图片进行缩放。<br>python3下安装命令：<br>~$:pip install opencv-python</p><h3 id="示例代码"><a href="#示例代码" class="headerlink" title="示例代码"></a>示例代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">img = np.random.rand(<span class="number">210</span>, <span class="number">160</span> ,<span class="number">3</span>)</span><br><span class="line">print(img.shape)</span><br><span class="line">img_scale = cv2.resize(img, (<span class="number">84</span>, <span class="number">84</span>))</span><br><span class="line">print(img_scale.shape)</span><br></pre></td></tr></table></figure><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;cv2-resize&quot;&gt;&lt;a href=&quot;#cv2-resize&quot; class=&quot;headerlink&quot; title=&quot;cv2.resize&quot;&gt;&lt;/a&gt;cv2.resize&lt;/h2&gt;&lt;p&gt;cv2是python的opencv包，实现的功能是对一个图片进行缩放。&lt;br
      
    
    </summary>
    
      <category term="python" scheme="http://mxxhcm.github.io/categories/python/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="opencv" scheme="http://mxxhcm.github.io/tags/opencv/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow model save load</title>
    <link href="http://mxxhcm.github.io/2019/05/09/tensorflow-model-save-load/"/>
    <id>http://mxxhcm.github.io/2019/05/09/tensorflow-model-save-load/</id>
    <published>2019-05-09T07:14:26.000Z</published>
    <updated>2019-09-28T10:25:35.998Z</updated>
    
    <content type="html"><![CDATA[<h2 id="tf-train-saver保存和恢复模型">tf.train.Saver保存和恢复模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">saver = tf.train.Saver()</span><br><span class="line">saver.save()</span><br></pre></td></tr></table></figure><p>调用上述代码之后会存存储以下几个文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">checkpoint</span><br><span class="line">model.ckpt.data-00000-of-00001</span><br><span class="line">model.ckpt.index</span><br><span class="line">model.ckpt.meta</span><br></pre></td></tr></table></figure><p>其中checkpoint文件存储的是最近保存的文件的名字，meta文件存放的是计算图的定义，index和data文件存放的是权重文件。</p><p>下面介绍一下上述代码中出现的两个API，tf.train.Saver()和tf.train.Saver().save()。</p><h3 id="tf-train-saver">tf.train.Saver()</h3><p>Saver是类，不是函数。可以用来保存，恢复variable和model，Saver对象提供save()和restore()等函数，save()保存模型，restore()加载模型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">__init__(</span><br><span class="line">    var_list=<span class="literal">None</span>, <span class="comment"># 指定要保存的variablelist</span></span><br><span class="line">    reshape=<span class="literal">False</span>,</span><br><span class="line">    sharded=<span class="literal">False</span>,</span><br><span class="line">    max_to_keep=<span class="number">5</span>, <span class="comment"># 最多保留最近的几个checkpoints</span></span><br><span class="line">    keep_checkpoint_every_n_hours=<span class="number">10000.0</span>,</span><br><span class="line">    name=<span class="literal">None</span>,</span><br><span class="line">    restore_sequentially=<span class="literal">False</span>,</span><br><span class="line">    saver_def=<span class="literal">None</span>,</span><br><span class="line">    builder=<span class="literal">None</span>,</span><br><span class="line">    defer_build=<span class="literal">False</span>,</span><br><span class="line">    allow_empty=<span class="literal">False</span>,</span><br><span class="line">    write_version=tf.train.SaverDef.V2,</span><br><span class="line">    pad_step_number=<span class="literal">False</span>,</span><br><span class="line">    save_relative_paths=<span class="literal">False</span>,</span><br><span class="line">    filename=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="tf-train-saver-save">tf.train.Saver.save()</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">save(</span><br><span class="line">    sess,       \\传入当前要保存的session</span><br><span class="line">    save_path,  \\指定checkpoint的路径</span><br><span class="line">    global_step=<span class="literal">None</span>,   \\当前存的model的step</span><br><span class="line">    latest_filename=<span class="literal">None</span>,</span><br><span class="line">    meta_graph_suffix=<span class="string">'meta'</span>,</span><br><span class="line">    write_meta_graph=<span class="literal">True</span>,  \\指定是否要保存计算图</span><br><span class="line">    write_state=<span class="literal">True</span>,</span><br><span class="line">    strip_default_attrs=<span class="literal">False</span>,</span><br><span class="line">    save_debug_info=<span class="literal">False</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>这里说一下save_path，如果不指定的话，文件名默认是空的，在linux下是以.开头的（即当前目录），所以会显示成隐藏文件。通常情况下我们指定checkpoint要保存的路径，以及名字，比如叫model.ckpt，在load的时候还使用这个名字就行。指定了global_step之后，tf会自动在路径后面加上step进行区分。</p><h2 id="读取graph">读取graph</h2><h3 id="读取图的定义">读取图的定义</h3><p>meta文件中存放了计算图的定义，可以直接使用API tf.train.import_meta_graph()函数调用：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    saver = tf.train.import_meta_graph(<span class="string">"model.ckpt.meta"</span>)</span><br></pre></td></tr></table></figure><p>这时计算图就已经定义在当前sess中了。上述代码会保留原始的device信息，如果迁移到其他设备时，可能由于没有指定设备出错，这个问题可以通过指定一个特殊的参数clear_devices解决：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    saver = tf.train.import_meta_graph(<span class="string">"model.ckpt.meta"</span>, clear_devices=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>这样子就和device无关了。</p><h3 id="访问graph中的参数">访问graph中的参数</h3><h4 id="通过collection访问计算图中collection的键">通过collection访问计算图中collection的键</h4><p>这里的键指的是graph中都有哪些<a href>collections</a>。</p><ul><li><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(sess.graph.get_all_collection_keys())</span><br></pre></td></tr></table></figure></li><li><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(sess.graph.collections)</span><br></pre></td></tr></table></figure></li><li><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.get_default_graph().get_all_collection_keys()</span><br></pre></td></tr></table></figure></li></ul><h4 id="访问collection">访问collection</h4><ul><li><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sess.graph.get_collection(<span class="string">"summaries"</span>)</span><br></pre></td></tr></table></figure></li><li><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.get_collection(<span class="string">""</span>)</span><br></pre></td></tr></table></figure></li></ul><p>示例</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment">#saver = tf.train.Saver()</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    new_saver = tf.train.import_meta_graph(<span class="string">"saver1.ckpt.meta"</span>)</span><br><span class="line">    print(sess.graph)</span><br><span class="line">    <span class="keyword">for</span> var <span class="keyword">in</span> tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES):</span><br><span class="line">        print(var)</span><br></pre></td></tr></table></figure><h4 id="通过operation访问">通过operation访问</h4><ul><li><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sess.graph.get_opeartions()</span><br></pre></td></tr></table></figure></li><li><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> op <span class="keyword">in</span> sess.graph.get_opeartions():</span><br><span class="line">    print(op.name, op.values())</span><br></pre></td></tr></table></figure></li><li><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">sess.graph.get_operation_by_name(<span class="string">"op_name"</span>).node_def</span><br><span class="line">``` </span><br><span class="line"></span><br><span class="line"><span class="comment">## 保存和恢复variables</span></span><br><span class="line"><span class="comment">### 保存和恢复全部variables</span></span><br><span class="line">- 恢复variable时，无需初始化。</span><br><span class="line">- 恢复variable时，使用的是variable的name，不是op的name。只要知道variable的name即可。save和restore的op name不需要相同，只要variable name相同即可。</span><br><span class="line">- 对于使用tf.Variable()创建的variable，如果没有指定variable名字的话，系统会为其生成默认名字，在恢复的时候，需要使用tf.get_variable()恢复variable，同时传variable name和shape。</span><br><span class="line"></span><br><span class="line"><span class="comment">#### 保存全部variables</span></span><br><span class="line">``` python</span><br><span class="line">saver = tf.train.Saver()</span><br><span class="line">saver.save(sess, save_path) <span class="comment"># 需要指定的是checkpoint的名字而不是目录</span></span><br></pre></td></tr></table></figure></li></ul><h4 id="恢复全部variables">恢复全部variables</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">saver = tf.train.Saver()</span><br><span class="line">saver.restore(sess, save_path)</span><br></pre></td></tr></table></figure><h3 id="保存和恢复部分variables">保存和恢复部分variables</h3><h4 id="保存全部variable">保存全部variable</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">saver = tf.train.Saver(&#123;<span class="string">"variable_name1"</span>: op_name1,..., <span class="string">"variable_namen"</span>: op_namen&#125;)</span><br><span class="line">saver.save(sess, save_path) <span class="comment"># 需要指定的是checkpoint的名字而不是目录</span></span><br></pre></td></tr></table></figure><h4 id="恢复全部variable">恢复全部variable</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">saver = tf.train.Saver(&#123;<span class="string">"variable_name1"</span>: op_name1,..., <span class="string">"variable_namen"</span>: op_namen&#125;)</span><br><span class="line">saver.restore(sess, save_path)</span><br></pre></td></tr></table></figure><h2 id="保存和恢复模型">保存和恢复模型</h2><p>其实和保存恢复变量没有什么区别。只是把整个模型的variables都save和restore了。</p><h2 id="代码示例">代码示例</h2><p><a href="https://github.com/mxxhcm/code/tree/master/tf/some_ops/saver_restore" target="_blank" rel="noopener">代码地址</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">graph = tf.Graph()</span><br><span class="line"><span class="keyword">with</span> graph.as_default():</span><br><span class="line">    W = tf.Variable([<span class="number">0.3</span>], dtype=tf.float32)</span><br><span class="line">    b = tf.Variable([<span class="number">-0.3</span>], dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># input and output</span></span><br><span class="line">    x = tf.placeholder(tf.float32)</span><br><span class="line">    y = tf.placeholder(tf.float32)</span><br><span class="line">    predicted_y = W*x+b</span><br><span class="line"></span><br><span class="line">    <span class="comment"># MSE loss</span></span><br><span class="line">    loss = tf.reduce_mean(tf.square(y - predicted_y))</span><br><span class="line">    <span class="comment"># optimizer</span></span><br><span class="line">    optimizer = tf.train.GradientDescentOptimizer(<span class="number">0.01</span>)</span><br><span class="line">    train_op = optimizer.minimize(loss)</span><br><span class="line"></span><br><span class="line">inputs = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]</span><br><span class="line">outputs = [<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session(graph=graph) <span class="keyword">as</span> sess:</span><br><span class="line">    saver = tf.train.Saver()</span><br><span class="line">    init_op = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5000</span>):</span><br><span class="line">        sess.run(train_op, feed_dict=&#123;x: inputs, y: outputs&#125;)</span><br><span class="line">    l_, W_, b_ = sess.run([loss, W, b], feed_dict=&#123;x: inputs, y: outputs&#125;)</span><br><span class="line">    print(<span class="string">"loss: "</span>, l_, <span class="string">"w: "</span>, W_, <span class="string">"b:"</span>, b_)</span><br><span class="line">    checkpoint = <span class="string">"./checkpoint/saver1.ckpt"</span></span><br><span class="line">    save_path = saver.save(sess, checkpoint)</span><br><span class="line">    print(<span class="string">"Model has been saved in %s."</span> % save_path)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session(graph=graph) <span class="keyword">as</span> sess:</span><br><span class="line">    saver = tf.train.Saver()</span><br><span class="line">    saver.restore(sess, checkpoint)</span><br><span class="line">    l_, W_, b_ = sess.run([loss, W, b], feed_dict=&#123;x: inputs, y: outputs&#125;)</span><br><span class="line">    print(<span class="string">"loss: "</span>, l_, <span class="string">"w: "</span>, W_, <span class="string">"b:"</span>, b_)</span><br><span class="line">    print(<span class="string">"Model has been restored."</span>)</span><br></pre></td></tr></table></figure><h2 id="获取最新的checkpoint文件">获取最新的checkpoint文件</h2><h3 id="tf-train-get-checkpoint-state">tf.train.get_checkpoint_state()</h3><p>给出checkpoint文件所在目录，可以使用get_checkpoint_state()获得最新的checkpoint文件：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ckpt = tf.train.get_checkpoint_state(checkpoint_dir)</span><br><span class="line"><span class="keyword">if</span> ckpt <span class="keyword">and</span> ckpt.model_checkpoint_path:</span><br><span class="line">    save.restore(sess, ckpt.model_checkpoint_path)</span><br></pre></td></tr></table></figure><h3 id="使用inspect-checkpoint库">使用inspect_checkpoint库</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># import the inspect_checkpoint library</span></span><br><span class="line"><span class="keyword">from</span> tensorflow.python.tools <span class="keyword">import</span> inspect_checkpoint <span class="keyword">as</span> chkp</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印checkpoint文件中所有variable</span></span><br><span class="line">chkp.print_tensors_in_checkpoint_file(<span class="string">"saver/variables/all_variables.ckpt"</span>, tensor_name=<span class="string">''</span>, all_tensors=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印变量"v1"</span></span><br><span class="line">chkp.print_tensors_in_checkpoint_file(<span class="string">"saver/variables/all_variables.ckpt"</span>, tensor_name=<span class="string">'v1'</span>, all_tensors=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">chkp.print_tensors_in_checkpoint_file(<span class="string">"saver/variables/all_variables.ckpt"</span>, tensor_name=<span class="string">'v2'</span>, all_tensors=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><h2 id="模型的冻结">模型的冻结</h2><p>模型的冻结是不在训练模型，只用于正向推导，所以把变量转换成常量后，和计算图一起保存在协议缓冲区文件(.pb)文件中，因此需要在计算图中预先定义输出节点的名称，示例如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">output_nodes = [<span class="string">"Accuracy/prediction"</span>, <span class="string">"Metric/Dice"</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载计算图</span></span><br><span class="line">saver = tf.train.import_meta_graph(<span class="string">"model.ckpt.meta"</span>, clear_devices=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    input_graph_def = sess.graph.as_graph_def()</span><br><span class="line">    <span class="comment"># load model</span></span><br><span class="line">    saver.restore(sess, <span class="string">"model.ckpt"</span>)</span><br><span class="line">    <span class="comment"># 将变量转换为常量</span></span><br><span class="line">    output_graph_def = tf.graph_util.convert_variables_to_constants(sess, input_graph_def, output_nodes)</span><br><span class="line">    <span class="comment"># 写入pb文件</span></span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">"frozen_model.pb"</span>, <span class="string">"wb"</span>) <span class="keyword">as</span> f:</span><br><span class="line">        f.write(output_graph_def.SerializeToString())</span><br></pre></td></tr></table></figure><h2 id="模型的执行">模型的执行</h2><p>从协议缓冲区文件(.pb)文件中读取模型，导入计算图</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 读取模型并保存到序列化模型对象中</span></span><br><span class="line"><span class="keyword">with</span> open(frozen_graph_path, <span class="string">"rb"</span>) <span class="keyword">as</span> f:</span><br><span class="line">    graph_def = tf.GraphDef()</span><br><span class="line">    graph_def.ParseFromString(f.read())</span><br><span class="line"><span class="comment"># 导入计算图</span></span><br><span class="line">graph = tf.Graph()</span><br><span class="line"><span class="keyword">with</span> graph.as_default():</span><br><span class="line">    tf.import_graph_def(graph_def, name=<span class="string">"Test"</span>)</span><br></pre></td></tr></table></figure><p>获取输入和输出的张量，然后将测试数据feed给输入张量，得到结果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">x_tensor = graph.get_tensor_by_name(<span class="string">"Test/input/image-input:0"</span>)</span><br><span class="line">y_tensor = graph.get_tensor_by_name(<span class="string">"Test/input/label-input:0"</span>)</span><br><span class="line">keep_prob = graph.get_tensor_by_name(<span class="string">"Test/dropout/Placeholder:0"</span>)</span><br><span class="line">acc_op = graph.get_tensor_by_name(<span class="string">"Test/accuracy/prediction:0"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"mnist_data"</span>, one_hot=<span class="literal">True</span>)</span><br><span class="line">x_values, y_values = mnist.test.next_batch(<span class="number">10000</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session(graph=graph) <span class="keyword">as</span> sess:</span><br><span class="line">    accuracy = sess.run(acc_op, feed_dict=&#123;x_tensor: x_values,</span><br><span class="line">                                          y_tensor: y_values,</span><br><span class="line">                                          keep_prob: <span class="number">1.0</span>&#125;)</span><br><span class="line">    print(accuracy)</span><br></pre></td></tr></table></figure><h2 id="参考文献">参考文献</h2><p>1.<a href="https://www.jarvis73.cn/2018/04/25/Tensorflow-Model-Save-Read/" target="_blank" rel="noopener">https://www.jarvis73.cn/2018/04/25/Tensorflow-Model-Save-Read/</a><br>2.<a href="https://www.tensorflow.org/guide/saved_model" target="_blank" rel="noopener">https://www.tensorflow.org/guide/saved_model</a><br>3.<a href="https://www.tensorflow.org/api_docs/python/tf/train/Saver" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/train/Saver</a><br>4.<a href="https://www.bilibili.com/read/cv681031/" target="_blank" rel="noopener">https://www.bilibili.com/read/cv681031/</a><br>5.<a href="https://cv-tricks.com/tensorflow-tutorial/save-restore-tensorflow-models-quick-complete-tutorial/" target="_blank" rel="noopener">https://cv-tricks.com/tensorflow-tutorial/save-restore-tensorflow-models-quick-complete-tutorial/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;tf-train-saver保存和恢复模型&quot;&gt;tf.train.Saver保存和恢复模型&lt;/h2&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;l
      
    
    </summary>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/categories/tensorflow/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/tags/tensorflow/"/>
    
      <category term="Saver" scheme="http://mxxhcm.github.io/tags/Saver/"/>
    
      <category term="save" scheme="http://mxxhcm.github.io/tags/save/"/>
    
      <category term="restore" scheme="http://mxxhcm.github.io/tags/restore/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow reduction</title>
    <link href="http://mxxhcm.github.io/2019/05/09/tensorflow-reduction/"/>
    <id>http://mxxhcm.github.io/2019/05/09/tensorflow-reduction/</id>
    <published>2019-05-09T03:19:00.000Z</published>
    <updated>2019-05-12T12:36:46.605Z</updated>
    
    <content type="html"><![CDATA[<h2 id="tf-reduction">tf.Reduction</h2><ul><li>tf.reduce_sum(input_tensor, reduction_indices=None, keep_dims=False, name=None)  # 计算input_tensor的和，可指定dim。</li><li>tf.reduce_mean(input_tensor, reduction_indices=None, keep_dims=False, name=None) # 计算input_tensor的均值，可指定dim。</li><li>tf.reduce_min(input_tensor, reduction_indices=None, keep_dims=False, name=None) # 计算input_tensor的最小值，可指定dim。</li><li>tf.reduce_max(input_tensor, reduction_indices=None, keep_dims=False, name=None) # 计算input_tensor的最大值，可指定dim。</li><li>tf.recude_proc(input_tensor, reduction_indices=None, keep_dims=False, name=None) # 计算input_tensor的乘积，可指定dim。</li><li>tf.reduce_all(input_tensor, reduction_indices=None, keep_dims=False, name=None) # 计算input_tensor中所有元素的逻辑与，可指定dim。</li><li>tf.reduce_any(input_tensor, reduction_indices=None, keep_dims=False, name=None) # 计算input_tensor的所有元素的逻辑或，可指定dim。</li><li>tf.accumulate_n(inputs, shape=None, tensor_dtype=None, name=None) # 计算inputs的和。</li><li>tf.cumsum(x, axis=0, exclusive=False, reverse=False, name=None) # 计算input_tensor的累积和。</li></ul><h2 id="代码示例">代码示例</h2><h3 id="tf-reduce-sum">tf.reduce_sum()</h3><p><a href="https://github.com/mxxhcm/code/blob/master/tf/some_ops/tf_reduce_sum.py" target="_blank" rel="noopener">代码地址</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x = tf.placeholder(dtype=tf.float32, shape=[<span class="literal">None</span>, <span class="number">2</span>])</span><br><span class="line">y = tf.log(x)</span><br><span class="line"><span class="comment"># 对所有y求和</span></span><br><span class="line">loss = tf.reduce_sum(y)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess :</span><br><span class="line">    <span class="comment"># inputs = tf.constant([1.0, 2.0])</span></span><br><span class="line">    inputs = np.array([[<span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line">    l = sess.run(loss, feed_dict=&#123;x: inputs&#125;)</span><br><span class="line">    print(l)</span><br></pre></td></tr></table></figure><h2 id="参考文献">参考文献</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;tf-reduction&quot;&gt;tf.Reduction&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;tf.reduce_sum(input_tensor, reduction_indices=None, keep_dims=False, name=None)  # 计算input_
      
    
    </summary>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/categories/tensorflow/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow assign</title>
    <link href="http://mxxhcm.github.io/2019/05/08/tensorflow-assign/"/>
    <id>http://mxxhcm.github.io/2019/05/08/tensorflow-assign/</id>
    <published>2019-05-08T12:48:06.000Z</published>
    <updated>2019-05-08T13:51:32.261Z</updated>
    
    <content type="html"><![CDATA[<h2 id="tf-assign">tf.assign</h2><h3 id="简单解释">简单解释</h3><p>op = x.assign(y)<br>将y的值赋值给x，执行sess.run(op)后，x的值就变成和y一样了。</p><h3 id="代码示例">代码示例</h3><p><a href="https://github.com/mxxhcm/code/blob/master/tf/some_ops/tf_assign.py" target="_blank" rel="noopener">代码地址</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 声明两个Variable</span></span><br><span class="line">x1 = tf.Variable([<span class="number">3</span>,<span class="number">4</span>])</span><br><span class="line">x2 = tf.Variable([<span class="number">9</span>,<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># y是将x2 assign 给x1的op</span></span><br><span class="line">y = x1.assign(x2)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  sess.run(tf.global_variables_initializer())</span><br><span class="line">  xx1 = sess.run(x1)</span><br><span class="line">  <span class="comment"># 输出x1</span></span><br><span class="line">  print(xx1)</span><br><span class="line">  <span class="comment"># [3 4]</span></span><br><span class="line"></span><br><span class="line">  xx2 = sess.run(x2)</span><br><span class="line">  <span class="comment"># 输出x2</span></span><br><span class="line">  print(xx2)</span><br><span class="line">  <span class="comment"># [9 1]</span></span><br><span class="line"></span><br><span class="line">  print(sess.run(x1))</span><br><span class="line">  <span class="comment"># [3 4]</span></span><br><span class="line">  </span><br><span class="line">  <span class="comment"># 执行y操作</span></span><br><span class="line">  yy = sess.run(y)</span><br><span class="line">  print(yy)</span><br><span class="line">  <span class="comment"># [9 1]</span></span><br><span class="line">  </span><br><span class="line">  <span class="comment"># 发现x1已经用x2赋值了</span></span><br><span class="line">  print(sess.run(x1))</span><br><span class="line">  <span class="comment"># [9 1]</span></span><br><span class="line">  print(sess.run(x2))</span><br><span class="line">  <span class="comment"># [9 1]</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;tf-assign&quot;&gt;tf.assign&lt;/h2&gt;
&lt;h3 id=&quot;简单解释&quot;&gt;简单解释&lt;/h3&gt;
&lt;p&gt;op = x.assign(y)&lt;br&gt;
将y的值赋值给x，执行sess.run(op)后，x的值就变成和y一样了。&lt;/p&gt;
&lt;h3 id=&quot;代码示例&quot;&gt;代码
      
    
    </summary>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/categories/tensorflow/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow Tensor</title>
    <link href="http://mxxhcm.github.io/2019/05/08/tensorflow-Tensor/"/>
    <id>http://mxxhcm.github.io/2019/05/08/tensorflow-Tensor/</id>
    <published>2019-05-08T12:47:50.000Z</published>
    <updated>2019-05-23T08:01:57.086Z</updated>
    
    <content type="html"><![CDATA[<h2 id="tf-tensor">tf.Tensor</h2><h3 id="目的">目的</h3><ul><li>当做另一个op的输入，各个op通过Tensor连接起来，形成数据流。</li><li>可以使用t.eval()得到Tensor的值。。。</li></ul><h3 id="属性">属性</h3><ul><li>数据类型，float32, int32, string等</li><li>形状</li></ul><p>tf.Tensor一般是各种op操作后产生的变量，如tf.add,tf.log等运算，它的值是不可以改变的，没有assign()方法。</p><h2 id="维度">维度</h2><ul><li>0 标量</li><li>1 向量</li><li>2 矩阵</li><li>3 3阶张量</li><li>n n阶张量</li></ul><h3 id="创建0维">创建0维</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">string_scalar = tf.Variable(<span class="string">"Elephat"</span>, tf.string)</span><br><span class="line">int_scalar = tf.Variable(<span class="number">414</span>, tf.int16)</span><br><span class="line">float_scalar = tf.Variable(<span class="number">3.2345</span>, tf.float64)</span><br><span class="line"><span class="comment"># complex_scalar = tf.Variable(12.3 - 5j, tf.complex64)</span></span><br></pre></td></tr></table></figure><h3 id="创建1维">创建1维</h3><p>需要列表作为初值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">string_vec = tf.Variable([<span class="string">"Elephat"</span>], tf.string)</span><br><span class="line">int_vec = tf.Variable([<span class="number">414</span>, <span class="number">32</span>], tf.int16)</span><br><span class="line">float_vec = tf.Variable([<span class="number">3.2345</span>, <span class="number">32</span>], tf.float64)</span><br><span class="line"><span class="comment"># complex_vec = tf.Variable([12.3 - 5j, 1 + j], tf.complex64)</span></span><br></pre></td></tr></table></figure><h3 id="创建2维">创建2维</h3><p>至少需要包含一行和一列</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">bool_mat = tf.Variable([[<span class="literal">True</span>], [<span class="literal">False</span>]], tf.bool)</span><br><span class="line">string_mat = tf.Variable([<span class="string">"Elephat"</span>], tf.string)</span><br><span class="line">int_mat = tf.Variable([[<span class="number">414</span>], [<span class="number">32</span>]], tf.int16)</span><br><span class="line">float_mat = tf.Variable([[<span class="number">3.2345</span>, <span class="number">32</span>]], tf.float64)</span><br><span class="line"><span class="comment"># complex_mat = tf.Variable([[12.3 - 5j], [1 + j]], tf.complex64)</span></span><br></pre></td></tr></table></figure><h3 id="获取维度">获取维度</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.rank(tensor)</span><br></pre></td></tr></table></figure><h2 id="切片">切片</h2><p>0阶标量不需要索引，本身就是一个数字<br>1阶向量，可以传递一个索引访问某个数字<br>2阶矩阵，可以传递两个数字，返回一个标量，传递1个数字返回一个向量。<br>可以使用:访问，表示不操作该维度。</p><h2 id="获得tensor的shape">获得Tensor的shape</h2><ul><li>tf.Tensor.shape</li><li>tf.shape(tensor) # 返回tensor的shape</li><li>tf.Tensor.get_shape()</li></ul><h2 id="改变tensor的shape">改变tensor的shape</h2><h3 id="api">api</h3><p>tf.reshape(tensor, shape, name=None)</p><ul><li>tensor 输入待操作tensor</li><li>shape reshape后的shape</li></ul><h3 id="代码示例">代码示例</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># t = [1, 2, 3, 4, 5, 6, 7, 8, 9]</span></span><br><span class="line">tf.reshape(t, [<span class="number">3</span>, <span class="number">3</span>])  <span class="comment"># [[1, 2, 3,], [4, 5, 6], [7, 8, 9]]</span></span><br></pre></td></tr></table></figure><h2 id="增加数据维度">增加数据维度</h2><h3 id="api-v2">API</h3><p>tf.expand_dims(input, axis=None, name=None, dim=None)</p><h3 id="代码示例-v2">代码示例</h3><p><a href="https://github.com/mxxhcm/code/blob/master/tf/some_ops/tf_expand_dims.py" target="_blank" rel="noopener">代码地址</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x = tf.placeholder(tf.int32, [<span class="literal">None</span>, <span class="number">10</span>])</span><br><span class="line">y1 = tf.expand_dims(x, <span class="number">0</span>)</span><br><span class="line">y2 = tf.expand_dims(x, <span class="number">1</span>)</span><br><span class="line">y3 = tf.expand_dims(x, <span class="number">2</span>)</span><br><span class="line">y4 = tf.expand_dims(x, <span class="number">-1</span>) <span class="comment"># -1表示最后一维</span></span><br><span class="line"><span class="comment"># y5 = tf.expand_dims(x, 3) error</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">   inputs = np.random.rand(<span class="number">12</span>, <span class="number">10</span>)</span><br><span class="line">   r1, r2, r3, r4 = sess.run([y1, y2, y3, y4], feed_dict=&#123;x: inputs&#125;)</span><br><span class="line">   print(r1.shape)</span><br><span class="line">   print(r2.shape)</span><br><span class="line">   print(r3.shape)</span><br><span class="line">   print(r4.shape)</span><br></pre></td></tr></table></figure><h2 id="改变数据类型">改变数据类型</h2><h3 id="api-v3">API</h3><p>tf.cast(x, dtype, name=None)</p><ul><li>x  # 待转换数据</li><li>dtype # 待转换数据类型</li></ul><h3 id="代码示例-v3">代码示例</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = tf.constant([<span class="number">1.8</span>, <span class="number">2.2</span>], dtype=tf.float32)</span><br><span class="line">tf.cast(x, tf.int32)</span><br></pre></td></tr></table></figure><h2 id="评估张量">评估张量</h2><p>tf.Tensor.eval() 返回一个与Tensor内容相同的numpy数组</p><h3 id="代码示例-v4">代码示例</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">constant = tf.constant([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">tensor = constant * constant</span><br><span class="line">print(tensor.eval()) <span class="comment"># 注意，只有eval()处于活跃的Session中才会起作用。</span></span><br></pre></td></tr></table></figure><h2 id="特殊类型">特殊类型</h2><ul><li>tf.Variable 和tf.Tensor还不一样，<a href>点击查看tf.Variable详细介绍</a></li><li>tf.constant</li><li>tf.placeholder</li><li>tf.SparseTensor</li></ul><h3 id="tf-placeholder">tf.placeholder</h3><h4 id="api-v4">API</h4><p>返回一个Tensor<br>tf.placeholder(dtype, shape=None, name = None)</p><ul><li>dtype  # 类型</li><li>shape  # 形状</li></ul><h4 id="代码示例-v5">代码示例</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x = tf.placeholder(tf.float32, shape=(<span class="literal">None</span>, <span class="number">1024</span>))</span><br><span class="line">y = tf.matmul(x, x)</span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line"><span class="comment"># print(sess.run(y))  this will fail</span></span><br><span class="line">rand_array = np.random.rand(<span class="number">1024</span>, <span class="number">1024</span>)</span><br><span class="line">print(sess.run(y, feed_dict=&#123;x: rand_array&#125;))</span><br></pre></td></tr></table></figure><h3 id="tf-constant">tf.constant</h3><h4 id="api-v5">api</h4><p>tf.constant(values, dtype=None, shape=None, name=‘Const’, verify_shape=False)<br>返回一个constant的Tensor。</p><ul><li>values # 初始值</li><li>dtype # 类型</li><li>shape # 形状</li><li>name  # 可选</li><li>verify_shape</li></ul><h4 id="代码示例-v6">代码示例</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor = tf.constant([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>])</span><br><span class="line">tensor = tf.constant(<span class="number">-1.0</span>, shape=[<span class="number">3</span>, <span class="number">4</span>])</span><br></pre></td></tr></table></figure><h3 id="tf-variable">tf.Variable</h3><h4 id="api-v6">api</h4><p>tf.Variable.__init__(initial_value=None, trainable=True, collections=None, validate_shape=True, caching_device=None, name=None, …)</p><h4 id="代码示例-v7">代码示例</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor1 = tf.Variable([[<span class="number">1</span>,<span class="number">2</span>], [<span class="number">3</span>,<span class="number">5</span>]])</span><br><span class="line">tensor2 = tf.Variable(tf.constant([[<span class="number">1</span>,<span class="number">2</span>], [<span class="number">3</span>,<span class="number">5</span>]]))</span><br><span class="line">sess.run(tf.global_variables_initializer())</span><br><span class="line">sess.run(tensor1)</span><br><span class="line">sess.run(tensor2)</span><br></pre></td></tr></table></figure><h3 id="创建常量tensor">创建常量Tensor</h3><ul><li><p>tf.ones(shape, dtype=tf.float32, name=None)</p></li><li><p>tf.zeros(shape, dtype=tf.float32, name=None)</p></li><li><p>tf.fill(shape, value, name=None)</p></li><li><p>tf.constant(value, dtype=None, shape=None, name=‘Const’)</p></li><li><p>tf.ones_like(tensor, dtype=None, name=None)</p></li><li><p>tf.zeros_like(tensor, dtype=None, name=None)</p></li><li><p>tf.linspace()</p></li></ul><h3 id="创建随机tensor">创建随机Tensor</h3><ul><li>tf.random_uniform(shape, minval=0, maxval=None, dtype=tf.float32, seed=None, name=None)<br><a href="https://www.tensorflow.org/versions/r1.8/api_docs/python/tf/random_uniform" target="_blank" rel="noopener">https://www.tensorflow.org/versions/r1.8/api_docs/python/tf/random_uniform</a></li><li>tf.random_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name=None)<br>均值为mean，方差为stddev的正态分布<br><a href="https://www.tensorflow.org/versions/r1.8/api_docs/python/tf/random_normal" target="_blank" rel="noopener">https://www.tensorflow.org/versions/r1.8/api_docs/python/tf/random_normal</a></li><li>tf.truncated_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name=None)<br>均值为mean，方差为stddev的正态分布，保留[mean-2*stddev, mean+2*stddev]之内的随机数。</li><li>tf.random_shuffle(value, seed=None, name=None)<br>对value的第一维重新排列</li></ul><h2 id="代码示例-v8">代码示例</h2><p><a href="https://github.com/mxxhcm/code/blob/master/tf/some_ops/tf_tensor.py" target="_blank" rel="noopener">代码地址</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line"></span><br><span class="line">x = tf.constant([[<span class="number">3</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">8</span>]])</span><br><span class="line">print(sess.run(tf.constant([<span class="number">3</span>,<span class="number">4</span>])))</span><br><span class="line"><span class="comment"># [3 4]</span></span><br><span class="line"></span><br><span class="line">print(sess.run(tf.ones_like(x)))</span><br><span class="line">[[<span class="number">1</span> <span class="number">1</span>]</span><br><span class="line"> [<span class="number">1</span> <span class="number">1</span>]]</span><br><span class="line"></span><br><span class="line">print(sess.run(tf.zeros_like(x)))</span><br><span class="line">[[<span class="number">0</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">0</span> <span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出正态分布的随机采样值</span></span><br><span class="line">print(sess.run(tf.random_normal([<span class="number">2</span>,<span class="number">2</span>])))</span><br><span class="line"><span class="comment"># [[-0.5188188   0.77538687]</span></span><br><span class="line"> [ <span class="number">1.2343276</span>  <span class="number">-0.58534193</span>]]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出均匀[0,1]分布的随机采样值。</span></span><br><span class="line">print(sess.run(tf.random_uniform([<span class="number">2</span>,<span class="number">2</span>])))</span><br><span class="line">[[<span class="number">0.8851745</span>  <span class="number">0.12824357</span>]</span><br><span class="line"> [<span class="number">0.28489232</span> <span class="number">0.76961493</span>]]</span><br><span class="line"></span><br><span class="line">print(sess.run(tf.random_uniform([<span class="number">2</span>,<span class="number">2</span>], dtype=tf.int32, maxval=<span class="number">4</span>)))</span><br><span class="line">[[<span class="number">0</span> <span class="number">2</span>]</span><br><span class="line"> [<span class="number">2</span> <span class="number">1</span>]]</span><br><span class="line"></span><br><span class="line">print(sess.run(tf.ones([<span class="number">3</span>, <span class="number">4</span>])))</span><br><span class="line">[[<span class="number">1.</span> <span class="number">1.</span> <span class="number">1.</span> <span class="number">1.</span>]</span><br><span class="line"> [<span class="number">1.</span> <span class="number">1.</span> <span class="number">1.</span> <span class="number">1.</span>]</span><br><span class="line"> [<span class="number">1.</span> <span class="number">1.</span> <span class="number">1.</span> <span class="number">1.</span>]]</span><br><span class="line"></span><br><span class="line">print(sess.run(tf.zeros([<span class="number">2</span>,<span class="number">2</span>])))</span><br><span class="line">[[<span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span>]]</span><br></pre></td></tr></table></figure><h2 id="参考文献">参考文献</h2><p>1.<a href="https://www.tensorflow.org/guide/tensors?hl=zh_cn" target="_blank" rel="noopener">https://www.tensorflow.org/guide/tensors?hl=zh_cn</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;tf-tensor&quot;&gt;tf.Tensor&lt;/h2&gt;
&lt;h3 id=&quot;目的&quot;&gt;目的&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;当做另一个op的输入，各个op通过Tensor连接起来，形成数据流。&lt;/li&gt;
&lt;li&gt;可以使用t.eval()得到Tensor的值。。。&lt;/li&gt;
&lt;/
      
    
    </summary>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/categories/tensorflow/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow cnn demo</title>
    <link href="http://mxxhcm.github.io/2019/05/08/tensorflow-cnn-demo/"/>
    <id>http://mxxhcm.github.io/2019/05/08/tensorflow-cnn-demo/</id>
    <published>2019-05-08T11:35:01.000Z</published>
    <updated>2019-05-08T12:43:31.108Z</updated>
    
    <content type="html"><![CDATA[<h2 id="tf-nn-conv2d">tf.nn.conv2d</h2><h3 id="代码示例">代码示例</h3><p><a href="https://github.com/mxxhcm/code/blob/master/tf/some_ops/tf_conv2d.py" target="_blank" rel="noopener">代码地址</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv</span><span class="params">(img)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> len(img.shape) == <span class="number">3</span>:</span><br><span class="line">        img = tf.reshape(img, [<span class="number">1</span>]+img.get_shape().as_list())</span><br><span class="line">    fiter = tf.random_normal([<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">1</span>])</span><br><span class="line">    img = tf.nn.conv2d(img, fiter, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line">    print(img.get_shape())</span><br><span class="line">    <span class="keyword">return</span> img</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> skimage <span class="keyword">import</span> data</span><br><span class="line"><span class="comment"># img = data.text()</span></span><br><span class="line">img = data.astronaut()</span><br><span class="line">print(img.shape)</span><br><span class="line">plt.imshow(img)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">x = tf.placeholder(tf.float32, shape=(img.shape))</span><br><span class="line">result = tf.squeeze(conv(x)).eval(feed_dict=&#123;x:img&#125;)</span><br><span class="line">plt.imshow(result)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;tf-nn-conv2d&quot;&gt;tf.nn.conv2d&lt;/h2&gt;
&lt;h3 id=&quot;代码示例&quot;&gt;代码示例&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/mxxhcm/code/blob/master/tf/some_ops/tf_conv2d
      
    
    </summary>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/categories/tensorflow/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow basic operation</title>
    <link href="http://mxxhcm.github.io/2019/05/08/tensorflow-basic-operation/"/>
    <id>http://mxxhcm.github.io/2019/05/08/tensorflow-basic-operation/</id>
    <published>2019-05-08T10:57:41.000Z</published>
    <updated>2019-05-16T01:08:47.714Z</updated>
    
    <content type="html"><![CDATA[<h2 id="创建session">创建Session</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">n = <span class="number">32</span></span><br><span class="line">x = tf.linspace(<span class="number">-3.0</span>, <span class="number">3.0</span>, n)</span><br></pre></td></tr></table></figure><h3 id="普通session">普通Session</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sess = tf.Session()</span><br></pre></td></tr></table></figure><h3 id="交互式session">交互式Session</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">sess = tf.InteractiveSession()</span><br></pre></td></tr></table></figure><h3 id="在sess内执行op">在sess内执行op</h3><h4 id="方法1">方法1</h4><p>sess.run(tf.global_variables_initializer())<br>sess.run(op)<br>代码示例</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sess.run(tf.global_variables_initializer())</span><br><span class="line">result  = sess.run(x)</span><br></pre></td></tr></table></figure><h4 id="方法2">方法2</h4><p>tf.global_variables_initializer().run()<br>sess.run(op)<br>op.eval()<br>代码示例</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tf.global_variables_initializer().run()</span><br><span class="line">x.eval(session=sess)</span><br><span class="line">sess.close()</span><br></pre></td></tr></table></figure><h2 id="新op添加到默认图上">新op添加到默认图上</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">sigma = <span class="number">1.0</span></span><br><span class="line">mean = <span class="number">0.0</span></span><br><span class="line"><span class="comment"># 和x的shape是一样的</span></span><br><span class="line">z = (tf.exp(tf.negative(tf.pow(x - mean, <span class="number">2.0</span>) /</span><br><span class="line">                        (<span class="number">2.0</span> * tf.pow(sigma, <span class="number">2.0</span>)))) *</span><br><span class="line">     (<span class="number">1.0</span> / (sigma * tf.sqrt(<span class="number">2.0</span> * <span class="number">3.1415</span>))))</span><br><span class="line">print(type(z))</span><br><span class="line">print(z.graph <span class="keyword">is</span> tf.get_default_graph())</span><br><span class="line"></span><br><span class="line">plt.plot(z.eval())</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h2 id="查看shape">查看shape</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">print(z.shape)</span><br><span class="line">print(z.get_shape())</span><br><span class="line">print(z.get_shape().as_list())</span><br><span class="line">print(tf.shape(z).eval())</span><br></pre></td></tr></table></figure><h2 id="常用function">常用function</h2><h3 id="tf-stack">tf.stack</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">print(tf.stack([tf.shape(z),tf.shape(z),[<span class="number">3</span>]]).eval())</span><br><span class="line"><span class="comment"># tf.reshape, tf.matmul</span></span><br><span class="line">z_ = tf.matmul(tf.reshape(z, (n, <span class="number">1</span>)), tf.reshape(z, (<span class="number">1</span>, n)))</span><br><span class="line">plt.imshow(z_.eval()) plt.show()</span><br></pre></td></tr></table></figure><h3 id="tf-ones-like-tf-multiply">tf.ones_like, tf.multiply</h3><p>tf.ones_like返回与输入tensor具有相同shape的tensor</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">x = tf.reshape(tf.sin(tf.linspace(- <span class="number">3.0</span>, <span class="number">3.0</span>, n)), (n, <span class="number">1</span>))</span><br><span class="line">print(x.shape)</span><br><span class="line">y = tf.reshape(tf.ones_like(x), (<span class="number">1</span>, n))</span><br><span class="line">print(y.shape)</span><br><span class="line">print(y.eval())</span><br><span class="line">z = tf.multiply(tf.matmul(x,y), z_)</span><br><span class="line">print(z.shape)</span><br><span class="line">plt.imshow(z.eval())</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h2 id="列出graph中所有操作">列出graph中所有操作</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">ops = tf.get_default_graph().get_operations()</span><br><span class="line">print([op <span class="keyword">for</span> op <span class="keyword">in</span> ops])</span><br></pre></td></tr></table></figure><h2 id="代码">代码</h2><p><a href="https://github.com/mxxhcm/code/blob/master/tf/some_ops/tf_basic.py" target="_blank" rel="noopener">完整地址</a></p><h2 id="参考文献">参考文献</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;创建session&quot;&gt;创建Session&lt;/h2&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span c
      
    
    </summary>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/categories/tensorflow/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow boolean_mask</title>
    <link href="http://mxxhcm.github.io/2019/05/08/tensorflow-boolean-mask/"/>
    <id>http://mxxhcm.github.io/2019/05/08/tensorflow-boolean-mask/</id>
    <published>2019-05-08T09:46:26.000Z</published>
    <updated>2019-05-12T09:04:21.196Z</updated>
    
    <content type="html"><![CDATA[<h2 id="tf-boolean-mask">tf.boolean_mask</h2><h3 id="简单解释">简单解释</h3><p>用一个mask数组和输入的tensor做与操作，忽略为0的值。</p><h3 id="api">api</h3><p>定义在tensorflow/python/ops/array_ops.py</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tf.boolean_mask(</span><br><span class="line">    tensor, <span class="comment"># 要处理的tensor</span></span><br><span class="line">    mask, <span class="comment"># 掩码，也需要是一个tensor</span></span><br><span class="line">    name=<span class="string">'boolean_mask'</span>, <span class="comment"># 这个op的名字</span></span><br><span class="line">    axis=<span class="literal">None</span> <span class="comment">#</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="代码示例">代码示例</h3><p><a href="https://github.com/mxxhcm/code/blob/master/tf/some_ops/tf_boolean_mask.py" target="_blank" rel="noopener">代码地址</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line">a = tf.Variable([1, 2, 3])</span><br><span class="line">b = tf.Variable([2, 1.0, 4.0])</span><br><span class="line">c = tf.Variable([2, 1.0, 0.0])</span><br><span class="line">d = tf.Variable([2, 0.0, 4.0])</span><br><span class="line">e = tf.Variable([0, 1.0, 4.0])</span><br><span class="line">f = tf.Variable([0, 1.0, 0.0])</span><br><span class="line">g = tf.Variable([0, 0.0, 0.0])</span><br><span class="line"></span><br><span class="line">sess.run(tf.global_variables_initializer())</span><br><span class="line">print(&quot;a: &quot;, sess.run(a))</span><br><span class="line">print(&quot;b: &quot;, sess.run(b))</span><br><span class="line">print(&quot;c: &quot;, sess.run(c))</span><br><span class="line">print(&quot;d: &quot;, sess.run(d))</span><br><span class="line">print(&quot;e: &quot;, sess.run(e))</span><br><span class="line">print(&quot;f: &quot;, sess.run(f))</span><br><span class="line">print(&quot;g: &quot;, sess.run(g))</span><br><span class="line"># c = tf.maximum(a, b)</span><br><span class="line">a1 = tf.boolean_mask(a, b)</span><br><span class="line">a2 = tf.boolean_mask(a, c)</span><br><span class="line">a3 = tf.boolean_mask(a, d)</span><br><span class="line">a4 = tf.boolean_mask(a, e)</span><br><span class="line">a5 = tf.boolean_mask(a, f)</span><br><span class="line">a6 = tf.boolean_mask(a, g)</span><br><span class="line"></span><br><span class="line">print(&quot;tf.boolean(a, b):\n  &quot;, sess.run(a1))</span><br><span class="line">print(&quot;tf.boolean(a, c):\n  &quot;, sess.run(a2))</span><br><span class="line">print(&quot;tf.boolean(a, d):\n  &quot;, sess.run(a3))</span><br><span class="line">print(&quot;tf.boolean(a, e):\n  &quot;, sess.run(a4))</span><br><span class="line">print(&quot;tf.boolean(a, f):\n  &quot;, sess.run(a5))</span><br><span class="line">print(&quot;tf.boolean(a, g):\n  &quot;, sess.run(a6))</span><br></pre></td></tr></table></figure><p>输出如下：</p><blockquote><p>a:  [1 2 3]<br>b:  [2. 1. 4.]<br>c:  [2. 1. 0.]<br>d:  [2. 0. 4.]<br>e:  [0. 1. 4.]<br>f:  [0. 1. 0.]<br>g:  [0. 0. 0.]<br>tf.boolean(a, b):<br>[1 2 3]<br>tf.boolean(a, c):<br>[1 2]<br>tf.boolean(a, d):<br>[1 3]<br>tf.boolean(a, e):<br>[2 3]<br>tf.boolean(a, f):<br>[2]<br>tf.boolean(a, g):<br>[]</p></blockquote><h2 id="参考文献">参考文献</h2><p>1.<a href="http://landcareweb.com/questions/27920/zai-tensorflowzhong-ru-he-cong-pythonde-zhang-liang-zhong-huo-qu-fei-ling-zhi-ji-qi-suo-yin" target="_blank" rel="noopener">http://landcareweb.com/questions/27920/zai-tensorflowzhong-ru-he-cong-pythonde-zhang-liang-zhong-huo-qu-fei-ling-zhi-ji-qi-suo-yin</a><br>2.<a href="https://www.tensorflow.org/api_docs/python/tf/boolean_mask" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/boolean_mask</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;tf-boolean-mask&quot;&gt;tf.boolean_mask&lt;/h2&gt;
&lt;h3 id=&quot;简单解释&quot;&gt;简单解释&lt;/h3&gt;
&lt;p&gt;用一个mask数组和输入的tensor做与操作，忽略为0的值。&lt;/p&gt;
&lt;h3 id=&quot;api&quot;&gt;api&lt;/h3&gt;
&lt;p&gt;定义在ten
      
    
    </summary>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/categories/tensorflow/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow summary</title>
    <link href="http://mxxhcm.github.io/2019/05/08/tensorflow-summary/"/>
    <id>http://mxxhcm.github.io/2019/05/08/tensorflow-summary/</id>
    <published>2019-05-08T09:39:43.000Z</published>
    <updated>2019-06-30T15:19:34.526Z</updated>
    
    <content type="html"><![CDATA[<h2 id="tf-summary">tf.summary</h2><h3 id="目的">目的</h3><p>该模块定义在tensorflow/_api/v1/summary/__init__.py文件中，主要用于可视化。<br>每次运行完一个op之后，调用writer.add_summary()将其写入事件file。因为summary操作实在数据流的外面进行操作的，并不会操作数据，所以需要每次运行完之后，都调用一次写入函数。</p><h2 id="常用api">常用API</h2><h3 id="函数">函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1.定义一个summary scalar op，同时会将这个op加入到tf.GraphKeys.SUMMARIES collection中。</span></span><br><span class="line">tf.summary.scalar(</span><br><span class="line">name, </span><br><span class="line">tensor, <span class="comment"># 一个实数型的Tensor，包含单个的值。</span></span><br><span class="line">collections=<span class="literal">None</span>, <span class="comment"># 可选项，是graph collections keys的list，新的summary op会被添加到这个list of collection。默认的list是[GraphKeys.SUMMARIES]。</span></span><br><span class="line">family=<span class="literal">None</span></span><br><span class="line">)</span><br><span class="line"><span class="comment"># 2.定义一个summary histogram op，同时会将这个op加入到tf.GraphKeys.SUMMARIES collection中。</span></span><br><span class="line">tf.summary.histogram(</span><br><span class="line">    name,</span><br><span class="line">    values, <span class="comment"># 一个实数型的Tensor，任意shape，用来生成直方图。</span></span><br><span class="line">    collections=<span class="literal">None</span>, <span class="comment"># 可选项，是graph collections keys的list，新的summary op会被添加到这个list of collection。默认的list是[GraphKeys.SUMMARIES].</span></span><br><span class="line">    family=<span class="literal">None</span></span><br><span class="line">)</span><br><span class="line"><span class="comment"># 3.将所有定义的summary op集中到一块，如scalar，text，histogram等。</span></span><br><span class="line">tf.summary.merge_all(</span><br><span class="line">    key=tf.GraphKeys.SUMMARIES, <span class="comment">#指定用哪个GraphKey来collect summaries。默认设置为GraphKeys.SUMMARIES.并不是说将他们加入到哪个GraphKey的意思，tf.summary.scalar()等会将op加入到相应的colleection。</span></span><br><span class="line">    scope=<span class="literal">None</span>, <span class="comment">#</span></span><br><span class="line">    name=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h4 id="scalar和histogram的区别">scalar和histogram的区别</h4><p>scalar记录的是一个标量。<br>而histogram记录的是一个分布，可以是任何shape。</p><h4 id="函数示例">函数示例</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">summary_loss = tf.summary.scalar(<span class="string">'loss'</span>, loss)</span><br><span class="line">summary_weights = tf.summary.scalar(<span class="string">'weights'</span>, weights)</span><br><span class="line"><span class="comment"># merged可以代替sumary_loss和summary_weights op。</span></span><br><span class="line">merged = tf.summary.merge_all()</span><br></pre></td></tr></table></figure><p>关于tf.summary.histogram()的示例，<a href="https://github.com/mxxhcm/code/blob/master/tf/some_ops/tf_summary_histogram.py" target="_blank" rel="noopener">可以点击查看。</a></p><h3 id="类">类</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义将Summary数据写入event文件的类</span></span><br><span class="line">tf.summary.FileWriter(</span><br><span class="line">self, </span><br><span class="line">logdir,　</span><br><span class="line">graph=<span class="literal">None</span>, </span><br><span class="line">max_queue=<span class="number">10</span>,</span><br><span class="line">flush_secs=<span class="number">120</span>, </span><br><span class="line">graph_def=<span class="literal">None</span>, </span><br><span class="line">filename_suffix=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h4 id="类内函数">类内函数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将summary op的输出存到event文件(Adds a Summary protocol buffer to the event file.)</span></span><br><span class="line">tf.summary.FileWriter.add_summary(</span><br><span class="line">self,</span><br><span class="line">summary,  <span class="comment"># 一个Summary protocol buffer，一般是sess.run(summary_op)的结果</span></span><br><span class="line">global_step=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h4 id="类示例">类示例</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">writer = tf.summary.FileWriter(<span class="string">"./summary/"</span>)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    summ = sess.run([merged], feed_dict=&#123;x: inputs, y: outputs&#125;)</span><br><span class="line">    writer.add_summary(summ, global_step=i)</span><br></pre></td></tr></table></figure><h2 id="使用流程">使用流程</h2><ol><li>summary_op = tf.summary_scalar() # 声明summary op，会将该op变量加入tf.GraphKeys.SUMMARIES collection</li><li>merged = tf.summary.merge_all() # 将所有summary op合并</li><li>writer = tf.summary.FileWriter() # 声明一个FileWrite文件，用于将Summary数据写入event文件</li><li>output = sess.run([merged]) # 运行merge后的summary op</li><li>writer.add_summary(output) # 将op运行后的结果写入事件文件</li></ol><h2 id="代码示例">代码示例</h2><p><a href="https://github.com/mxxhcm/code/blob/master/tf/some_ops/tf_summary.py" target="_blank" rel="noopener">代码地址</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">graph = tf.Graph()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> graph.as_default():</span><br><span class="line">    <span class="comment"># model parameters</span></span><br><span class="line">    w = tf.Variable([<span class="number">0.3</span>], name=<span class="string">"w"</span>, dtype=tf.float32)</span><br><span class="line">    b = tf.Variable([<span class="number">0.2</span>], name=<span class="string">"b"</span>, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">    x = tf.placeholder(tf.float32, name=<span class="string">"inputs"</span>)</span><br><span class="line">    y = tf.placeholder(tf.float32, name=<span class="string">"outputs"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">'linear_model'</span>):</span><br><span class="line">        linear = w * x + b</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">'cal_loss'</span>):</span><br><span class="line">        loss = tf.reduce_mean(input_tensor=tf.square(y - linear), name=<span class="string">'loss'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">'add_summary'</span>):</span><br><span class="line">        summary_loss = tf.summary.scalar(<span class="string">'MSE'</span>, loss)</span><br><span class="line">        summary_b = tf.summary.scalar(<span class="string">'b'</span>, b[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">'train_model'</span>):</span><br><span class="line">        optimizer = tf.train.GradientDescentOptimizer(<span class="number">0.01</span>)</span><br><span class="line">        train = optimizer.minimize(loss)</span><br><span class="line"></span><br><span class="line">inputs = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]</span><br><span class="line">outputs = [<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session(graph=graph) <span class="keyword">as</span> sess:</span><br><span class="line">    writer = tf.summary.FileWriter(<span class="string">"./summary/"</span>, graph)</span><br><span class="line">    merged = tf.summary.merge_all()</span><br><span class="line"></span><br><span class="line">    init_op = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5000</span>):</span><br><span class="line">        _, summ = sess.run([train, merged], feed_dict=&#123;x: inputs, y: outputs&#125;)</span><br><span class="line">        writer.add_summary(summ, global_step=i)</span><br><span class="line"></span><br><span class="line">    w_, b_, l_ = sess.run([w, b, loss], feed_dict=&#123;x: inputs, y: outputs&#125;)</span><br><span class="line">    print(<span class="string">"w: "</span>, w_, <span class="string">"b: "</span>, b_, <span class="string">"loss: "</span>, l_)</span><br><span class="line">    <span class="keyword">for</span> var <span class="keyword">in</span> tf.get_collection(tf.GraphKeys.SUMMARIES):</span><br><span class="line">    <span class="comment">#for var in tf.get_collection(tf.GraphKeys.MODEL_VARIABLES):</span></span><br><span class="line">        print(var)</span><br></pre></td></tr></table></figure><p>使用tensorboard --logdir ./summary/打开tensorboard<br>打开之后在每个图中会看到两个曲线，一个深色，一个浅色，浅色的是真实的值，深色的是在真实值的基础上进行了平滑。在左侧可以调整平滑系数，默认是0.6，如果是0表示不进行平滑，如果是1就成了一条直线。<br>如果多次运行的话，多次的结果都会在图中显示出来，鼠标移动到图中只能看到最新的那次结果。浅色的线是最新运行的结果的真实值，深色的线是平滑后的，设置为0可以看到深色和浅色重合了。横轴STEP表示按步长，RELATIVE表示按相对时间，WALL表示将它们分开显示。<br>对于histogram来说的话，这个它是把每一步中list的值做成了一个直方图，统计在每个范围内出现的值的个数，然后按照时间步展现出来每一步的直方图。但是这个直方图是做了一定优化的，如果拿几个值来测试，最后的结果跟你想的并不一定一样。<br>所以histogram就是展现出了每一步list的值主要集中在哪个地方。有两个mode，overlay和offset，overlay是重叠的。<br>overlay中横轴是bin的取值，纵轴是每个bin的频率，所有的时间步都在一起，每一条线都代表一个时间步的直方图，鼠标悬停上去会显示每一条线的时间步。<br>offset中横轴是bin的取值，纵轴是时间步，所有的直方图按照时间步进行展开，每一时间步都是一条单独的线，鼠标悬停上去会显示每一条线的频率。<br>。</p><h3 id="官网示例">官网示例</h3><p>加了一定注释，<a href="https://github.com/mxxhcm/code/blob/master/tf/ops/tf_summary_example.py" target="_blank" rel="noopener">可以点击查看</a></p><h2 id="所有api">所有API</h2><h3 id="类-v2">类</h3><ul><li>class Event: A ProtocolMessage</li><li>class FileWriter: Writes Summary protocol buffers to event files.</li><li>class FileWriterCache: Cache for file writers.</li><li>class SessionLog: A ProtocolMessage</li><li>class Summary: A ProtocolMessage</li><li>class SummaryDescription: A ProtocolMessage</li><li>class TaggedRunMetadata: A ProtocolMessage</li></ul><h3 id="函数-v2">函数</h3><ul><li>scalar(…): Outputs a Summary protocol buffer containing a single scalar value.</li><li>histogram(…): Outputs a Summary protocol buffer with a histogram.</li><li>image(…): Outputs a Summary protocol buffer with images.</li><li>tensor_summary(…): Outputs a Summary protocol buffer with a serialized tensor.proto.</li><li>audio(…): Outputs a Summary protocol buffer with audio.</li><li>text(…): Summarizes textual data.</li><li>merge(…): Merges summaries.</li><li>merge_all(…): Merges all summaries collected in the default graph.</li><li>get_summary_description(…): Given a TensorSummary node_def, retrieve its SummaryDescription.</li></ul><h2 id="参考文献">参考文献</h2><p>1.<a href="https://www.tensorflow.org/api_docs/python/tf/summary" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/summary</a><br>2.<a href="https://www.tensorflow.org/api_docs/python/tf/summary/scalar" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/summary/scalar</a><br>3.<a href="https://www.tensorflow.org/api_docs/python/tf/summary/histogram" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/summary/histogram</a><br>4.<a href="https://www.tensorflow.org/api_docs/python/tf/summary/merge_all" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/summary/merge_all</a><br>5.<a href="https://www.tensorflow.org/guide/graphs#visualizing_your_graph" target="_blank" rel="noopener">https://www.tensorflow.org/guide/graphs#visualizing_your_graph</a><br>6.<a href="https://www.tensorflow.org/guide/summaries_and_tensorboard" target="_blank" rel="noopener">https://www.tensorflow.org/guide/summaries_and_tensorboard</a><br>7.<a href="https://www.tensorflow.org/tensorboard/r1/histograms" target="_blank" rel="noopener">https://www.tensorflow.org/tensorboard/r1/histograms</a><br>8.<a href="https://ask.csdn.net/questions/760881" target="_blank" rel="noopener">https://ask.csdn.net/questions/760881</a><br>9.<a href="https://gaussic.github.io/2017/08/16/tensorflow-tensorboard/" target="_blank" rel="noopener">https://gaussic.github.io/2017/08/16/tensorflow-tensorboard/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;tf-summary&quot;&gt;tf.summary&lt;/h2&gt;
&lt;h3 id=&quot;目的&quot;&gt;目的&lt;/h3&gt;
&lt;p&gt;该模块定义在tensorflow/_api/v1/summary/__init__.py文件中，主要用于可视化。&lt;br&gt;
每次运行完一个op之后，调用writer
      
    
    </summary>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/categories/tensorflow/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow math</title>
    <link href="http://mxxhcm.github.io/2019/05/08/tensorflow-math/"/>
    <id>http://mxxhcm.github.io/2019/05/08/tensorflow-math/</id>
    <published>2019-05-08T09:38:46.000Z</published>
    <updated>2019-05-10T11:37:24.676Z</updated>
    
    <content type="html"><![CDATA[<h2 id="tf-math">tf.math</h2><ul><li>tf.add(x, y, name=None) # 求和</li><li>tf.sub(x, y, name=None) # 减法</li><li>tf.mul(x, y, name=None) # 乘法</li><li>tf.div(x, y, name=None) # 除法</li><li>tf.mod(x, y, name=None) # 取模</li><li>tf.maximumd(x, y, name=None) # x &gt; y?x:y</li><li>tf.minimum(x, y, name=None) # x &lt; y?x:y</li><li>tf.abs(x, name=None) # 求绝对值</li><li>tf.neg(x, name=None) # 取负</li><li>tf.sign(x, name=None) # 返回符号</li><li>tf.inv(x, name=None) # 取反</li><li>tf.square(x, name=None) # 平方</li><li>tf.round(x, name=None) # 四舍五入</li><li>tf.sqrt(x, name=None) # 开根号</li><li>tf.pow(x, name=None) #</li><li>tf.exp(x, name=None) #</li><li>tf.log(x, name=None) #</li><li>tf.sin(x, name=None) #</li><li>tf.cos(x, name=None) #</li><li>tf.tan(x, name=None) #</li><li>tf.atan(x, name=None) #</li></ul><h2 id="代码示例">代码示例</h2><h3 id="tf-maximum">tf.maximum</h3><p>比较两个tensor，返回element-wise两个tensor的最大值。<br><a href="https://github.com/mxxhcm/myown_code/blob/master/tf/some_ops/tf_maximum.py" target="_blank" rel="noopener">代码地址示例</a>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line">a = tf.Variable([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">b = tf.Variable([<span class="number">2</span>, <span class="number">1</span>, <span class="number">4</span>])</span><br><span class="line"></span><br><span class="line">sess.run(tf.global_variables_initializer())</span><br><span class="line">print(<span class="string">"a: "</span>, sess.run(a))</span><br><span class="line">print(<span class="string">"b: "</span>, sess.run(b))</span><br><span class="line">c = tf.maximum(a, b)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"tf.maximum(a, b):\n  "</span>, sess.run(c))</span><br></pre></td></tr></table></figure><p>输出如下：</p><blockquote><p>a:  [1 2 3]<br>b:  [2 1 4]<br>tf.maximum(a, b):<br>[2 2 4]</p></blockquote><h2 id="参考文献">参考文献</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;tf-math&quot;&gt;tf.math&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;tf.add(x, y, name=None) # 求和&lt;/li&gt;
&lt;li&gt;tf.sub(x, y, name=None) # 减法&lt;/li&gt;
&lt;li&gt;tf.mul(x, y, name=None) #
      
    
    </summary>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/categories/tensorflow/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow multinomial</title>
    <link href="http://mxxhcm.github.io/2019/05/08/tensorflow-multinomial/"/>
    <id>http://mxxhcm.github.io/2019/05/08/tensorflow-multinomial/</id>
    <published>2019-05-08T09:37:45.000Z</published>
    <updated>2019-05-12T09:12:06.018Z</updated>
    
    <content type="html"><![CDATA[<h2 id="tf-multinomial-1-tf-random-categorical-2">tf.multinomial[1] (tf.random.categorical[2])</h2><p>多项分布，采样。</p><h3 id="更新">更新</h3><p>在tensorflow 13.1版本中，提示这个API在未来会被弃用，需要使用tf.random.categorical替代。</p><h3 id="api">API</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tf.multinomial(</span><br><span class="line">    logits, <span class="comment"># 指定样本概率的tf.Tensor</span></span><br><span class="line">    num_samples, <span class="comment"># 样本个数</span></span><br><span class="line">    seed=<span class="literal">None</span>, <span class="comment">#, 0-D</span></span><br><span class="line">    name=<span class="literal">None</span>,</span><br><span class="line">    output_dtype=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="代码示例">代码示例</h3><p><a href="https://github.com/mxxhcm/myown_code/blob/master/tf/some_ops/tf_multinominal.py" target="_blank" rel="noopener">代码地址</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># tf.multinomial(logits, num_samples, seed=None, name=None)</span></span><br><span class="line"><span class="comment"># logits 是一个二维张量，指定概率，num_samples是采样个数</span></span><br><span class="line">sess = tf.Session()</span><br><span class="line">sample = tf.multinomial([[<span class="number">5.0</span>, <span class="number">5.0</span>, <span class="number">5.0</span>], [<span class="number">5.0</span>, <span class="number">4</span>, <span class="number">3</span>]], <span class="number">10</span>) <span class="comment"># 注意logits必须是float</span></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">  print(sess.run(sample))</span><br></pre></td></tr></table></figure><p>输出结果如下:</p><blockquote><p>[[2 1 2 1 0 2 1 1 1 0]<br>[1 0 0 1 0 1 0 1 0 0]]<br>[[2 2 0 2 2 0 2 0 1 2]<br>[1 0 0 2 0 1 0 1 1 0]]<br>[[0 0 0 2 0 0 1 2 0 1]<br>[0 0 0 1 0 1 0 0 0 0]]<br>[[2 1 0 1 1 1 0 0 2 0]<br>[1 0 0 2 0 0 0 0 0 1]]<br>[[1 0 1 0 0 1 2 2 0 0]<br>[1 0 0 0 0 1 1 1 2 0]]</p></blockquote><h2 id="参考文献">参考文献</h2><p>1.<a href="https://www.tensorflow.org/api_docs/python/tf/random/multinomial" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/random/multinomial</a><br>2.<a href="https://www.tensorflow.org/api_docs/python/tf/random/categorical" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/random/categorical</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;tf-multinomial-1-tf-random-categorical-2&quot;&gt;tf.multinomial[1] (tf.random.categorical[2])&lt;/h2&gt;
&lt;p&gt;多项分布，采样。&lt;/p&gt;
&lt;h3 id=&quot;更新&quot;&gt;更新&lt;/h3&gt;
&lt;p&gt;在
      
    
    </summary>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/categories/tensorflow/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow app</title>
    <link href="http://mxxhcm.github.io/2019/05/08/tensorflow-app/"/>
    <id>http://mxxhcm.github.io/2019/05/08/tensorflow-app/</id>
    <published>2019-05-08T09:35:39.000Z</published>
    <updated>2019-05-08T14:09:23.063Z</updated>
    
    <content type="html"><![CDATA[<h2 id="tf-app-flags">tf.app.flags</h2><h3 id="代码示例">代码示例</h3><p><a href="https://github.com/mxxhcm/code/blob/master/tf/some_ops/tf_app.py" target="_blank" rel="noopener">代码地址</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">flags.py</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">flags = tf.app.flags</span><br><span class="line">flags.DEFINE_string(<span class="string">'model'</span>, <span class="string">'mxx'</span>, <span class="string">'Type of model'</span>)</span><br><span class="line">flags.DEFINE_boolean(<span class="string">'gpu'</span>,<span class="string">'True'</span>, <span class="string">'use gpu?'</span>)</span><br><span class="line">FLAGS = flags.FLAGS</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(_)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> k,v <span class="keyword">in</span> FLAGS.flag_values_dict().items():</span><br><span class="line">        print(k, v)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    tf.app.run(main)</span><br></pre></td></tr></table></figure><p>传递参数的方法有两种，一种是命令行~$:python <a href="http://flags.py" target="_blank" rel="noopener">flags.py</a> --model hhhh ，一种是pycharm中传递参数。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;tf-app-flags&quot;&gt;tf.app.flags&lt;/h2&gt;
&lt;h3 id=&quot;代码示例&quot;&gt;代码示例&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/mxxhcm/code/blob/master/tf/some_ops/tf_app.py
      
    
    </summary>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/categories/tensorflow/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow where</title>
    <link href="http://mxxhcm.github.io/2019/05/08/tensorflow-where/"/>
    <id>http://mxxhcm.github.io/2019/05/08/tensorflow-where/</id>
    <published>2019-05-08T09:34:47.000Z</published>
    <updated>2019-05-08T14:12:24.529Z</updated>
    
    <content type="html"><![CDATA[<h2 id="tf-where">tf.where</h2><h3 id="简单解释">简单解释</h3><p>tf.where(conditon) 返回条件为True的下标。<br>tf.where(condition, x=X, y=Y) 条件为True的对应位置值替换为1,为False替换成0。</p><h3 id="api">API</h3><p>定义在tensorflow/python/ops/array_ops.py中。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tf.where(</span><br><span class="line">    condition, <span class="comment"># 条件</span></span><br><span class="line">    x=<span class="literal">None</span>,  <span class="comment"># 操作数1</span></span><br><span class="line">    y=<span class="literal">None</span>,  <span class="comment"># 操作数2</span></span><br><span class="line">    name=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="tf-where-condition-代码示例">tf.where(condition)代码示例</h3><p><a href="https://github.com/mxxhcm/code/blob/master/tf/some_ops/tf_where.py" target="_blank" rel="noopener">代码地址</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">X = tf.placeholder(tf.int32, [<span class="literal">None</span>, <span class="number">7</span>])</span><br><span class="line"></span><br><span class="line">zeros = tf.zeros_like(X)</span><br><span class="line">index = tf.not_equal(X, zeros)</span><br><span class="line">loc = tf.where(index)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    inputs = np.array([[<span class="number">1</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">0</span>, <span class="number">8</span>, <span class="number">6</span>], [<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>]])</span><br><span class="line">    out = sess.run(loc, feed_dict=&#123;X: inputs&#125;)</span><br><span class="line">    print(np.array(out))</span><br><span class="line">    <span class="comment"># 输出12个坐标，表示这个数组中不为0元素的索引。</span></span><br></pre></td></tr></table></figure><h3 id="tf-where-condition-x-x-y-y-代码示例">tf.where(condition, x=X, y=Y)代码示例</h3><p><a href="https://github.com/mxxhcm/code/blob/master/tf/some_ops/tf_where.py" target="_blank" rel="noopener">代码地址</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">inputs = np.array([[<span class="number">1</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">0</span>, <span class="number">8</span>, <span class="number">6</span>], [<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>]])</span><br><span class="line">X = tf.placeholder(tf.int32, [<span class="literal">None</span>, <span class="number">7</span>])</span><br><span class="line">zeros = tf.zeros_like(X)</span><br><span class="line">ones = tf.ones_like(X)</span><br><span class="line">loc = tf.where(inputs, x=ones, y=zeros)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    out = sess.run(loc, feed_dict=&#123;X: inputs&#125;)</span><br><span class="line">    print(np.array(out))</span><br></pre></td></tr></table></figure><h2 id="参考文献">参考文献</h2><p>1.<a href="https://www.tensorflow.org/api_docs/python/tf/where" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/where</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;tf-where&quot;&gt;tf.where&lt;/h2&gt;
&lt;h3 id=&quot;简单解释&quot;&gt;简单解释&lt;/h3&gt;
&lt;p&gt;tf.where(conditon) 返回条件为True的下标。&lt;br&gt;
tf.where(condition, x=X, y=Y) 条件为True的对应位置值替
      
    
    </summary>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/categories/tensorflow/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>reinforcement learning an introduction 第5章笔记</title>
    <link href="http://mxxhcm.github.io/2019/04/29/reinforcement-learning-an-introduction-%E7%AC%AC5%E7%AB%A0%E7%AC%94%E8%AE%B0/"/>
    <id>http://mxxhcm.github.io/2019/04/29/reinforcement-learning-an-introduction-第5章笔记/</id>
    <published>2019-04-29T07:53:02.000Z</published>
    <updated>2019-08-04T06:58:54.818Z</updated>
    
    <content type="html"><![CDATA[<h2 id="mc-methods">MC Methods</h2><p>这章主要介绍了MC算法，MC算法通过采样，估计state-value function或者action value function。为了找到最好的policy，需要让policy不断的进行探索，但是我们还需要找到最好的action，减少exploration。这两个要求是矛盾的，这一章主要介绍了两种方法来尽量满足这两个要求。一种是on-policy的方法，使用soft policy，即有一定概率随机选择action，其余情况下选择最好的action。这种情况下学习到的policy不是greedy的，同时也能进行一定的exploration。一种是off-policy的方法，这种方法使用两个不同的policy，一个用来采样的behaviour policy，一个用来评估的target policy。target policy是一个deterministic policy，而behaviour policy用来exploration。<br>MC方法通过采样估计值函数有三个优势，从真实experience中学习，从仿真环境中学习，以及每个state value的计算独立于其他state。<br>MC和DP不一样的是，它不需要环境的信息，只需要experience即可，不管是从真实交互还是从仿真环境中得到的state,action,reward序列都行。从真实交互中学习不需要环境的信息，从仿真环境中学习需要一个model，但是这个model只用于生成sample transition，并不需要像DP那样需要所有transition的完整概率分布。在很多情况下，生成experience sample要比显示的得到概率分布容易很多。<br>MC基于average sample returns估计值函数。为了保证returns是可用的，这里定义蒙特卡洛算法是episodic的，即所有的experience都有一个terminal state。只有在一个episode结束的时候，value estimate和policy才会改变。蒙塔卡洛算法可以在episode和episode实现增量式，不能在step和step之间实现增量式。(Monte Carlo methods can thus be incremental in an episode-by-episode sense, but not in a step-by-step online sense.)<br>在一个state采取action得到的return取决于同一个episode后续状态的action，因为所有的action都是在不断学习中采取，从早期state的角度来看，这个问题是non-stationary的。为了解决non-stationary问题，采用GPI中的idea。DP从已知的MDP中计算value function，蒙特卡洛使用MDP的sample returns学习value function。然后value function和对应的policy交互获得好的value和policy。<br>这一章就是把DP中的各种想法推广到了MC上，解决prediction和control问题，DP使用的是整个MDP，而MC使用的是MDP的采样。</p><h2 id="mc-prediction">MC Prediction</h2><p>Prediction problem就是估计value function，value function又分为state value function和action value function。这里会分别给出state value function和action value function的估计方法。</p><h3 id="state-value-function">State value function</h3><p>从state value function说起。最简单的想法就是使用experience估计value function，通过对每个state experience中return做个average。</p><h4 id="first-visti-mc-method">First visti MC method</h4><p>这里主要介绍两个算法，一个叫做first visit MC method，另一个是every visit MC method。比如要估计策略$\pi$下的$v(s)$，使用策略$\pi$采样一系列经过$s$的episodes，$s$在每一个episode中出现一次叫做一个visit，一个$s$可能在一个episode中出现多次。First visit就是只取第一次visit估计$v(s)$，every visit就是每一次visit都用。<br>下面给出first visit的算法：<br>算法1 <strong>First visit MC preidction</strong><br><strong>输入</strong> 被评估的policy $\pi$<br><strong>初始化</strong>:<br>$\qquad V(s)\in R,\forall s \in S$<br>$\qquad Returns(s) \leftarrow empty list,\forall s \in S$<br><strong>Loop</strong> for each episeode:<br>$\qquad$生成一个episode<br>$\qquad G\leftarrow 0$<br>$\qquad$<strong>Loop</strong> for each step, $t= T-1,T-2, \cdots, 1$<br>$\qquad\qquad G\leftarrow G + \gamma R_t$<br>$\qquad\qquad$ IF $S_t$没有在$S_0, \cdots , S_{t-1}$中出现过<br>$\qquad\qquad\qquad Returns(S_t).apppend(G)$<br>$\qquad\qquad\qquad V(S_t)\leftarrow average(Returns(S_t))$<br>$\qquad\qquad END IF$<br>Every visit算法的话，不用判断$S_t$是否出现。当$s$的visit趋于无穷的时候，first vist和every visit算法$v_{\pi}(s)$都能收敛。First visit中，每一个return都是$v_{\pi}(s)$的一个独立同分布估计。根据大数定律，估计平均值（$average(Returns(S_0),\cdots, average(Returns(S_t)$）的序列收敛于它的期望。每一个average都是它自己的一个无偏估计，标准差是$\frac{1}{\sqrt{n}}$。every visit的收敛更难直观的去理解，但是它二次收敛于$v_{\pi}(s)$。<br>补充一点：<br>大数定律：无论抽象分布如何，均值服从正态分布。<br>中心极限定理：样本大了，抽样分布近似于整体分布。</p><p>这里再次对比一下DP和MC，在扑克牌游戏中，我们知道环境的所有信息，但是我们不知道摸到下一张牌的概率，比如我们手里有很多牌了，我们知道下一张摸到什么牌会赢，但是我们不知道这件事发生的概率。使用MC可以采样获得，所以说，即使有时候知道环境信息，MC方法可能也比DP方法好。</p><h4 id="mc-backup-diagram">MC backup diagram</h4><p>能不能推广DP中的backup图到MC中？什么是backup图？backup图顶部是一个root节点，表示要被更新的节点，下面是所有的transitions，leaves是对于更新有用的reward或者estimated values。<br>MC中的backup图，root节点是一个state，下面是一个episode中的所有transtion轨迹，以terminal state为终止节点。DP backup diagram展示了所有可能的transitions，而MC backup diagram只展示了采样的那个episode；DP backup diagram只包含一步的transitions，而MC backup diagram包含一个episode的所有序列。<br><img src="/2019/04/29/reinforcement-learning-an-introduction-第5章笔记/" alt="mc backup"><br><img src="/2019/04/29/reinforcement-learning-an-introduction-第5章笔记/" alt="dp backup page 59"></p><h4 id="mc的特点">MC的特点</h4><p>DP中每个state的估计都依赖于它的后继state，而MC中每个state value的计算都不依赖于任何其他state value（MC算法不进行bootstrap），所以可以单独估计某一个state或者states的一个子集。而且估计单个state的计算复杂度和states的数量无关，我们可以只取感兴趣的states子集进行评估，这是MC的第三个优势。前两个优势是从actural experience中学习和从simulated的experience中学习。</p><h3 id="action-value-function">Action value function</h3><p>如果没有model的话，需要估计state-action value而不是state value。有model的话，只有state value就可以确定policy，选择使reward和next_state value加起来最大的action即可。没有model的话，只有state value是不够的，因为不知道下一个state是什么。而使用action value，就可以确定policy，选择$q$值最大的那个action value，取相应的action即可。<br>所以这一节的目标是学习action value function。有一个问题是许多state-action可能一次也没有被访问过，如果$\pi$是deterministic的，每一个state只输出一个action，其他action的MC估计没有returns进行平均，就无法进行更新。所以，我们需要估计每一个state对应的所有action，这是exploration问题。<br>对于action value的policy evaluation，必须保证continual exploration。一种实现方式是指定episode开始的state-action pair，每一个pair都有大于$0$的概率被选中,这就保证了每一个action-pair在无限个episode中会被访问无限次，这叫做exploring starts。这种假设有时候有用，但是在某些时候，我们无法控制环境产生的experience，可行的方法是使用stochastic policy。</p><h2 id="mc-control">MC Control</h2><p>MC control使用的还是GPI的想法，估计当前policy的action value，基于action value改进policy，不断迭代。考虑经典的policy iteration，执行一次完全的iterative policy evaluation，再执行一次完全的policy improvement，不断迭代。对于policy evaluation，每次evaluation都使用多个episodes的experience，每次action value都会离true value function更近。假设我们有无限个exploring starts生成的episodes，满足这些条件时，对于任意$\pi_k$都会精确计算出$q_{\pi_k}$。进行policy improvement时，只要对于当前的action value function进行贪心即可，即：<br>$$\pi(s) = arg\ max_a q(s,a)\tag{1}$$<br>第$4$章给出了证明，即policy improvement theorem。在每一轮improvement中，对所有的$s\in S$，执行：<br>\begin{align*}<br>q_{\pi_k}(s,\pi_{k+1}(s)) &amp;=q_{\pi_k}(s, argmax_a q_{\pi_k}(s,a))\\<br>&amp;=max_a q_{\pi_k}(s,a)\\<br>&amp;\ge q_{\pi_k}(s, \pi_k(s))\\<br>&amp;\ge v_{\pi_k}(s)\\<br>\end{align*}<br>MC算法的收敛保证需要满足两个假设，一个是exploring start，一个是policy evaluation需要无限个episode的experience。但是现实中，这两个条件是不可能满足的，我们需要替换掉这些条件近似接近最优解。</p><h3 id="mc-control-without-infinte-episodes">MC Control without infinte episodes</h3><p>无限个episodes的条件比较容易去掉，在DP方法中也有这些问题。在DP和MC任务中，都有两种方法去掉无限episode的限制，第一种方法是像iterative policy evaluation一样，规定一个误差的bound，在每一次evaluation迭代，逼近$q_{\pi_k}$，通过足够多的迭代确保误差小于bound，可能需要很多个episode才能达到这个bound。第二种是进行不完全的policy evaluation，和DP一样，使用小粒度的policy evaluation，可以只执行iterative policy evaluation的一次迭代，也可以执行一次单个state的improvement和evaluation。对于MC方法来说，很自然的就想到基于一个episode进行evaluation和improvement。每经历一个episode，执行该episode内相应state的evaluation和improvement。也就是说一个是规定每次迭代的bound，一个是规定每次迭代的次数。</p><h4 id="伪代码">伪代码</h4><p>算法2 <strong>First visit MCES</strong><br><strong>初始化</strong><br>$\qquad$任意初始化$\pi(s)\in A(s), \forall s\in S$<br>$\qquad$任意初始化$Q(s, a)\in R, \forall s\in S, \forall a \in A(s)$<br>$\qquad$Returns(s,a)$\leftarrow$ empty list, $\forall s\in S, \forall a \in A(s)$<br><strong>Loop forever(for each episode)</strong><br>$\qquad$随机选择满足$S_0\in S, A_0\in A(S_0)$的state-action$(S_0,A_0)$，满足概率大于$0$<br>$\qquad$从$S_0,A_0$生成策略$\pi$下的一个episode，$S_0,A_0,R_1,\cdots,S_{T-1},A_{T-1},R_T$<br>$\qquad G\leftarrow 0$<br>$\qquad$<strong>Loop for each step of episode</strong>,$t=T-1,T-2,\cdots,0$<br>$\qquad\qquad G\leftarrow \gamma G+R_{t+1}$<br>$\qquad\qquad$如果$S_t,A_t$没有在$S_0,A_0,\cdots, S_{t-1},A_{t-1}$中出现过<br>$\qquad\qquad\qquad$Returns($S_t,A_t$).append(G)<br>$\qquad\qquad\qquad Q(S_t,A_t) \leftarrow average(Returns(S_t, A_t)$<br>$\qquad\qquad\qquad \pi(S_t) \leftarrow argmax_a Q(S_t,a)$<br>这个算法一定会收敛到全局最优解，因为如果收敛到一个suboptimal policy，value function在迭代过程中会收敛到该policy的true value function，接下来的policy improvement会改进该suboptimal policy。</p><h2 id="on-policy-mc-control-without-es">On-policy MC Control without ES</h2><p>上节主要是去掉了无穷个episode的限制，这节需要去掉ES的限制，解决方法是需要agents一直能够去选择所有的actions。目前有两类方法实现，一种是on-policy，一种是off-policy。</p><h3 id="on-policy和off-policy">on-policy和off-policy</h3><p>On-policy算法中，用于evaluation或者improvement的policy和用于决策的policy是相同的，而off-policy算法中，evaluation和improvement的policy和决策的policy是不同的。</p><h3 id="varepsilon-soft和-varepsilon-greedy">$\varepsilon$ soft和$\varepsilon$ greedy</h3><p>在on-policy算法中，policy一般是soft的，整个policy整体上向一个deterministic policy偏移。<br>在$\varepsilon$ soft算法中，只要满足$\pi(a|s)\gt 0,\forall s\in S, a\in A$即可。<br>在$\varepsilon$ greedy算法中，用$\frac{\varepsilon}{|A(s)|}$的概率选择non-greedy的action，使用$1 -\varepsilon + \frac{\varepsilon}{|A(s)|}$的概率选择greedy的action。<br>$\varepsilon$ greedy是$\varepsilon$ soft算法中的一类，可以看成一种特殊的$\varepsilon$ soft算法。<br>本节介绍的on policy方法使用$\varepsilon$ greedy算法。</p><h3 id="on-policy-first-visit-mc">On-policy first visit MC</h3><p>本节介绍的on policy MC算法整体的思路还是GPI，首先使用first visit MC估计当前policy的action value function。去掉exploring starting条件之后，为了保证exploration，不能直接对所有的action value进行贪心，使用$\varepsilon$ greedy算法保持exploration。<br>算法3 <strong>On policy first visit MC Control</strong><br>$\varepsilon \gt 0$<br><strong>初始化</strong><br>$\qquad$用任意$\varepsilon$ soft算法初始化$\pi$<br>$\qquad$任意初始化$Q(s, a)\in R, \forall s\in S, \forall a \in A(s)$<br>$\qquad$Returns(s,a) $\leftarrow$ empty list, $\forall s\in S, \forall a \in A(s)$<br><strong>Loop forever(for each episode)</strong><br>$\qquad$根据policy $\pi$生成一个episode，$S_0,A_0,R_1,\cdots,S_{T-1},A_{T-1},R_T$<br>$\qquad G\leftarrow 0$<br>$\qquad$<strong>Loop for each step of episode</strong>,$t=T-1,T-2,\cdots,0$<br>$\qquad\qquad G\leftarrow \gamma G+R_{t+1}$<br>$\qquad\qquad$如果$S_t,A_t$没有在$S_0,A_0,\cdots, S_{t-1},A_{t-1}$中出现过<br>$\qquad\qquad\qquad$Returns($S_t,A_t$).append(G)<br>$\qquad\qquad\qquad Q(S_t,A_t) \leftarrow average(Returns(S_t, A_t)$<br>$\qquad\qquad\qquad A^{*}\leftarrow argmax_a Q(S_t,a)$<br>$\qquad\qquad\qquad$<strong>For all</strong> $a \in A(S_t) : $<br>$\qquad\qquad\qquad\qquad\pi(a|S_t)\leftarrow \begin{cases}1-\varepsilon+\frac{\varepsilon}{|A(S_t)|}\qquad if\ a = A^{*}\\ \frac{\varepsilon}{|A(S_t)|}\qquad a\neq A^{*}\end{cases}$</p><p>对于任意的$\varepsilon$ soft policy $\pi$，相对于$q_{\pi}$的$\varepsilon$ greedy算法至少和$\pi$一样好。用$\pi’$表示$\varepsilon$ greedy policy，对于$\forall s\in S$，都满足policy improvement theorem的条件：<br>\begin{align*}<br>q_{\pi}(s,\pi’(s))&amp;=\sum_a\pi’(a|s)q_{\pi}(s,a)\\<br>&amp;=\frac{\varepsilon}{|A(s)|} \sum_aq_{\pi}(s,a) + (1- \varepsilon) max_a q_{\pi}(s,a) \tag{2}\\<br>&amp;\ge \frac{\varepsilon}{|A(s)|} \sum_aq_{\pi}(s,a) + (1-\varepsilon) \sum_a\frac{\pi(a|s) - \frac{\varepsilon}{|A(s)|}}{1-\varepsilon}q_{\pi}(s,a) \tag{3}\\<br>&amp;=\frac{\varepsilon}{|A(s)|} \sum_aq_{\pi}(s,a) - \frac{\varepsilon}{|A(s)|} \sum_aq_{\pi}(s,a) + \sum_a \pi(a|s)\sum_aq_{\pi}(s,a)\\<br>&amp;=v(s)<br>\end{align*}<br>式子2到式子3是怎么变换的，我有点没看明白！！！（不懂）。后来终于想明白了，式子3的第二项分子服从的是$\pi(a|s)$，而式子2的第二项这个$a$是新的$\pi’(a|s)$。<br>接下来证明，当$\pi$和$\pi’$都是optimal $\varepsilon$ policy的时候，可以取到等号。这个我看这没什么意思，就不证明了。。在p102。</p><h2 id="off-policy-prediction-via-importance-sampling">Off-policy Prediction via Importance Sampling</h2><p>所有的control方法都要面临一个问题：一方面需要选择optimal的action估计action value，另一方面需要exploration，不能一直选择optimal action，那么该如何控制这两个问题之间的比重。on-policy方法采样的方法是学习一个接近但不是optimal的policy保持exploriation。off-policy的方法使用两个policy，一个用于采样的behavior policy，一个用于evaluation的target policy。用于学习target policy的data不是target policy自己产生的，所以叫做off-policy learning。</p><h3 id="on-policy-vs-off-policy">on-policy vs off-policy</h3><p>on policy更简单，off policy使用两个不同的policy，所以variance更大，收敛的更慢，但是off-policy效果更好，更通用。On-policy可以看成off-policy的特例，target policy和behaviour policy是相同的。Off-policy可以使用非学习出来的data，比如人工生成的data。</p><h3 id="off-policy-prediction-problem">off-policy prediction problem</h3><p>对于prediction problem，target policy和behaviour policy都是固定的。$\pi$是target policy，$b$是behaviour policy，我们要使用$b$生成的episode去估计$q_{\pi}$或者$v_{\pi}$。为了使用$b$生成的episodes估计$\pi$，需要满足一个假设，policy $\pi$中采取的action在$b$中也要能有概率被采取，即$\pi(a|s)\gt 0$表明$b(a|s) \gt 0$，这是coverage假设。<br>在control问题中，target policy通常是相对于当前action value的deterministic greedy policy，最后target policy是一个deterministic optimal policy而behaviour policy通常是$\varepsilon$ greedy的探索策略。</p><h3 id="importance-sampling和importance-sampling-ratio">importance sampling和importance sampling ratio</h3><p>很多off policy方法使用importance sampling，利用一个distribution的samples估计另一个distribution的value function。Importance sampling通过计算trajectoried在target和behaviour policy中出现的概率比值对returns进行加权，这个相对概率称为importance sampling ratio。给定以$S_t$为初始状态的sate-action trajectory，它在任何一个policy $\pi$中发生的概率如下：<br>\begin{align*}<br>&amp;Pr\{A_t, S_{t+1},A_{t+1},\cdots,S_T|A_{t:T-1}\sim \pi,S_t\}\\<br>=&amp;\pi(A_t|S_t)p(S_{t+1}|S_t,A_t)\pi(A_{t+1}|S_{t+1})\cdots p(S_T|S_{T-1},A_{T-1})\\<br>=&amp;\prod_{k=t}^{T-1}\pi(A_k|S_k)p(S_{k+1}|S_k,A_k)<br>\end{align*}<br>其中$p$是状态转换概率，imporrance sampling计算如下：<br>$$\rho_{t:T-1}=\frac{\prod_{k=t}^{T-1} \pi(A_k|S_k)p(S_{k+1}|S_k,A_k)}{\prod_{k=t}^{T-1} b(A_k|S_k)p(S_{k+1}|S_k,A_k)}=\prod_{k=t}^{T-1}\frac{\pi(A_k|S_k)}{b(A_k|S_k}\tag{2}$$<br>因为p跟policy无关，所以可以直接消去。importance sampling ratio只和policies以及sequences有关。<br>根据behaviour policy的returns $G_t$，我们可以得到一个Expectation，即$\mathbb{E}[G_t|S_t=s]=v_b(s)$，显然，这是b的value function而不是$\pi$的value function，这个时候就用到了importance sampling，ratio $\rho_{t:T-1}$对b的returns进行转换，得到了另一个期望：<br>$$\mathbb{E}[\rho_{t:T-1}G_t|S_t=s]=v_{\pi}(s)\tag{3}$$</p><h3 id="符号定义">符号定义</h3><p>假设我们想要从policy b 中的一些episodes中估计$v_{\pi}(s)$，</p><ul><li>用$t$表示episode中的每一步，有些不同的是，$t$在不同episode之间是连续的，比如第$1$个episode有$100$个timesteps，第$2$个episode的timsteps从$101$开始。</li><li>用$J(s)$表示state $s$在不同episodes中第一次出现的$t$。</li><li>用$T(t)$表示从$t$所在那个episode的terminal timestep。</li><li>用$\left\{G_t\right\}_{t\in J(s)}$表示所有state $s$的return list。</li><li>用$\left\{\rho_{t:T(t)-1}\right\}_{t\in J(s)}$表示相应的importance ratio。</li></ul><h3 id="importance-sampling">importance sampling</h3><p>有两种importance sampling方法估计$v_{\pi}(s)$，一种是oridinary importance sampling，一种是weighted importance sampling。</p><h4 id="oridinary-importance-sampling">oridinary importance sampling</h4><p>直接对多个结果进行平均<br>$$V(s) = \frac{\sum_{t\in J(s)}\rho_{t:T(t)-1} G_t}{|J(s)|}\tag{4}$$</p><h4 id="weighted-importance-sampling">weighted importance sampling</h4><p>对多个结果进行加权平均<br>$$V(s) = \frac{\sum_{t\in J(s)}\rho_{t:T(t)-1} G_t}{\sum_{t\in J(s)}\rho_{t:T(t)-1}}\tag{5}$$</p><h4 id="异同点">异同点</h4><p>为了比较这两种importance sampling的异同，考虑state s只有一个returns的first vist MC方法，在加权平均中，ratio会约分约掉，这个returns的expectation是$v_b(s)$而不是$v_{\pi}(s)$，是一个有偏估计；而普通平均，returns的expectation还是$v_{\pi}(s)$，是一个无偏估计，但是可能会很极端，比如ratio是$10$，就说明$v_{\pi}(s)$是$v_b(s)$的$10$倍，可能与实际相差很大。<br>在fisrt visit算法中，就偏差和方差来说。普通平均的偏差是无偏的，而加权平均的偏差是有偏的（逐渐趋向$0$）。普通平均的方差是unbounded，因为ratio可以是unbounded，而加权平均对于每一个returns来说，权重最大是$1$。事实上，假定returns是bounded，即使ratios的方差是infinite，加权平均的方差也会趋于$0$。实践中，加权平均有更小的方差，通常更多的被采用。<br>在every visit算法中，普通平均和加权平均都是有偏的，随着样本的增加，偏差也趋向于$0$。在实践中，因为every visit不需要记录哪个状态是否被记录过，所以要比first visit常用。</p><h3 id="无穷大方差">无穷大方差</h3><p><img src="/2019/04/29/reinforcement-learning-an-introduction-第5章笔记/figure_5_4.png" alt="example of oridinary importance ratio"><br>考虑一个例子。只有一个non-terminal state s，两个ation，left和right，right action是deterministic transition到termination，left action有$0.9$的概率回到s，有$0.1$的概率到termination。left action回到termination会产生$+1$的reward，其他操作的reward是$0$。所有target policy策略下的episodes都会经过一些次回到state s然后到达terminal state，总的returns是$1(\gamma = 1)$。使用behaviour policy等概率选择left和right action。<br>这个例子中returns的真实期望是$1$。first visit中weighted importance sampling中return的期望是$1$，因为behaviour policy中选择right的action 在target policy中概率为$0$，不满足之前假设的条件，所以没有影响。而oridinary importance sampling的returns期望也是$1$，但是可能经过了几百万个episodes之后，也不一定收敛到$1$。<br>接下来我们证明oridinary importance sampling中returns的variance是infinite。<br>$$Var(X) = \mathbb{E}\left[(X-\bar{X})^2\right] = \mathbb{E}\left[X^2-2\bar{X}X +\bar{x}^2\right]= \mathbb{E}\left[X^2\right]-\bar{X}^2 \tag{6}$$<br>如果mean是finite，只有当random variable的平方的Expectation为infinte时variance是infinte。所以，我们需要证明：<br>$$\mathbb{E}_b\left[\left(\prod_{t=0}^{T-1}\frac{\pi(A_t|S_t)}{b(A_t|S_t)}G_0\right)^2\right] \tag{7}$$<br>是infinte的。<br>这里我们按照一个episode一个episode的进行计算。但是需要注意的是，behaviour policy可以选择right action，而target policy只有left action，当behaviour policy选择right的话，ratio是$0$。我们只需要考虑那些一直选择left action回到state s，然后通过left action到达terminal state的episodes。按照下式计算期望，注意这个和上面用oridinary important ratio估计$v_{\pi}(s)$可不一样，上面是用采样估计$v_{\pi}(s)$，这个是计算真实的$v_{\pi}(s)$的期望，不对，是它的平方的期望。<br>\begin{align*}<br>\mathbb{E}_b\left[\left( \prod_{t=0}^{T-1}\frac{\pi(A_t|S_t)}{b(A_t|S_t)}G_0\right)^2\right] = &amp; \frac{1}{2}\cdot 0.1 \left(\frac{1}{0.5}\right)^2\tag{长度为1的episode}\\<br>&amp;+\frac{1}{2}\cdot 0.9\cdot\frac{1}{2}\cdot 0.1 \left(\frac{1}{0.5}\frac{1}{0.5}\right)^2\tag{长度为2的episode}\\<br>&amp;+\frac{1}{2}\cdot 0.9\cdot \frac{1}{2} \cdot 0.9 \frac{1}{2}\cdot 0.1 \left(\frac{1}{0.5}\frac{1}{0.5}\frac{1}{0.5}\right)^2\tag{长度为3的episode}\\<br>&amp;+ \cdots\\<br>=&amp;0.1 \sum_{k=0}^{\infty}0.9^k\cdot 2^k \cdot 2\\<br>=&amp;0.2 \sum_{k=0}^{\infty}1.8^k\\<br>=&amp;\infty \tag{8}\<br>\end{align*}</p><h3 id="incremental-implementation">Incremental Implementation</h3><p>Monte Carlo prediction可以增量式实现，用episode-by-episode bias。<br>在on-policy算法中，$V_t$的估计通过直接对多个episode的$G_t$进行平均得到。<br>$$V_n(s) = \frac{G_1 + G_2 + \cdots + G_{n-1}}{n - 1} \tag{9}$$<br>其中$V_n(s)$表示在第$n$个epsisode估计的state $s$的value function，$n-1$表示采样得到的总共$n-$个episode，$G_1$表示每个episode中第一次遇到$s$时的Return。<br>在第$n+1$个episodes估计$V(s)$时：<br>\begin{align*}<br>V_{n+1}(s) &amp;= \frac{G_1 + G_2 + \cdots + G_n}{n}\\<br>nV_{n+1}(s)&amp;= G_1 + G_2 + \cdots + G_{n - 1} + G_n\tag{上式两边同时乘上n}\\<br>(n-1)V_n(s)&amp;= G_1 + G_2 + \cdots + G_{n - 1}\tag{用n-1代替n}\\<br>nV_{n+1}(s)&amp;= G_1 + G_2 + \cdots + G_{n - 1} + G_n\tag{分解V_{n+1}(s)}\\<br>&amp;= (G_1 + G_2 + \cdots + G_{n - 1}) + G_n\\<br>&amp;= (n-1)V_n(s) + G_n\\<br>\frac{nV_{n+1}(s)}{n}&amp;= \frac{(n-1)V_n(s) + G_n}{n}\tag{上式两边同时除以n}\\<br>V_{n+1}(s)&amp;= \frac{(n-1)V_n(s) + G_n}{n}\\<br>&amp; = V_n(s) +\frac{G_n-V_n(s)}{n} \tag{10}<br>\end{align*}<br>这个更新规则的一般形式如下：<br>$$NewEstimate \leftarrow OldEstimate + StepSize \left[Target - OldEstimate\right] \tag{11}$$<br>表达式$\left[Target - OldEstimate\right]$是一个estimate error，通过向&quot;Target&quot;走一步减小error。这个&quot;Target&quot;给定了更新的方向，当然也有可能是noisy，在式子$10$中，target是第$n$个episode中state s的return。式子$10$的更新规则中StepSize$\frac{1}{n}$是在变的，一般我们叫它步长或者学习率，用$\alpha$表示。<br>在off-policy算法中，odrinary importance sampling和weighted importance sampling要分开。因为odirinary importance sampling只是对ratio缩放后的不同returns做了平均，还可以使用上面的公式。而对于weighted imporatance sampling，假设一系列episodes的returns是$G_1,G_2,\cdots, G_{n-1}$，对应的权重为$W_i$（比如$W_i=\rho_{t_i:T(t_i)-1}$），有：<br>$$V_n = \frac{\sum_{k=1}^{n-1}W_kG_k}{\sum_{k=1}^{n-1}W_k} \tag{11}$$<br>用$C_n$表示前$n$个episode returns的权重和，即$C_n=\sum_{k=1}^nW_k$，$V_n$的更新规则如下：<br>\begin{align*}<br>V_{n+1}&amp;=\frac{\sum_{k=1}^{n}W_kG_k}{\sum_{k=1}^{n}W_k}\\<br>&amp;=\frac{\sum_{k=1}^{n-1}W_kG_k + W_nG_n}{\sum_{k=1}^{n}W_k}\\<br>&amp;=\frac{1}{\sum_{k=1}^{n}W_k} \cdot \left(\sum_{k=1}^{n-1}W_kG_k + W_nG_n\right)\\<br>&amp;=\frac{1}{\sum_{k=1}^{n}W_k} \cdot \left(\frac{\sum_{k=1}^{n-1}W_kG_k}{\sum_{k=1}^{n-1}W_k}(\sum_{k=1}^{n-1}W_k) + W_nG_n\right)\\<br>&amp;=\frac{1}{\sum_{k=1}^{n}W_k} \cdot \left(V_n\cdot(\sum_{k=1}^{n-1}W_k) + W_nG_n\right)\\<br>&amp;=\frac{1}{\sum_{k=1}^{n}W_k} \cdot \left(V_n\cdot(\sum_{k=1}^{n-1}W_k + W_n - W_n) + W_nG_n\right)\\<br>&amp;=\frac{1}{\sum_{k=1}^{n}W_k} \cdot \left(V_n\cdot(\sum_{k=1}^{n}W_k - W_n) + W_nG_n\right)\\<br>&amp;=\frac{1}{\sum_{k=1}^{n}W_k} \cdot \left(V_n\cdot(\sum_{k=1}^{n}W_k) + W_nG_n - W_nV_n\right)\\<br>&amp;=\frac{V_n\cdot(\sum_{k=1}^{n}W_k)}{\sum_{k=1}^{n}W_k} + \frac{W_nG_n-W_nV_n}{\sum_{k=1}^{n}W_k}\\<br>&amp;=V_n + \frac{W_n}{C_n}(G_n-V_n)\\<br>\end{align*}<br>其中$C_0=0, C_{n+1} = C_n + W_{n+1}$，事实上，在$W_k=1$的情况下，即$\pi=b$时，上面的公式就变成了on-policy的公式。接下来给出一个episode-by-episode的MC  policy evaluation incremental algorithm，使用的是weighted importance sampling。</p><h3 id="off-policy-mc-prediction-算法">Off-policy MC Prediction 算法</h3><p>算法 4 Off-policy MC prediction(policy evaluation)<br>输入: 一个任意的target policy $\pi$<br>初始化，$Q(s,a)\in \mathbb{R}, C(s,a) = 0, \forall s\in S, a\in A(s)$<br><strong>Loop</strong> forever (for each episode)<br>$\qquad$$b\leftarrow$ 任意覆盖target policy $\pi$的behaviour policy<br>$\qquad$用behaviour policy $b$生成一个episode，$S_0,A_0,R_1,\cdots, S_{T-1},A_{T-1},R_T$<br>$\qquad$$G\leftarrow 0$<br>$\qquad$$W\leftarrow 1$<br>$\qquad$<strong>FOR</strong> $t \in T-1,T-2,\cdots, 0$并且$W\neq 0$<br>$\qquad\qquad$$G\leftarrow G+\gamma R_{t+1}$<br>$\qquad\qquad$$W\leftarrow = W\cdot \frac{\pi(A_t|S_t)}{b(A_t|S_t)}$！！！原书中这个是放在最后一行的，我怎么觉得应该放在这里。。<br>$\qquad\qquad$$C(S_t, A_t)\leftarrow C(S_t, A_t)+W$<br>$\qquad\qquad$$Q(S_t, A_t)\leftarrow Q(S_t, A_t)+ \frac{W}{C(S_t,A_t)}(G_t-Q(S_t,A_t))$<br>$\qquad$<strong>END FOR</strong><br><strong>思考：这里怎么把它转换为first-visit的算法</strong></p><h2 id="off-policy-mc-control">Off-policy MC Control</h2><p>这一节给出一个off-policy的MC control算法，target policy是greedy算法，而behaviour policy是soft算法，在不同的episode中可以采用不同的behaviour policy。<br>算法 5 Off-policy MC control<br>初始化，$Q(s,a)\in \mathbb{R}, C(s,a) = 0, \forall s\in S, a\in A(s), \pi(s)\leftarrow arg max_aQ(s, a)$<br><strong>Loop</strong> forever (for each episode)<br>$\qquad$$b\leftarrow$ 任意覆盖target policy $\pi$的behaviour policy<br>$\qquad$用behaviour policy $b$生成一个episode，$S_0,A_0,R_1,\cdots, S_{T-1},A_{T-1},R_T$<br>$\qquad$$G\leftarrow 0$<br>$\qquad$$W\leftarrow 1$<br>$\qquad$<strong>for</strong> $t \in T-1,T-2,\cdots, 0$并且$W\neq 0$<br>$\qquad\qquad$$G\leftarrow G+\gamma R_{t+1}$<br>$\qquad\qquad$$C(S_t, A_t)\leftarrow C(S_t, A_t)+W$<br>$\qquad\qquad$$Q(S_t, A_t)\leftarrow Q(S_t, A_t)+ \frac{W}{C(S_t,A_t)}(G_t-Q(S_t, A_t)$<br>$\qquad\qquad\pi(s)\leftarrow arg max_aQ(S_t,a)$<br>$\qquad\qquad$<strong>if</strong> $A_t\neq\pi(S_t)$ then<br>$\qquad\qquad\qquad$break for循环<br>$\qquad\qquad$<strong>end if</strong><br>$\qquad\qquad$$W\leftarrow = W\cdot \frac{1}{b(A_t|S_t)}$这个为什么放最后一行，我能理解要进行一下if判断，但是放在这里importance ratio不就不对了吗。。<br>$\qquad$<strong>end for</strong></p><h2 id="discounting-aware-importance-sampling">Discounting-aware Importance Sampling</h2><p>这一节介绍了discounting的importance sampling，假设有$100$个steps的一个episode，$\gamma=0$，其实它的returns在第一步以后就确定了，后面的$99$步已经没有影响了，因为$\gamma=0$，这里就介绍了discount importance sampling。<br>…</p><h2 id="per-decision-importance-sampling">Per-decision Importance Sampling</h2><p>根据每一个Reward确定进行importance sampling，而不是根据每一个returns。<br>…</p><h2 id="summary">Summary</h2><p>MC相对于DP的好处</p><ol><li>model-free</li><li>sample比较容易</li><li>很容易focus在一个我们需要的subset上</li><li>不进行bootstrap</li></ol><p>在MC control算法中，估计的是action-value fucntion，因为action value function能够在不知道model dynamic的情况下改进policy。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;mc-methods&quot;&gt;MC Methods&lt;/h2&gt;
&lt;p&gt;这章主要介绍了MC算法，MC算法通过采样，估计state-value function或者action value function。为了找到最好的policy，需要让policy不断的进行探索，但是我
      
    
    </summary>
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="蒙特卡洛" scheme="http://mxxhcm.github.io/tags/%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B/"/>
    
  </entry>
  
  <entry>
    <title>python defaultdict</title>
    <link href="http://mxxhcm.github.io/2019/04/25/python-defaultdict/"/>
    <id>http://mxxhcm.github.io/2019/04/25/python-defaultdict/</id>
    <published>2019-04-25T02:24:36.000Z</published>
    <updated>2019-05-06T16:22:27.708Z</updated>
    
    <content type="html"><![CDATA[<h2 id="使用defaultdict创建字典的值默认类型"><a href="#使用defaultdict创建字典的值默认类型" class="headerlink" title="使用defaultdict创建字典的值默认类型"></a>使用defaultdict创建字典的值默认类型</h2><h3 id="使用defaultdict创建值类型为dict的字典"><a href="#使用defaultdict创建值类型为dict的字典" class="headerlink" title="使用defaultdict创建值类型为dict的字典"></a>使用defaultdict创建值类型为dict的字典</h3><p>如下示例<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"></span><br><span class="line">ddd = defaultdict(dict)</span><br><span class="line">print(ddd)</span><br><span class="line"></span><br><span class="line">m = ddd[<span class="string">'a'</span>]</span><br><span class="line">m[<span class="string">'step'</span>] = <span class="number">1</span></span><br><span class="line">m[<span class="string">'exp'</span>] = <span class="number">3</span></span><br><span class="line">print(type(m))</span><br><span class="line">print(ddd)</span><br><span class="line"></span><br><span class="line">m = ddd[<span class="string">'b'</span>]</span><br><span class="line">m[<span class="string">'step'</span>] = <span class="number">1</span></span><br><span class="line">m[<span class="string">'exp'</span>] = <span class="number">3</span></span><br><span class="line">print(ddd)</span><br></pre></td></tr></table></figure></p><p>上述代码创建了一个dict，dict的value类型还是一个dict</p><blockquote><p>defaultdict(class ‘dict’&amp;gt , {})<br>&amp;lt class ‘dict’&amp;gt<br>defaultdict(&amp;lt class ‘dict’&amp;gt , {‘a’: {‘step’: 1, ‘exp’: 3}})<br>defaultdict(&amp;lt class ‘dict’&amp;gt , {‘a’: {‘step’: 1, ‘exp’: 3}, ‘b’: {‘step’: 1, ‘exp’: 3}})</p></blockquote><h3 id="使用defaultdict创建值类型为list的dict"><a href="#使用defaultdict创建值类型为list的dict" class="headerlink" title="使用defaultdict创建值类型为list的dict"></a>使用defaultdict创建值类型为list的dict</h3><p>如下示例<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"></span><br><span class="line">ddl = defaultdict(list)</span><br><span class="line">print(ddl)</span><br><span class="line">m = ddl[<span class="string">'a'</span>]</span><br><span class="line">print(type(m))</span><br><span class="line">m.append(<span class="number">3</span>)</span><br><span class="line">m.append(<span class="string">'hhhh'</span>)</span><br><span class="line">print(ddl)</span><br></pre></td></tr></table></figure></p><p>上述代码创建了一个dict，dict的value类型是一个list，输出如下</p><blockquote><p>defaultdict(&amp;lt class ‘list’&amp;gt , {})<br>&amp;lt class ‘list’&amp;gt<br>defaultdict(&amp;lt class ‘list’&amp;gt , {‘a’: [3, ‘hhhh’]})</p></blockquote><h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><p>点击获得<a href="https://github.com/mxxhcm/myown_code/blob/master/tools/python/defaultdict_test.py" target="_blank" rel="noopener">完整代码</a></p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>1.<a href="http://www.cnblogs.com/dancesir/p/8142775.html" target="_blank" rel="noopener">http://www.cnblogs.com/dancesir/p/8142775.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;使用defaultdict创建字典的值默认类型&quot;&gt;&lt;a href=&quot;#使用defaultdict创建字典的值默认类型&quot; class=&quot;headerlink&quot; title=&quot;使用defaultdict创建字典的值默认类型&quot;&gt;&lt;/a&gt;使用defaultdict创建字典
      
    
    </summary>
    
      <category term="python" scheme="http://mxxhcm.github.io/categories/python/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>python multiprocessing</title>
    <link href="http://mxxhcm.github.io/2019/04/23/python-multiprocessing-vs-threading/"/>
    <id>http://mxxhcm.github.io/2019/04/23/python-multiprocessing-vs-threading/</id>
    <published>2019-04-23T07:46:14.000Z</published>
    <updated>2019-10-11T05:30:51.354Z</updated>
    
    <content type="html"><![CDATA[<h2 id="multiprocessing-vs-multithread"><a href="#multiprocessing-vs-multithread" class="headerlink" title="multiprocessing vs multithread"></a>multiprocessing vs multithread</h2><p>多个threads可以在一个process中。同一个process中的所有threads共享相同的memory。而不同的processes有不同的memory areas，每一个都有自己的variables，进程之间为了通信，需要使用其他的channels，比如files, pipes和sockets等。thread比process更容易创建和管理，thread之间的交流比processes之间的交流更快。<br>这一节首先介绍一些GIL，然后介绍两个python的package，一个是threading，一个是multiprocessing。threading主要提供了多线程的实现。multiprocessing 主要提供了多进程的实现，当然也有多线程实现。</p><h2 id="GIL"><a href="#GIL" class="headerlink" title="GIL"></a>GIL</h2><p>thread有一个东西，叫做GIL(Global Interpreter Lock)，阻止同一个process中不同threads的同时运行，所以python多线程并不是多线程。举个例子，如果你有8个cores，使用8个threads，CPU的利用率不会达到800%，也不会快8倍。它会使用100%CPU，速度和原来相同，甚至会更慢，因为需要对多个threads进行调度。当然，有一些例外，如果大量的计算不是使用python运行的，而是使用一些自定义的C code进行GIL handling，就会得到你想要的性能。对于网络服务器或者GUI应用来说，大部分的事件都在等待，而不是在计算，这个时候就可以使用多个thread，相当于把他们都放在后台运行，而不需要终止相应的主线程。<br>如果想用纯python代码进行大量的CPU计算，使用threads并不能起到什么作用。使用process就没有GIL的问题，每个process有自己的GIL。这个时候需要在多线程和多进程之间做个权衡，因为进程之间的通信比线程之间通信的代价大得多。</p><h2 id="CPython的GIL实现"><a href="#CPython的GIL实现" class="headerlink" title="CPython的GIL实现"></a>CPython的GIL实现</h2><p>CPython 2.7中GIL是这样一行代码：<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">static</span> PyThread_type_lock interpreter_lock = <span class="number">0</span>; <span class="comment">/* This is the GIL */</span></span><br></pre></td></tr></table></figure></p><p>在Unix类系统中，PyThread_type_lock是标准的C lock mutex_t的别名。它的初始化方式如下：<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">void</span></span><br><span class="line">PyEval_InitThreads(<span class="keyword">void</span>)</span><br><span class="line">&#123;</span><br><span class="line">    interpreter_lock = PyThread_allocate_lock();</span><br><span class="line">    PyThread_acquire_lock(interpreter_lock);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>解释器中执行python的C代码必须持有这个lock。GIL的作用就是让你的程序足够简单：一个thread执行python代码，其他N个thread sleep或者等待I/O。或者可以等待threading.Lock或者其他同步操作。<br>那么什么时候threads进程切换呢？当一个thread 准备sleep或者进入等待I/O的时候，它释放GIL，其他thread请求GIL，执行相应的代码。这种任务叫做cooperative multitasking。还有一种是preemptive multitasking：在python2中一个thread不间断的执行1000个bytecode，或者python3中不间断的执行15 ms，然后放弃GIL让另一个thread运行。接下来举两个例子。</p><h2 id="cooperative-multithread"><a href="#cooperative-multithread" class="headerlink" title="cooperative multithread"></a>cooperative multithread</h2><p>在网络I/O中，具有很强的不确定性，当一个拥有GIL的thread请求网络I/O时，它释放GIL，这样子其他thread可以获得GIL继续执行，等到I/O完成时，该thread请求GIL继续执行。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">do_connect</span><span class="params">()</span>:</span></span><br><span class="line">    s = socket.socket()</span><br><span class="line">    s.connect((<span class="string">'python.org'</span>, <span class="number">80</span>))  <span class="comment"># drop the GIL</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">    t = threading.Thread(target=do_connect)</span><br><span class="line">    t.start()</span><br></pre></td></tr></table></figure></p><p>在上面的例子中，同一时刻只能有一个拥有GIL的thread执行python代码，但是一旦拥有GIL的thread开始connect，它就drop GIL，另一个thread可以申请GIL。但是所有的threads都可以drop GIL，也就是多个thread可以一起并行的等待sockets连接。<br>具体python在connect socket的时候是怎么drop GIL的，我们可以看一下socketmodule的c代码：<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* s.connect((host, port)) method */</span></span><br><span class="line"><span class="keyword">static</span> PyObject *</span><br><span class="line">sock_connect(PySocketSockObject *s, PyObject *addro)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">sock_addr_t</span> addrbuf;</span><br><span class="line">    <span class="keyword">int</span> addrlen;</span><br><span class="line">    <span class="keyword">int</span> res;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* convert (host, port) tuple to C address */</span></span><br><span class="line">    getsockaddrarg(s, addro, SAS2SA(&amp;addrbuf), &amp;addrlen);</span><br><span class="line"></span><br><span class="line">    Py_BEGIN_ALLOW_THREADS</span><br><span class="line">    res = connect(s-&gt;sock_fd, addr, addrlen);</span><br><span class="line">    Py_END_ALLOW_THREADS</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* error handling and so on .... */</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>其中Py_BEGIN_ALLOW_THREADS宏就是drop GIL，它的定义如下：<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">PyThread_release_lock(interpreter_lock);</span><br></pre></td></tr></table></figure></p><p>同样，Py_END_ALLOW_THREADS宏是请求GIL。thread可以在这里block，等待GIL被释放，申请GIL继续执行。</p><h2 id="preemptive-multithread"><a href="#preemptive-multithread" class="headerlink" title="preemptive multithread"></a>preemptive multithread</h2><p>除了自动释放GIL外，还可以强制的释放GIL。python代码的执行有两步，第一步将python源代码编译成二进制的bytecode；第二步，python interpreter的main loop，一个叫做PyEval_EvalFrameEx()的函数，读取bytecode，并且一个一个的执行。<br>在多线程的模式下，interpreter强制周期性的drop GIL。如下所示，是thread判断是否释放GIl的代码：<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (;;) &#123;</span><br><span class="line">    <span class="keyword">if</span> (--ticker &lt; <span class="number">0</span>) &#123;</span><br><span class="line">        ticker = check_interval;</span><br><span class="line">    </span><br><span class="line">        <span class="comment">/* Give another thread a chance */</span></span><br><span class="line">        PyThread_release_lock(interpreter_lock);</span><br><span class="line">    </span><br><span class="line">        <span class="comment">/* Other threads may run now */</span></span><br><span class="line">    </span><br><span class="line">        PyThread_acquire_lock(interpreter_lock, <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    bytecode = *next_instr++;</span><br><span class="line">    <span class="keyword">switch</span> (bytecode) &#123;</span><br><span class="line">        <span class="comment">/* execute the next instruction ... */</span> </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>默认设置下是1000个bytecode。所有的threads周期性的获取GIL，然后释放。在python3下，所有thread获得15ms的GIL，而不是1000个bytecode。</p><h2 id="python的thread-safety"><a href="#python的thread-safety" class="headerlink" title="python的thread safety"></a>python的thread safety</h2><p>但是，如果买票等之类的，必须保证操作的atomic，否则就会出现问题。对于sort() operation来说，它是atomic，所以无序担心。看下面一个code snippet<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">n = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">foo</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">global</span> n</span><br><span class="line">    n += <span class="number">1</span></span><br></pre></td></tr></table></figure></p><p>我们查看foo对应的bytecode：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> dis</span><br><span class="line"></span><br><span class="line">print(dis.dis(foo))</span><br><span class="line"></span><br><span class="line"><span class="comment">#   7           0 LOAD_GLOBAL              0 (n)</span></span><br><span class="line"><span class="comment">#               2 LOAD_CONST               1 (1)</span></span><br><span class="line"><span class="comment">#               4 INPLACE_ADD</span></span><br><span class="line"><span class="comment">#               6 STORE_GLOBAL             0 (n)</span></span><br><span class="line"><span class="comment">#               8 LOAD_CONST               0 (None)</span></span><br><span class="line"><span class="comment">#              10 RETURN_VALUE</span></span><br></pre></td></tr></table></figure></p><p>可以看出，foo有6个bytecode，如果在第三个bytecode处，强制释放了GIL锁，其他thread改了n的值，等到切回这个thread的时候，就会出错。。所以，为了保证不出问题，需要手动加一个lock，保证不会在这个时候释放GIL。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"></span><br><span class="line">n = <span class="number">0</span></span><br><span class="line">lock = threading.Lock()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">foo</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">global</span> n</span><br><span class="line">    <span class="keyword">with</span> lock:</span><br><span class="line">        n += <span class="number">1</span></span><br></pre></td></tr></table></figure></p><p>当然，如果operation本身就是atomic的话，就不需要了。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">l = [<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">2</span>]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">foo</span><span class="params">()</span>:</span></span><br><span class="line">    l.sort()</span><br></pre></td></tr></table></figure></p><h2 id="threading"><a href="#threading" class="headerlink" title="threading"></a>threading</h2><p>threading是python多线程的一个package。</p><h3 id="threading-Thread"><a href="#threading-Thread" class="headerlink" title="threading.Thread"></a>threading.Thread</h3><h4 id="代码示例"><a href="#代码示例" class="headerlink" title="代码示例"></a>代码示例</h4><p><a href="thread_Thread.py">代码地址</a><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"><span class="keyword">import</span> socket</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">do_connect</span><span class="params">(website)</span>:</span></span><br><span class="line">    s = socket.socket()</span><br><span class="line">    info = s.connect((website, <span class="number">80</span>))  <span class="comment"># drop the GIL</span></span><br><span class="line">    print(type(info))</span><br><span class="line">    print(info)</span><br><span class="line">    print(os.getpid())</span><br><span class="line"></span><br><span class="line">websites = [<span class="string">'python.org'</span>, <span class="string">'baidu.com'</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(websites)):</span><br><span class="line">    t = threading.Thread(target=do_connect, args=(websites[i],))</span><br><span class="line">    t.start()</span><br></pre></td></tr></table></figure></p><h3 id="threading-Lock"><a href="#threading-Lock" class="headerlink" title="threading.Lock"></a>threading.Lock</h3><h4 id="代码示例-1"><a href="#代码示例-1" class="headerlink" title="代码示例"></a>代码示例</h4><p><a href="threading_Lock.py">代码地址</a><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line">hhhh = <span class="number">100</span></span><br><span class="line">lock = threading.Lock()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_number</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">global</span> hhhh</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">        <span class="keyword">with</span> lock:</span><br><span class="line">            hhhh += <span class="number">1</span></span><br><span class="line">            print(<span class="string">"add: "</span>, hhhh)</span><br><span class="line">            time.sleep(<span class="number">0.015</span>)</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">subtract_number</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">global</span> hhhh</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">        <span class="keyword">with</span> lock:</span><br><span class="line">            hhhh -= <span class="number">1</span></span><br><span class="line">            print(<span class="string">"subtract:"</span>, hhhh)</span><br><span class="line">            time.sleep(<span class="number">0.015</span>)</span><br><span class="line"> </span><br><span class="line">job_list = []</span><br><span class="line">job_list.append(threading.Thread(target=subtract_number, args=()))</span><br><span class="line">job_list.append(threading.Thread(target=add_number, args=()))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> job_list:</span><br><span class="line">    t.start()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> job_list:</span><br><span class="line">    t.join()</span><br><span class="line"> </span><br><span class="line">print(<span class="string">"Done"</span>)</span><br></pre></td></tr></table></figure></p><h2 id="multiprocessing"><a href="#multiprocessing" class="headerlink" title="multiprocessing"></a>multiprocessing</h2><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><div class="table-container"><table><thead><tr><th>方法</th><th>并行</th><th>是否直接阻塞</th><th>目标函数</th><th>函数返回值</th><th>适用场景</th></tr></thead><tbody><tr><td>mp.Pool.apply</td><td>否</td><td>是</td><td>只能有一个函数</td><td>函数返回值</td></tr><tr><td>mp.Pool.apply_async</td><td>是</td><td>否，调用join()进行阻塞</td><td>可以相同可以不同</td><td>返回AysncResult对象</td></tr><tr><td>mp.Pool.map</td><td>是</td><td>是</td><td>目标函数相同，参数不同</td><td>所有processes完成后直接返回有序结果</td></tr><tr><td>mp.Pool.map_async</td><td>是</td><td>否，调用join()阻塞</td><td>不知道。。</td><td>返回AysncResult对象</td></tr><tr><td>mp.Process</td><td>是</td><td>否</td><td>可以相同可以不同</td><td>无直接返回值</td><td>适用于线程数量比较小</td></tr></tbody></table></div><p>mp.Pool适用于线程数量远大于cpu数量，mp.Process适用于线程数量小于或者等于cpu数量的场景。<br>mp.Pool.apply   适用于非并行，调用apply()直接阻塞，process执行结束后直接返回结果。<br>mp.Pool.apply_async 适用于并行，异步执行，目标函数可以相同可以不同，返回AysncResult对象，因为AsyncResult对象是有序的，所以调用get得到的结果也是有序的。调用join()进行阻塞，调用get()方法获得返回结果，get()方法也是阻塞方法。<br>mp.Pool.map     适用于并行，异步，目标函数相同，参数不同。调用map()函数直接阻塞，等待所有processes完成后直接返回有序结果。<br>mp.Pool.map_async   也是调用join()和get()都能阻塞。<br>mp.Process  适用于并行，异步，目标函数可以相同可以不同，返回的结果需要借助mp.Queue()等工具，mp.Queue()存储的结果是无序的，mp.Manager()存储的结果是有序的。无序的结果可以使用特殊方法进行排序。</p><h3 id="统计cpu数量"><a href="#统计cpu数量" class="headerlink" title="统计cpu数量"></a>统计cpu数量</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cpus = mp.cpu_count()</span><br></pre></td></tr></table></figure><h3 id="实现并行的几种常用方法"><a href="#实现并行的几种常用方法" class="headerlink" title="实现并行的几种常用方法"></a>实现并行的几种常用方法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 方式1</span></span><br><span class="line">pool.apply_async</span><br><span class="line"><span class="comment"># 方式2</span></span><br><span class="line">pool.map</span><br><span class="line"><span class="comment"># 方式3</span></span><br><span class="line">mp.Process</span><br></pre></td></tr></table></figure><h3 id="retrieve并行结果"><a href="#retrieve并行结果" class="headerlink" title="retrieve并行结果"></a>retrieve并行结果</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 方式1</span></span><br><span class="line">results_obj = [pool.apply_async(f, args=(x,)) <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">3</span>)]</span><br><span class="line">results = [result_obj.get() <span class="keyword">for</span> result_obj <span class="keyword">in</span> results_obj]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 方式2</span></span><br><span class="line">results = pool.map(f, range(<span class="number">7</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 方式3</span></span><br><span class="line">output = Queue()</span><br><span class="line">pool.Process(target=f, args=(output))</span><br></pre></td></tr></table></figure><h2 id="mp-Pool"><a href="#mp-Pool" class="headerlink" title="mp.Pool"></a>mp.Pool</h2><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>指定占用的CPU核数，进程的个数可以多于CPU的核数，Pool会负责调用。如果CPU核数小于进程数，一般遵循FIFO的原则进行调用。</p><h3 id="API"><a href="#API" class="headerlink" title="API"></a>API</h3><ul><li>Pool.apply, </li><li>Pool.apply_async, </li><li>Pool.map, </li><li>Pool.map_async。</li></ul><h4 id="python-apply"><a href="#python-apply" class="headerlink" title="python apply"></a>python apply</h4><p>在老版本的python中，调用具有任意参数的function要使用apply函数，<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">apply(f, args, kwargs)</span><br></pre></td></tr></table></figure></p><p>甚至在2.7版本中还存在apply函数，但是基本上不怎么用了，3版本中已经没有了这种形式，现在都是直接使用函数名：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">f(*args, **kwargs)</span><br></pre></td></tr></table></figure></p><h4 id="mp-Pool-apply-vs-mp-Pool-apply-async"><a href="#mp-Pool-apply-vs-mp-Pool-apply-async" class="headerlink" title="mp.Pool.apply vs mp.Pool.apply_async"></a>mp.Pool.apply vs mp.Pool.apply_async</h4><p>multiprocessing.Pool中也有类似的interface。Pool.apply和python内置的apply挺像的，只不过Pool.apply会在一个单独的process执行，并且该函数会阻塞直到进程调用结束，所以Pool.apply不能异步执行。可以使用apply_async使用多个workers并行处理。<br>Pool.apply_async和apply基本一样，只不过它会在调用后立即返回一个AsyncResult对象，不用等到进程结束再返回。然后使用get()方法获得函数调用的返回值，get()方法会阻塞直到process结束。也就是说Pool.apply(func, args, kwargs)和pool.apply_async(func, args, kwargs).get()等价。Pool.apply_async可以调用很多个不同的函数。<br>Pool.apply_async返回值是无序的。</p><h4 id="mp-Pool-map-vs-mp-Pool-map-async"><a href="#mp-Pool-map-vs-mp-Pool-map-async" class="headerlink" title="mp.Pool.map vs mp.Pool.map_async"></a>mp.Pool.map vs mp.Pool.map_async</h4><p>Pool.map应用于同一个函数的不同参数，它的返回值顺序和调用顺序是一致的。Pool.map(func, iterable)和Pool.map_async(func, iterable).get()是一样的。</p><h4 id="mp-Pool-map-vs-mp-Pool-apply"><a href="#mp-Pool-map-vs-mp-Pool-apply" class="headerlink" title="mp.Pool.map vs mp.Pool.apply"></a>mp.Pool.map vs mp.Pool.apply</h4><p>Pool.apply(f, args): f函数仅仅被process pool中的一个worker执行。<br>Pool.map(f, iterable): 将iterable分割成多个单独的task，就是相当于同一个函数，给定不同的参数，每一组是一个task，然后使用pool中所有的processes执行这些taskes。所以map也能实现并行处理，而且是有序结果。</p><h4 id="mp-Pool-map-vs-mp-Pool-apply-async"><a href="#mp-Pool-map-vs-mp-Pool-apply-async" class="headerlink" title="mp.Pool.map vs mp.Pool.apply_async"></a>mp.Pool.map vs mp.Pool.apply_async</h4><p>Pool.map返回的结果是有序的；<br>Pool.apply_async返回的结果是无序的。<br>Pool.map处理相同的函数，不同的参数；</p><blockquote><p>pool.map() is a completely different kind of animal, because it distributes a bunch of arguments to the same function (asynchronously), across the pool processes, and then waits until all function calls have completed before returning the list of results.<br>Pool.apply_async处理不同的参数。</p></blockquote><h3 id="retrieve-return-value"><a href="#retrieve-return-value" class="headerlink" title="retrieve return value"></a>retrieve return value</h3><p>Pool.apply()会直接返回结果。<br>Pool.apply_async()会返回一个AsyncResult，然后使用get()方法获得结果。</p><h3 id="其他问题"><a href="#其他问题" class="headerlink" title="其他问题"></a>其他问题</h3><p>pool.map传递多个参数，或者重复参数，使用他的另一个版本，pool.starmap()<br>如下示例，<a href>代码地址</a><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Pool</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> repeat</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(string, x)</span>:</span></span><br><span class="line">    print(string)</span><br><span class="line">    <span class="keyword">return</span> x*x</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    <span class="keyword">with</span> Pool(processes=<span class="number">4</span>) <span class="keyword">as</span> pool:</span><br><span class="line">        number = <span class="number">10</span></span><br><span class="line">        s = <span class="string">"hello"</span></span><br><span class="line">        print(pool.starmap(f, zip(repeat(s), range(number))))</span><br></pre></td></tr></table></figure></p><h3 id="使用流程"><a href="#使用流程" class="headerlink" title="使用流程"></a>使用流程</h3><ol><li>创建Pool进程池，指定cpu核数<br>pool = Pool(cpu_core) </li><li>使用apply_async添加进程<br>processes = [p1, p2, p3]<br>results = []<br>for p in processes:<br> results.append(pool.apply_async(p, args=()))</li><li>关闭进程池<br>pool.close()</li><li>等待所有进程执行完毕<br>pool.join()</li><li>访问结果<br>for res in results:<br> print(res.get())</li></ol><h3 id="代码示例-2"><a href="#代码示例-2" class="headerlink" title="代码示例"></a>代码示例</h3><p><a href>代码地址</a></p><h2 id="mp-Process"><a href="#mp-Process" class="headerlink" title="mp.Process"></a>mp.Process</h2><h3 id="简介-1"><a href="#简介-1" class="headerlink" title="简介"></a>简介</h3><p>每个进程占用一个CPU核。</p><h3 id="retrieve结果"><a href="#retrieve结果" class="headerlink" title="retrieve结果"></a>retrieve结果</h3><p>使用mp.Queue()或者mp.Pipe()等对象记录结果。Queue()不保证结果的顺序和task的执行顺序一致。</p><h3 id="使用流程-1"><a href="#使用流程-1" class="headerlink" title="使用流程"></a>使用流程</h3><h3 id="代码示例-3"><a href="#代码示例-3" class="headerlink" title="代码示例"></a>代码示例</h3><p><a href="Process.py">代码地址</a></p><h2 id="mp-Pool-vs-mp-Process"><a href="#mp-Pool-vs-mp-Process" class="headerlink" title="mp.Pool vs mp.Process"></a>mp.Pool vs mp.Process</h2><ol><li>Pool会负责对cpu进行调度，即tasks数量可以远大于worker数量，一个worker占用一个cpu核。而Process的task必须小于worker，每个worker只能运行一个task。</li><li>如果执行多个task的时候，Process一定会使用多个seperate workes，但是对于Pool来说，可能会使用同一个worker去执行多个task。如下示例，p1和p2一定是两个wrokers运行两个process，而pool中，pool中有两个worker，foo可以是第一个worker也可以是第二个worker运行的process解决的，而bar也可以是这两个中任意一个worker解决的，这种情况发生在foo已经运行结束了，两个worker都是空闲的，给bar任意分配一个worker。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">foo</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bar</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">p1 = Process(target=foo, args=())</span><br><span class="line">p2 = Process(target=bar, args=())</span><br><span class="line"></span><br><span class="line">p1.start()</span><br><span class="line">p2.start()</span><br><span class="line">p1.join()</span><br><span class="line">p2.join()</span><br><span class="line"></span><br><span class="line">pool = Pool(processes=<span class="number">2</span>)             </span><br><span class="line">r1 = pool.apply_async(foo)</span><br><span class="line">r2 = pool.apply_async(bar)</span><br></pre></td></tr></table></figure></li></ol><h3 id="代码示例-4"><a href="#代码示例-4" class="headerlink" title="代码示例"></a>代码示例</h3><p><a href="Pool_Process.py">代码地址</a></p><h2 id="join方法"><a href="#join方法" class="headerlink" title="join方法"></a>join方法</h2><h3 id="简介-2"><a href="#简介-2" class="headerlink" title="简介"></a>简介</h3><p>用来阻塞当前进程，直到该进程执行完毕，再继续执行后续代码。</p><h3 id="代码示例-5"><a href="#代码示例-5" class="headerlink" title="代码示例"></a>代码示例</h3><p><a href="https://github.com/mxxhcm/myown_code/blob/master/tools/py_process_thread/mp/mp_join.py" target="_blank" rel="noopener">代码地址</a><br>可以看出来，调用join()函数的时候，会等子进程执行完之后再继续执行；而不使用join()函数的话，在子进程开始执行的时候，就会继续向后执行了。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>1.<a href="https://www.cnblogs.com/lipijin/p/3709903.html" target="_blank" rel="noopener">https://www.cnblogs.com/lipijin/p/3709903.html</a><br>2.<a href="https://www.ellicium.com/python-multiprocessing-pool-process/" target="_blank" rel="noopener">https://www.ellicium.com/python-multiprocessing-pool-process/</a><br>3.<a href="https://stackoverflow.com/questions/8533318/multiprocessing-pool-when-to-use-apply-apply-async-or-map" target="_blank" rel="noopener">https://stackoverflow.com/questions/8533318/multiprocessing-pool-when-to-use-apply-apply-async-or-map</a><mp pool apply, apply_async, map用法><br>4.<a href="https://stackoverflow.com/questions/31711378/python-multiprocessing-how-to-know-to-use-pool-or-process" target="_blank" rel="noopener">https://stackoverflow.com/questions/31711378/python-multiprocessing-how-to-know-to-use-pool-or-process</a><mp process和pool.map获得不同目标函数process的结果，对mp.process无序结果进行排序><br>5.<a href="https://stackoverflow.com/questions/18176178/python-multiprocessing-process-or-pool-for-what-i-am-doing" target="_blank" rel="noopener">https://stackoverflow.com/questions/18176178/python-multiprocessing-process-or-pool-for-what-i-am-doing</a><mp pool.apply_async, process不同函数的多process><br>6.<a href="https://stackoverflow.com/questions/10415028/how-can-i-recover-the-return-value-of-a-function-passed-to-multiprocessing-proce" target="_blank" rel="noopener">https://stackoverflow.com/questions/10415028/how-can-i-recover-the-return-value-of-a-function-passed-to-multiprocessing-proce</a>&lt;获得传递给mp Process函数返回值的方法&gt;<br>7.<a href="https://docs.python.org/3/library/multiprocessing.html#sharing-state-between-processes" target="_blank" rel="noopener">https://docs.python.org/3/library/multiprocessing.html#sharing-state-between-processes</a><br>8.<a href="https://sebastianraschka.com/Articles/2014_multiprocessing.html" target="_blank" rel="noopener">https://sebastianraschka.com/Articles/2014_multiprocessing.html</a><br>9.<a href="https://opensource.com/article/17/4/grok-gil" target="_blank" rel="noopener">https://opensource.com/article/17/4/grok-gil</a><gil解释></gil解释></mp></mp></mp></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;multiprocessing-vs-multithread&quot;&gt;&lt;a href=&quot;#multiprocessing-vs-multithread&quot; class=&quot;headerlink&quot; title=&quot;multiprocessing vs multithread&quot;&gt;
      
    
    </summary>
    
      <category term="python" scheme="http://mxxhcm.github.io/categories/python/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="Pool" scheme="http://mxxhcm.github.io/tags/Pool/"/>
    
      <category term="Process" scheme="http://mxxhcm.github.io/tags/Process/"/>
    
      <category term="multiprocessing" scheme="http://mxxhcm.github.io/tags/multiprocessing/"/>
    
      <category term="threading" scheme="http://mxxhcm.github.io/tags/threading/"/>
    
  </entry>
  
  <entry>
    <title>python 类和函数的属性</title>
    <link href="http://mxxhcm.github.io/2019/04/14/python-%E7%B1%BB%E5%92%8C%E5%87%BD%E6%95%B0%E7%9A%84%E5%B1%9E%E6%80%A7/"/>
    <id>http://mxxhcm.github.io/2019/04/14/python-类和函数的属性/</id>
    <published>2019-04-14T06:49:41.000Z</published>
    <updated>2019-05-06T16:22:27.708Z</updated>
    
    <content type="html"><![CDATA[<h2 id="函数和类的默认属性"><a href="#函数和类的默认属性" class="headerlink" title="函数和类的默认属性"></a>函数和类的默认属性</h2><p>这里主要介绍类和函数的一些属性。<br><strong>dict</strong>用来描述对象的属性。对于类来说，它内部的变量就是它的数量，注意，不是它的member variable，但是对于函数来说不是。对于类来说，而对于类对象来说，输出的是整个类的属性，而<strong>dict</strong>输出的是self.variable的内容。</p><p>python中的函数有很多特殊的属性（包括自定义的函数和库函数）</p><ul><li><strong>doc</strong>  输出用户定义的关于函数的说明</li><li><strong>name</strong> 输出函数名字</li><li><strong>module</strong> 输出函数所在模块的名字</li><li><strong>dict</strong> 输出函数中的字典</li></ul><p>示例：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">myfunc</span><span class="params">()</span>:</span></span><br><span class="line">   <span class="string">'this func is to test the __doc__'</span></span><br><span class="line">   myfunc.func_attr = <span class="string">"attr"</span></span><br><span class="line">   print(<span class="string">"hhhh"</span>)</span><br><span class="line"> </span><br><span class="line">myfunc.func_attr1 = <span class="string">"first1"</span></span><br><span class="line">myfunc.func_attr2 = <span class="string">"first2"</span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">  print(myfunc.__doc__)</span><br><span class="line">  print(myfunc.__name__)</span><br><span class="line">  print(myfunc.__module__)</span><br><span class="line">  print(myfunc.__dict__)</span><br></pre></td></tr></table></figure></p><p>输出：</p><blockquote><p>this func is to test the <strong>doc</strong><br>myfunc<br><strong>main</strong><br>{‘func_attr1’: ‘first1’, ‘func_attr2’: ‘first2’}</p></blockquote><p>类也有很多特殊的属性（包括自定义的类和库中的类）</p><ul><li><strong>doc</strong>  输出用户定义的类的说明</li><li><strong>module</strong> 输出类所在模块的名字</li><li><strong>dict</strong> 输出类中的字典</li></ul><p>示例：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyClass</span>:</span></span><br><span class="line">  <span class="string">"""This is my class __doc__"""</span></span><br><span class="line">  class_name = <span class="string">"cllll"</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, test=None)</span>:</span></span><br><span class="line">     self.test = test</span><br><span class="line">  <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">  print(MyClass.__dict__)</span><br><span class="line">  print(MyClass.__doc__)</span><br><span class="line">  print(MyClass.__module__)</span><br></pre></td></tr></table></figure></p><p>输出：</p><blockquote><p>{‘<strong>module</strong>‘: ‘<strong>main</strong>‘, ‘<strong>doc</strong>‘: ‘This is my class <strong>doc</strong>‘, ‘class_name’: ‘cllll’, ‘<strong>init</strong>‘: \<function myclass.__init__ at 0x7f1349d44510\>, ‘<strong>dict</strong>‘: \<attribute '__dict__' of 'myclass' objects\>, ‘<strong>weakref</strong>‘: \<attribute '__weakref__' of 'myclass' objects\>}<br>This is my class <strong>doc</strong><br><strong>main</strong></attribute></attribute></function></p></blockquote><p>类的对象的属性</p><ul><li><strong>doc</strong>  输出用户定义的类的说明</li><li><strong>module</strong> 输出类对象所在模块的名字</li><li><strong>dict</strong> 输出类对象中的字典</li></ul><p>示例<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"> <span class="number">1</span> <span class="class"><span class="keyword">class</span> <span class="title">MyClass</span>:</span></span><br><span class="line"> <span class="number">2</span>   <span class="string">"""This is my class __doc__"""</span></span><br><span class="line"> <span class="number">3</span>   class_name = <span class="string">"cllll"</span></span><br><span class="line"> <span class="number">4</span>   <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, test=None)</span>:</span></span><br><span class="line"> <span class="number">5</span>      self.test = test</span><br><span class="line"> <span class="number">6</span>   <span class="keyword">pass</span></span><br><span class="line"> <span class="number">7</span> </span><br><span class="line"> <span class="number">8</span> <span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line"> <span class="number">9</span> </span><br><span class="line"><span class="number">10</span>   cl = MyClass()</span><br><span class="line"><span class="number">11</span>   print(cl.__dict__)</span><br><span class="line"><span class="number">12</span>   print(cl.__doc__)</span><br><span class="line"><span class="number">13</span>   print(cl.__module__)</span><br></pre></td></tr></table></figure></p><p>输出</p><blockquote><p>{‘test’: None}<br>This is my class <strong>doc</strong><br><strong>main</strong></p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;函数和类的默认属性&quot;&gt;&lt;a href=&quot;#函数和类的默认属性&quot; class=&quot;headerlink&quot; title=&quot;函数和类的默认属性&quot;&gt;&lt;/a&gt;函数和类的默认属性&lt;/h2&gt;&lt;p&gt;这里主要介绍类和函数的一些属性。&lt;br&gt;&lt;strong&gt;dict&lt;/strong&gt;用
      
    
    </summary>
    
      <category term="python" scheme="http://mxxhcm.github.io/categories/python/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>python zip和enumerate</title>
    <link href="http://mxxhcm.github.io/2019/04/13/python-zip%E5%92%8Cenumerate/"/>
    <id>http://mxxhcm.github.io/2019/04/13/python-zip和enumerate/</id>
    <published>2019-04-13T06:59:12.000Z</published>
    <updated>2019-05-06T16:22:27.708Z</updated>
    
    <content type="html"><![CDATA[<h2 id="zip-function"><a href="#zip-function" class="headerlink" title="zip function"></a>zip function</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">a = np.zeros((<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>))</span><br><span class="line">b = np.zeros((<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>))</span><br><span class="line">c = np.zeros((<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>))</span><br><span class="line">d = zip(a,b,c)   </span><br><span class="line">print(list(d))        </span><br><span class="line">d = list(zip(a,b,c))</span><br><span class="line">e,f,g = d</span><br></pre></td></tr></table></figure><p>这里d是一个什么呢，是多个tuple，数量是min(len(a),len(b),len(c))，每一个element是一个tuple，这个tuple的内容为(a[0],b[0],c[0])，….<br>打印出list(d)是一个list，这个list的长度为min(len(a),len(b),len(c))每一个element是一个tuple，tuple的形状是((2,2),(2,2),(2,2))<br>用zip的话，就是看一下它的len，然后在第一维上对他们进行拼接，形成多个新的元组。<br>例子<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a = (<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">b = (<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">c = (<span class="number">4</span>,<span class="number">5</span>)</span><br><span class="line">d = zip(a,b,c)</span><br><span class="line">print(list(c))</span><br></pre></td></tr></table></figure></p><blockquote><p>[(2,3),(3,4),(4,5)]    </p></blockquote><p>相当于吧tuple a和tuple b分别当做一个list的一个元组，然后结合成一个新的tuple的list，</p><h2 id="enumerate-iterable-start-0"><a href="#enumerate-iterable-start-0" class="headerlink" title="enumerate(iterable, start=0)"></a>enumerate(iterable, start=0)</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">seasons = [<span class="string">'Spring'</span>, <span class="string">'Summer'</span>, <span class="string">'Fall'</span>, <span class="string">'Winter'</span>]</span><br><span class="line">print(list(enumerate(seasons)))</span><br><span class="line">print(list(enumerate(seasons, start=<span class="number">1</span>)))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> enumerate(seansons):</span><br><span class="line">   print(i)</span><br></pre></td></tr></table></figure><blockquote><p>[(0, ‘Spring’), (1, ‘Summer’), (2, ‘Fall’), (3, ‘Winter’)]<br>[(1, ‘Spring’), (2, ‘Summer’), (3, ‘Fall’), (4, ‘Winter’)]<br>(0, ‘Spring’)<br>(1, ‘Summer’)<br>(2, ‘Fall’)<br>(3, ‘Winter’)</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;zip-function&quot;&gt;&lt;a href=&quot;#zip-function&quot; class=&quot;headerlink&quot; title=&quot;zip function&quot;&gt;&lt;/a&gt;zip function&lt;/h2&gt;&lt;figure class=&quot;highlight python&quot;&gt;
      
    
    </summary>
    
      <category term="python" scheme="http://mxxhcm.github.io/categories/python/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>python time</title>
    <link href="http://mxxhcm.github.io/2019/04/13/python-time/"/>
    <id>http://mxxhcm.github.io/2019/04/13/python-time/</id>
    <published>2019-04-13T06:52:30.000Z</published>
    <updated>2019-05-06T16:22:27.708Z</updated>
    
    <content type="html"><![CDATA[<h2 id="time（time-library-datetime-library-panda-Timestamp-）"><a href="#time（time-library-datetime-library-panda-Timestamp-）" class="headerlink" title="time（time library,datetime library,panda.Timestamp()）"></a>time（time library,datetime library,panda.Timestamp()）</h2><p>import time</p><h3 id="获得当前时间"><a href="#获得当前时间" class="headerlink" title="获得当前时间"></a>获得当前时间</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">time.time()        <span class="comment">#获得当前timestamp</span></span><br></pre></td></tr></table></figure><h3 id="time-localtime-timestamp"><a href="#time-localtime-timestamp" class="headerlink" title="time.localtime(timestamp)"></a>time.localtime(timestamp)</h3><p>得到一个struct_time<br>time.struct_time(tm_year=2018….)</p><h3 id="将struct-time转换成string"><a href="#将struct-time转换成string" class="headerlink" title="将struct time转换成string"></a>将struct time转换成string</h3><blockquote><p>Convert a tuple or struct_time representing a time as returned by gmtime() or localtime() to a string as specified by the format argument.If t is not provided,the current time as returned by localtime() is used.</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line">time.strftime(format,t)        <span class="comment">#将一个struct_time表示为一个格式化字符串</span></span><br><span class="line">time.strftime(<span class="string">"%Y-%m-%d %H:%M:%S"</span>,time.localtime())</span><br></pre></td></tr></table></figure><h3 id="将一个string类型的事件转换成struct-time"><a href="#将一个string类型的事件转换成struct-time" class="headerlink" title="将一个string类型的事件转换成struct time"></a>将一个string类型的事件转换成struct time</h3><blockquote><p>Parse a string representing a time accroding to a format.The return value is a struct_time as returned by gmtime() or localtime()</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line">time.strptime(<span class="string">"a string representing a time"</span>,<span class="string">"a format"</span>)    <span class="comment">#将某个format表示的time转化为一个struct_time()</span></span><br><span class="line">time.strptime(<span class="string">"2014-02-01 00:00:00"</span>,<span class="string">"%Y-%m-%d %H:%M:%S"</span>)</span><br></pre></td></tr></table></figure><h3 id="time-mktime"><a href="#time-mktime" class="headerlink" title="time.mktime()"></a>time.mktime()</h3><p>将时间t转换成timestamp</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;time（time-library-datetime-library-panda-Timestamp-）&quot;&gt;&lt;a href=&quot;#time（time-library-datetime-library-panda-Timestamp-）&quot; class=&quot;headerl
      
    
    </summary>
    
      <category term="python" scheme="http://mxxhcm.github.io/categories/python/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>python文件和目录操作(os和shutil)</title>
    <link href="http://mxxhcm.github.io/2019/04/13/python-file-dir/"/>
    <id>http://mxxhcm.github.io/2019/04/13/python-file-dir/</id>
    <published>2019-04-13T06:51:26.000Z</published>
    <updated>2019-10-11T05:30:51.354Z</updated>
    
    <content type="html"><![CDATA[<h2 id="文件和目录操作（os库和shutil库）"><a href="#文件和目录操作（os库和shutil库）" class="headerlink" title="文件和目录操作（os库和shutil库）"></a>文件和目录操作（os库和shutil库）</h2><p>import os</p><h2 id="查看信息"><a href="#查看信息" class="headerlink" title="查看信息"></a>查看信息</h2><p>不是函数，而是属性<br>os.linesep   #列出当前平台的行终止符<br>os.name    #列出当前的平台信息</p><h2 id="环境变量"><a href="#环境变量" class="headerlink" title="环境变量"></a>环境变量</h2><p>os.getenv(key, default=None)<br>如果key存在，返回key对应的值，否则返回默认值None，也可以指定默认返回值。</p><h2 id="列出目录"><a href="#列出目录" class="headerlink" title="列出目录"></a>列出目录</h2><p>file_dir_list = os.listdir(parent_dir)    #列出某个目录下的文件和目录，默认的话为当前目录<br>parent_dir 是一个目录<br>file_dir_list是一个list</p><p>os.path.exists(pathname)    #判断pathname是否存在<br>os.path.isdir(pathname)    #判断pathname是否是目录<br>os.path.isfile(pathname)    #判断pathname是否是文件<br>os.path.isabs(pathname)    #判断pathname是否是绝对路径</p><p>os.path.basename(pathname)    # 列出pathname的dir<br>os.path.dirname(pathname)        # 列出pathname的file name<br>os.path.split(pathname)    #将pathname分为dir和filename<br>os.path.split(pathname)    #将pathname的扩展名分离出来</p><p>os.path.join(“dir_name”,”file_name”)    # 拼接两个路径</p><p>os.getcwd()    #获得当前路径<br>os.chdir(pathname)    #改变当前路径<br>os.path.expanduser(pathname)    #如果pathname中包含”~”，将其替换成/homre/user/</p><h2 id="创建和删除"><a href="#创建和删除" class="headerlink" title="创建和删除"></a>创建和删除</h2><p>os.mkdir(pathname)    #创建新目录<br>os.rmdir(pathname)    #删除目录<br>os.makedirs(“/home/mxxhcm/Documents/“)    #创建多级目录<br>os.removedirs()    #删除多个目录<br>os.remove(file_pathname)    #删除文件</p><p>os.rename(old_pathname,new_pathname)    #重命名</p><h2 id="打开文件"><a href="#打开文件" class="headerlink" title="打开文件"></a>打开文件</h2><p>对于open文件来说，共有三种模式，分别为w,a,r<br>r的话，为只读，读取一个不存在的文件，会报错<br>r+的话，为可读写，读取一个不存在的文件，会报错<br>a的话，为追加读，读取一个不存在的文件，会创建该文件<br>w的话，为写入文件，读取一个不存在的文件，会创建改文件，打开一个存在的同名文件，会删除该文件，创建一个新的文件</p><h2 id="读取文件"><a href="#读取文件" class="headerlink" title="读取文件"></a>读取文件</h2><p>fp = open(file_path_name,”r+”)</p><h3 id="read-将文件读到一个字符串中"><a href="#read-将文件读到一个字符串中" class="headerlink" title="read()将文件读到一个字符串中"></a>read()将文件读到一个字符串中</h3><p>file_str = fp.read()<br>fp.read()会返回一个字符串，包含换行符</p><h3 id="readline"><a href="#readline" class="headerlink" title="readline()"></a>readline()</h3><p>for file_str in fp:<br>    print(file_str)<br>这里的file_str是一个str类型变量</p><h3 id="readlines-将文件读到一个列表中"><a href="#readlines-将文件读到一个列表中" class="headerlink" title="readlines()将文件读到一个列表中"></a>readlines()将文件读到一个列表中</h3><p>list(fp)<br>file_list = fp.readlines()<br>filt_list是一个list变量</p><h2 id="关闭文件"><a href="#关闭文件" class="headerlink" title="关闭文件"></a>关闭文件</h2><p>fp.close()<br>或者<br>with open(file_pathname, “r”) as f:<br>    file_str = fp.read()<br>当跳出这个语句块的时候，文件已经别关闭了。</p><h2 id="复制文件"><a href="#复制文件" class="headerlink" title="复制文件"></a>复制文件</h2><p>shutil.move(‘test’,’test_move’)    # 递归的将文件或者目录移动到另一个位置。如果目标位置是一个目录，移动到这个目录里，如果目标已经存在而且不是一个目录，可能会用os.rename()重命名<br>shutil.copyfile(src,dst) #复制文件内容，metadata没有复制<br>shutil.copymode(src,dst) #copy权限。文件内容，owner和group不变。<br>shutil.copystat(src,dst)    #copy权限，各种时间以及flags位。文件内容，owner，group不变<br>shutil.copy(src,dst)    #copy file,权限为也会被copied<br>shutil.copy2(src,dst)  #和先后调用shutil.copy()和shutil.copystat()函数一样<br>shutil.copytree(src,dst,symlinks=False,ignore=None)  #递归的将str目录结构复制到dst，dst位置必须不存在，目录的权限和时间用copystat来复制，文件的赋值用copy2()来复制<br>shutil.rmtree(path[,ignore_errors[,onerror]])   #删除一个完整的目录，无论目录是否为空</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>1.<a href="https://www.zhihu.com/question/48161511/answer/445852429" target="_blank" rel="noopener">https://www.zhihu.com/question/48161511/answer/445852429</a><br>2.<a href="https://www.geeksforgeeks.org/python-os-getenv-method/" target="_blank" rel="noopener">https://www.geeksforgeeks.org/python-os-getenv-method/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;文件和目录操作（os库和shutil库）&quot;&gt;&lt;a href=&quot;#文件和目录操作（os库和shutil库）&quot; class=&quot;headerlink&quot; title=&quot;文件和目录操作（os库和shutil库）&quot;&gt;&lt;/a&gt;文件和目录操作（os库和shutil库）&lt;/h2&gt;&lt;
      
    
    </summary>
    
      <category term="python" scheme="http://mxxhcm.github.io/categories/python/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>python regex</title>
    <link href="http://mxxhcm.github.io/2019/04/13/python-regex/"/>
    <id>http://mxxhcm.github.io/2019/04/13/python-regex/</id>
    <published>2019-04-13T06:50:41.000Z</published>
    <updated>2019-06-06T07:47:18.093Z</updated>
    
    <content type="html"><![CDATA[<h2 id="regex-examples"><a href="#regex-examples" class="headerlink" title="regex examples"></a>regex examples</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> regex <span class="keyword">as</span> re</span><br><span class="line"></span><br><span class="line"><span class="comment"># 找出每一行的数字</span></span><br><span class="line">string = <span class="string">"""a9apple1234</span></span><br><span class="line"><span class="string">2banana5678</span></span><br><span class="line"><span class="string">a3coconut9012"""</span></span><br><span class="line">pattern = <span class="string">"[0-9]+"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># search</span></span><br><span class="line">result = re.search(pattern, string)</span><br><span class="line">print(type(result))</span><br><span class="line">print(result[<span class="number">0</span>])</span><br><span class="line">print(result.group(<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># match</span></span><br><span class="line"><span class="comment"># 即使设置了MULTILINE模式，也只会匹配string的开头而不是每一行的开头</span></span><br><span class="line">result = re.match(pattern, string, re.S| re.M)  </span><br><span class="line">print(type(result))</span><br><span class="line"><span class="comment"># print(result[0])</span></span><br><span class="line"><span class="comment"># print(result.group(0))</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># findall</span></span><br><span class="line">result = re.findall(pattern, string)</span><br><span class="line">print(type(result))</span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure><h2 id="语法"><a href="#语法" class="headerlink" title="语法"></a>语法</h2><p>.   匹配除了newline的任意character，如果要匹配newline，需要添加re.DOTALL flag<br>*  重复至少$0$次<br>+  重复至少$1$次<br>?   重复$0$次或者$1$次<br>{}  重复多少次，如a{3,5}表示重复$3-5$次<br>[]  匹配方括号内的内容,如[1-9]表示匹配$1-9$中任意一个<br>^   matching the start of the string<br>$   matching the end os the string<br>+,*.?    都是贪婪匹配，如果加一个?为非贪婪匹配<br>+?,*?,??    为非贪婪匹配<br>()  匹配括号内的正则表达式，表示一个group的开始和结束<br>|   或<br>\number<br>\b  匹配empty string<br>\B<br>\d  匹配数字<br>\D  匹配非数字<br>\s  匹配空白符[ \t\n\r\f\v]<br>\S  匹配非空白符<br>\w  匹配unicode<br>\W<br>\A<br>\Z</p><h2 id="模块"><a href="#模块" class="headerlink" title="模块"></a>模块</h2><ul><li>re.compile(patern, flags=0)</li><li>re.match(pattern, string, flags=0)</li><li>re.fullmatch(pattern,string,flags=0)</li><li>re.search(pattern, string, flags=0)</li><li>re.split(pattern, string, maxflit=0, flags=0)</li><li>re.findall(pattern,string,flags=0)</li><li>re.sub(pattern,repl,string,count=0,flags=0)</li><li>re.subn(pattern,repl,string,count=0,flags=0)</li></ul><h3 id="flags"><a href="#flags" class="headerlink" title="flags"></a>flags</h3><blockquote><p>flags can be re.DEBUG, re.I, re.IGNORECASE, re.L, re.LOCALE, re.M, re.MULTILINE, re.S, re.DOTALL, re.U, re.UNICODE, re.X, re.VERBOSE</p></blockquote><ul><li>re.I(re.IGNORECASE) 忽略大小写</li><li>re.L(re.LOCALE) </li><li>re.M(re.MULTILINE) 多行模式，设置以后.匹配newline。指定re.S时，’^’匹配string的开始和each line的开始(紧跟着each newline); ‘$’匹配string的结束和each line的结束($在newline之前，immediately preceding each newline)。如果不指定的话, ‘^’只匹配string的开始,’$’只匹配string的结束和immediately before the newline (if any) at the end of the string，对应inline flag (?m).</li><li>re.S(re.DOTALL)  </li><li>re.U(re.UNICODE)</li><li>re.X(re.VERBOSE)</li><li>re.DEBUG</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> regex <span class="keyword">as</span> re</span><br><span class="line"></span><br><span class="line">print(re.I)</span><br><span class="line">print(re.IGNORECASE)</span><br><span class="line">print(re.L)</span><br><span class="line">print(re.LOCALE)</span><br><span class="line">print(re.M)</span><br><span class="line">print(re.MULTILINE)</span><br><span class="line">print(re.S)</span><br><span class="line">print(re.DOTALL)</span><br><span class="line">print(re.U)</span><br><span class="line">print(re.UNICODE)</span><br><span class="line">print(re.X)</span><br><span class="line">print(re.VERBOSE)</span><br><span class="line">print(re.DEBUG)</span><br><span class="line"></span><br><span class="line">print(re.M <span class="keyword">is</span> re.MULTILINE)</span><br><span class="line">print(re.I <span class="keyword">is</span> re.IGNORECASE)</span><br></pre></td></tr></table></figure><p>re.M例子<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">text = <span class="string">"""First line.</span></span><br><span class="line"><span class="string">Second line.</span></span><br><span class="line"><span class="string">Third line."""</span></span><br><span class="line"></span><br><span class="line">pattern = <span class="string">"^.*$"</span>  <span class="comment"># 匹配从开始到结束的任何字符</span></span><br><span class="line"><span class="comment"># 默认情况下， . 不匹配newlines，所以默认情况下不会有任何匹配结果，因为$之前有newline，而.不能匹配</span></span><br><span class="line"><span class="comment"># re.search(pattern, text) is None  # Nothing matches!</span></span><br><span class="line">print(re.search(pattern, text))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果设置MULTILINE模式, $匹配每一行的结尾，这个时候第一行就满足要求了，设置MULTILINE模式后，$匹配string的结尾和每一行的结尾（each newline之前)</span></span><br><span class="line">print(re.search(pattern, text, re.M).group())</span><br><span class="line"><span class="comment"># First line.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果同时设置MULTILINE和DOTALL模式, .能够匹配newlines，所以第一行和第二行的newline都匹配了，在贪婪模式下，就匹配了整个字符串。</span></span><br><span class="line">print(re.search(pattern, text, re.M | re.S).group())</span><br><span class="line"><span class="comment"># First line.</span></span><br><span class="line"><span class="comment"># Second line.</span></span><br><span class="line"><span class="comment"># Third line.</span></span><br></pre></td></tr></table></figure></p><h3 id="re-compile-patern-flags-0"><a href="#re-compile-patern-flags-0" class="headerlink" title="re.compile(patern, flags=0)"></a>re.compile(patern, flags=0)</h3><p>将一个正则表达式语句编译成一个正则表达式对象，可以调用正则表达式的match()和search()函数进行matching。</p><blockquote><p>complie a regular expression pattern into a regular expression object,which can be used for matching using its match() and search()</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">str = <span class="string">"https://abc https://dcdf https://httpfn https://hello"</span></span><br><span class="line"><span class="keyword">import</span> regex <span class="keyword">as</span> re</span><br><span class="line"></span><br><span class="line">prog = re.compile(pattern)</span><br><span class="line">results = prog.match(string)</span><br><span class="line"><span class="comment"># 上面两行等价于下面一行</span></span><br><span class="line"></span><br><span class="line">results = re.match(pattern, string)</span><br></pre></td></tr></table></figure><h3 id="re-match-pattern-string-flags-0-or-re-fullmatch-pattern-string-flags-0"><a href="#re-match-pattern-string-flags-0-or-re-fullmatch-pattern-string-flags-0" class="headerlink" title="re.match(pattern, string, flags=0) or re.fullmatch(pattern,string,flags=0)"></a>re.match(pattern, string, flags=0) or re.fullmatch(pattern,string,flags=0)</h3><p>在给定的string开始位置进行查找，返回一个match object。<strong>即使设置了MULTILINE mode, re.match()也只会在string的开始而不是each line的每一行开始匹配。</strong></p><h3 id="re-search-pattern-string-flags-0"><a href="#re-search-pattern-string-flags-0" class="headerlink" title="re.search(pattern, string, flags=0)"></a>re.search(pattern, string, flags=0)</h3><p>在给定的string任意位置进行查找，返回一个match object。</p><blockquote><p>locat a match anywhere in string</p></blockquote><h3 id="search-vs-match"><a href="#search-vs-match" class="headerlink" title="search() vs. match()"></a>search() vs. match()</h3><p>re.macth()在string的开头查找，而re.search在string的任意位置查找，他们都返回match object对象。如果不匹配，返回None。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">import re</span><br><span class="line"></span><br><span class="line">match1 = re.match(&quot;cd&quot;, &quot;abcdef&quot;)     # match</span><br><span class="line">match2 = re.search(&quot;cd&quot;, &quot;abcdef&quot;)    # search</span><br><span class="line">print(match1)</span><br><span class="line">print(match2)</span><br><span class="line">print(match2.group(0))</span><br><span class="line"># None</span><br><span class="line"># &lt;regex.Match object; span=(2, 4), match=&apos;cd&apos;&gt;</span><br><span class="line"># cd</span><br><span class="line"></span><br><span class="line">with open(&quot;content.txt&quot;, &quot;r&quot;) as f:</span><br><span class="line">    s = f.read()</span><br><span class="line">match3 = re.match(&quot;cd&quot;, s)     # match</span><br><span class="line">match4 = re.search(&quot;cd&quot;, s)</span><br><span class="line">print(match3)</span><br><span class="line">print(match4)</span><br><span class="line"># None</span><br><span class="line"># &lt;regex.Match object; span=(4, 6), match=&apos;cd&apos;&gt;</span><br></pre></td></tr></table></figure></p><h3 id="re-findall-pattern-string-flags-0"><a href="#re-findall-pattern-string-flags-0" class="headerlink" title="re.findall(pattern,string,flags=0)"></a>re.findall(pattern,string,flags=0)</h3><p>查找字符string所有匹配pattern的字符<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> regex <span class="keyword">as</span> re</span><br><span class="line">str = <span class="string">"https://abc https://dcdf https://httpfn https://hello"</span></span><br><span class="line">p2 = <span class="string">"https.+? "</span>    <span class="comment"># pay attention to space here</span></span><br><span class="line">results = re.findall(p2,str)</span><br></pre></td></tr></table></figure></p><blockquote><p>[‘<a href="https://abc" target="_blank" rel="noopener">https://abc</a> ‘, ‘<a href="https://dcdf" target="_blank" rel="noopener">https://dcdf</a> ‘, ‘<a href="https://httpfn" target="_blank" rel="noopener">https://httpfn</a> ‘]    # pay attention to the last ,since the end of str is \n</p></blockquote><h3 id="re-split-pattern-string-maxflit-0-flags-0"><a href="#re-split-pattern-string-maxflit-0-flags-0" class="headerlink" title="re.split(pattern, string, maxflit=0, flags=0)"></a>re.split(pattern, string, maxflit=0, flags=0)</h3><p>按照patten对string进行分割<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> regex <span class="keyword">as</span> re</span><br><span class="line">str = <span class="string">"https://abc https://dcdf https://httpfn https://hello"</span></span><br><span class="line">p1 = <span class="string">" "</span></span><br><span class="line">results = re.split(p1,str)</span><br></pre></td></tr></table></figure></p><blockquote><p>[‘<a href="https://abc" target="_blank" rel="noopener">https://abc</a>‘, ‘<a href="https://dcdf" target="_blank" rel="noopener">https://dcdf</a>‘, ‘<a href="https://httpfn" target="_blank" rel="noopener">https://httpfn</a>‘, ‘<a href="https://hello" target="_blank" rel="noopener">https://hello</a>‘]</p></blockquote><h3 id="re-sub-pattern-repl-string-count-0-flags-0"><a href="#re-sub-pattern-repl-string-count-0-flags-0" class="headerlink" title="re.sub(pattern,repl,string,count=0,flags=0)"></a>re.sub(pattern,repl,string,count=0,flags=0)</h3><h3 id="re-subn-pattern-repl-string-count-0-flags-0"><a href="#re-subn-pattern-repl-string-count-0-flags-0" class="headerlink" title="re.subn(pattern,repl,string,count=0,flags=0)"></a>re.subn(pattern,repl,string,count=0,flags=0)</h3><h3 id="…"><a href="#…" class="headerlink" title="…"></a>…</h3><h2 id="正则表达式对象-regular-express-object"><a href="#正则表达式对象-regular-express-object" class="headerlink" title="正则表达式对象(regular express object)"></a>正则表达式对象(regular express object)</h2><p>class re.RegexObject<br>只有re.compile()函数会产生正则表达式对象，正则</p><blockquote><p>only re.compile() will create a direct regular express object,<br>it’s a special class which design for re.compile().<br>正则表达式对象支持下列方法和属性</p><ul><li>match(string[,pos[,endpos]])</li><li>search(string[,pos[,endpos]])</li><li>findall(string[,pos[,endpos]])</li><li>split(string,maxsplit=0)</li><li>sub()</li><li>flags</li><li>groups</li><li>groupindex</li><li>pattern</li></ul></blockquote><h3 id="match-string-pos-endpos"><a href="#match-string-pos-endpos" class="headerlink" title="match(string[,pos[,endpos]])"></a>match(string[,pos[,endpos]])</h3><h3 id="search-string-pos-endpos"><a href="#search-string-pos-endpos" class="headerlink" title="search(string[,pos[,endpos]])"></a>search(string[,pos[,endpos]])</h3><h3 id="findall-string-pos-endpos"><a href="#findall-string-pos-endpos" class="headerlink" title="findall(string[,pos[,endpos]])"></a>findall(string[,pos[,endpos]])</h3><h3 id="split-string-maxsplit-0"><a href="#split-string-maxsplit-0" class="headerlink" title="split(string,maxsplit=0)"></a>split(string,maxsplit=0)</h3><h3 id="sub"><a href="#sub" class="headerlink" title="sub()"></a>sub()</h3><h3 id="flags-1"><a href="#flags-1" class="headerlink" title="flags"></a>flags</h3><h3 id="groups"><a href="#groups" class="headerlink" title="groups"></a>groups</h3><h3 id="groupindex"><a href="#groupindex" class="headerlink" title="groupindex"></a>groupindex</h3><h3 id="pattern"><a href="#pattern" class="headerlink" title="pattern"></a>pattern</h3><h2 id="匹配对象-match-objects"><a href="#匹配对象-match-objects" class="headerlink" title="匹配对象(match objects)"></a>匹配对象(match objects)</h2><p>class re.MatchObject<br>匹配是否成功</p><blockquote><p>match objects have a boolean value of True.</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">match = re.search(pattern, string)</span><br><span class="line">if match:</span><br><span class="line">   processs(match)</span><br></pre></td></tr></table></figure><p>MatchObject支持以下方法和属性</p><ul><li>group([group1,..])</li><li>groups([default=None])</li><li>groupdict(default=None)</li><li>start([group])</li><li>end([group])</li><li>span([group])</li><li>pos</li><li>endpos</li><li>lstindex</li><li>lastgroup</li><li>re</li><li>string</li></ul><h3 id="group-group1"><a href="#group-group1" class="headerlink" title="group([group1,..])"></a>group([group1,..])</h3><p>group的话pattern需要多个()</p><h3 id="groups-default"><a href="#groups-default" class="headerlink" title="groups([default])"></a>groups([default])</h3><p>返回一个元组</p><blockquote><p>return a tuple containing all the subgroups of the match.</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">re.match(<span class="string">r"(\d+)\.(\d+)"</span>,<span class="string">"24.1632"</span>)</span><br><span class="line">m.groups()</span><br></pre></td></tr></table></figure><blockquote><p>(‘24’,’1632’)</p></blockquote><p>show default<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">m = re.match(<span class="string">r"(\d+)\.?(\d+)?"</span>, <span class="string">"24"</span>)</span><br><span class="line">m.groups()      <span class="comment"># Second group defaults to None.</span></span><br></pre></td></tr></table></figure></p><blockquote><p>(‘24’, None)</p></blockquote><p>change default to 0<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">m.groups(<span class="string">'0'</span>)  <span class="comment"># Now, the second group defaults to '0'.</span></span><br><span class="line">(<span class="string">'24'</span>, <span class="string">'0'</span>)</span><br></pre></td></tr></table></figure></p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>1.<a href="https://stackoverflow.com/questions/180986/what-is-the-difference-between-re-search-and-re-match" target="_blank" rel="noopener">https://stackoverflow.com/questions/180986/what-is-the-difference-between-re-search-and-re-match</a><br>2.<a href="https://devdocs.io/python~3.7/library/re" target="_blank" rel="noopener">https://devdocs.io/python~3.7/library/re</a><br>3.<a href="https://mail.python.org/pipermail/python-list/2014-July/674576.html" target="_blank" rel="noopener">https://mail.python.org/pipermail/python-list/2014-July/674576.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;regex-examples&quot;&gt;&lt;a href=&quot;#regex-examples&quot; class=&quot;headerlink&quot; title=&quot;regex examples&quot;&gt;&lt;/a&gt;regex examples&lt;/h2&gt;&lt;figure class=&quot;highlight 
      
    
    </summary>
    
      <category term="python" scheme="http://mxxhcm.github.io/categories/python/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="正则表达式" scheme="http://mxxhcm.github.io/tags/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/"/>
    
  </entry>
  
  <entry>
    <title>python数组初始化</title>
    <link href="http://mxxhcm.github.io/2019/04/13/python-%E6%95%B0%E7%BB%84%E5%88%9D%E5%A7%8B%E5%8C%96/"/>
    <id>http://mxxhcm.github.io/2019/04/13/python-数组初始化/</id>
    <published>2019-04-13T06:49:35.000Z</published>
    <updated>2019-06-09T03:10:21.669Z</updated>
    
    <content type="html"><![CDATA[<h2 id="array-initialize"><a href="#array-initialize" class="headerlink" title="array initialize"></a>array initialize</h2><p>array_one_dimension =  [ 0 for i in range(cols)]<br>array_multi_dimension  = [[0 for i in range(cols)] for j in range(rows)]</p><h2 id="numpy"><a href="#numpy" class="headerlink" title="numpy"></a>numpy</h2><ul><li>numpy.array()</li><li>numpy.zeros()</li><li>numpy.empty()</li></ul><p>返回np.ndarray数组</p><h3 id="np-ndarray属性"><a href="#np-ndarray属性" class="headerlink" title="np.ndarray属性"></a>np.ndarray属性</h3><p>ndarray.shape        #array的shape<br>ndarray.ndim            #array的维度<br>ndarray.size            #the number of ndarray in array<br>ndarray.dtype        #type of the number in array<br>ndarray.itemsize        #size of the element in array<br>array[array &gt; 0].size    #统计一个数组有多少个非零元素，不论array的维度是多少</p><h3 id="numpy-array"><a href="#numpy-array" class="headerlink" title="numpy.array()"></a>numpy.array()</h3><p>np.array(object,dtype=None,copy=True,order=False,subok=False,ndim=0)</p><h3 id="numpy-zeros"><a href="#numpy-zeros" class="headerlink" title="numpy.zeros()"></a>numpy.zeros()</h3><p>np.zeros(shape,dtype=float,order=’C’)</p><h3 id="numpy-empty"><a href="#numpy-empty" class="headerlink" title="numpy.empty()"></a>numpy.empty()</h3><p>np.empty(shape,dtype=float,order=’C’)</p><h3 id="numpy-random-randn-shape"><a href="#numpy-random-randn-shape" class="headerlink" title="numpy.random.randn(shape)"></a>numpy.random.randn(shape)</h3><p>np.random.randn(3,4)</p><h3 id="numpy-arange"><a href="#numpy-arange" class="headerlink" title="numpy.arange()"></a>numpy.arange()</h3><h3 id="numpy-linspace"><a href="#numpy-linspace" class="headerlink" title="numpy.linspace()"></a>numpy.linspace()</h3>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;array-initialize&quot;&gt;&lt;a href=&quot;#array-initialize&quot; class=&quot;headerlink&quot; title=&quot;array initialize&quot;&gt;&lt;/a&gt;array initialize&lt;/h2&gt;&lt;p&gt;array_one_dime
      
    
    </summary>
    
      <category term="python" scheme="http://mxxhcm.github.io/categories/python/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="numpy" scheme="http://mxxhcm.github.io/tags/numpy/"/>
    
  </entry>
  
  <entry>
    <title>python2和python3中的dict</title>
    <link href="http://mxxhcm.github.io/2019/04/13/python-dict/"/>
    <id>http://mxxhcm.github.io/2019/04/13/python-dict/</id>
    <published>2019-04-13T06:46:26.000Z</published>
    <updated>2019-05-06T16:22:27.708Z</updated>
    
    <content type="html"><![CDATA[<h2 id="python2和python3的dict"><a href="#python2和python3的dict" class="headerlink" title="python2和python3的dict"></a>python2和python3的dict</h2><h3 id="将object转换为dict"><a href="#将object转换为dict" class="headerlink" title="将object转换为dict"></a>将object转换为dict</h3><p>vars([object]) -&gt; dictionary</p><h3 id="python2-dict"><a href="#python2-dict" class="headerlink" title="python2 dict"></a>python2 dict</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">m_dict = &#123;&apos;a&apos;: 10, &apos;b&apos;: 20&#125;</span><br><span class="line"></span><br><span class="line">values = m_dict.values()</span><br><span class="line">print(type(values))</span><br><span class="line">print(values)</span><br><span class="line">print(&quot;\n&quot;)</span><br><span class="line"></span><br><span class="line">items = m_dict.items()</span><br><span class="line">print(type(items))</span><br><span class="line">print(items)</span><br><span class="line">print(&quot;\n&quot;)</span><br><span class="line"></span><br><span class="line">keys = m_dict.keys()</span><br><span class="line">print(type(keys))</span><br><span class="line">print(keys)</span><br><span class="line">print(&quot;\n&quot;)</span><br><span class="line"></span><br><span class="line">l_values = list(values)</span><br><span class="line">print(type(l_values))</span><br><span class="line">print(l_values)</span><br><span class="line"></span><br><span class="line">输出：</span><br></pre></td></tr></table></figure><h3 id="python3-dict"><a href="#python3-dict" class="headerlink" title="python3 dict"></a>python3 dict</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">m_dict = &#123;&apos;a&apos;: 10, &apos;b&apos;: 20&#125;</span><br><span class="line"></span><br><span class="line">values = m_dict.values()</span><br><span class="line">print(type(values))</span><br><span class="line">print(values) print(&quot;\n&quot;)</span><br><span class="line"></span><br><span class="line">items = m_dict.items()</span><br><span class="line">print(type(items))</span><br><span class="line">print(items)</span><br><span class="line">print(&quot;\n&quot;)</span><br><span class="line"></span><br><span class="line">keys = m_dict.keys()</span><br><span class="line">print(type(keys))</span><br><span class="line">print(keys)</span><br><span class="line">print(&quot;\n&quot;)</span><br><span class="line"></span><br><span class="line">l_values = list(values)</span><br><span class="line">print(type(l_values))</span><br><span class="line">print(l_values)</span><br></pre></td></tr></table></figure><p>输出：</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;python2和python3的dict&quot;&gt;&lt;a href=&quot;#python2和python3的dict&quot; class=&quot;headerlink&quot; title=&quot;python2和python3的dict&quot;&gt;&lt;/a&gt;python2和python3的dict&lt;/h2&gt;&lt;
      
    
    </summary>
    
      <category term="python" scheme="http://mxxhcm.github.io/categories/python/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>python中的深复制和浅复制</title>
    <link href="http://mxxhcm.github.io/2019/04/13/python-%E6%B7%B1%E5%A4%8D%E5%88%B6%E5%92%8C%E6%B5%85%E5%A4%8D%E5%88%B6/"/>
    <id>http://mxxhcm.github.io/2019/04/13/python-深复制和浅复制/</id>
    <published>2019-04-13T06:43:31.000Z</published>
    <updated>2019-05-06T16:22:27.708Z</updated>
    
    <content type="html"><![CDATA[<h2 id="简单赋值，浅拷贝，深拷贝"><a href="#简单赋值，浅拷贝，深拷贝" class="headerlink" title="简单赋值，浅拷贝，深拷贝"></a>简单赋值，浅拷贝，深拷贝</h2><h3 id="简单赋值"><a href="#简单赋值" class="headerlink" title="简单赋值"></a>简单赋值</h3><h4 id="str"><a href="#str" class="headerlink" title="str"></a>str</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = <span class="string">'hello'</span></span><br><span class="line">b = <span class="string">'hello'</span></span><br><span class="line">c = a</span><br><span class="line">print(id(a),id(b),id(c))</span><br></pre></td></tr></table></figure><blockquote><p>2432356754632  2432356754632  2432356754632</p></blockquote><p>这里打印出a，b，c的id是一样的，因为他们全是指向’hello’这个字符串在内存中的地址<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = <span class="string">'world'</span></span><br><span class="line">print(id(a),id(b),id(c))</span><br></pre></td></tr></table></figure></p><blockquote><p>2432356757376  2432356754632  2432356754632</p></blockquote><p>将a指向一个新的字符串’world’,所以变量a的地址就改变了，指向字符串’world’的地址，但是b和c还是指向字符串’hello’的地址。</p><h4 id="list"><a href="#list" class="headerlink" title="list"></a>list</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = [<span class="string">'hello'</span>]</span><br><span class="line">b = [<span class="string">'hello'</span>]</span><br><span class="line">c = a</span><br><span class="line">print(id(a),id(b),id(c))</span><br></pre></td></tr></table></figure><blockquote><p>2432356788424 2432356797064 2432356788424</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">b = [<span class="string">'world'</span>]</span><br><span class="line">print(id(a),id(b),id(c))</span><br></pre></td></tr></table></figure><blockquote><p>2432356798024 2432356797064 2432356788424</p></blockquote><h4 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h4><p>简单赋值是先给一个变量分配内存，然后把变量的地址赋值给一个变量名。<br>对于一些不可变的类型，比如str，int等，某一个值在内存中的地址是固定的，如果用赋值操作直接指向一个值的话，那么变量名指向的就是这个值在内存中地址。<br>比如a=’hello’,b=’hello’,这样a和b的id是相同的，都指向内存中hello的地址<br>对于一些可变的类型，比如list，因为他是可变的，所以如果用赋值操作指向同一个值的话，那么这几个变量的地址也不一样<br>比如a =[‘hello’],b=[‘hello’],这样a和b的id是不同的，虽然他们指向的值是一样的，</p><h3 id="浅拷贝"><a href="#浅拷贝" class="headerlink" title="浅拷贝"></a>浅拷贝</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">a = [<span class="string">'hello'</span> , [<span class="number">123</span>] ]</span><br><span class="line">b = a[:]</span><br><span class="line">a = [<span class="string">'hello'</span> , [<span class="number">123</span>] ]</span><br><span class="line">b = a[:]</span><br><span class="line">print(a,b)</span><br><span class="line">print(id(a),id(b))</span><br><span class="line">print(id(a[<span class="number">0</span>]),id(a[<span class="number">1</span>]))</span><br><span class="line">print(id(b[<span class="number">0</span>]),id(b[<span class="number">1</span>]))</span><br></pre></td></tr></table></figure><blockquote><p>[‘hello’, [123]] [‘hello’, [123]]<br>2432356775368 2432356775432 2432356754632 2432356774984<br>2432356754632 2432356774984</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;a[<span class="number">0</span>] = <span class="string">'world'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(a,b)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(id(a),id(b))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(id(a[<span class="number">0</span>]),id(a[<span class="number">1</span>]))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(id(b[<span class="number">0</span>]),id(b[<span class="number">1</span>]))</span><br></pre></td></tr></table></figure><blockquote><p>[‘world’, [123]] [‘hello’, [123]]<br>2432356775368 2432356775432<br>2432356756424 2432356774984<br>2432356754632 2432356774984</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a[<span class="number">1</span>].append(<span class="number">3</span>)</span><br><span class="line">print(a,b)</span><br><span class="line">print(id(a),id(b))</span><br><span class="line">print(id(a[<span class="number">0</span>]),id(a[<span class="number">1</span>]))</span><br><span class="line">print(id(b[<span class="number">0</span>]),id(b[<span class="number">1</span>]))</span><br></pre></td></tr></table></figure><blockquote><p>[‘world’, [123, 3]] [‘hello’, [123, 3]]<br>2432356775368 2432356775432<br>2432356756424 2432356774984<br>2432356754632 2432356774984</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">a[<span class="number">1</span>] = [<span class="number">123</span>]</span><br><span class="line">print(a,b)</span><br><span class="line">print(id(a),id(b))</span><br><span class="line">print(id(a[<span class="number">0</span>]),id(a[<span class="number">1</span>]))</span><br><span class="line">print(id(b[<span class="number">0</span>]),id(b[<span class="number">1</span>]))</span><br><span class="line">``` </span><br><span class="line">&gt; [<span class="string">'world'</span>, [<span class="number">123</span>]] [<span class="string">'hello'</span>, [<span class="number">123</span>, <span class="number">3</span>]]</span><br><span class="line"><span class="number">2432356775368</span> <span class="number">2432356775432</span></span><br><span class="line"><span class="number">2432356756424</span> <span class="number">2432356822984</span></span><br><span class="line"><span class="number">2432356754632</span> <span class="number">2432356774984</span></span><br><span class="line"></span><br><span class="line"><span class="comment">### 深拷贝</span></span><br><span class="line">``` python</span><br><span class="line"><span class="keyword">from</span> copy <span class="keyword">import</span> deepcopy</span><br><span class="line">a = [<span class="string">'hello'</span>,[<span class="number">123</span>,<span class="number">234</span>]</span><br><span class="line">b = deepcopy(a)</span><br></pre></td></tr></table></figure><p>a，b以及a，b中任何元素（除了str，int等类型）的地址都是不一样的</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;简单赋值，浅拷贝，深拷贝&quot;&gt;&lt;a href=&quot;#简单赋值，浅拷贝，深拷贝&quot; class=&quot;headerlink&quot; title=&quot;简单赋值，浅拷贝，深拷贝&quot;&gt;&lt;/a&gt;简单赋值，浅拷贝，深拷贝&lt;/h2&gt;&lt;h3 id=&quot;简单赋值&quot;&gt;&lt;a href=&quot;#简单赋值&quot; cla
      
    
    </summary>
    
      <category term="python" scheme="http://mxxhcm.github.io/categories/python/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>python special method</title>
    <link href="http://mxxhcm.github.io/2019/04/13/python-special-method/"/>
    <id>http://mxxhcm.github.io/2019/04/13/python-special-method/</id>
    <published>2019-04-13T06:41:38.000Z</published>
    <updated>2019-05-06T16:22:27.708Z</updated>
    
    <content type="html"><![CDATA[<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>print(object)就是调用了类对象object的<strong>repr</strong>()函数<br>如下代码<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Tem</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">     <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__repr__</span><span class="params">(self)</span>:</span></span><br><span class="line">     <span class="keyword">return</span> <span class="string">"tem class"</span></span><br></pre></td></tr></table></figure></p><p>声明类对象 </p><blockquote><blockquote><blockquote><p>Tem tem<br>下面两行代码的功能是一样的。<br>print(tem)<br>print(repr(tem))</p></blockquote></blockquote></blockquote><h2 id="基本的自定义方法"><a href="#基本的自定义方法" class="headerlink" title="基本的自定义方法"></a>基本的自定义方法</h2><h3 id="object-new"><a href="#object-new" class="headerlink" title="object.new"></a>object.<strong>new</strong></h3><h3 id="object-init"><a href="#object-init" class="headerlink" title="object.init"></a>object.<strong>init</strong></h3><h3 id="object-repr和object-str"><a href="#object-repr和object-str" class="headerlink" title="object.repr和object.str"></a>object.<strong>repr</strong>和object.<strong>str</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Tem</span><span class="params">(object)</span>:</span></span><br><span class="line">  <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TemStr</span><span class="params">(object)</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__str__</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">'foo'</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TemRepr</span><span class="params">(object)</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__repr__</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">'foo'</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TemStrRepr</span><span class="params">(object)</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__repr__</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">'foo'</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__str__</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">'foo_str'</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>: </span><br><span class="line">   tem = Tem() </span><br><span class="line">   print(str(tem)) </span><br><span class="line">   print(repr(tem)) </span><br><span class="line">   tem_str = TemStr() </span><br><span class="line">   print(str(tem_str)) </span><br><span class="line">   print(repr(tem_str)) </span><br><span class="line">   tem_repr = TemRepr() </span><br><span class="line">   print(str(tem_repr)) </span><br><span class="line">   print(repr(tem_repr)) </span><br><span class="line">   tem_str_repr = TemStrRepr() </span><br><span class="line">   print(str(tem_str_repr)) </span><br><span class="line">   print(repr(tem_str_repr))</span><br></pre></td></tr></table></figure><p>单独重载<strong>repr</strong>，<strong>str</strong>也会调用<strong>repr</strong>，<br>但是单独重载<strong>str</strong>,<strong>repr</strong>不会调用它。<br><strong>repr</strong>面向的是程序员，而<strong>str</strong>面向的是普通用户。它们都用来返回一个字符串，这个字符串可以是任何字符串，我觉得这个函数的目的就是将对象转化为字符串。</p><h3 id="object-bytes"><a href="#object-bytes" class="headerlink" title="object.bytes"></a>object.<strong>bytes</strong></h3><h2 id="自定义属性方法"><a href="#自定义属性方法" class="headerlink" title="自定义属性方法"></a>自定义属性方法</h2><h3 id="object-getattr-self-name"><a href="#object-getattr-self-name" class="headerlink" title="object.getattr(self, name)"></a>object.<strong>getattr</strong>(self, name)</h3><h3 id="object-setattr-self-name"><a href="#object-setattr-self-name" class="headerlink" title="object.setattr(self, name)"></a>object.<strong>setattr</strong>(self, name)</h3><h2 id="比较"><a href="#比较" class="headerlink" title="比较"></a>比较</h2><h3 id="object-eq-self-others"><a href="#object-eq-self-others" class="headerlink" title="object.eq(self, others)"></a>object.<strong>eq</strong>(self, others)</h3><h3 id="object-lt-self-others"><a href="#object-lt-self-others" class="headerlink" title="object.lt(self, others)"></a>object.<strong>lt</strong>(self, others)</h3><h3 id="object-le-self-others"><a href="#object-le-self-others" class="headerlink" title="object.le(self, others)"></a>object.<strong>le</strong>(self, others)</h3><h3 id="object-ne-self-others"><a href="#object-ne-self-others" class="headerlink" title="object.ne(self, others)"></a>object.<strong>ne</strong>(self, others)</h3><h3 id="object-gt-self-others"><a href="#object-gt-self-others" class="headerlink" title="object.gt(self, others)"></a>object.<strong>gt</strong>(self, others)</h3><h3 id="object-ge-self-others"><a href="#object-ge-self-others" class="headerlink" title="object.ge(self, others)"></a>object.<strong>ge</strong>(self, others)</h3><h2 id="特殊属性"><a href="#特殊属性" class="headerlink" title="特殊属性"></a>特殊属性</h2><h3 id="object-dict"><a href="#object-dict" class="headerlink" title="object.dict"></a>object.<strong>dict</strong></h3><h3 id="instance-class"><a href="#instance-class" class="headerlink" title="instance.class"></a>instance.<strong>class</strong></h3>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;结论&quot;&gt;&lt;a href=&quot;#结论&quot; class=&quot;headerlink&quot; title=&quot;结论&quot;&gt;&lt;/a&gt;结论&lt;/h2&gt;&lt;p&gt;print(object)就是调用了类对象object的&lt;strong&gt;repr&lt;/strong&gt;()函数&lt;br&gt;如下代码&lt;br&gt;&lt;figu
      
    
    </summary>
    
      <category term="python" scheme="http://mxxhcm.github.io/categories/python/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>reinforcement learning an introduction 第4章笔记</title>
    <link href="http://mxxhcm.github.io/2019/04/07/reinforcement-learning-an-introduction-%E7%AC%AC4%E7%AB%A0%E7%AC%94%E8%AE%B0/"/>
    <id>http://mxxhcm.github.io/2019/04/07/reinforcement-learning-an-introduction-第4章笔记/</id>
    <published>2019-04-07T15:46:17.000Z</published>
    <updated>2019-10-18T12:29:17.952Z</updated>
    
    <content type="html"><![CDATA[<h2 id="原理">原理</h2><p>Policy iteration有两种方式实现，一种是使用两个数组，一个保存原来的值，一个用来进行更新，这种方法是雅克比方法，或者叫同步的方法，因为他可以并行的进行。<br>In-place的方法是高斯赛德尔方法。就是用来解方程组的迭代法。</p><h2 id="dynamic-programming">Dynamic Programming</h2><p>DP指的是给定环境的模型，通常是一个MDP，计算智能体最优策略的一类算法。经典的DP算法应用场景有限，因为它需要环境的模型，计算量很高，但是DP的思路是很重要的。许多其他的算法都是在尽量减少计算量和对环境信息情况，尽可能获得和DP接近的性能。<br>通常我们假定环境是一个有限(finite)的MDP，也就是state, action, reward都是有限的。尽管DP可以应用于连续(continuous)的state和action space，但是只能应用在几个特殊的场景上。一个常见的做法是将连续state和action quantize(量化)，然后使用有限MDP。<br>DP关键在于使用value function寻找好的policy，在找到了满足Bellman optimal equation的optimal value function之后，可以找到optimal policy，参见<a href="https://mxxhcm.github.io/2018/12/21/reinforcement-learning-an-introduction-%E7%AC%AC3%E7%AB%A0%E7%AC%94%E8%AE%B0/">第三章推导</a>：<br>Bellman optimal equation:<br>\begin{align*}<br>v_{*}(s) &amp;= max_a\mathbb{E}\left[R_{t+1}+\gamma v_{*}(S_{t+1})|S_t=s,A_t=a\right] \\<br>&amp;= max_a \sum_{s’,r} p(s’,r|s,a){*}\left[r+\gamma v_{*}(s’)\right]  \tag{1}<br>\end{align*}</p><p>\begin{align*}<br>q_{*}(s,a) &amp;= \mathbb{E}\left[R_{t+1}+\gamma max_{a’}q_{*}(S_{t+1},a’)|S_t=s,A_t = a\right]\\<br>&amp;= \sum_{s’,r} p(s’,r|s,a) \left[r + \gamma max_a q_{*}(s’,a’)\right] \tag{2}<br>\end{align*}</p><h2 id="policy-evaluation-prediction">Policy Evaluation(Prediction)</h2><p>给定一个policy，计算state value function的过程叫做policy evaluation或者prediction problem。<br>根据$v(s)$和它的后继状态$v(s’)$之间的关系：<br>\begin{align*}<br>v_{\pi}(s) &amp;= \mathbb{E}_{\pi}[G_t|S_t = s]\\<br>&amp;= \mathbb{E}_{\pi}\left[R_{t+1}+\gamma G_{t+1}|S_t = s\right]\\<br>&amp;= \sum_a \pi(a|s)\sum_{s’}\sum_rp(s’,r|s,a) \left[r + \gamma \mathbb{E}_{\pi}\left[G_{t+1}|S_{t+1}=s’\right]\right] \tag{3}\\<br>&amp;= \sum_a \pi(a|s)\sum_{s’,r}p(s’,r|s,a) \left[r + \gamma v_{\pi}(s’) \right] \tag{4}\\<br>\end{align*}<br>只要$\gamma \lt 1$或者存在terminal state，那么$v_{\pi}$的必然存在且唯一。这个我觉得是迭代法解方程的条件。数值分析上有证明。<br>如果环境的转换概率$p$是已知的，可以列出方程组，直接求解出每个状态$s$的$v(s)$。这里采用迭代法求解，随机初始化$v_0$，使用式子$(4)$进行更新：<br>\begin{align*}<br>v_{k+1}(s) &amp;= \mathbb{E}\left[R_{t+1} + \gamma v_k(S_{t+1})\ S_t=s\right]\\<br>&amp;= \sum_a \pi(a|s)\sum_{s’,r}p(s’,r|s,a) \left[r + \gamma v_k(s’) \right] \tag{5}<br>\end{align*}<br>直到$v_k=v_{\pi}$到达fixed point，Bellman equation满足这个条件。当$k\rightarrow \infty$时收敛到$v_{\pi}$。这个算法叫做iterative policy evaluation。<br>在每一次$v_k$到$v_{k+1}$的迭代过程中，所有的$v(s)$都会被更新，$s$的旧值被后继状态$s’$的旧值加上reward替换，正如公式$(5)$中体现的那样。这个目标值被称为expected update，因为它是基于所有$s’$的期望计算出来的（利用环境的模型），而不是通过对$s’$采样计算的。<br>在实现iterative policy evaluation的时候，每一次迭代，都需要重新计算所有$s$的值。这里有一个问题，就是你在每次更新$s$的时候，使用的$s’$如果在本次迭代过程中已经被更新过了，那么是使用更新过的$s’$，还是使用没有更新的$s’$，这就和迭代法中的雅克比迭代以及高斯赛德尔迭代很像，如果使用更新后的$s’$，这里我们叫它in-place的算法，否则就不是。具体那种方法收敛的快，还是要看应用场景的，并不是in-place的就一定收敛的快，这是在数值分析上学到的。<br>下面给出in-place版本的iterative policy evation算法伪代码。<br><strong>iterative policy evation 算法</strong><br><strong>输入</strong>需要evaluation的policy $\pi$<br>给出算法的参数：阈值$\theta\gt 0$，当两次更新的差值小于这个阈值的时候，就停止迭代，随机初始化$V(s),\forall s\in S^{+}$，除了$V(terminal) = 0$。<br><strong>Loop</strong><br>$\qquad \delta \leftarrow 0$<br>$\qquad$ <strong>for</strong> each $s\in S$<br>$\qquad\qquad v\leftarrow V(s)$ （保存迭代之前的$V(s)$）<br>$\qquad\qquad V(s)\leftarrow\sum_a \pi(a|s)\sum_{s’,r}p(s’,r|s,a) \left[r + \gamma v_k(s’) \right] $<br>$\qquad\qquad \nabla \leftarrow max(\delta,|v-V(s)|)$<br>$\qquad$<strong>end for</strong><br><strong>until</strong> $\delta \lt \theta$</p><h2 id="policy-improvement">Policy Improvement</h2><p>为什么要进行policy evaluation，或者说为什么要计算value function？<br>其中一个原因是为了找到更好的policy。假设我们已经知道了一个deterministic的策略$\pi$，但是在其中一些状态，我们想要知道是不是有更好的action选择，如$a\neq \pi(s)$的时候，是不是这个改变后的策略会更好。好该怎么取评价，这个时候就可以使用值函数进行评价了，在某个状态，我们选择$a \neq \pi(s)$，在其余状态，依然遵循策略$\pi$。用公式表示为：<br>\begin{align*}<br>q_{\pi}(s,a) &amp;= \mathbb{E}\left[R_{t+1}+\gamma v_{\pi}(S_{t+1})|S_t=s,A_t = a\right]\\<br>&amp;=\sum_{s’,r}p(s’,r|s,a)\left[r+\gamma v_{\pi}(s’)\right] \tag{6}<br>\end{align*}<br>那么，这个值是是比$v(s)$要大还是要小呢？如果比$v(s)$要大，那么这个新的策略就比$\pi$要好。<br>用$\pi$和$\pi’$表示任意一对满足下式的deterministic policy：<br>$$q_{\pi}(s,\pi’(s)) \ge v_{\pi}(s) \tag{7}$$<br>那么$\pi’$至少和$\pi$一样好。可以证明，任意满足$(7)$的$s$都满足下式：<br>$$v_{\pi’}(s) \ge v_{\pi}(s) \tag{8}$$<br>对于我们提到的$\pi$和$\pi’$来说，除了在状态$s$处，$v_{\pi’}(s) = a \neq v_{\pi}(s)$，在其他状态处$\pi$和$\pi’$是一样的，都有$q_{\pi}(s,\pi’(s)) = v_{\pi}(s)$。而在状态$s$处，如果$q_{\pi}(s,a) \gt v_{\pi}(s)$，注意这里$a=\pi’(s)$，那么$\pi’$一定比$\pi$好。<br>证明：<br>\begin{align*}<br>v_{\pi}(s) &amp;\le q_{\pi}(s,\pi’(s))\\<br>&amp; = \mathbb{E}\left[R_{t+1} + \gamma v_{\pi}(S_{t+1})|S_t = s, A_t = \pi’(s) \right]\\<br>&amp; = \mathbb{E}_{\pi’}\left[R_{t+1} + \gamma v_{\pi}(S_{t+1})|S_t = s \right]\\<br>&amp; \le \mathbb{E}_{\pi’}\left[R_{t+1} + \gamma q_{\pi}(S_{t+1},\pi’(S_{t+1}))|S_t = s \right]\\<br>&amp; = \mathbb{E}_{\pi’}\left[ R_{t+1} + \gamma \mathbb{E}_{\pi’}\left[R_{t+2} +\gamma v_{\pi}(S_{t+2})|S_{t+1}, A_{t+1}=\pi’(S_{t+1})|S_t = s \right]\right]\\<br>&amp; = \mathbb{E}_{\pi’}\left[ R_{t+1} + \gamma R_{t+2} +\gamma^2 v_{\pi}(S_{t+2})|S_t = s \right]\\<br>&amp; \le \mathbb{E}_{\pi’}\left[ R_{t+1} + \gamma R_{t+2} +\gamma^2 R_{t+3}  +\gamma^3 v_{\pi}(S_{t+3})|S_t = s \right]\\<br>&amp; \le \mathbb{E}_{\pi’}\left[ R_{t+1} + \gamma R_{t+2} +\gamma^2 R_{t+3}  +\gamma^3 R_{t+4} + \cdots |S_t = s \right]\\<br>&amp;=v_{\pi’}(s)<br>\end{align*}<br>所以，在计算出一个policy的value function的时候，很容易我们就直到某个状态$s$处的变化是好还是坏。扩展到所有状态和所有action的时候，在每个state，根据$q_{\pi}(s,a)$选择处最好的action，这样就得到了一个greedy策略$\pi’$，给出如下定义：<br>\begin{align*}<br>\pi’(s’) &amp;= argmax_{a} q_{\pi}(s,a)\\<br>&amp; = argmax_{a} \mathbb{E}\left[R_{t+1} + \gamma v_{\pi}(S_{t+1} |S_t=a,A_t=a)\right] \tag{9}\\<br>&amp; = argmax_{a} \sum_{s’,r}p(s’,r|s,a)\left[r+v_{\pi}(s’) \right]<br>\end{align*}<br>可以看出来，该策略的定义一定满足式子$(7)$，所以$\pi’$比$\pi$要好或者相等，这就叫做policy improvement。当$\pi’$和$\pi$相等时，，根据式子$(9)$我们有：<br>\begin{align*}<br>v_{\pi’}(s’)&amp; = max_{a} \mathbb{E}\left[R_{t+1} + \gamma v_{\pi’}(S_{t+1} |S_t=a,A_t=a)\right] \tag{9}\\<br>&amp; = max_{a} \sum_{s’,r}p(s’,r|s,a)\left[r+v_{\pi’}(s’) \right]<br>\end{align*}<br>这和贝尔曼最优等式是一样的？？？殊途同归！！！<br>但是，需要说的一点是，目前我们假设的$\pi$和$\pi’$是deterministic，当$\pi$是stochastic情况的时候，其实也是一样的。只不过，原来我们每次选择的是使得$v_{\pi}$最大的action。对于stochastic的情况来说，输出的是每个动作的概率，可能有几个动作都能使得value function最大，那就让这几个动作的概率一样大，比如是$n$个动作，都是$\frac{1}{n}$。</p><h2 id="policy-iteration">Policy Iteration</h2><p>我们已经讲了Policy Evaluation和Policy Improvement，Evalution会计算出一个固定$\pi$的value function，Improvment会根据value function改进这个policy，然后计算出一个新的policy $\pi’$，对于新的策略，我们可以再次进行Evaluation，然后在Improvement，就这样一直迭代，对于有限的MDP，我们可以求解出最优的value function和policy。这就是Policy Iteration算法。</p><p><strong>Policy Iteration算法</strong><br><strong>1.初始化</strong><br>$V(s)\in R,\pi(s) in A(s)$<br>$\qquad$<br><strong>2.Policy Evaluation</strong><br><strong>Loop</strong><br>$\qquad\Delta\leftarrow 0 $<br>$\qquad$ <strong>For</strong> each $s\in S$<br>$\qquad\qquad v\leftarrow V(s)$<br>$\qquad\qquad V(s)\leftarrow \sum_{s’,r}p(s’,r|s,a)\left[r+\gamma V(s’)\right]$<br>$\qquad\qquad \Delta \leftarrow max(\Delta, |v-V(s)|) $<br><strong>until</strong> $\Delta \lt \theta$<br><strong>3.Policy Improvement</strong><br>$policy-stable\leftarrow true$<br><strong>For</strong> each $s \in S$<br>$\qquad old_action = \pi(s)$<br>$\qquad \pi(s) = argmax_a \sum_{s’,a’}p(s’,r|s,a)\left[r+\gamma V(s’)\right]$<br>$\qquad If\ old_action \neq \pi(s), policy-stable\leftarrow false$<br><strong>If policy-stable</strong>，停止迭代，返回$V$和$\pi$，否则回到2.Policy Evalution继续执行。</p><h2 id="value-iteration">Value Iteration</h2><p>从Policy Iteration算法中我们可以看出来，整个算法分为两步，第一步是Policy Evaluation，第二步是Policy Improvement。而每一次Policy Evaluation都要等到Value function收敛到一定程度才结束，这样子就会非常慢。一个替代的策略是我们尝试每一次Policy Evaluation只进行几步的话，一种特殊情况就是每一个Policy Evaluation只进行一步，这种就叫做Value Iteration。给出如下定义：<br>\begin{align*}<br>v_{k+1}(s) &amp;= max_a \mathbb{E}\left[R_{t+1} + \gamma v_k(S_{t+1})| S_t=s, A_t = a\right]\\<br>&amp;= max_a \sum_{s’,r}p(s’,r|s,a) \left[r+\gamma v_k(s’)\right] \tag{10}<br>\end{align*}<br>它其实就是把两个步骤给合在了一起，原来分开是：<br>\begin{align*}<br>v_{\pi}(s) &amp;= \mathbb{E}\left[R_{t+1} + \gamma v_k(S_{t+1})| S_t=s, A_t = a\right]\\<br>&amp;= \sum_{s’,r}p(s’,r|s,a) \left[r+\gamma v_k(s’)\right]\\<br>v_{\pi’}(s) &amp;= max_a \sum_{s’,r}p(s’,r|s,a) \left[r+\gamma v_{\pi}(s’)\right]\\<br>\end{align*}<br>另一种方式理解式$(10)$可以把它看成是使用贝尔曼最优等式进行迭代更新，Policy Evaluation用的是贝尔曼期望等式进行更新。下面给出完整的Value Iteration算法</p><p><strong>Value Iteration 算法</strong><br><strong>初始化</strong><br>阈值$\theta$，以及随机初始化的$V(s), s\in S^{+}$，$V(terminal)=0$。<br><strong>Loop</strong><br>$\qquad v\leftarrow V(s)$<br>$\qquad$<strong>Loop</strong> for each $s\in S$<br>$\qquad\qquad V(s) = max_a\sum_{s’,r}p(s’,r|s,a)\left[r+\gamma V(s’)\right]$<br>$\qquad\qquad\Delta \leftarrow max(Delta, |v-V(s)|)$<br><strong>until</strong> $\Delta \lt \theta$<br><strong>返回</strong> 输出一个策略$\pi\approx\pi_{*}$，这里书中说是deterministic，我觉得都可以，$\pi$也可以是stochastic的，最后得到的$\pi$满足:<br>$\pi(s) = argmax_a\sum_{s’,r}p(s’,r|s,a)\left[r+\gamma V(s’)\right]$</p><h2 id="asychronous-dynamic-programming">Asychronous Dynamic Programming</h2><p>之前介绍的这些DP方法，在每一次操作的时候，都有对所有的状态进行处理，这就很耗费资源。所以这里就产生了异步的DP算法，这类算法在更新的时候，不会使用整个的state set，而是使用部分state进行更新，其中一些state可能被访问了很多次，而另一些state一次也没有被访问过。<br>其中一种异步DP算法就是在plicy evalutaion的过程中，只使用一个state。<br>使用DP算法并不代表一定能减少计算量，他只是减少在策略没有改进之前陷入无意义的evaluation的可能。尽量选取那些重要的state用来进行更新。<br>同时，异步DP方便进行实时的交互。在使用异步DP更新的时候，同时使用一个真实场景中的agent经历进行更新。智能体的experience可以被用来确定使用哪些state进行更新，DP更新后的值也可以用来指导智能体的决策。</p><h2 id="generalized-policy-iteration">Generalized Policy Iteration</h2><p>之前介绍了三类方法，Policy Iteration,Value iteration以及Asychronous DP算法，它们都有两个过程在不断的迭代进行。一个是evaluation，一个是improvement，这类算法统一的被称为Generalized Policy Iteration(GPI)，可以根据不同的粒度进行细分。基本上所有的算法都是GPI，policy使用value function进行改进，value function朝着policy的真实值函数改进，如果value function和policy都稳定之后，那么说他们都是最优的了。<br>GPI中evalution和improvemetnt可以看成既有竞争又有合作。竞争是因为evaluation和improment的方向通常是相对的，policy改进意味着value function不适用于当前的policy,value function更新意味着policy不是greedy的。然后长期来说，他们共同作用，想要找到最优的值函数和policy。<br>GPI可以看成两个目标的交互过程，这两个目标不是正交的，改进一个目标也会使用另一个目标有所改进，直到最后，这两个交互过程使得总的目标变成最优的。</p><h2 id="efficiency-of-dynamic-programming">Efficiency of Dynamic Programming</h2><p>用$n$和$k$表示MDP的状态数和动作数，DP算法保证在多项式时间内找到最优解，即使策略的总数是$k^n$个。<br>DP比任何在policy space内搜索的算法要快上指数倍，因为policy space搜索需要检查每一个算法。Linear Programming算法也可以用来解MDP问题，在某些情况下最坏的情况还要比DP算法快，但是LP要比只适合解决state数量小的问题。而DP也能处理states很大的情况。</p><h2 id="summary">Summary</h2><ul><li>使用贝尔曼公式更新值函数，可以使用backup diagram看他们的直观表示。</li><li>基本上所有的强化学习算法都可以看成GPI(generalized policy iteraion)，先评估某个策略，然后改进这个策略，评估新的策略…这样子循环下去，直到收敛，找到一个不在变化的最优值函数和策略。<br>GPI不一定是收敛的，本章介绍的这些大多都是收敛的，但是还有一些没有被证明收敛。</li><li>可以使用异步的DP算法。</li><li>所有的DP算法都有一个属性叫做bootstrapping，即基于其他states的估计更新每一个state的值。因为每一个state value的更新都需要用到他们的successor state的估计。</li></ul><blockquote><p>They update estimates onthe basis of other estimates。</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;原理&quot;&gt;原理&lt;/h2&gt;
&lt;p&gt;Policy iteration有两种方式实现，一种是使用两个数组，一个保存原来的值，一个用来进行更新，这种方法是雅克比方法，或者叫同步的方法，因为他可以并行的进行。&lt;br&gt;
In-place的方法是高斯赛德尔方法。就是用来解方程组的
      
    
    </summary>
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="动态规划" scheme="http://mxxhcm.github.io/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"/>
    
  </entry>
  
  <entry>
    <title>reinforcement learning an introduction 第9章笔记</title>
    <link href="http://mxxhcm.github.io/2019/04/04/reinforcement-learning-an-introduction-%E7%AC%AC9%E7%AB%A0%E7%AC%94%E8%AE%B0/"/>
    <id>http://mxxhcm.github.io/2019/04/04/reinforcement-learning-an-introduction-第9章笔记/</id>
    <published>2019-04-04T02:14:08.000Z</published>
    <updated>2019-08-30T03:44:59.932Z</updated>
    
    <content type="html"><![CDATA[<h2 id="on-policy-prediction-with-approximation">On-policy Prediction with Approximation</h2><p>这一章讲的是利用on-policy的数据估计函数形式的值函数，on-policy就是说利用一个已知的policy $\pi$生成的experience来估计$v_{\pi}$。和之前讲的不同的是，前面几章讲的是表格形式的值函数，而这一章是使用参数为$\mathbf{w}\in R^d$的函数表示。即$\hat{v}(s,\mathbf{w})\approx v_{\pi}(s)$表示给定一个权值vector $\mathbf{w}$，state $s$的状态值。这个函数可以是任何形式的，可以是线性函数，也可以是神经网络，还可以是决策树。</p><h2 id="值函数估计">值函数估计</h2><p>目前这本书介绍的所有prediction方法都是更新某一个state的估计值函数向backed-up value（或者叫update target）值移动。我们用符号$s\mapsto u$表示一次更新。其中$s$是要更新的状态，$u$是$s$的估计值函数的update target。例如，Monte Carlo更新的value prediction是：$S_t \mapsto G_t$，TD(0)的update是：$S_t \mapsto R_{t+1} + \gamma \hat{v}(S_{t+1}, \mathbf{w}_t)$，$n$-step TD update是：$S_t \mapsto G_{t:t+n}$。在DP policy evaluation update中是：$s\mapsto E_{\pi}[R_{t+1}+\gamma\hat{v}(S_{t+1}, \mathbf{w}_t)| S_t =s]$，任意一个状态$s$被更新了，同时在其他真实experience中遇到的$S_t$也被更新了。</p><p>之前表格的更新太trivial了，更次更新$s$向$u$移动，其他状态的值都保持不变。现在使用函数实现更新，在状态$s$处的更新，可以一次性更新很多个其他状态的值。就像监督学习学习input和output之间的映射一样，我们可以把$s\mapsto g$的更新看做一个训练样本。这样就可以使用很多监督学习的方法学习这样一个函数。<br>但是并不是所有的方法都适用于强化学习，因为许多复杂的神经网络和统计学方法都假设训练集是静态不变的。然而强化学习中，学习是online的，即智能体不断地与环境进行交互产生新的数据，这就需要这个方法能够从不断增加的数据中高效的学习。<br>此外，强化学习通常需要function approximation能够处理target function不稳定的情况，即target function随着事件在不断的变化。比如，在基于GPI的control方法中，在$\pi$不断变化的情况下，我们想要学习出$q_{\pi}$。即使policy保持不变，如果使用booststrapping方法（DP和TD学习），训练样本的target value也在不断的改变，因为下一个state的value值在不断的改变。所以不能处理这些不稳定情况的方法有点不适合强化学习。</p><h2 id="预测目标-the-prediction-objective">预测目标(The Prediction Objective)</h2><p>表格形式的值函数最终都会收敛到真值，状态值之间也都是解耦的，即更新一个state不影响另一个state。<br>但是使用函数拟合，更新一个state的估计值就会影响很多个其他状态，并且不可能精确的估计所有states的值。假设我们的states比weights多的多，让一个state的估计更精确也意味着使得其他的state越不accurate。我们用一个state $s$上的分布,$\mu(s)\ge 0,\sum_s\mu(s)=1$代表对每个state上error的权重。然后使用$\mu(s)$对approximate value $\hat{v}(s,\mathbf{w})$和true value $v_{\pi}(s)$的squared error进行加权，得到Mean Squared Value Error，表示为$\bar{VE}$：<br>$$\bar{VE}(\mathbf{w}) = \sum_{s\in S}\mu(s)[v_{\pi}(s) - \hat{v}(s, \mathbf{w})]^2$$<br>通常情况下，$\mu(s)$是在state $s$处花费时间的百分比。在on-policy训练中，这叫做on-policy分布。在continuing tasks中，策略$\pi$下的on-policy分布是一个stationary distribution。<br>在episodic tasks中，on-policy分布有一些不同，因为它还取决于每个episodic的初始状态，用$h(s)$表示在一个episodic开始状态为$s$的概率，用$\eta(s)$表示在一个回合中，state $s$平均被访问的次数。<br>$$\eta(s) = h(s) + \sum_{\bar{s}}\eta(\bar{s})\sum_a\pi(a|\bar{s})p(s|\bar{s},a), forall\ s \in S$$<br>其中$\bar{s}$是$s$的前一个状态，$s$处的时间为以状态$s$开始的概率$h(s)$加上它由前一个状态$\bar{s}$转换过来消耗的时间。<br>列出一个方程组，可以解出来$\eta(s)$的期望值。然后进行归一化，得到：<br>$$\mu(s)=\frac{\eta{s}}{\sum_{s’}\eta{s’}}, \forall s \in S.$$<br>这是没有折扣因子的式子，如果有折扣因子的话，可以看成一种形式的</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;on-policy-prediction-with-approximation&quot;&gt;On-policy Prediction with Approximation&lt;/h2&gt;
&lt;p&gt;这一章讲的是利用on-policy的数据估计函数形式的值函数，on-policy就是说
      
    
    </summary>
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="函数近似" scheme="http://mxxhcm.github.io/tags/%E5%87%BD%E6%95%B0%E8%BF%91%E4%BC%BC/"/>
    
      <category term="on-policy" scheme="http://mxxhcm.github.io/tags/on-policy/"/>
    
      <category term="值函数" scheme="http://mxxhcm.github.io/tags/%E5%80%BC%E5%87%BD%E6%95%B0/"/>
    
  </entry>
  
  <entry>
    <title>reinforcement learning an introduction 第13章笔记.md</title>
    <link href="http://mxxhcm.github.io/2019/04/03/reinforcement-learning-an-introduction-%E7%AC%AC13%E7%AB%A0%E7%AC%94%E8%AE%B0/"/>
    <id>http://mxxhcm.github.io/2019/04/03/reinforcement-learning-an-introduction-第13章笔记/</id>
    <published>2019-04-03T01:46:49.000Z</published>
    <updated>2019-07-25T06:31:58.572Z</updated>
    
    <content type="html"><![CDATA[<h2 id="policy-gradient">Policy gradient</h2><p>这章介绍的是使用一个参数化策略(parameterized policy)直接给出action，而不用借助一个value funciton选择action。但是需要说一下的是，Policy gradient方法也可以学习一个Value function，但是value function是用来帮助学习policy parameters的，而不是用来选择action。我们用$\mathbf{\theta} \in R^{d’}$表示policy’s parameters vector，用$\pi(a|s, \mathbf{\theta}) = Pr[A_t = a|S_t = s, \mathbf{\theta}_t = \mathbf{\theta}]$表示environment在时刻$t$处于state $s$时，智能体根据参数为$\mathbf{\theta}$的策略$\pi$选择action $a$。<br>如果policy gradient方法使用了一个value function,它的权重用$\mathbf{w} \in R^d$表示，即$\hat{v}(s,\mathbf{w})$。</p><p>用$J(\mathbf{\theta})$表示policy parameters的标量performance measure。使用梯度上升(gradient ascent) 方法来最大化这个performance：<br>$$\mathbf{\theta}_{t+1} = \mathbf{\theta}_t + \alpha \widehat{\nabla J(\mathbf{\theta}_t}),\tag{1}$$<br>其中$\widehat{\nabla J(\mathbf{\theta}_t)} \in R^{d’}$是一个随机估计(stachastic estimate)，它的期望是performance measure对$\mathbf{\theta_t}$的梯度。不管它们是否使用value function，这种方法就叫做policy gradient方法。既学习policy，又学习value function的方法被称为actor-critic，其中actor指的是学到的policy，critic指的是学习到的value funciton,通常是state value function。</p><h2 id="policy估计和它的优势">policy估计和它的优势</h2><h3 id="参数化policy的条件">参数化policy的条件</h3><p>policy可以用任何方式参数化，只要$\pi(a|s,\mathbf{\theta}),\mathbf{\theta}\in R^{d’}$对于它的参数$\mathbf{\theta}$是可导的，即只要$\nabla_{\pi}(a|s,\mathbf{\theta})$（即：$\pi(a|s,\mathbf{\theta})$相对于$\mathbf{\theta}$的偏导数列向量）存在，并且$\forall s\in S, a\in A(s)$偏导数都是有限的即可。</p><h3 id="stochastic-policy">stochastic policy</h3><p>为了保证exploration，通常策略是stochastic，而不是deterministic，即$\forall s,a,\mathbf{\theta}, \pi(a|s,\mathbf{\theta})\in (0,1)$</p><h3 id="参数化方式的选择">参数化方式的选择</h3><h4 id="softmax">softmax</h4><p>对于有限且离散的action space，一个很自然的参数化方法就是对于每一个state-action对都计算一个参数化的数值偏好$h(s,a,\mathbf{\theta})\in R$。通过计算一个exponetial softmax，这个数值大的动作有更大的概率被选中：<br>$$\pi(a|s,\mathbf{\theta}) = \frac{e^{h(s,a,\mathbf{\theta} )}}{\sum_be^{h(s,b,\mathbf{\theta} )}}, \tag{2}$$<br>其中$b$是在state $s$下所有可能采取的动作，它们的概率加起来为$1$，这种方法叫做softmax in aciton preferences。</p><h4 id="nn和线性方法">NN和线性方法</h4><p>参数化还可以选择其他各种各样的方法，如AlphaGo中使用的NN，或者可以使用如下的线性方法：<br>$$h(s,a, \mathbf{\theta}) = \mathbf{\theta}^Tx(s,a), \tag{3}$$</p><h3 id="优势">优势</h3><p>和action value方法相比，policy gradient有多个优势。<br>第一个优势是使用action preferences的softmax，同时用$\epsilon-greedy$算法用$\epsilon$的概率随机选择action得到的策略可以接近一个deterministic policy。<br>而单单使用action values的方法并不会使得策略接近一个deterministic policy，但是action-value方法会逐渐收敛于它的true values，翻译成概率来表示就是在$0$和$1$之间的一个概率值。但是action preferences方法不收敛于任何值，它们产生optimal stochastic policy，如果optimal policy是deterministic，那么optimal action的preferences应该比其他所有suboptimal actions都要高。</p><p>第二个优势是使用action preferences方法得到的参数化策略可以使用任意的概率选择action。在某些问题中，最好的approximate policy可能是stochastic的，actor-value方法不能找到一个stochastic optimal policy，它总是根据action value值选出来一个值最大的action，但是这时候的结果通常不是最优的。</p><p>第三个优势是policy parameterization可能比action value parameterization更容易学习。当然，也有时候可能是action value更容易。这个要根据情况而定</p><p>第四个优势是policy parameterizaiton比较容易添加先验知识到policy中。</p><h2 id="policy-gradient理论">policy gradient理论</h2><p>除了上节说的实用优势之外，还有理论优势。policy parameterization学到关于参数的一个连续函数，action probability概率可以平滑的变化。然而$\epsilon-greedy$算法中，action-value改变以后，action probability可能变化很大。很大程度上是因为policy gradient方法的收敛性要比action value方法强的多。因为policy的连续性依赖于参数，使得policy gradient方法接近于gradient ascent。<br>这里讨论episodic情况。定义perfromance measure是episode初始状态的值。假设每一个episode，都从state $s_0$开始，定义：<br>$$J(\mathbf{\theta}) = v_{\pi_\mathbf{\theta}}(s_0), \tag{4}$$<br>其中$v_{\pi_\mathbf{\theta}}(s_0)$是由参数$\mathbf{\theta}$确定的策略$\pi_{\mathbf{\theta}}$的true value function。假设在episodic情况下，$\gamma=1$。</p><p>使用function approximation，一个需要解决的问题就是如何确保每次更新policy parameter，performance measure都有improvement。因为performence不仅仅依赖于action的选择，还取决于state的分布，然后它们都受policy parameter的影响。给定一个state，policy parameter对于actions，reward的影响，都可以相对直接的利用参数知识计算出来。但是policy parameter对于state 分布的影响是一个环境的函数，通常是不知道的。当梯度依赖于policy改变对于state分布的影响未知时，我们该如何估计performance相对于参数的梯度。</p><h3 id="episodic-case证明">Episodic case证明</h3><p>为了简化表示，用$\pi$表示参数为$\theta$的policy，所有的梯度都是相对于$\mathbf{\theta}$求的<br>\begin{align*}<br>\nabla v_{\pi}(s) &amp;= \nabla [ \sum_a \pi(a|s)q_{\pi}(s,a)], \forall s\in S \tag{5}\\<br>&amp;= \sum_a [\nabla\pi(a|s)q_{\pi}(s,a)], \forall s\in S \tag{6}\\<br>&amp;= \sum_a[\nabla\pi(a|s)q_{\pi}(s,a) + \pi(a|s)\nabla q_{\pi}(s,a)] \tag{7}\\<br>&amp;= \sum_a[\nabla\pi(a|s)q_{\pi}(s,a) + \pi(a|s)\nabla \sum_{s’,r}p(s’,r|s,a)(r+\gamma v_{\pi}(s’))] \tag{8}\\<br>&amp;= \sum_a[\nabla\pi(a|s)q_{\pi}(s,a) + \pi(a|s) \nabla \sum_{s’,r}p(s’,r|s,a)r + \pi(a|s)\nabla \sum_{s’,r}p(s’,r|s,a)\gamma v_{\pi}(s’))] \tag{9}\\<br>&amp;= \sum_a[\nabla\pi(a|s)q_{\pi}(s,a) + 0 + \pi(a|s)\sum_{s’}\gamma p(s’|s,a)\nabla v_{\pi}(s’) ] \tag{10}\\<br>&amp;= \sum_a[\nabla\pi(a|s)q_{\pi}(s,a) + 0 + \pi(a|s)\sum_{s’}\gamma p(s’|s,a)\\<br>&amp;\ \ \ \ \ \ \ \ \sum_{a’}[\nabla\pi(a’|s’)q_{\pi}(s’,a’) + \pi(a’|s’)\sum_{s’’}\gamma p(s’’|s’,a’)\nabla v_{\pi}(s’’))] ],  \tag{11}展开\\<br>&amp;= \sum_{x\in S}\sum_{k=0}^{\infty}Pr(s\rightarrow x, k,\pi)\sum_a\nabla\pi(a|x)q_{\pi}(x,a) \tag{12}<br>\end{align*}<br>第(5)式使用了$v_{\pi}(s) = \sum_a\pi(a|s)q(s,a)$进行展开。第(6)式将梯度符号放进求和里面。第(7)步使用product rule对q(s,a)求导。第(8)步利用$q_{\pi}(s, a) =\sum_{s’,r}p(s’,r|s,a)(r+v_{\pi}(s’)$ 对$q_{\pi}(s,a)$进行展开。第(9)步将(8)式进行分解。第(10)步对式(9)进行计算，因为$\sum_{s’,r}p(s’,r|s,a)r$是一个定制，求偏导之后为$0$。第(11)步对生成的$v_{\pi}(s’)$重复(5)-(10)步骤，得到式子(11)。如果对式子(11)中的$v_{\pi}(s)$一直展开，就得到了式子(12)。式子(12)中的$Pr(s\rightarrow x, k, \pi)$是在策略$\pi$下从state $s$经过$k$步转换到state $x$的概率，这里我有一个问题，就是为什么，$k$可以取到$\infty$，后来想了想，因为对第(11)步进行展开以后，可能会有重复的state，重复的意思就是从状态$s$开始，可能会多次到达某一个状态$x$，$k$就能取很多次，大不了$k=\infty$的概率为$0$就是了。</p><p>所以，对于$v_{\pi}(s_0)$，就有：<br>\begin{align*}<br>\nabla J(\mathbf{\theta}) &amp;= \nabla_{v_{\pi}}(s_0)\\<br>&amp;= \sum_{s\in S}( \sum_{k=0}^{\infty}Pr(s_0\rightarrow s,k,\pi) ) \sum_a\nabla_{\pi}(a|s)q_{\pi}(s,a)\\<br>&amp;=\sum_{s\in S}\eta(s)\sum_a \nabla_{\pi}(a|s)q_{\pi}(s,a)\\<br>&amp;=\sum_{s’\in S}\eta(s’)\sum_s\frac{\eta(s)}{\sum_{s’}\eta(s’)}\sum_a \nabla_{\pi}(a|s)q_{\pi}(s,a)\\<br>&amp;=\sum_{s’\in S}\eta(s’)\sum_s\mu(s)\sum_a \nabla_{\pi}(a|s)q_{\pi}(s,a)\\<br>&amp;\propto \sum_{s\in S}\mu(s)\sum_a\nabla\pi(a|s)q_{\pi}(s,a)<br>\end{align*}<br>最后，我们可以看出来performance对policy求导不涉及state distribution的导数。Episodic 情况下的策略梯度如下所示：<br>$$\nabla J(\mathbf{\theta})\propto \sum_{s\in S}\mu(s)\sum_aq_{\pi}(s,a)\nabla\pi(a|s,\mathbf{\theta}), \tag{13}$$<br>其中梯度是performacne指标$J$关于$\mathbf{\theta}$的偏导数列向量，$\pi$是参数$\mathbf{\theta}$对应的策略。在episodic情况下，比例常数是一个episode的平均长度，在continuing情况下，常数是$1$，实际上这个正比于就是一个等式。分布$\mu$是策略$\pi$下的on-policy分布。</p><h2 id="reinforce-monte-carlo-policy-gradient">REINFORCE: Monte Carlo Policy Gradient</h2><p>对于式子(1)，我们需要进行采样，让样本梯度的期望正比于performance measure对于$\mathbf{\theta}$的真实梯度。比例系数不需要确定，因为步长$\alpha$的大小是手动设置的。Policy gradient理论给出了一个正比于gradient的精确表达式，我们要做的就是选择采样方式，它的期望等于或者接近policy gradient理论给出的值。</p><h3 id="all-actions">all-actions</h3><p>使用随机变量的期望替换对随机变量求和的取值，我们可以将式子(13)进行如下变化：<br>\begin{align*}<br>\nabla J(\mathbf{\theta})&amp;\propto \sum_{s\in S}\mu(s)\nabla\pi(a|s,\mathbf{\theta})\sum_aq_{\pi}(s,a)\\<br>&amp;=\mathbb{E}_{\pi}\left[\nabla\pi(a|S_t,\mathbf{\theta})\sum_aq_{\pi}(S_t,a)\right]\tag{14}<br>\end{align*}<br>接下来，我们可以实例化该方法：<br>$$\mathbf{\theta}_{t+1} = \mathbf{\theta}_t+\alpha\sum_a\hat{q}(S_t,s,\mathbf{w})\nabla\pi(a|S_t,\mathbf{\theta}), \tag{15}$$<br>其中$\hat{q}$是$q_{\pi}$的估计值，这个算法被称为all-actions方法，因为它的更新涉及到了所有的action。然而，我们这里介绍的REINFORCE仅仅使用了$t$时刻的action $A_t$。。</p><h3 id="reinforce">REINFORCE</h3><p>和引入$S_t$的方法一样，使用随机变量的期望代替对与随机变量的可能取值进行求和，我们在式子(14)中引入$A_t$，<br>\begin{align*}<br>\nabla J(\mathbf{\theta}) &amp;= \mathbb{E}_{\pi}\left[\sum_aq_{\pi}(S_t,a)\nabla\pi(a|S_t,\mathbf{\theta})\right]\\<br>&amp; = \mathbb{E}_{\pi}\left[\sum_aq_{\pi}(S_t,a)\pi(a|S_t,\mathbf{\theta})\frac{\nabla\pi(a|S_t,\mathbf{\theta})}{\pi(a|S_t,\mathbf{\theta})}\right]\\<br>&amp; = \mathbb{E}_{\pi}\left[q_{\pi}(S_t,A_t)\frac{\nabla\pi(A_t|S_t,\mathbf{\theta})}{\pi(A_t|S_t,\mathbf{\theta})}\right]\\<br>\end{align*}</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;policy-gradient&quot;&gt;Policy gradient&lt;/h2&gt;
&lt;p&gt;这章介绍的是使用一个参数化策略(parameterized policy)直接给出action，而不用借助一个value funciton选择action。但是需要说一下的是，Pol
      
    
    </summary>
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="Policy Gradient" scheme="http://mxxhcm.github.io/tags/Policy-Gradient/"/>
    
  </entry>
  
  <entry>
    <title>matplotlib笔记</title>
    <link href="http://mxxhcm.github.io/2019/03/21/python-matplotlib%E7%AC%94%E8%AE%B0/"/>
    <id>http://mxxhcm.github.io/2019/03/21/python-matplotlib笔记/</id>
    <published>2019-03-21T07:29:17.000Z</published>
    <updated>2019-07-08T02:26:38.208Z</updated>
    
    <content type="html"><![CDATA[<h2 id="show"><a href="#show" class="headerlink" title="show()"></a>show()</h2><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>show()函数是一个阻塞函数，调用该函数，显示当前已经绘制的图像，然后需要手动关闭打开的图像，程序才会继续执行。</p><h3 id="代码示例"><a href="#代码示例" class="headerlink" title="代码示例"></a>代码示例</h3><p><a href="https://github.com/mxxhcm/code/blob/master/tools/matplotlib/1_show.py" target="_blank" rel="noopener">代码地址</a><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x = np.arange(<span class="number">0</span>,<span class="number">10</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">y1 = x**<span class="number">2</span></span><br><span class="line">y2 = <span class="number">2</span>*x +<span class="number">5</span></span><br><span class="line"></span><br><span class="line">plt.plot(x,y1)</span><br><span class="line">plt.savefig(<span class="string">"0_1.png"</span>)</span><br><span class="line">plt.show()  <span class="comment"># 调用show()会阻塞，然后关掉打开的图片，程序继续执行</span></span><br><span class="line"></span><br><span class="line">plt.plot(x,y2)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><h2 id="savefig"><a href="#savefig" class="headerlink" title="savefig()"></a>savefig()</h2><h3 id="介绍-1"><a href="#介绍-1" class="headerlink" title="介绍"></a>介绍</h3><p>该文件接收一个参数，作为文件保存的路径。</p><h3 id="代码示例-1"><a href="#代码示例-1" class="headerlink" title="代码示例"></a>代码示例</h3><p><a href="https://github.com/mxxhcm/code/blob/master/tools/matplotlib/2_savefig.py" target="_blank" rel="noopener">代码地址</a><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x = np.arange(<span class="number">0</span>, <span class="number">10</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">y1 = x**<span class="number">2</span></span><br><span class="line">y2 = <span class="number">2</span>*x +<span class="number">5</span></span><br><span class="line"></span><br><span class="line">plt.plot(x,y1)</span><br><span class="line">plt.savefig(<span class="string">"2.png"</span>) <span class="comment"># 保存图像，名字为2.png</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><h2 id="figure"><a href="#figure" class="headerlink" title="figure()"></a>figure()</h2><h3 id="介绍-2"><a href="#介绍-2" class="headerlink" title="介绍"></a>介绍</h3><p>figure()函数相当于生成一张画布。如果不显示调用的话，所有的图像都会绘制在默认的画布上。可以通过调用figure()函数将函数图像分开。figure()会接受几个参数，num是生成图片的序号，figsize指定图片的大小。</p><h3 id="代码示例-2"><a href="#代码示例-2" class="headerlink" title="代码示例"></a>代码示例</h3><p><a href="https://github.com/mxxhcm/code/blob/master/tools/matplotlib/3_figure.py" target="_blank" rel="noopener">代码地址</a><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x = np.arange(<span class="number">0</span>,<span class="number">10</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">y1 = x**<span class="number">2</span></span><br><span class="line">y2 = <span class="number">2</span>*x +<span class="number">5</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># figure</span></span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(x,y1)</span><br><span class="line"></span><br><span class="line">plt.figure(num=<span class="number">6</span>,figsize=(<span class="number">10</span>,<span class="number">10</span>))</span><br><span class="line">plt.plot(x,y2)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><h2 id="imshow"><a href="#imshow" class="headerlink" title="imshow()"></a>imshow()</h2><h3 id="介绍-3"><a href="#介绍-3" class="headerlink" title="介绍"></a>介绍</h3><p>该函数用来显示图像，接受一个图像矩阵。调用完该函数之后还需要调用show()函数。</p><h3 id="代码示例-3"><a href="#代码示例-3" class="headerlink" title="代码示例"></a>代码示例</h3><p><a href="https://github.com/mxxhcm/code/blob/master/tools/matplotlib/4_image.py" target="_blank" rel="noopener">代码地址</a><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">img = np.random.randint(<span class="number">0</span>, <span class="number">255</span>, [<span class="number">32</span>, <span class="number">32</span>])</span><br><span class="line">print(img.shape)</span><br><span class="line"></span><br><span class="line">plt.imshow(img)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><h2 id="subplot"><a href="#subplot" class="headerlink" title="subplot()"></a>subplot()</h2><h3 id="介绍-4"><a href="#介绍-4" class="headerlink" title="介绍"></a>介绍</h3><p>绘制$m\times n$个子图</p><h3 id="代码示例-4"><a href="#代码示例-4" class="headerlink" title="代码示例"></a>代码示例</h3><p><a href="https://github.com/mxxhcm/code/blob/master/tools/matplotlib/5_subplot.py" target="_blank" rel="noopener">代码地址</a><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x = np.arange(<span class="number">0</span>, <span class="number">10</span>, <span class="number">1</span>)</span><br><span class="line">y1 = <span class="number">2</span> * x</span><br><span class="line">y2 = <span class="number">3</span> * x</span><br><span class="line">y3 = <span class="number">4</span> * x</span><br><span class="line">y4 = <span class="number">5</span> * x</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">plt.plot(x, y1, marker=<span class="string">'s'</span>, lw=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">plt.plot(x, y2, ls=<span class="string">'-.'</span>)</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">plt.plot(x, y3, color=<span class="string">'r'</span>)</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">2</span>, <span class="number">2</span>, <span class="number">4</span>)</span><br><span class="line">plt.plot(x, y4, ms=<span class="number">10</span>, marker=<span class="string">'o'</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><h2 id="subplots"><a href="#subplots" class="headerlink" title="subplots()"></a>subplots()</h2><h3 id="介绍-5"><a href="#介绍-5" class="headerlink" title="介绍"></a>介绍</h3><p>将一张图分成$m\times n$个子图。</p><h3 id="代码示例-5"><a href="#代码示例-5" class="headerlink" title="代码示例"></a>代码示例</h3><p><a href="https://github.com/mxxhcm/code/blob/master/tools/matplotlib/6_subplots.py" target="_blank" rel="noopener">代码地址</a><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">figure,axes = plt.subplots(<span class="number">2</span>, <span class="number">3</span>, figsize=[<span class="number">40</span>,<span class="number">20</span>])</span><br><span class="line">axes = axes.flatten()</span><br><span class="line"></span><br><span class="line">x = np.arange(<span class="number">0</span>, <span class="number">20</span>) </span><br><span class="line">y1 = pow(x, <span class="number">2</span>)</span><br><span class="line">axes[<span class="number">0</span>].plot(x, y1) </span><br><span class="line"></span><br><span class="line">y5 = pow(x, <span class="number">3</span>)</span><br><span class="line">axes[<span class="number">5</span>].plot(x, y5) </span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><h2 id="ax"><a href="#ax" class="headerlink" title="ax()"></a>ax()</h2><h3 id="介绍-6"><a href="#介绍-6" class="headerlink" title="介绍"></a>介绍</h3><p>获得当前figure的坐标轴，用来绘制。</p><h3 id="代码示例-6"><a href="#代码示例-6" class="headerlink" title="代码示例"></a>代码示例</h3><p><a href="https://github.com/mxxhcm/code/blob/master/tools/matplotlib/7_axes.py" target="_blank" rel="noopener">代码地址</a><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x = np.arange(<span class="number">-3.5</span>,<span class="number">3.5</span>,<span class="number">0.5</span>)</span><br><span class="line">y1 = np.abs(<span class="number">2</span> * x)</span><br><span class="line">y2 = np.abs(x)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>,<span class="number">10</span>))</span><br><span class="line">ax = plt.gca() <span class="comment"># gca = get current axis</span></span><br><span class="line">ax.spines[<span class="string">'right'</span>].set_color(<span class="string">'none'</span>)</span><br><span class="line">ax.spines[<span class="string">'top'</span>].set_color(<span class="string">'red'</span>)</span><br><span class="line">ax.xaxis.set_ticks_position(<span class="string">"bottom"</span>)</span><br><span class="line">ax.yaxis.set_ticks_position(<span class="string">"left"</span>)</span><br><span class="line">ax.spines[<span class="string">'bottom'</span>].set_position((<span class="string">'data'</span>,<span class="number">0</span>))</span><br><span class="line">ax.spines[<span class="string">'left'</span>].set_position((<span class="string">'data'</span>,<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># both work</span></span><br><span class="line">ax.plot(x,y1,lw=<span class="number">2</span>,marker=<span class="string">'-'</span>,ms=<span class="number">8</span>)</span><br><span class="line">plt.plot(x,y2,lw=<span class="number">3</span>,marker=<span class="string">'^'</span>,ms=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># xlim and ylim</span></span><br><span class="line"><span class="comment"># ax.xlim([-3.8, 3.3])</span></span><br><span class="line"><span class="comment"># AttributeError: 'AxesSubplot' object has no attribute 'xlim'</span></span><br><span class="line">plt.xlim([<span class="number">-3.8</span>, <span class="number">3.3</span>])</span><br><span class="line">plt.ylim([<span class="number">0</span>, <span class="number">7.2</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># xlabel and ylabel</span></span><br><span class="line"><span class="comment"># ax.xlabel('x',fontsize=20)</span></span><br><span class="line"><span class="comment"># AttributeError: 'AxesSubplot' object has no attribute 'xlabel'</span></span><br><span class="line">plt.xlabel(<span class="string">'x'</span>,fontsize=<span class="number">20</span>)</span><br><span class="line">plt.ylabel(<span class="string">'y = 2x '</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># xticklabel and yticaklabel</span></span><br><span class="line"><span class="comment"># ax.xticks(x,('a','b','c','d','e','f','g','h','i','j','k','l','m','n'),fontsize=20)</span></span><br><span class="line"><span class="comment"># AttributeError: 'AxesSubplot' object has no attribute 'xticks'</span></span><br><span class="line">plt.xticks(x,(<span class="string">'a'</span>,<span class="string">'b'</span>,<span class="string">'c'</span>,<span class="string">'d'</span>,<span class="string">'e'</span>,<span class="string">'f'</span>,<span class="string">'g'</span>,<span class="string">'h'</span>,<span class="string">'i'</span>,<span class="string">'j'</span>,<span class="string">'k'</span>,<span class="string">'l'</span>,<span class="string">'m'</span>,<span class="string">'n'</span>),fontsize=<span class="number">20</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># both work</span></span><br><span class="line">ax.legend([<span class="string">'t1'</span>,<span class="string">'t2'</span>])</span><br><span class="line">plt.legend([<span class="string">'y1'</span>,<span class="string">'y2'</span>])</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><h2 id="ion-和ioff"><a href="#ion-和ioff" class="headerlink" title="ion()和ioff()"></a>ion()和ioff()</h2><h3 id="介绍-7"><a href="#介绍-7" class="headerlink" title="介绍"></a>介绍</h3><p>交互式绘图，可以在一张图上不断的更新。</p><h3 id="代码示例-7"><a href="#代码示例-7" class="headerlink" title="代码示例"></a>代码示例</h3><p><a href="https://github.com/mxxhcm/code/blob/master/tools/matplotlib/8_plt_ion_ioff.py" target="_blank" rel="noopener">代码地址</a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">count = 1</span><br><span class="line">flag = True</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">ax = plt.gca()</span><br><span class="line">x = np.arange(20)</span><br><span class="line">plt.figure()</span><br><span class="line">ax2 = plt.gca()</span><br><span class="line"></span><br><span class="line">while flag:</span><br><span class="line">    plt.ion()</span><br><span class="line">    y = pow(x[:count], 2)</span><br><span class="line">    temp = x[:count]</span><br><span class="line">    ax.plot(temp, y, linewidth=1)</span><br><span class="line">    plt.pause(1)</span><br><span class="line">    plt.ioff()</span><br><span class="line"></span><br><span class="line">    ax2.plot(x, x+count)</span><br><span class="line">    count += 1</span><br><span class="line">    if count &gt; 20:</span><br><span class="line">        break</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><h2 id="seanborn"><a href="#seanborn" class="headerlink" title="seanborn"></a>seanborn</h2><h3 id="介绍-8"><a href="#介绍-8" class="headerlink" title="介绍"></a>介绍</h3><p>对matplotlib进行了一层封装</p><h3 id="代码示例-8"><a href="#代码示例-8" class="headerlink" title="代码示例"></a>代码示例</h3><p><a href="https://github.com/mxxhcm/code/blob/master/tools/matplotlib/9_seanborn.py" target="_blank" rel="noopener">代码地址</a><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">values = np.zeros((<span class="number">21</span>,<span class="number">21</span>), dtype=np.int)</span><br><span class="line">fig, axes = plt.subplots(<span class="number">2</span>, <span class="number">3</span>, figsize=(<span class="number">40</span>,<span class="number">20</span>))</span><br><span class="line">plt.subplots_adjust(wspace=<span class="number">0.1</span>, hspace=<span class="number">0.2</span>)</span><br><span class="line">axes = axes.flatten()</span><br><span class="line"></span><br><span class="line"><span class="comment"># cmap is the paramter to specify color type, ax is the parameter to specify where to show the picture</span></span><br><span class="line"><span class="comment"># np.flipud(matrix), flip the column in the up/down direction, rows are preserved</span></span><br><span class="line">figure = sns.heatmap(np.flipud(values), cmap=<span class="string">"YlGnBu"</span>, ax=axes[<span class="number">0</span>])</span><br><span class="line">figure.set_xlabel(<span class="string">"cars at second location"</span>, fontsize=<span class="number">30</span>)</span><br><span class="line">figure.set_title(<span class="string">"policy"</span>, fontsize=<span class="number">30</span>)</span><br><span class="line">figure.set_ylabel(<span class="string">"cars at first location"</span>, fontsize=<span class="number">30</span>)</span><br><span class="line">figure.set_yticks(list(reversed(range(<span class="number">21</span>))))</span><br><span class="line"></span><br><span class="line">figure = sns.heatmap(np.flipud(values), ax=axes[<span class="number">1</span>])</span><br><span class="line">figure.set_ylabel(<span class="string">"cars at first location"</span>, fontsize=<span class="number">30</span>)</span><br><span class="line">figure.set_yticks(list(reversed(range(<span class="number">21</span>))))</span><br><span class="line">figure.set_title(<span class="string">"policy"</span>, fontsize=<span class="number">30</span>)</span><br><span class="line">figure.set_xlabel(<span class="string">"cars at second location"</span>, fontsize=<span class="number">30</span>)</span><br><span class="line"></span><br><span class="line">plt.savefig(<span class="string">"hello.pdf"</span>)</span><br><span class="line">plt.show()</span><br><span class="line">plt.close()</span><br></pre></td></tr></table></figure></p><h2 id="color"><a href="#color" class="headerlink" title="color"></a>color</h2><h3 id="介绍-9"><a href="#介绍-9" class="headerlink" title="介绍"></a>介绍</h3><p>指定线条的颜色，用color=’’实现。常见的颜色有：’b’, ‘g’, ‘r’, ‘c’, ‘m’, ‘y’, ‘k’, ‘w’。</p><h3 id="代码示例-9"><a href="#代码示例-9" class="headerlink" title="代码示例"></a>代码示例</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">color = [<span class="string">'b'</span>, <span class="string">'g'</span>, <span class="string">'r'</span>, <span class="string">'c'</span>, <span class="string">'m'</span>, <span class="string">'y'</span>, <span class="string">'k'</span>, <span class="string">'w'</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(color)):</span><br><span class="line">    x = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line">    y = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line">    plt.plot(x, y+i, color=color[i])</span><br><span class="line"></span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">plt.plot(range(<span class="number">10</span>), range(<span class="number">10</span>), color=<span class="string">'w'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h3 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h3><p>color=’w’，’w’是white，所以画出来的图你是看不到的。。。这困扰了我好久。。。。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;show&quot;&gt;&lt;a href=&quot;#show&quot; class=&quot;headerlink&quot; title=&quot;show()&quot;&gt;&lt;/a&gt;show()&lt;/h2&gt;&lt;h3 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介
      
    
    </summary>
    
      <category term="python" scheme="http://mxxhcm.github.io/categories/python/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="matplotlib" scheme="http://mxxhcm.github.io/tags/matplotlib/"/>
    
  </entry>
  
  <entry>
    <title>pandas笔记</title>
    <link href="http://mxxhcm.github.io/2019/03/18/python-pandas%E7%AC%94%E8%AE%B0/"/>
    <id>http://mxxhcm.github.io/2019/03/18/python-pandas笔记/</id>
    <published>2019-03-18T07:15:54.000Z</published>
    <updated>2019-08-16T08:59:53.844Z</updated>
    
    <content type="html"><![CDATA[<h2 id="pd-read"><a href="#pd-read" class="headerlink" title="pd.read_*()"></a>pd.read_<em>*</em>()</h2><h3 id="pd-read-csv"><a href="#pd-read-csv" class="headerlink" title="pd.read_csv()"></a>pd.read_csv()</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas</span><br><span class="line">pandas.read_csv(filepath_or_buffer, sep=<span class="string">', '</span>, delimiter=<span class="literal">None</span>, header=<span class="string">'infer'</span>, names=<span class="literal">None</span>, index_col=<span class="literal">None</span>, usecols=<span class="literal">None</span>, squeeze=<span class="literal">False</span>, prefix=<span class="literal">None</span>, mangle_dupe_cols=<span class="literal">True</span>, dtype=<span class="literal">None</span>, engine=<span class="literal">None</span>, converters=<span class="literal">None</span>, true_values=<span class="literal">None</span>, false_values=<span class="literal">None</span>, skipinitialspace=<span class="literal">False</span>, skiprows=<span class="literal">None</span>, nrows=<span class="literal">None</span>, na_values=<span class="literal">None</span>, keep_default_na=<span class="literal">True</span>, na_filter=<span class="literal">True</span>, verbose=<span class="literal">False</span>, skip_blank_lines=<span class="literal">True</span>, parse_dates=<span class="literal">False</span>, infer_datetime_format=<span class="literal">False</span>, keep_date_col=<span class="literal">False</span>, date_parser=<span class="literal">None</span>, dayfirst=<span class="literal">False</span>, iterator=<span class="literal">False</span>, chunksize=<span class="literal">None</span>, compression=<span class="string">'infer'</span>, thousands=<span class="literal">None</span>, decimal=<span class="string">b'.'</span>, lineterminator=<span class="literal">None</span>, quotechar=<span class="string">'"'</span>, quoting=<span class="number">0</span>, escapechar=<span class="literal">None</span>, comment=<span class="literal">None</span>, encoding=<span class="literal">None</span>, dialect=<span class="literal">None</span>, tupleize_cols=<span class="literal">None</span>, error_bad_lines=<span class="literal">True</span>, warn_bad_lines=<span class="literal">True</span>, skipfooter=<span class="number">0</span>, skip_footer=<span class="number">0</span>, doublequote=<span class="literal">True</span>, delim_whitespace=<span class="literal">False</span>, as_recarray=<span class="literal">None</span>, compact_ints=<span class="literal">None</span>, use_unsigned=<span class="literal">None</span>, low_memory=<span class="literal">True</span>, buffer_lines=<span class="literal">None</span>, memory_map=<span class="literal">False</span>, float_precision=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure><p>filepath_or_buffer: 文件路径，或者一个字符串，url等等<br>sep: str,分隔符，默认是’,’<br>delimiter: str,定界符，如果指定该参数，sep参数失效<br>delimiter_whitespace: boolean,指定是否吧空格作为分界符如果指定该参数，则delimiter失效<br>header: int or list of ints,指定列名字，默认是header=0,表示把第一行当做列名，如果header=[0,3,4],表示吧第0,3,4行都当做列名，真正的数据从第二行开始，如果没有列名，指定header=None<br>index_col: int or sequence or False,指定哪几列作为index，index_col=[0,1],表示用前两列的值作为一个index，去访问后面几列的值。<br>prefix: str,如果header为None的话，可以指定列名。<br>parse_dates: boolean or list of ints or names,or list of lists, or dict 如果是True，解析index，如果是list of ints，把每一个int代表的列都分别当做一个日期解析，如果是list of lists，将list中的list作为一个日期解析，如果是字典的话，将dict中key作为一个新的列名，value为这个新的列的值。<br>keep_date_col: boolean,如果parser_dates中是将多个列合并为一个日期的话，是否保留原始列<br>date_parser: function,用来解析parse_dates中给出的日期列，是自己写的函数，函数参数个数和一个日期的列数相同。</p><p>chunksize: 如果文件太大的话，分块读入<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data = pd.read_csv(<span class="string">"input.csv"</span>,chunksize=<span class="number">1000</span>)</span><br><span class="line"><span class="keyword">for</span>  i  <span class="keyword">in</span>  data:</span><br><span class="line">  ...</span><br></pre></td></tr></table></figure></p><h2 id="DataFrame"><a href="#DataFrame" class="headerlink" title="DataFrame"></a>DataFrame</h2><h3 id="声明一个DataFrame"><a href="#声明一个DataFrame" class="headerlink" title="声明一个DataFrame"></a>声明一个DataFrame</h3><p>data = pandas.DataFrame(numpy.arange(16).reshape(4,4),index=list(‘abcd’),columns=(‘wxyz’)<br>    w  x  y  z<br>a  0  1  2  3<br>b  4  5  6  7<br>c  8  9  10  11<br>d  12  13  14  15<br>index 是index列的值<br>columns 是列名</p><h3 id="访问某一列"><a href="#访问某一列" class="headerlink" title="访问某一列"></a>访问某一列</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data = pandas.DataFrame(numpy.arange(<span class="number">16</span>).reshape(<span class="number">4</span>,<span class="number">4</span>),index=list(<span class="string">'abcd'</span>),columns=(<span class="string">'wxyz'</span>)</span><br><span class="line">data[<span class="string">'w'</span>]</span><br><span class="line">data.w</span><br></pre></td></tr></table></figure><h3 id="写入某一列"><a href="#写入某一列" class="headerlink" title="写入某一列"></a>写入某一列</h3><p>只能先访问列 再访问行<br>data[‘w’] = []   # =左右两边shape必须一样<br>data[‘w’][0]  #某一列的第0行</p><h3 id="groupby"><a href="#groupby" class="headerlink" title="groupby"></a>groupby</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">data = pandas.DataFrame(np.arange(<span class="number">16</span>).reshape(<span class="number">4</span>,<span class="number">4</span>),index=list(<span class="string">'abcd'</span>),columns=(<span class="string">'wxyz'</span>))</span><br><span class="line"><span class="keyword">for</span> key,value <span class="keyword">in</span> data.groupby(<span class="string">"w"</span>):  <span class="comment"># group by 列名什么的，就是说某一列的值一样分一组</span></span><br><span class="line">  value = value.values  <span class="comment"># value是一个numpy数组</span></span><br><span class="line">  value_list = value.tolist()  <span class="comment">#将numpy数组转换为一个list</span></span><br><span class="line">  <span class="keyword">for</span> single_list <span class="keyword">in</span> value_list:</span><br><span class="line">     single_list = str(single_list)</span><br><span class="line">     ...</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;pd-read&quot;&gt;&lt;a href=&quot;#pd-read&quot; class=&quot;headerlink&quot; title=&quot;pd.read_*()&quot;&gt;&lt;/a&gt;pd.read_&lt;em&gt;*&lt;/em&gt;()&lt;/h2&gt;&lt;h3 id=&quot;pd-read-csv&quot;&gt;&lt;a href=&quot;#pd-re
      
    
    </summary>
    
      <category term="python" scheme="http://mxxhcm.github.io/categories/python/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="pandas" scheme="http://mxxhcm.github.io/tags/pandas/"/>
    
  </entry>
  
  <entry>
    <title>argparse笔记</title>
    <link href="http://mxxhcm.github.io/2019/03/18/python-argparse%E7%AC%94%E8%AE%B0/"/>
    <id>http://mxxhcm.github.io/2019/03/18/python-argparse笔记/</id>
    <published>2019-03-18T07:15:41.000Z</published>
    <updated>2019-06-26T03:27:59.464Z</updated>
    
    <content type="html"><![CDATA[<h2 id="简单的例子"><a href="#简单的例子" class="headerlink" title="简单的例子"></a>简单的例子</h2><h3 id="创建一个parser"><a href="#创建一个parser" class="headerlink" title="创建一个parser"></a>创建一个parser</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">parser = argparse.ArgumentParser(description=<span class="string">'Process Intergers'</span>)</span><br></pre></td></tr></table></figure><h3 id="添加参数"><a href="#添加参数" class="headerlink" title="添加参数"></a>添加参数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">parser.add_argument(,,)</span><br></pre></td></tr></table></figure><h3 id="解析参数"><a href="#解析参数" class="headerlink" title="解析参数"></a>解析参数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">arglist = parser.parse_args()</span><br></pre></td></tr></table></figure><h3 id="代码示例"><a href="#代码示例" class="headerlink" title="代码示例"></a>代码示例</h3><p>完整代码如下<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_args</span><span class="params">()</span>:</span></span><br><span class="line">    parser = argparse.ArgumentParser(<span class="string">"input parameters"</span>)</span><br><span class="line">    parser.add_argument(<span class="string">"--batch_size"</span>, type=int, default=<span class="number">32</span>)</span><br><span class="line">    parser.add_argument(<span class="string">"--episodes"</span>, type=int, default=<span class="number">1</span>)</span><br><span class="line">    parser.add_argument(<span class="string">"--lr"</span>, type=float, default=<span class="number">0.01</span>)</span><br><span class="line">    parser.add_argument(<span class="string">"--momentum"</span>, type=float, default=<span class="number">0.9</span>)</span><br><span class="line">    args_list = parser.parse_args()</span><br><span class="line">    <span class="keyword">return</span> args_list</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(args_list)</span>:</span></span><br><span class="line">print(args_list.batch_size)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    args_list = parse_args()</span><br><span class="line">    main(args_list)</span><br></pre></td></tr></table></figure></p><h2 id="ArgumentParser-objects"><a href="#ArgumentParser-objects" class="headerlink" title="ArgumentParser objects"></a>ArgumentParser objects</h2><blockquote><p>The ArgumentParser object will hold all the information necessary to parse the command line into python data types</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">argparse</span>.<span class="title">ArgumentParser</span><span class="params">(</span></span></span><br><span class="line"><span class="class"><span class="params">prog=None,</span></span></span><br><span class="line"><span class="class"><span class="params">usage=None,</span></span></span><br><span class="line"><span class="class"><span class="params">description=None,</span></span></span><br><span class="line"><span class="class"><span class="params">epilog=None,</span></span></span><br><span class="line"><span class="class"><span class="params">parents=[],</span></span></span><br><span class="line"><span class="class"><span class="params">formatter_class=argparse.HelpFormatter,</span></span></span><br><span class="line"><span class="class"><span class="params">prefix_chars=<span class="string">'-'</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">fromfile_prefix_chars=None,</span></span></span><br><span class="line"><span class="class"><span class="params">argument_default=None,</span></span></span><br><span class="line"><span class="class"><span class="params">conflict_handler=<span class="string">'error'</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">add_help=True</span></span></span><br><span class="line"><span class="class"><span class="params">)</span></span></span><br></pre></td></tr></table></figure><h3 id="创建一个名为test-py的程序如下"><a href="#创建一个名为test-py的程序如下" class="headerlink" title="创建一个名为test.py的程序如下"></a>创建一个名为test.py的程序如下</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">args = parser.parse_args()</span><br></pre></td></tr></table></figure><p>~#:python test.py -h</p><blockquote><p>usage: test.py [-h]<br>optional arguments:<br>  -h, —help  show this help message and exit</p></blockquote><h3 id="prog参数"><a href="#prog参数" class="headerlink" title="prog参数"></a>prog参数</h3><p>设置显示程序的名称</p><h4 id="直接使用默认显示的程序名"><a href="#直接使用默认显示的程序名" class="headerlink" title="直接使用默认显示的程序名"></a>直接使用默认显示的程序名</h4><p>~#:python test.py -h</p><blockquote><p>usage: test.py [-h]<br>optional arguments:<br>  -h, —help  show this help message and exit</p></blockquote><h4 id="使用prog参数进行设置"><a href="#使用prog参数进行设置" class="headerlink" title="使用prog参数进行设置"></a>使用prog参数进行设置</h4><p>修改test.py的程序如下<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser(prog=<span class="string">"mytest"</span>)</span><br><span class="line">args = parser.parse_args()</span><br></pre></td></tr></table></figure></p><p>~#:python test.py -h</p><blockquote><p>usage: mytest [-h]<br> optional arguments:<br>  -h, —help  show this help message and exit</p></blockquote><p>usage后的名称变为我们prog参数指定的名称</p><h3 id="usage"><a href="#usage" class="headerlink" title="usage"></a>usage</h3><h4 id="使用默认的usage"><a href="#使用默认的usage" class="headerlink" title="使用默认的usage"></a>使用默认的usage</h4><h4 id="使用指定的usage"><a href="#使用指定的usage" class="headerlink" title="使用指定的usage"></a>使用指定的usage</h4><h3 id="description"><a href="#description" class="headerlink" title="description"></a>description</h3><h4 id="使用默认的description"><a href="#使用默认的description" class="headerlink" title="使用默认的description"></a>使用默认的description</h4><h4 id="使用指定的description"><a href="#使用指定的description" class="headerlink" title="使用指定的description"></a>使用指定的description</h4><h3 id="epilog"><a href="#epilog" class="headerlink" title="epilog"></a>epilog</h3><h4 id="使用默认的epilog"><a href="#使用默认的epilog" class="headerlink" title="使用默认的epilog"></a>使用默认的epilog</h4><h4 id="使用指定的epilog"><a href="#使用指定的epilog" class="headerlink" title="使用指定的epilog"></a>使用指定的epilog</h4><h3 id="parents"><a href="#parents" class="headerlink" title="parents"></a>parents</h3><h3 id="formatter-class"><a href="#formatter-class" class="headerlink" title="formatter_class"></a>formatter_class</h3><h3 id="prefix-chars"><a href="#prefix-chars" class="headerlink" title="prefix_chars"></a>prefix_chars</h3><p>指定其他的prefix，默认的是-，比如可以指定可选参数的前缀为+</p><h3 id="fromfile-prefix-chars"><a href="#fromfile-prefix-chars" class="headerlink" title="fromfile_prefix_chars"></a>fromfile_prefix_chars</h3><h3 id="argument-default"><a href="#argument-default" class="headerlink" title="argument_default"></a>argument_default</h3><h3 id="conflict-handler"><a href="#conflict-handler" class="headerlink" title="conflict_handler"></a>conflict_handler</h3><p>将conflict_handler设置为resolve就可以防止override原来older arguments<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser(conflict_handler=<span class="string">'resolve'</span>)</span><br><span class="line">parser.add_argument(<span class="string">'--foo'</span>,<span class="string">'-f'</span>,help=<span class="string">"old help"</span>)</span><br><span class="line">parser.add_argument(<span class="string">'-f'</span>,help=<span class="string">"new_help"</span>)</span><br><span class="line">parser.print_help()</span><br></pre></td></tr></table></figure></p><h3 id="add-help"><a href="#add-help" class="headerlink" title="add_help"></a>add_help</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser(add_help=<span class="literal">False</span>)</span><br><span class="line">parser.print_help()</span><br></pre></td></tr></table></figure><p>输出</p><blockquote><p>usage: [-h]<br> optional arguments:<br>  -h, —help  show this help message and exit<br>将add_help设置为false</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser(add_help=<span class="literal">False</span>)</span><br><span class="line">parser.print_help()</span><br></pre></td></tr></table></figure><p>输出</p><blockquote><p>usage: </p></blockquote><h2 id="The-add-argument-method"><a href="#The-add-argument-method" class="headerlink" title="The add_argument() method"></a>The add_argument() method</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">ArgumentParser.add_argument(</span><br><span class="line">name <span class="keyword">or</span> flags...</span><br><span class="line">[,action],</span><br><span class="line">[,nargs],</span><br><span class="line">[,const],</span><br><span class="line">[,default],</span><br><span class="line">[,type],</span><br><span class="line">[,choices],</span><br><span class="line">[,required],</span><br><span class="line">[,help],</span><br><span class="line">[,metavar],</span><br><span class="line">[,dest]</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">'-f'</span>,<span class="string">'-foo'</span>,<span class="string">'-a'</span>, defaults=, type=, help=)</span><br><span class="line">parser.add_argument(<span class="string">'hello'</span>)</span><br><span class="line">parser.add_argument(<span class="string">'hi'</span>)</span><br><span class="line">args = parser.parse_args([<span class="string">'Hello'</span>,<span class="string">'-f'</span>,<span class="string">'123'</span>,<span class="string">'Hi'</span>])</span><br><span class="line">print(args)</span><br></pre></td></tr></table></figure><h3 id="name-or-flags"><a href="#name-or-flags" class="headerlink" title="name or flags"></a>name or flags</h3><h4 id="添加可选参数"><a href="#添加可选参数" class="headerlink" title="添加可选参数"></a>添加可选参数</h4><p>parser.add_argument(‘-f’, ‘—foo’, ‘-fooo’)</p><h4 id="添加必选参数"><a href="#添加必选参数" class="headerlink" title="添加必选参数"></a>添加必选参数</h4><p>parser.add_argument(‘bar’)</p><h4 id="调用parse-args"><a href="#调用parse-args" class="headerlink" title="调用parse_args()"></a>调用parse_args()</h4><p>当parse_args()函数被调用的时候，可选参数会被-prefix所识别，剩下的参数会被分配给必选参数的位置。如下代码中，’3’对应的就是’hello’的参数，’this is hi’对应的就是’hi’的参数，而’123’是’-f’的参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">'-f'</span>,<span class="string">'-foo'</span>,<span class="string">'-a'</span>)</span><br><span class="line">parser.add_argument(<span class="string">'hello'</span>, type=int)</span><br><span class="line">parser.add_argument(<span class="string">'hi'</span>)</span><br><span class="line">args = parser.parse_args([<span class="string">'3'</span>,<span class="string">'-f'</span>,<span class="string">'123'</span>,<span class="string">'this is hi'</span>])</span><br><span class="line">print(args)</span><br></pre></td></tr></table></figure><p>输出</p><blockquote><p>Namespace(f=’123’, hello=’Hello’, hi=’Hi’)</p></blockquote><h3 id="action"><a href="#action" class="headerlink" title="action"></a>action</h3><h4 id="store-the-default-action"><a href="#store-the-default-action" class="headerlink" title="store,the default action"></a>store,the default action</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">'--foo'</span>)</span><br><span class="line">args = parser.parse_args([<span class="string">'--foo'</span>,<span class="string">'1'</span>])</span><br><span class="line">print(args)</span><br></pre></td></tr></table></figure><p>输出</p><blockquote><p>Namespace(foo=’1’)</p></blockquote><h4 id="store-const"><a href="#store-const" class="headerlink" title="store_const"></a>store_const</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">'--foo'</span>, action=<span class="string">'store_const'</span>, const=<span class="number">42</span>)</span><br><span class="line">args = parser.parse_args([<span class="string">'--foo'</span>)</span><br><span class="line">print(args)</span><br></pre></td></tr></table></figure><p>输出</p><blockquote><p>Namespace(foo=42)</p></blockquote><h4 id="store-true-and-store-false"><a href="#store-true-and-store-false" class="headerlink" title="store_true and store_false"></a>store_true and store_false</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">'--foo'</span>, action=<span class="string">'store_true'</span>)</span><br><span class="line">parser.add_argument(<span class="string">'--bar'</span>, action=<span class="string">'store_false'</span>)</span><br><span class="line">parser.add_argument(<span class="string">'--baz'</span>, action=<span class="string">'store_false'</span>)</span><br><span class="line">args = parser.parse_args(<span class="string">'--foo --bar'</span>.split())</span><br><span class="line">print(args)</span><br></pre></td></tr></table></figure><p>输出</p><blockquote><p>Namespace(bar=False, baz=True, foo=True)</p></blockquote><p>这里为什么是这样呢，因为默认存储的都是True，当你调用—bar,—foo参数时，会执行action操作，会把action指定的动作执行</p><h4 id="d-append"><a href="#d-append" class="headerlink" title="d.append"></a>d.append</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">'--foo'</span>, action=<span class="string">'append'</span>)</span><br><span class="line">args = parser.parse_args(<span class="string">'--foo 1 --foo 2 --foo 3'</span>.split())</span><br><span class="line">print(args)</span><br></pre></td></tr></table></figure><p>输出</p><blockquote><p>Namespace(foo=[‘1’, ‘2’, ‘3’])</p></blockquote><h4 id="append-const"><a href="#append-const" class="headerlink" title="append_const"></a>append_const</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">'--str'</span>, action=<span class="string">'append_const'</span>,const=str)</span><br><span class="line">parser.add_argument(<span class="string">'--int'</span>, action=<span class="string">'append_const'</span>,const=int)</span><br><span class="line">args = parser.parse_args(<span class="string">'--str --int'</span>.split())</span><br><span class="line">print(args)</span><br></pre></td></tr></table></figure><p>输出</p><blockquote><p>Namespace(int=[<class 'int'>], str=[<class 'str'>])</class></class></p></blockquote><h4 id="count"><a href="#count" class="headerlink" title="count"></a>count</h4><p>统计一个keyword argument出现了多少次<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">'--co'</span>, <span class="string">'-c'</span>,action=<span class="string">'count'</span>)</span><br><span class="line">args = parser.parse_args([<span class="string">'-ccc'</span>])</span><br><span class="line">print(args)</span><br></pre></td></tr></table></figure></p><p>输出</p><blockquote><p>Namespace(co=3)</p></blockquote><h4 id="help"><a href="#help" class="headerlink" title="help"></a>help</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">args = parser.parse_args(<span class="string">'--help'</span>.split())</span><br><span class="line">print(args)</span><br></pre></td></tr></table></figure><p>输出，如果是交互式环境的话，会退出python</p><blockquote><p>usage: [-h]</p><p>optional arguments:<br>  -h, —help  show this help message and exit</p></blockquote><h4 id="version"><a href="#version" class="headerlink" title="version"></a>version</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">'--version'</span>, action=<span class="string">'version'</span>,version=<span class="string">'version 3'</span>)</span><br><span class="line">args = parser.parse_args(<span class="string">'--version'</span>.split())</span><br><span class="line">print(args)</span><br></pre></td></tr></table></figure><p>输出,如果是交互式环境的话，会退出python</p><blockquote><p>version 3</p></blockquote><h3 id="nargs-指定参数个数"><a href="#nargs-指定参数个数" class="headerlink" title="nargs 指定参数个数"></a>nargs 指定参数个数</h3><h4 id="N"><a href="#N" class="headerlink" title="N"></a>N</h4><p>如果是可选参数的话，或者不指定这个参数，或者必须指定N个参数<br>如果是必选参数的话，必须指定N个参数，不能多也不能少，也不能为0个<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">'--foo'</span>,nargs=<span class="number">3</span>)</span><br><span class="line">parser.add_argument(<span class="string">'bar'</span>,nargs=<span class="number">4</span>)</span><br><span class="line">args = parser.parse_args(<span class="string">'bar 3 4 5'</span>.split())</span><br><span class="line">print(args)</span><br></pre></td></tr></table></figure></p><p>输出</p><blockquote><p>Namespace(bar=[‘bar’, ‘3’, ‘4’, ‘5’], foo=None)</p></blockquote><h4 id><a href="#" class="headerlink" title="?"></a>?</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">'--foo'</span>,nargs=<span class="string">'?'</span>,const=<span class="string">'c'</span>,default=<span class="string">'d'</span>)</span><br><span class="line">parser.add_argument(<span class="string">'bar'</span>,nargs=<span class="string">'?'</span>,default=<span class="string">'d'</span>)</span><br><span class="line">args = parser.parse_args(<span class="string">'3'</span>.split())</span><br><span class="line">print(args)</span><br><span class="line">args = parser.parse_args(<span class="string">'3 --foo'</span>.split())</span><br><span class="line">print(args)</span><br></pre></td></tr></table></figure><p>输出</p><blockquote><p>Namespace(bar=’3’, foo=’d’)<br>Namespace(bar=’3’, foo=’c’)</p></blockquote><p>如果显式指定可选参数，但是不给它参数，那么如果有const的话，就会显示const的值，否则就会显示None</p><h4 id="-1"><a href="#-1" class="headerlink" title="*"></a>*</h4><p>nargs设置为*的话，不能直接用const=’’来设置const参数，需要使用其他方式<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">'--foo'</span>,nargs=<span class="string">'*'</span>,default=<span class="string">'d'</span>)</span><br><span class="line">parser.add_argument(<span class="string">'bar'</span>,nargs=<span class="string">'*'</span>,default=<span class="string">'d'</span>)</span><br><span class="line">args = parser.parse_args(<span class="string">'3 --foo 3 4'</span>.split())</span><br><span class="line">print(args)</span><br></pre></td></tr></table></figure></p><p>输出</p><blockquote><p>Namespace(bar=[‘3’], foo=[‘3’, ‘4’])</p></blockquote><h4 id="-2"><a href="#-2" class="headerlink" title="+"></a>+</h4><p>nargs设置为+，参数个数必须大于等于1<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">'--foo'</span>,nargs=<span class="string">'+'</span>,default=<span class="string">'d'</span>)</span><br><span class="line">parser.add_argument(<span class="string">'bar'</span>,nargs=<span class="string">'+'</span>,default=<span class="string">'d'</span>)</span><br><span class="line">args = parser.parse_args(<span class="string">'3 3'</span>.split())</span><br><span class="line">print(args)</span><br><span class="line">args = parser.parse_args(<span class="string">'--foo 3'</span>.split())</span><br><span class="line">print(args)</span><br></pre></td></tr></table></figure></p><p>输出</p><blockquote><p>Namespace(bar=[‘3’], foo=’d’)<br>Namespace(bar=[‘3’], foo=[‘3’])</p></blockquote><h3 id="const"><a href="#const" class="headerlink" title="const"></a>const</h3><h4 id="action-’’store-const”-or-action-”append-const”"><a href="#action-’’store-const”-or-action-”append-const”" class="headerlink" title="action=’’store_const” or action=”append_const”"></a>action=’’store_const” or action=”append_const”</h4><p>the examples are in the action<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">'--foo'</span>, action=<span class="string">'store_const'</span>, const=<span class="number">42</span>)</span><br><span class="line">args = parser.parse_args([<span class="string">'--foo'</span>)</span><br><span class="line">print(args)</span><br></pre></td></tr></table></figure></p><p>输出</p><blockquote><p>Namespace(foo=42)</p></blockquote><h4 id="like-f-or-—foo-and-nargs-’-’"><a href="#like-f-or-—foo-and-nargs-’-’" class="headerlink" title="like -f or —foo and nargs=’?’"></a>like -f or —foo and nargs=’?’</h4><p>the examples are the same as examples in the nargs=’?’<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">'--foo'</span>,nargs=<span class="string">'?'</span>,const=<span class="string">'c'</span>,default=<span class="string">'d'</span>)</span><br><span class="line">parser.add_argument(<span class="string">'bar'</span>,nargs=<span class="string">'?'</span>,default=<span class="string">'d'</span>)</span><br><span class="line">args = parser.parse_args(<span class="string">'3'</span>.split())</span><br><span class="line">print(args)</span><br><span class="line">args = parser.parse_args(<span class="string">'3 --foo'</span>.split())</span><br><span class="line">print(args)</span><br></pre></td></tr></table></figure></p><p>输出</p><blockquote><p>Namespace(bar=’3’, foo=’d’)<br>Namespace(bar=’3’, foo=’c’)<br>如果显式指定可选参数，但是不给它参数，那么如果有const的话，就会显示const的值，否则就会显示None</p></blockquote><h3 id="default"><a href="#default" class="headerlink" title="default"></a>default</h3><p>default对于可选参数来说，是有用的，当可选参数没有在command line中显示出来时被使用，但是对于必选参数来说，只有nargs=?或者*才能起作用。</p><h4 id="对于可选参数"><a href="#对于可选参数" class="headerlink" title="对于可选参数"></a>对于可选参数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">'--foo'</span>,default=<span class="number">43</span>)</span><br><span class="line">args = parser.parse_args([])</span><br><span class="line">print(args)</span><br><span class="line">args = parser.parse_args(<span class="string">'--foo 3'</span>.split())</span><br><span class="line">print(args)</span><br></pre></td></tr></table></figure><p>输出</p><blockquote><p>Namespace(foo=’43’)<br>Namespace(foo=’3’)</p></blockquote><h4 id="对于必选参数"><a href="#对于必选参数" class="headerlink" title="对于必选参数"></a>对于必选参数</h4><p>对于nargs=‘+’是会出错<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(&apos;bar&apos;,nargs=&apos;+&apos;,default=&apos;d&apos;)</span><br><span class="line">args = parser.parse_args([])</span><br><span class="line">print(args)</span><br></pre></td></tr></table></figure></p><p>输出</p><blockquote><p>usage: [-h] bar [bar …]<br>: error: the following arguments are required: bar</p></blockquote><p>对于nargs=‘*’或者nargs=’?’就行了<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">'bar'</span>,nargs=<span class="string">'?'</span>,default=<span class="string">'d'</span>)</span><br><span class="line">args = parser.parse_args([])</span><br><span class="line">print(args)</span><br></pre></td></tr></table></figure></p><p>输出</p><blockquote><p>Namespace(bar=’d’)</p></blockquote><h3 id="type"><a href="#type" class="headerlink" title="type"></a>type</h3><p>将输入的字符串参数转换为你想要的参数类型<br>对于文件类型来说，这个文件必须在当前目录存在。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">'--door'</span>,type=int)</span><br><span class="line">parser.add_argument(<span class="string">'filename'</span>,type=file)</span><br><span class="line">parser.parse_args([<span class="string">'--door'</span>,<span class="string">'3'</span>,<span class="string">'hello.txt'</span>])</span><br></pre></td></tr></table></figure></p><p>输出</p><blockquote><p>Namespace(door=3)<br>这里的door就是int类型的</p></blockquote><h3 id="choices"><a href="#choices" class="headerlink" title="choices"></a>choices</h3><p>输入的参数必须在choices这个范围中，否则就会报错<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParse()</span><br><span class="line">parser.add_argument(<span class="string">'--door'</span>,type=int,choices=range(<span class="number">1</span>,<span class="number">9</span>))</span><br><span class="line">parser.parse_args([<span class="string">'--door'</span>,<span class="string">'3'</span>])</span><br></pre></td></tr></table></figure></p><p>输出</p><blockquote><p>Namespace(door=3)</p></blockquote><h3 id="required"><a href="#required" class="headerlink" title="required"></a>required</h3><p>如果将required设置为True的话，那么这个可选参数必须要设置的<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">'-f'</span>, <span class="string">'--foo-bar'</span>, <span class="string">'--foo'</span>,required=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure></p><h3 id="help-1"><a href="#help-1" class="headerlink" title="help"></a>help</h3><p>help可以设置某个参数的简要介绍。<br>使用help=argparse.SUPRESS可以在help界面中不显示这个参数的介绍<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">'-f'</span>, <span class="string">'--foo-bar'</span>, <span class="string">'--foo'</span>,help=<span class="string">'fool you '</span>)</span><br><span class="line">parser.add_argument(<span class="string">'-xs'</span>, <span class="string">'--y'</span>,help=argparse.SUPPRESS)</span><br><span class="line">parser.print_help()</span><br></pre></td></tr></table></figure></p><p>输出</p><blockquote><p>usage: [-h] [-f FOO_BAR]</p><p>optional arguments:<br>  -h, —help            show this help message and exit<br>  -f FOO_BAR, —foo-bar FOO_BAR, —foo FOO_BAR<br>                        fool you</p></blockquote><h3 id="dest"><a href="#dest" class="headerlink" title="dest"></a>dest</h3><p>dest就是在help输出时显示的optional和positional参数后跟的名字（没有指定metavar时）<br>如下,dest就是FOO<br>-foo FOO</p><h4 id="positional-argument"><a href="#positional-argument" class="headerlink" title="positional argument"></a>positional argument</h4><p>dest is normally supplied as the first argument to add_argument()</p><h4 id="可选参数"><a href="#可选参数" class="headerlink" title="可选参数"></a>可选参数</h4><p>对于optional argument选择，—参数最长的一个作为dest，如果没有最长的，选择第一个出现的，如果没有—参数名，选择-参数。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">'-f'</span>, <span class="string">'--foo-bar'</span>, <span class="string">'--foo'</span>)</span><br><span class="line">parser.add_argument(<span class="string">'-xs'</span>, <span class="string">'--y'</span>)</span><br><span class="line">args = parser.parse_args(<span class="string">'-f 1 -xs 2'</span>.split())</span><br><span class="line">print(args)</span><br><span class="line">args = parser.parse_args(<span class="string">'--foo 1 --y 2'</span>.split())</span><br><span class="line">print(args)</span><br><span class="line">parser.print_help()</span><br></pre></td></tr></table></figure></p><p>输出</p><blockquote><p>Namespace(foo_bar=’1’, y=’2’)<br>Namespace(foo_bar=’1’, y=’2’)<br>usage: [-h] [-f FOO_BAR] [-xs Y]</p><p>optional arguments:<br>  -h, —help            show this help message and exit<br>  -f FOO_BAR, —foo-bar FOO_BAR, —foo FOO_BAR<br>  -xs Y, —y Y</p></blockquote><h3 id="metavar"><a href="#metavar" class="headerlink" title="metavar"></a>metavar</h3><p>如果指定metavar变量名的话，那么help输出的postional和positional参数后跟的名字就是metavar的名字而不是dest的名字</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">'-f'</span>, <span class="string">'--foo-bar'</span>, <span class="string">'--foo'</span>,metavar=<span class="string">"FOO"</span>)</span><br><span class="line">parser.add_argument(<span class="string">'-xs'</span>, <span class="string">'--y'</span>,metavar=<span class="string">'XY'</span>)</span><br><span class="line">args = parser.parse_args(<span class="string">'-f 1 -xs 2'</span>.split())</span><br><span class="line">print(args)</span><br><span class="line">args = parser.parse_args(<span class="string">'--foo 1 --y 2'</span>.split())</span><br><span class="line">print(args)</span><br><span class="line">parser.print_help()</span><br></pre></td></tr></table></figure><p>输出</p><blockquote><p>Namespace(foo_bar=’1’, y=’2’)<br>Namespace(foo_bar=’1’, y=’2’)<br>usage: [-h] [-f FOO] [-xs XY]</p><p>optional arguments:<br>  -h, —help            show this help message and exit<br>  -f FOO, —foo-bar FOO, —foo FOO<br>  -xs XY, —y XY</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;简单的例子&quot;&gt;&lt;a href=&quot;#简单的例子&quot; class=&quot;headerlink&quot; title=&quot;简单的例子&quot;&gt;&lt;/a&gt;简单的例子&lt;/h2&gt;&lt;h3 id=&quot;创建一个parser&quot;&gt;&lt;a href=&quot;#创建一个parser&quot; class=&quot;headerlink&quot; 
      
    
    </summary>
    
      <category term="python" scheme="http://mxxhcm.github.io/categories/python/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="argparse" scheme="http://mxxhcm.github.io/tags/argparse/"/>
    
  </entry>
  
  <entry>
    <title>numpy笔记</title>
    <link href="http://mxxhcm.github.io/2019/03/18/python-numpy%E7%AC%94%E8%AE%B0/"/>
    <id>http://mxxhcm.github.io/2019/03/18/python-numpy笔记/</id>
    <published>2019-03-18T07:15:29.000Z</published>
    <updated>2019-10-21T06:51:19.712Z</updated>
    
    <content type="html"><![CDATA[<h2 id="numpy-ndarray"><a href="#numpy-ndarray" class="headerlink" title="numpy.ndarray"></a>numpy.ndarray</h2><h3 id="attribute-of-the-np-ndarray"><a href="#attribute-of-the-np-ndarray" class="headerlink" title="attribute of the np.ndarray"></a>attribute of the np.ndarray</h3><p>ndarray.shape        #array的shape<br>ndarray.ndim            #array的维度<br>ndarray.size            #the number of ndarray in array<br>ndarray.dtype        #type of the number in array，dtype可以是’S’,int等<br>ndarray.itemsize        #size of the element in array<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array[array&gt;<span class="number">0</span>].size    <span class="comment">#统计一个数组有多少个非零元素，不论array的维度是多少</span></span><br></pre></td></tr></table></figure></p><h3 id="改变数组数据类型"><a href="#改变数组数据类型" class="headerlink" title="改变数组数据类型"></a>改变数组数据类型</h3><p>将整形数组改为字符型<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = numpy.zeros((<span class="number">3</span>,<span class="number">4</span>),dtype=<span class="string">'i'</span>)</span><br><span class="line">a.astype(<span class="string">'S'</span>)</span><br></pre></td></tr></table></figure></p><h3 id="将numpy转为list"><a href="#将numpy转为list" class="headerlink" title="将numpy转为list"></a>将numpy转为list</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">a = np.zeros((<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>))</span><br><span class="line">b = a.tolist()</span><br><span class="line">print(b)</span><br><span class="line">print(len(b))</span><br><span class="line">print(len(b[<span class="number">0</span>]))</span><br><span class="line"><span class="comment"># [[[0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0]], [[0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0]], [[0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0]]]</span></span><br><span class="line"><span class="comment"># 3</span></span><br><span class="line"><span class="comment"># 4</span></span><br></pre></td></tr></table></figure><h3 id="reshape"><a href="#reshape" class="headerlink" title="reshape"></a>reshape</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">a = np.zeros((<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>))</span><br><span class="line">a.reshape(<span class="number">-1</span>, <span class="number">1</span>)</span><br></pre></td></tr></table></figure><h3 id="flatten"><a href="#flatten" class="headerlink" title="flatten"></a>flatten</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">a = np.zeros((<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>))</span><br><span class="line">a.flatten()</span><br></pre></td></tr></table></figure><h2 id="numpy数组初始化"><a href="#numpy数组初始化" class="headerlink" title="numpy数组初始化"></a>numpy数组初始化</h2><ul><li>numpy.array()</li><li>numpy.zeros()</li><li>numpy.empty()</li><li>numpy.random()</li></ul><h3 id="numpy-array"><a href="#numpy-array" class="headerlink" title="numpy.array()"></a>numpy.array()</h3><h4 id="API"><a href="#API" class="headerlink" title="API"></a>API</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">np.array(</span><br><span class="line">    object,</span><br><span class="line">    dtype=<span class="literal">None</span>,</span><br><span class="line">    copy=<span class="literal">True</span>,</span><br><span class="line">    order=<span class="literal">False</span>,</span><br><span class="line">    subok=<span class="literal">False</span>,</span><br><span class="line">    ndim=<span class="number">0</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="numpy-zeros"><a href="#numpy-zeros" class="headerlink" title="numpy.zeros()"></a>numpy.zeros()</h3><h4 id="API-1"><a href="#API-1" class="headerlink" title="API"></a>API</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">np.zeros(</span><br><span class="line">    shape,</span><br><span class="line">    dtype=float,</span><br><span class="line">    order=<span class="string">'C'</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h4 id="代码示例"><a href="#代码示例" class="headerlink" title="代码示例"></a>代码示例</h4><p><a href>代码地址</a><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">np.zeros((<span class="number">3</span>, <span class="number">4</span>),dtype=<span class="string">'i'</span>)</span><br><span class="line">``` </span><br><span class="line"></span><br><span class="line"><span class="comment">### numpy.empty()</span></span><br><span class="line"><span class="comment">#### API</span></span><br><span class="line">``` python</span><br><span class="line">np.empty(</span><br><span class="line">    shape,</span><br><span class="line">    dtype=float,</span><br><span class="line">    order=<span class="string">'C'</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure></p><h4 id="代码示例-1"><a href="#代码示例-1" class="headerlink" title="代码示例"></a>代码示例</h4><p><a href>代码地址</a><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.empty((<span class="number">3</span>, <span class="number">4</span>),dtype=<span class="string">'f'</span>)</span><br></pre></td></tr></table></figure></p><h3 id="numpy-random"><a href="#numpy-random" class="headerlink" title="numpy.random"></a>numpy.random</h3><h4 id="numpy-random-randn"><a href="#numpy-random-randn" class="headerlink" title="numpy.random.randn()"></a>numpy.random.randn()</h4><p>返回标准正态分布的一个样本<br>numpy.random.randn(d0, d1, …, dn)<br>例子<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.random.randn(<span class="number">3</span>,<span class="number">4</span>)</span><br></pre></td></tr></table></figure></p><blockquote><p>array([[ 0.47203644, -0.0869761 , -1.02814481, -0.45945482],<br>       [ 0.34586502, -0.63121119,  0.35510786,  0.82975136],<br>       [-2.00253326, -0.63773715, -0.82700167,  1.80724647]])</p></blockquote><h4 id="numpy-random-rand"><a href="#numpy-random-rand" class="headerlink" title="numpy.random.rand()"></a>numpy.random.rand()</h4><p>创建一个给定shape的数组，从区间[0,1)上的均匀分布中随机采样</p><blockquote><p>create an array of the given shape and populate it with random samples from a uniform disctribution over [0,1)</p></blockquote><p>numpy.random.rand(d0,d1,…,dn)<br>例子<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.random.rand(<span class="number">3</span>,<span class="number">4</span>)</span><br></pre></td></tr></table></figure></p><h4 id="numpy-random-random"><a href="#numpy-random-random" class="headerlink" title="numpy.random.random()"></a>numpy.random.random()</h4><p>返回区间[0.0, 1.0)之间的随机浮点数</p><blockquote><p>return random floats in the half-open interval [0.0,1.0)</p></blockquote><p>numpy.random.random(size=None)<br>例子<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.random.random((<span class="number">3</span>,<span class="number">4</span>))</span><br></pre></td></tr></table></figure></p><h5 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h5><p>注意，random.random()和random.rand()实现的功能都是一样的，就是输入的参数不同。见参考文献[1]。</p><h4 id="numpy-random-ranf"><a href="#numpy-random-ranf" class="headerlink" title="numpy.random.ranf()"></a>numpy.random.ranf()</h4><p>我觉得它和random.random()没啥区别</p><h4 id="numpy-random-randint"><a href="#numpy-random-randint" class="headerlink" title="numpy.random.randint()"></a>numpy.random.randint()</h4><blockquote><p>return random integers from low(inclusive) to high(exclusive),[low,high) if high is None,then results are from [0,low)</p></blockquote><p>numpy.random.randint(low,high=None,size=None,dtype=’l’)<br>例子<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">np.random.randint(<span class="number">3</span>,size=[<span class="number">3</span>,<span class="number">4</span>])</span><br><span class="line">np.random.randint(<span class="number">4</span>,<span class="number">6</span>,size=[<span class="number">6</span>,<span class="number">2</span>])</span><br></pre></td></tr></table></figure></p><h4 id="numpy-random-RandomState"><a href="#numpy-random-RandomState" class="headerlink" title="numpy.random.RandomState()"></a>numpy.random.RandomState()</h4><blockquote><p>class numpy.random.RandomState(seed=None)</p></blockquote><p>这是一个类，给定一个种子，它接下来产生的一系列随机数都是固定的。每次需要重新产生随机数的时候，就重置种子。<br>通过一个例子来看：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">rdm = np.randrom.RandomState()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">   rdm.seed(<span class="number">3</span>)</span><br><span class="line">   print(rdm.rand())</span><br><span class="line">   print(rdm.rand())</span><br><span class="line">   print(rdm.rand())</span><br><span class="line">    print(<span class="string">"\n"</span>)</span><br><span class="line"><span class="comment"># 0.9670298390136767</span></span><br><span class="line"><span class="comment"># 0.5472322491757223</span></span><br><span class="line"><span class="comment"># 0.9726843599648843</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 0.9670298390136767</span></span><br><span class="line"><span class="comment"># 0.5472322491757223</span></span><br><span class="line"><span class="comment"># 0.9726843599648843</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 0.9670298390136767</span></span><br><span class="line"><span class="comment"># 0.5472322491757223</span></span><br><span class="line"><span class="comment"># 0.9726843599648843</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 0.9670298390136767</span></span><br><span class="line"><span class="comment"># 0.5472322491757223</span></span><br><span class="line"><span class="comment"># 0.9726843599648843</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 0.9670298390136767</span></span><br><span class="line"><span class="comment"># 0.5472322491757223</span></span><br><span class="line"><span class="comment"># 0.9726843599648843</span></span><br></pre></td></tr></table></figure></p><h3 id="创建bool类型数组"><a href="#创建bool类型数组" class="headerlink" title="创建bool类型数组"></a>创建bool类型数组</h3><p>np.ones([2, 2], dtype=bool)<br>np.zeros([2, 2], dtype=bool)</p><h3 id="others"><a href="#others" class="headerlink" title="others"></a>others</h3><h4 id="numpy-arange"><a href="#numpy-arange" class="headerlink" title="numpy.arange()"></a>numpy.arange()</h4><h4 id="numpy-linspace"><a href="#numpy-linspace" class="headerlink" title="numpy.linspace()"></a>numpy.linspace()</h4><h2 id="np-random-binomial"><a href="#np-random-binomial" class="headerlink" title="np.random.binomial"></a>np.random.binomial</h2><h3 id="API-2"><a href="#API-2" class="headerlink" title="API"></a>API</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">numpy.random.binomial(</span><br><span class="line">n, </span><br><span class="line">p, </span><br><span class="line">size=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>二项分布，共有三个参数，前两个是必选参数，第三个是可选参数。$n$是实验的个数，比如同时扔三枚硬币，这里就是$n=3$,$p$是为$1$的概率。$size$是总共进行多少次实验。<br>返回值是在每次试验中，trival成功的个数。如果是一个scalar，代表$size=1$，如果是一个list，代表$size\gt 1$。</p><h3 id="代码示例-2"><a href="#代码示例-2" class="headerlink" title="代码示例"></a>代码示例</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    rand = np.random.binomial(<span class="number">2</span>, <span class="number">0.9</span>)</span><br><span class="line">    print(rand)</span><br><span class="line"><span class="comment"># 可以看成扔2个硬币，每个硬币正面向上的概率是0.9,最后有几个硬币正面向上。</span></span><br><span class="line"><span class="comment"># 2</span></span><br><span class="line"><span class="comment"># 2</span></span><br><span class="line"><span class="comment"># 2</span></span><br><span class="line"><span class="comment"># 2</span></span><br><span class="line"><span class="comment"># 2</span></span><br><span class="line"><span class="comment"># 2</span></span><br><span class="line"><span class="comment"># 2</span></span><br><span class="line"><span class="comment"># 1</span></span><br><span class="line"><span class="comment"># 1</span></span><br><span class="line"><span class="comment"># 2</span></span><br><span class="line"></span><br><span class="line">rand = np.random.binomial(<span class="number">3</span>, <span class="number">0.9</span>, <span class="number">5</span>)</span><br><span class="line">print(rand)</span><br><span class="line"><span class="comment"># 可以看成扔3个硬币，每个硬币正面向上的概率是0.9,最后有几个硬币正面向上。一共进行5次实验。</span></span><br><span class="line"><span class="comment"># [2 2 3 3 2]</span></span><br></pre></td></tr></table></figure><h2 id="np-random-choice"><a href="#np-random-choice" class="headerlink" title="np.random.choice"></a>np.random.choice</h2><h3 id="API-3"><a href="#API-3" class="headerlink" title="API"></a>API</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">numpy.random.choice(</span><br><span class="line">    a,  <span class="comment"># 1d array或者int，如果是一个数组，从其中生成样本；如果是一个整数，从np.arange(a)中生成样本</span></span><br><span class="line">    size=<span class="literal">None</span>,  <span class="comment"># output shape，比如是(m, n, k)的话，总共要m*n*k个样本，默认是None,返回一个样本。</span></span><br><span class="line">    replace=<span class="literal">True</span>,   <span class="comment"># 是否使用replacement，设置为False的话所有元素不重复。</span></span><br><span class="line">    p=<span class="literal">None</span>  <span class="comment"># 概率分布，相加必须等于1，默认是从一个均匀分布中采样。</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="代码示例-3"><a href="#代码示例-3" class="headerlink" title="代码示例"></a>代码示例</h3><p><a href>代码地址</a><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">a0 = np.random.choice([<span class="number">8</span>, <span class="number">9</span>, <span class="number">-1</span>, <span class="number">2</span>, <span class="number">0</span>], <span class="number">3</span>)</span><br><span class="line">print(a0)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 从np.arange(5)从使用均匀分布采样一个shape为4的样本</span></span><br><span class="line">a1 = np.random.choice(<span class="number">5</span>, <span class="number">4</span>)</span><br><span class="line">print(a1)</span><br><span class="line"></span><br><span class="line">a2 = np.random.choice(<span class="number">5</span>, <span class="number">8</span>, p=[<span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.5</span>, <span class="number">0.2</span>, <span class="number">0</span>])</span><br><span class="line">print(a2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># replace 设置为False，相当于np.random.permutation()</span></span><br><span class="line">a3 = np.random.choice([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">8</span>, <span class="number">9</span>], <span class="number">5</span>, replace=<span class="literal">False</span>)</span><br><span class="line">print(a3)</span><br></pre></td></tr></table></figure></p><h2 id="np-random-permutation"><a href="#np-random-permutation" class="headerlink" title="np.random.permutation"></a>np.random.permutation</h2><h3 id="API-4"><a href="#API-4" class="headerlink" title="API"></a>API</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">np.random.permutation(</span><br><span class="line">    x   <span class="comment"># int或者array，如果是int，置换np.arange(x)。如果是array，make a copy，随机打乱元素。</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>对输入序列进行排列组合，如果输入是多维的话，只会在第一维重新排列。</p><h3 id="代码示例-4"><a href="#代码示例-4" class="headerlink" title="代码示例"></a>代码示例</h3><p><a href>代码地址</a><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">a1 = np.random.permutation(<span class="number">9</span>)</span><br><span class="line">print(a1)</span><br><span class="line"></span><br><span class="line">a2 = np.random.permutation([<span class="number">1</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">9</span>, <span class="number">8</span>])</span><br><span class="line">print(a2)</span><br><span class="line"></span><br><span class="line">a3 = np.random.permutation(np.arange(<span class="number">9</span>).reshape(<span class="number">3</span>, <span class="number">3</span>))</span><br><span class="line">print(a3)</span><br></pre></td></tr></table></figure></p><h2 id="np-random-normal"><a href="#np-random-normal" class="headerlink" title="np.random.normal"></a>np.random.normal</h2><h3 id="API-5"><a href="#API-5" class="headerlink" title="API"></a>API</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">numpy.random.normal(loc=0.0, scale=1.0, size=None)  </span><br><span class="line">loc:float，正态分布的均值，对应着整个分布的center</span><br><span class="line">scale:float，正态分布的标准差，对应于分布的宽度，scale越大越矮胖，scale越小，越瘦高</span><br><span class="line">size:int or tuple of ints，输出的shape，默认为None，只输出一个值</span><br><span class="line">np.random.randn(size)相当于np.random.normal(loc=0, scale=1, size)</span><br></pre></td></tr></table></figure><h2 id="np-argsort"><a href="#np-argsort" class="headerlink" title="np.argsort"></a>np.argsort</h2><h3 id="API-6"><a href="#API-6" class="headerlink" title="API"></a>API</h3><p>numpy.argsort(a, axis=-1, kind=’quicksort’, order=None)<br>axis:对哪个axis进行排序，默认是-1</p><h3 id="功能"><a href="#功能" class="headerlink" title="功能"></a>功能</h3><p>将数组排序后（默认是从小到大排序），返回排序后的数组在原数组中的位置。</p><p>参考文献<br>1.<a href="https://stackoverflow.com/questions/47231852/np-random-rand-vs-np-random-random" target="_blank" rel="noopener">https://stackoverflow.com/questions/47231852/np-random-rand-vs-np-random-random</a><br>2.<a href="https://stackoverflow.com/questions/21174961/how-to-create-a-numpy-array-of-all-true-or-all-false" target="_blank" rel="noopener">https://stackoverflow.com/questions/21174961/how-to-create-a-numpy-array-of-all-true-or-all-false</a><br>3.<a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.choice.html" target="_blank" rel="noopener">https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.choice.html</a><br>4.<a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.permutation.html" target="_blank" rel="noopener">https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.permutation.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;numpy-ndarray&quot;&gt;&lt;a href=&quot;#numpy-ndarray&quot; class=&quot;headerlink&quot; title=&quot;numpy.ndarray&quot;&gt;&lt;/a&gt;numpy.ndarray&lt;/h2&gt;&lt;h3 id=&quot;attribute-of-the-np-n
      
    
    </summary>
    
      <category term="python" scheme="http://mxxhcm.github.io/categories/python/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="numpy" scheme="http://mxxhcm.github.io/tags/numpy/"/>
    
  </entry>
  
  <entry>
    <title>h5py笔记</title>
    <link href="http://mxxhcm.github.io/2019/03/18/python-hdf5%E7%AC%94%E8%AE%B0/"/>
    <id>http://mxxhcm.github.io/2019/03/18/python-hdf5笔记/</id>
    <published>2019-03-18T07:12:03.000Z</published>
    <updated>2019-06-13T02:06:17.974Z</updated>
    
    <content type="html"><![CDATA[<h2 id="python包安装"><a href="#python包安装" class="headerlink" title="python包安装"></a>python包安装</h2><p>~$:pip install h5py</p><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><h3 id="创建和打开h5py文件"><a href="#创建和打开h5py文件" class="headerlink" title="创建和打开h5py文件"></a>创建和打开h5py文件</h3><p>f = h5py.File(“pathname”,”w”)<br>w     create file, truncate if exist<br>w- or x  create file,fail if exists<br>r         readonly, file must be exist r+        read/write,file must be exist<br>a        read/write if exists,create othrewise (default)</p><h3 id="删除一个dataset或者group"><a href="#删除一个dataset或者group" class="headerlink" title="删除一个dataset或者group"></a>删除一个dataset或者group</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">del</span> group[<span class="string">"dataset_name/group_name"</span>]</span><br></pre></td></tr></table></figure><h2 id="dataset"><a href="#dataset" class="headerlink" title="dataset"></a>dataset</h2><h3 id="什么是dataset"><a href="#什么是dataset" class="headerlink" title="什么是dataset"></a>什么是dataset</h3><p>datasets和numpy arrays挺像的</p><h3 id="创建一个dataset"><a href="#创建一个dataset" class="headerlink" title="创建一个dataset"></a>创建一个dataset</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">f = h5py.File(<span class="string">"pathname"</span>,<span class="string">"w"</span>)</span><br><span class="line">f.create_dataset(<span class="string">"dataset_name"</span>, (<span class="number">10</span>,), dtype=<span class="string">'i'</span>)</span><br><span class="line">f.create_dataset(<span class="string">"dataset_name"</span>, (<span class="number">10</span>,), dtype=<span class="string">'c'</span>)</span><br></pre></td></tr></table></figure><p>第一个参数是dataset的名字, 第二个参数是dataset的shape, dtype参数是dataset中元素的类型。</p><h3 id="如何访问一个dataset"><a href="#如何访问一个dataset" class="headerlink" title="如何访问一个dataset"></a>如何访问一个dataset</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dataset = f[<span class="string">"dataset_name"</span>]                           <span class="comment"># acess like a python dict</span></span><br><span class="line">dataset = f.create_dateset(<span class="string">"dataset_name"</span>)  <span class="comment"># or create a new dataset</span></span><br></pre></td></tr></table></figure><h3 id="dataset的属性"><a href="#dataset的属性" class="headerlink" title="dataset的属性"></a>dataset的属性</h3><p>dataset.name        #输出dataset的名字<br>dataset.tdype        #输出dataset中elements的type<br>dataset.shape        #输出dataset的shape<br>dataset.value<br>dataset doesn’t hava attrs like keys,values,items,etc..</p><h3 id="给h5py-dataset复制numpy-array"><a href="#给h5py-dataset复制numpy-array" class="headerlink" title="给h5py dataset复制numpy array"></a>给h5py dataset复制numpy array</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">array = np.zero((<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">h[<span class="string">'array'</span>] = array        <span class="comment"># in h5py file, you need't to explicit declare the shape of array, just assign it an object of numpy array</span></span><br></pre></td></tr></table></figure><h2 id="group"><a href="#group" class="headerlink" title="group"></a>group</h2><h3 id="什么是group"><a href="#什么是group" class="headerlink" title="什么是group"></a>什么是group</h3><p>group和字典挺像的</p><h3 id="创建一个group"><a href="#创建一个group" class="headerlink" title="创建一个group"></a>创建一个group</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">group = f.create_group(<span class="string">"group_name"</span>)    <span class="comment">#在f下创建一个group</span></span><br><span class="line">group.create_group(<span class="string">"group_name"</span>)        <span class="comment">#在group下创建一个group</span></span><br><span class="line">group.create_dataset(<span class="string">"dataset_name"</span>)    <span class="comment">#在group下创建一个dataset</span></span><br></pre></td></tr></table></figure><h3 id="访问一个group-the-same-as-dataset"><a href="#访问一个group-the-same-as-dataset" class="headerlink" title="访问一个group(the same as dataset)"></a>访问一个group(the same as dataset)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">group = f[<span class="string">"group_name"</span>]                           <span class="comment"># acess like a python dict</span></span><br><span class="line">group = f.create_dateset(<span class="string">"group_name"</span>)  <span class="comment"># or create a new group</span></span><br></pre></td></tr></table></figure><h3 id="group的属性和方法"><a href="#group的属性和方法" class="headerlink" title="group的属性和方法"></a>group的属性和方法</h3><p>group.name        #输出group的名字<br>以下内容分为python2和python3版本</p><h4 id="python-2-版本"><a href="#python-2-版本" class="headerlink" title="python 2 版本"></a>python 2 版本</h4><p>group.values()    #输出group的value<br>group.keys()        #输出gorup的keys<br>group.items()    #输出group中所有的item，包含group和dataste</p><h4 id="python-3-版本"><a href="#python-3-版本" class="headerlink" title="python 3 版本"></a>python 3 版本</h4><p>list(group.keys())<br>list(group.values())<br>list(group.items())</p><h2 id="属性"><a href="#属性" class="headerlink" title="属性"></a>属性</h2><h3 id="设置dataset属性"><a href="#设置dataset属性" class="headerlink" title="设置dataset属性"></a>设置dataset属性</h3><p>dataset.attrs[“attr_name”]=”attr_value”    #设置attr<br>print(dataset.attrs[“attr_name”])                #访问attr</p><h3 id="设置group属性"><a href="#设置group属性" class="headerlink" title="设置group属性"></a>设置group属性</h3><p>group.attrs[“attr_name”]=”attr_value”    #设置attr<br>print(group.attrs[“attr_name”])                #访问attr</p><h2 id="numpy-and-h5py"><a href="#numpy-and-h5py" class="headerlink" title="numpy and h5py"></a>numpy and h5py</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">f = h5py.File(pathname,<span class="string">"r"</span>)</span><br><span class="line"></span><br><span class="line">data = f[<span class="string">'data'</span>]    <span class="comment"># type 是dataset</span></span><br><span class="line">data = f[<span class="string">'data'</span>][:] <span class="comment">#type是numpy ndarray</span></span><br><span class="line">f.close()</span><br></pre></td></tr></table></figure><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>1.<a href="http://docs.h5py.org/en/latest/index.html" target="_blank" rel="noopener">http://docs.h5py.org/en/latest/index.html</a><br>2.<a href="https://stackoverflow.com/questions/31037088/discovering-keys-using-h5py-in-python3" target="_blank" rel="noopener">https://stackoverflow.com/questions/31037088/discovering-keys-using-h5py-in-python3</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;python包安装&quot;&gt;&lt;a href=&quot;#python包安装&quot; class=&quot;headerlink&quot; title=&quot;python包安装&quot;&gt;&lt;/a&gt;python包安装&lt;/h2&gt;&lt;p&gt;~$:pip install h5py&lt;/p&gt;
&lt;h2 id=&quot;简介&quot;&gt;&lt;a hre
      
    
    </summary>
    
      <category term="python" scheme="http://mxxhcm.github.io/categories/python/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="h5py" scheme="http://mxxhcm.github.io/tags/h5py/"/>
    
  </entry>
  
  <entry>
    <title>python 常见问题（不定期更新）</title>
    <link href="http://mxxhcm.github.io/2019/03/13/python-%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/"/>
    <id>http://mxxhcm.github.io/2019/03/13/python-常见问题/</id>
    <published>2019-03-13T02:40:03.000Z</published>
    <updated>2019-10-11T05:30:51.354Z</updated>
    
    <content type="html"><![CDATA[<h2 id="问题1-‘dict-values’-object-does-not-support-indexing’"><a href="#问题1-‘dict-values’-object-does-not-support-indexing’" class="headerlink" title="问题1-‘dict_values’ object does not support indexing’"></a>问题1-‘dict_values’ object does not support indexing’</h2><p>参考文献[1,2,3]</p><h3 id="报错"><a href="#报错" class="headerlink" title="报错"></a>报错</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&apos;dict_values&apos; object does not support indexing&apos;</span><br></pre></td></tr></table></figure><h3 id="原因"><a href="#原因" class="headerlink" title="原因"></a>原因</h3><p>The objects returned by dict.keys(), dict.values() and dict.items() are view objects. They provide a dynamic view on the dictionary’s entries, which means that when the dictionary changes, the view reflects these changes.<br>python3 中调用字典对象的一些函数，返回值是view objects。如果要转换为list的话，需要使用list()强制转换。<br>而python2的返回值直接就是list。</p><h3 id="代码示例"><a href="#代码示例" class="headerlink" title="代码示例"></a>代码示例</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">m_dict = &#123;<span class="string">'a'</span>: <span class="number">10</span>, <span class="string">'b'</span>: <span class="number">20</span>&#125;</span><br><span class="line">values = m_dict.values()</span><br><span class="line">print(type(values))</span><br><span class="line">print(values)</span><br><span class="line">print(<span class="string">"\n"</span>)</span><br><span class="line"></span><br><span class="line">items = m_dict.items()</span><br><span class="line">print(type(items))</span><br><span class="line">print(items)</span><br><span class="line">print(<span class="string">"\n"</span>)</span><br><span class="line"></span><br><span class="line">keys = m_dict.keys()</span><br><span class="line">print(type(keys))</span><br><span class="line">print(keys)</span><br><span class="line">print(<span class="string">"\n"</span>)</span><br></pre></td></tr></table></figure><p>如果使用python3执行以上代码，输出结果如下所示：</p><blockquote><p>class ‘dict_values’<br>dict_values([10, 20])<br>class ‘dict_items’<br>dict_items([(‘a’, 10), (‘b’, 20)])<br>class ‘dict_keys’<br>dict_keys([‘a’, ‘b’])</p></blockquote><p>如果使用python2执行以上代码，输出结果如下所示：</p><blockquote><p>type ‘list’<br>[10, 20]<br>type ‘list’<br>[(‘a’, 10), (‘b’, 20)]<br>type ‘list’<br>[‘a’, ‘b’]</p></blockquote><h2 id="问题2-‘TimeLimit’-object-has-no-attribute-‘ale’"><a href="#问题2-‘TimeLimit’-object-has-no-attribute-‘ale’" class="headerlink" title="问题2-‘TimeLimit’ object has no attribute ‘ale’"></a>问题2-‘TimeLimit’ object has no attribute ‘ale’</h2><p>参考文献[4,5,6]</p><h3 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h3><p>运行github clone 下来的<a href="https://github.com/devsisters/DQN-tensorflow" target="_blank" rel="noopener">DQN-tensorflow</a>，报错:</p><blockquote><p>AttributeError: ‘TimeLimit’ object has no attribute ‘ale’.</p></blockquote><h3 id="原因-1"><a href="#原因-1" class="headerlink" title="原因"></a>原因</h3><p>是因为gym版本原因，在gym 0.7版本中，可以使用env.ale.lives()访问ale属性，但是0.8版本以及以上，就没有了该属性，可以在系列函数中添加如下修改：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, config)</span>:</span></span><br><span class="line">    self.step_info = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_step</span><span class="params">(self, action)</span>:</span></span><br><span class="line">    self._screen, self.reward, self.terminal, self.step_info = self.env.step(action)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lives</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> self.step_info <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> self.step_info[<span class="string">'ale.lives'</span>]</span><br></pre></td></tr></table></figure></p><h3 id="ale属性是什么"><a href="#ale属性是什么" class="headerlink" title="ale属性是什么"></a>ale属性是什么</h3><p>我看官方文档也没有看清楚，但是我觉得就是生命值是否没有了</p><blockquote><p>info (dict): diagnostic information useful for debugging. It can sometimes be useful for learning (for example, it might contain the raw probabilities behind the environment’s last state change). However, official evaluations of your agent are not allowed to use this for learning.</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> gym</span><br><span class="line">env = gym.make(<span class="string">'CartPole-v0'</span>)</span><br><span class="line"><span class="keyword">for</span> i_episode <span class="keyword">in</span> range(<span class="number">20</span>):</span><br><span class="line">    observation = env.reset()</span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">        env.render()</span><br><span class="line">        print(observation)</span><br><span class="line">        action = env.action_space.sample()</span><br><span class="line">        observation, reward, done, info = env.step(action)</span><br><span class="line">        <span class="keyword">if</span> done:</span><br><span class="line">            print(<span class="string">"Episode finished after &#123;&#125; timesteps"</span>.format(t+<span class="number">1</span>))</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">env.close()</span><br></pre></td></tr></table></figure><h2 id="问题3-cannot-import-name"><a href="#问题3-cannot-import-name" class="headerlink" title="问题3-cannot import name ***"></a>问题3-cannot import name ***</h2><p>参考文献[7]</p><h3 id="报错-1"><a href="#报错-1" class="headerlink" title="报错"></a>报错</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cannot import name tqdm</span><br></pre></td></tr></table></figure><h3 id="问题原因"><a href="#问题原因" class="headerlink" title="问题原因"></a>问题原因</h3><p>谷歌了半天，没有发现原因，然后百度了一下，发现了原因，看来还是自己太菜了。。<br>因为自己起的文件名就叫tqdm，然后就和库中的tqdm冲突了，这也太蠢了吧。。。</p><h2 id="问题4-linux下python执行shell脚本输出重定向"><a href="#问题4-linux下python执行shell脚本输出重定向" class="headerlink" title="问题4-linux下python执行shell脚本输出重定向"></a>问题4-linux下python执行shell脚本输出重定向</h2><p><a href="https://mxxhcm.github.io/2019/06/03/linux-python调用shell脚本并将输出重定向到文件/">详细介绍</a></p><h2 id="问题4-ImportError-No-module-named-conda-cli’"><a href="#问题4-ImportError-No-module-named-conda-cli’" class="headerlink" title="问题4-ImportError: No module named conda.cli’"></a>问题4-ImportError: No module named conda.cli’</h2><h3 id="问题描述-1"><a href="#问题描述-1" class="headerlink" title="问题描述"></a>问题描述</h3><p>anaconda的python版本是3.7，执行了conda install python=3.6之后，运行conda命令出错。报错如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from conda.cli import main </span><br><span class="line">ModuleNotFoundError: No module named &apos;conda&apos;</span><br></pre></td></tr></table></figure></p><h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><p>找到anaconda安装包，加一个-u参数，如下所示。重新安装anaconda自带的package，自己安装的包不会丢失。<br>~$:sh xxx.sh -u</p><h2 id="问题5-python-pip使用国内源"><a href="#问题5-python-pip使用国内源" class="headerlink" title="问题5-python-pip使用国内源"></a>问题5-python-pip使用国内源</h2><h3 id="暂时使用国内pip源"><a href="#暂时使用国内pip源" class="headerlink" title="暂时使用国内pip源"></a>暂时使用国内pip源</h3><p>使用清华源<br>~\$:pip install -i <a href="https://pypi.tuna.tsinghua.edu.cn/simple" target="_blank" rel="noopener">https://pypi.tuna.tsinghua.edu.cn/simple</a> package-name<br>使用阿里源<br>~\$:pip install -i <a href="https://mirrors.aliyun.com/pypi/simple" target="_blank" rel="noopener">https://mirrors.aliyun.com/pypi/simple</a> package-name</p><h3 id="将国内pip源设为默认"><a href="#将国内pip源设为默认" class="headerlink" title="将国内pip源设为默认"></a>将国内pip源设为默认</h3><p>~\$:pip install pip -U<br>~\$:pip config set global.index-url <a href="https://pypi.tuna.tsinghua.edu.cn/simple" target="_blank" rel="noopener">https://pypi.tuna.tsinghua.edu.cn/simple</a><br>~\$:pip config set global.timeout 60</p><blockquote><p>Writing to /home/username/.config/pip/pip.conf</p></blockquote><h4 id="查看pip配置文件"><a href="#查看pip配置文件" class="headerlink" title="查看pip配置文件"></a>查看pip配置文件</h4><p>~\$:find / -name pip.conf<br>我的是在/home/username/.config/pip/pip.conf</p><h2 id="问题6-ImportError-lib-x86-64-linux-gnu-libc-so-6-version-GLIBC-2-28-not-found"><a href="#问题6-ImportError-lib-x86-64-linux-gnu-libc-so-6-version-GLIBC-2-28-not-found" class="headerlink" title="问题6-ImportError: /lib/x86_64-linux-gnu/libc.so.6: version GLIBC_2.28 not found"></a>问题6-ImportError: /lib/x86_64-linux-gnu/libc.so.6: version GLIBC_2.28 not found</h2><h3 id="问题描述-2"><a href="#问题描述-2" class="headerlink" title="问题描述"></a>问题描述</h3><p>安装roboschool之后，出现ImportError。报错如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ImportError: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.28&apos; not found (required by /usr/local/lib/python3.6/dist-packages/roboschool/.libs/libQt5Core.so.5)</span><br></pre></td></tr></table></figure></p><h3 id="解决方案-1"><a href="#解决方案-1" class="headerlink" title="解决方案"></a>解决方案</h3><p>在roboschool上找到一个issue，说从1.0.49版本退回到1.0.48即可。我退回之后，又出现以下错误：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ImportError: libpcre16.so.3: cannot open shared object file: No such file or directory</span><br></pre></td></tr></table></figure></p><p>安装相应的库即可。完整的命令如下<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">~$:pip install roboschool==1.0.48</span><br><span class="line">~$:sudo apt install libpcre3-dev</span><br></pre></td></tr></table></figure></p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>1.<a href="https://www.cnblogs.com/timxgb/p/8905290.html" target="_blank" rel="noopener">https://www.cnblogs.com/timxgb/p/8905290.html</a><br>2.<a href="https://docs.python.org/3/library/stdtypes.html#dictionary-view-objects" target="_blank" rel="noopener">https://docs.python.org/3/library/stdtypes.html#dictionary-view-objects</a><br>3.<a href="https://stackoverflow.com/questions/43663206/typeerror-unsupported-operand-types-for-dict-values-and-int" target="_blank" rel="noopener">https://stackoverflow.com/questions/43663206/typeerror-unsupported-operand-types-for-dict-values-and-int</a><br>4.<a href="https://github.com/devsisters/DQN-tensorflow/issues/29" target="_blank" rel="noopener">https://github.com/devsisters/DQN-tensorflow/issues/29</a><br>5.<a href="https://gym.openai.com/docs" target="_blank" rel="noopener">https://gym.openai.com/docs</a><br>6.<a href="https://github.com/openai/baselines/issues/42" target="_blank" rel="noopener">https://github.com/openai/baselines/issues/42</a><br>7.<a href="https://blog.csdn.net/m0_37561765/article/details/78714603" target="_blank" rel="noopener">https://blog.csdn.net/m0_37561765/article/details/78714603</a><br>8.<a href="https://blog.csdn.net/u014432608/article/details/79066813" target="_blank" rel="noopener">https://blog.csdn.net/u014432608/article/details/79066813</a><br>9.<a href="https://mirrors.tuna.tsinghua.edu.cn/help/pypi/" target="_blank" rel="noopener">https://mirrors.tuna.tsinghua.edu.cn/help/pypi/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;问题1-‘dict-values’-object-does-not-support-indexing’&quot;&gt;&lt;a href=&quot;#问题1-‘dict-values’-object-does-not-support-indexing’&quot; class=&quot;headerlin
      
    
    </summary>
    
      <category term="python" scheme="http://mxxhcm.github.io/categories/python/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="gym" scheme="http://mxxhcm.github.io/tags/gym/"/>
    
      <category term="常见问题" scheme="http://mxxhcm.github.io/tags/%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/"/>
    
      <category term="pip源" scheme="http://mxxhcm.github.io/tags/pip%E6%BA%90/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow 常见问题（不定期更新）</title>
    <link href="http://mxxhcm.github.io/2019/03/07/tensorflow-problems/"/>
    <id>http://mxxhcm.github.io/2019/03/07/tensorflow-problems/</id>
    <published>2019-03-07T06:51:01.000Z</published>
    <updated>2019-07-18T12:27:16.349Z</updated>
    
    <content type="html"><![CDATA[<h2 id="问题1-the-value-of-a-feed-cannot-be-a-tf-tensor-object">问题1-The value of a feed cannot be a tf.Tensor object</h2><h3 id="报错">报错</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">TypeError: The value of a feed cannot be a tf.Tensor object</span><br></pre></td></tr></table></figure><h3 id="问题原因">问题原因</h3><p>sess.run(op, feed_dict={})中的feed value不能是tf.Tensor类型。</p><h3 id="解决方法">解决方法</h3><p>sess.run(train, feed_dict={x:images, y:labels}的输入不能是tensor，可以使用sess.run(tensor)得到numpy.array形式的数据再喂给feed_dict。</p><blockquote><p>Once you have launched a sess, you can use your_tensor.eval(session=sess) or sess.run(your_tensor) to get you feed tensor into the format of numpy.array and then feed it to your placeholder.</p></blockquote><h2 id="问题2-could-not-create-cudnn-handle-cudnn-status-internal-error">问题2-Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR</h2><h3 id="配置">配置</h3><p>环境配置如下：</p><ul><li>Ubuntu 18.04</li><li>CUDA 10.0</li><li>CuDNN 7.4.2</li><li>Python3.7.3</li><li>Tensorflow 1.13.1</li><li>Nvidia Drivers 430.09</li><li>RTX2070</li></ul><h3 id="报错-v2">报错</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">2019-05-12 14:45:59.355405: E tensorflow/stream_executor/cuda/cuda_dnn.cc:334] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR</span><br><span class="line">2019-05-12 14:45:59.357698: E tensorflow/stream_executor/cuda/cuda_dnn.cc:334] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR</span><br><span class="line">...</span><br></pre></td></tr></table></figure><h3 id="问题原因-v2">问题原因</h3><p>GPU不够用了。</p><h3 id="解决方法-v2">解决方法</h3><p>在代码中添加下面几句：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">config = tf.ConfigProto()</span><br><span class="line">config.gpu_options.allow_growth = <span class="literal">True</span></span><br><span class="line">session = InteractiveSession(config=config)</span><br></pre></td></tr></table></figure><h2 id="问题3-libcublas-so-10-0-cannot-open-shared-object-file-no-such-file-or-directory">问题3-libcublas.so.10.0: cannot open shared object file: No such file or directory</h2><p>在命令行或者pycharm中import tensorflow报错</p><h3 id="报错-v3">报错</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ImportError: libcublas.so.10.0: cannot open shared object file: No such file or directory</span><br><span class="line">Failed to load the native TensorFlow runtime.</span><br></pre></td></tr></table></figure><h3 id="问题原因-v3">问题原因</h3><p>没有配置CUDA环境变量</p><h3 id="解决方法-v3">解决方法</h3><h4 id="命令行中">命令行中</h4><p>在.bashrc文件中加入下列语句：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export PATH=/usr/local/cuda/bin$&#123;PATH:+:$&#123;PATH&#125;&#125;</span><br><span class="line">export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$&#123;LD_LIBRARY_PATH:+:$&#123;LD_LIBRARY_PATH&#125;&#125;</span><br><span class="line">export CUDA_HOME=/usr/local/cuda</span><br></pre></td></tr></table></figure><h4 id="pycharm中">pycharm中</h4><h5 id="方法1-这种方法我没有实验成功-不知道为什么">方法1（这种方法我没有实验成功，不知道为什么）</h5><p>在左上角选中<br>File&gt;&gt;Settings&gt;&gt;Build.Execution,Deployment&gt;&gt;Console&gt;&gt;Python Console<br>在Environment下的Environment variables中添加<br>LD_LIBRARY_PATH=/usr/local/cuda/lib64:${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}即可。</p><h5 id="方法2">方法2</h5><p>修改完.bashrc文件后从终端中运行pycharm。</p><h2 id="问题4-dlerror-libcupti-so-10-0-cannot-open-shared-object-file-no-such-file-or-directory">问题4-dlerror: libcupti.so.10.0: cannot open shared object file: No such file or directory</h2><p>执行mnist_with_summary代码时报错</p><h3 id="报错-v4">报错</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">I tensorflow/stream_executor/dso_loader.cc:142] Couldn&apos;t open CUDA library libcupti.so.10.0. LD_LIBRARY_PATH: /usr/local/cuda/lib64:</span><br><span class="line">2019-05-13 23:04:10.620149: F tensorflow/stream_executor/lib/statusor.cc:34] Attempting to fetch value instead of handling error Failed precondition: could not dlopen DSO: libcupti.so.10.0; dlerror: libcupti.so.10.0: cannot open shared object file: No such file or directory</span><br><span class="line">Aborted (core dumped)</span><br></pre></td></tr></table></figure><h3 id="问题问题问题问题问题问题问题问题问题原因">问题问题问题问题问题问题问题问题问题原因</h3><p>libcupti.so.10.0包没找到</p><h3 id="解决方法-v4">解决方法</h3><p>执行以下命令，找到相关的依赖包：<br>~$:find /usr/local/cuda/ -name libcupti.so.10.0<br>输出如下：</p><blockquote><p>/usr/local/cuda/extras/CUPTI/lib64/libcupti.so.10.0</p></blockquote><p>然后修改~/.bashrc文件中相应的环境变量:<br>export LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64/😒{LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}<br>重新运行即可。</p><h2 id="问题5-unhashable-type-list">问题5-unhashable type: ‘list’</h2><p>sess.run(op, feed_dict={})中feed的数据中包含有list的时候会报错。</p><h3 id="报错-v5">报错</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">TypeError: unhashable type: &apos;list&apos;</span><br></pre></td></tr></table></figure><h3 id="问题原因-v4">问题原因</h3><p>feed_dict中不能的value不能是list。</p><h3 id="解决方法-v5">解决方法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">feed_dict = &#123;</span><br><span class="line">               placeholder : value </span><br><span class="line">                  <span class="keyword">for</span> placeholder, value <span class="keyword">in</span> zip(placeholder_list, inputs_list))</span><br><span class="line">            &#125;</span><br></pre></td></tr></table></figure><h3 id="代码示例">代码示例</h3><p><a href="https://github.com/mxxhcm/code/blob/master/tf/ops/tf_placeholder_list.py" target="_blank" rel="noopener">代码地址</a></p><h2 id="问题6-attempting-to-use-uninitialized-value">问题6-Attempting to use uninitialized value</h2><p>tf.Session()和tf.InteractiveSession()混用问题。</p><h3 id="报错-v6">报错</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensorflow.python.framework.errors_impl.FailedPreconditionError: Attempting to use uninitialized value prediction/l1/w</span><br><span class="line"> [[&#123;&#123;node prediction/l1/w/read&#125;&#125;]]</span><br><span class="line"> [[&#123;&#123;node prediction/LogSoftmax&#125;&#125;]]</span><br></pre></td></tr></table></figure><h3 id="问题原因-v5">问题原因</h3><p>声明了如下session:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sess = tf.Session()</span><br><span class="line">sess.run(tf.global_variables_initializer())</span><br></pre></td></tr></table></figure><p>在接下来的代码中，因为我声明的是tf.Session()，使用了op.eval()函数，这种用法是tf.InteractiveSession的用法，所以就相当于没有初始化。<br>result = op.eval(feed_dict={})<br>然后就报了未初始化的错误。<br>把代码改成：<br>result = sess.run([op], feeed_dct={})<br>即可，即上下文使用的session应该一致。</p><h3 id="解决方案">解决方案</h3><p>使用统一的session类型</p><h2 id="问题7-setting-an-array-element-with-a-sequence">问题7-setting an array element with a sequence</h2><p>feed_dict键值对中中值必须是numpy.ndarray，不能是其他类型。</p><h3 id="报错-v7">报错</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">value error setting an array element with a sequence,</span><br></pre></td></tr></table></figure><h3 id="问题原因-v6">问题原因</h3><p>feed_dict中key-value的value必须是numpy.ndarray，不能是其他类型，尤其不能是tf.Variable。</p><h3 id="解决方法-v6">解决方法</h3><p>检查sess.run(op, feed_dict={})中的feed_dict，确保他们的类型，不能是tf.Variable()类型的对象，需要是numpy.ndarray。</p><h2 id="问题8-访问tf-variable-的值">问题8-访问tf.Variable()的值</h2><p>如何获得tf.Variable()对象的值</p><h3 id="解决方法-v7">解决方法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">x = tf.Varialbe([<span class="number">1.0</span>, <span class="number">2.0</span>])</span><br><span class="line">sess = tf.Session()</span><br><span class="line">sess.run(tf.global_variables_initializer())</span><br><span class="line">value = sess.run(x)</span><br></pre></td></tr></table></figure><p>或者</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">x = tf.Varialbe([1.0, 2.0])</span><br><span class="line">sess = tf.InteractiveSession()</span><br><span class="line">sess.run(tf.global_variables_initializer())</span><br><span class="line">x.eval()</span><br></pre></td></tr></table></figure><h2 id="问题9-can-not-convert-a-ndarray-into-a-tensor-or-operation">问题9-Can not convert a ndarray into a Tensor or Operation</h2><h3 id="报错-v8">报错</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Can not convert a ndarray into a Tensor or Operation.</span><br></pre></td></tr></table></figure><h3 id="问题原因-v7">问题原因</h3><p>原因是sess.run()前后参数名重了，比如outputs = sess.run(outputs)，outputs本来是自己定义的一个op，但是sess.run(outputs)之后outputs就成了一个变量，就把定义的outputs op覆盖了。</p><h3 id="解决方法-v8">解决方法</h3><p>换个变量名字就行</p><h2 id="问题10-本地使用gpu-server的tensorboard">问题10-本地使用gpu server的tensorboard</h2><h3 id="问题描述">问题描述</h3><p>在gpu server跑的实验结果，然后summary的记录也在server上，但是又没办法可视化，只好在本地可视化。</p><h3 id="解决方法-v9">解决方法</h3><p>使用ssh进行映射好了。</p><h4 id="本机设置">本机设置</h4><p>~$:ssh -L 12345:10.1.114.50:6006 <a href="mailto:mxxmhh@127.0.0.1" target="_blank" rel="noopener">mxxmhh@127.0.0.1</a><br>将本机的12345端口映射到10.1.114.50的6006端口，中间服务器使用的是本机。<br>或者可以使用10.1.114.50作为中间服务器。<br>~$:ssh -L 12345:10.1.114.50:6006 <a href="mailto:liuchi@10.1.114.50" target="_blank" rel="noopener">liuchi@10.1.114.50</a><br>或者可以使用如下方法：<br>~$:ssh -L 12345:127.0.0.1:6006 <a href="mailto:liuchi@10.1.114.50" target="_blank" rel="noopener">liuchi@10.1.114.50</a><br>从这个方法中，可以看出127.0.0.1这个ip是中间服务器可以访问的ip。<br>以上三种方法中，-L后的端口号12345可以随意设置，只要不冲突即可。</p><h4 id="服务端设置">服务端设置</h4><p>然后在服务端运行以下命令：<br>~$:tensorboard --logdir logdir -port 6006<br>这个端口号也是可以任意设置的，不冲突即可。</p><h4 id="运行">运行</h4><p>然后在本机访问<br><a href="https://127.0.0.1:12345" target="_blank" rel="noopener">https://127.0.0.1:12345</a>即可。</p><h2 id="问题11-每一步summary一个list的每一个元素">问题11-每一步summary一个list的每一个元素</h2><h3 id="问题原因-v8">问题原因</h3><p>有一个tf list的placeholder，但是每一步只能生成其中的一个元素，所以怎么样summary中其中的某一个？</p><h3 id="解决方法-v10">解决方法</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">number = 3</span><br><span class="line">x_ph_list = []</span><br><span class="line">for i in range(number):</span><br><span class="line">    x_ph_list.append(tf.placeholder(tf.float32, shape=None))</span><br><span class="line"></span><br><span class="line">x_summary_list = []</span><br><span class="line">for i in range(number):</span><br><span class="line">    x_summary_list.append(tf.summary.scalar("x%s" % i, x_ph_list[i]))</span><br><span class="line"></span><br><span class="line">writer = tf.summary.FileWriter("./tf_summary/scalar_list_summary/sep")</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    scope = 10</span><br><span class="line">    inputs = np.arange(scope*number)</span><br><span class="line">    inputs = inputs.reshape(scope, number)</span><br><span class="line">    # inputs = np.random.randn(scope, number)</span><br><span class="line">    for i in range(scope):</span><br><span class="line">        for j in range(number):</span><br><span class="line">            out, xj_s = sess.run([x_ph_list[j], x_summary_list[j]], feed_dict=&#123;x_ph_list[j]: inputs[i][j]&#125;)</span><br><span class="line">            writer.add_summary(xj_s, global_step=i)</span><br></pre></td></tr></table></figure><h2 id="问题12-for-value-in-summary-value-attributeerror-list-object-has-no-attribute-value">问题12- for value in summary.value: AttributeError: ‘list’ object has no attribute ‘value’</h2><h3 id="问题描述-v2">问题描述</h3><p>writer.add_summary时报错</p><h3 id="报错-v9">报错</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">File &quot;/home/mxxmhh/anaconda3/lib/python3.7/site-packages/tensorflow/python/summary/writer/writer.py&quot;, line 127, in add_summary</span><br><span class="line">    for value in summary.value:</span><br><span class="line">AttributeError: &apos;list&apos; object has no attribute &apos;value&apos;</span><br></pre></td></tr></table></figure><h3 id="问题原因-v9">问题原因</h3><p>执行以下代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">s_ = sess.run([loss_summary], feed_dict=&#123;p_losses_ph: inputs1, q_losses_ph: inputs2&#125;)</span><br><span class="line">writer.add_summary(s_, global_step=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>因为[loss_summary]加了方括号，就把它当成了一个list。。返回值也是list，就报错了</p><h3 id="解决方法-v11">解决方法</h3><ul><li>方法1，在等号左边加一个逗号，取出list中的值</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">s_, = sess.run([loss_summary], feed_dict=&#123;p_losses_ph: inputs1, q_losses_ph: inputs2&#125;)</span><br></pre></td></tr></table></figure><ul><li>方法2，去掉loss_summary外面的中括号。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">s_ = sess.run(loss_summary, feed_dict=&#123;p_losses_ph: inputs1, q_losses_ph: inputs2&#125;)</span><br></pre></td></tr></table></figure><h2 id="问题13-tf-get-default-session-always-returns-none-type">问题13- tf.get_default_session() always returns None type:</h2><h3 id="问题描述-v3">问题描述</h3><p>调用tf.get_default_session()时，返回的是None</p><h3 id="报错-v10">报错</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">    tf.get_default_session().run(y)</span><br><span class="line">AttributeError: &apos;NoneType&apos; object has no attribute &apos;run&apos;</span><br></pre></td></tr></table></figure><h3 id="问题原因-v10">问题原因</h3><p>只有在设定default session之后，才能使用tf.get_default_session()获得当前的默认session，在我们写代码的时候，一般会按照下面的方式写：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    some operations</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure><p>这种情况下已经把tf.Session()生成的session当做了默认session，但是如果仅仅使用以下代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">sess =  tf.Session()</span><br><span class="line">sess.run(some operations)</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>是没有把tf.Session()当成默认session的，即只有在with block内，才会将这个session当做默认session。</p><h3 id="解决方案-v2">解决方案</h3><h2 id="参考文献">参考文献</h2><p>1.<a href="https://github.com/tensorflow/tensorflow/issues/4842" target="_blank" rel="noopener">https://github.com/tensorflow/tensorflow/issues/4842</a><br>2.<a href="https://github.com/tensorflow/tensorflow/issues/24496" target="_blank" rel="noopener">https://github.com/tensorflow/tensorflow/issues/24496</a><br>3.<a href="https://github.com/tensorflow/tensorflow/issues/9530" target="_blank" rel="noopener">https://github.com/tensorflow/tensorflow/issues/9530</a><br>4.<a href="https://stackoverflow.com/questions/51128427/how-to-feed-list-of-values-to-a-placeholder-list-in-tensorflow" target="_blank" rel="noopener">https://stackoverflow.com/questions/51128427/how-to-feed-list-of-values-to-a-placeholder-list-in-tensorflow</a><br>5.<a href="https://github.com/tensorflow/tensorflow/issues/11897" target="_blank" rel="noopener">https://github.com/tensorflow/tensorflow/issues/11897</a><br>6.<a href="https://stackoverflow.com/questions/34156639/tensorflow-python-valueerror-setting-an-array-element-with-a-sequence-in-t" target="_blank" rel="noopener">https://stackoverflow.com/questions/34156639/tensorflow-python-valueerror-setting-an-array-element-with-a-sequence-in-t</a><br>7.<a href="https://stackoverflow.com/questions/33679382/how-do-i-get-the-current-value-of-a-variable" target="_blank" rel="noopener">https://stackoverflow.com/questions/33679382/how-do-i-get-the-current-value-of-a-variable</a><br>8.<a href="https://blog.csdn.net/michael__corleone/article/details/79007425" target="_blank" rel="noopener">https://blog.csdn.net/michael__corleone/article/details/79007425</a><br>9.<a href="https://stackoverflow.com/questions/47721792/tensorflow-tf-get-default-session-after-sess-tf-session-is-none" target="_blank" rel="noopener">https://stackoverflow.com/questions/47721792/tensorflow-tf-get-default-session-after-sess-tf-session-is-none</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;问题1-the-value-of-a-feed-cannot-be-a-tf-tensor-object&quot;&gt;问题1-The value of a feed cannot be a tf.Tensor object&lt;/h2&gt;
&lt;h3 id=&quot;报错&quot;&gt;报错&lt;/h3&gt;

      
    
    </summary>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/categories/tensorflow/"/>
    
    
      <category term="机器学习" scheme="http://mxxhcm.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/tags/tensorflow/"/>
    
      <category term="深度学习" scheme="http://mxxhcm.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="常见问题" scheme="http://mxxhcm.github.io/tags/%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>reinforcement learning an introduction 第3章笔记</title>
    <link href="http://mxxhcm.github.io/2018/12/21/reinforcement-learning-an-introduction-%E7%AC%AC3%E7%AB%A0%E7%AC%94%E8%AE%B0/"/>
    <id>http://mxxhcm.github.io/2018/12/21/reinforcement-learning-an-introduction-第3章笔记/</id>
    <published>2018-12-21T07:13:38.000Z</published>
    <updated>2019-10-04T04:03:17.473Z</updated>
    
    <content type="html"><![CDATA[<h2 id="马尔科夫过程-markov-process-马尔科夫链-markov-chain">马尔科夫过程(markov process)、马尔科夫链(markov chain)</h2><p>马尔科夫过程或者马尔科夫链(markov chain)是一个tuple $\lt S,P\gt$,其中S是一个有限(或者无限)的状态集合,P是状态转移矩阵(transition probability matrix)或马尔科夫矩阵(markov matrix),$P_{ss’}= P[S_{t+1} = s’|S_t = s]$.</p><h2 id="马尔科夫奖励过程-markov-reward-process">马尔科夫奖励过程(markov reward process)</h2><p>马尔科夫奖励过程是一个tuple $\lt S,P,R,\gamma\gt$,和马尔科夫过程相比，它多了一个奖励R，R和某个具体的状态相关，MRP中的reward只和state有关,和action无关。<br>S是一个(有限)状态的集合。<br>P是一个状态转移概率矩阵。<br>R是一个奖励函数$R = \mathbb{E}[R_{t+1}|S_t = s]$, <strong>这里为什么是t+1时刻的reward?这仅仅是一个约定，为了描述RL问题中涉及到的observation，action，reward比较方便。这里可以理解为离开这个状态才能获得奖励而不是进入这个状态即获得奖励。如果改成$R_t$也是可以的，这时可以理解为进入这个状态获得的奖励。</strong><br>$\gamma$称为折扣因子(discount factor), $\gamma \epsilon [0,1]$.<strong>为什么引入$\gamma$，David Silver的公开课中提到了四个原因:(1)数学上便于计算回报(return)；(2)避免陷入无限循环；(3)长远利益具有一定的不确定性；(4)符合人类对眼前利益的追求。</strong></p><h3 id="奖励-reward">奖励(reward)</h3><p>每个状态s在一个时刻t立即可得到一个reward,reward的值需要由环境给出,这个值可正可负。目前的强化学习算法中reward都是人为设置的。</p><h3 id="回报-return">回报(return)</h3><p>回报是累积的未来的reward,其计算公式如下:<br>$$G_t = R_{t+1} + R_{t+2} + … = \sum_{k=0}^{\infty} {\gamma^k R_{t+k+1}} \tag{1}$$<br>它是一个马尔科夫链上从t时刻开始往后所有奖励的有衰减(带折扣因子)的总和。</p><h3 id="值函数-value-function">值函数(value function)</h3><p>值函数是回报(return)的期望(expected return), 一个MRP过程中某一状态的value function为从该状态开始的markov charin return的期望，即$v(s) = \mathbb{E}[G_t|S_t=s]$.<br>MRP的value function和MDP的value function是不同的, MRP的value function是对于state而言的，而MDP的value function是针对tuple $\lt$state, action$\gt$的。<br>这里为什么要取期望,因为policy是stotastic的情况时，在每个state时，采取每个action都是可能的，都有一定的概率，next state也是不确定的了，所以value funciton是一个随机变量，因此就引入期望来刻画随机变量的性质。<br>为什么在当前state就知道下一时刻的state了?对于有界的RL问题来说，return是在一个回合结束时候计算的；对于无界的RL问题来说，由于有衰减系数，只要reward有界，return就可以计算出来。</p><h3 id="马尔科夫奖励过程的贝尔曼方程-bellman-equation-for-mrp">马尔科夫奖励过程的贝尔曼方程(bellman equation for MRP)</h3><p>\begin{align*}<br>v(s) &amp;= \mathbb{E}[G_t|S_t = s]\\<br>&amp;= \mathbb{E}[R_{t+1} + \gamma R_{t+2} + … | S_t = s]\\<br>&amp;= \mathbb{E}[R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3} + …|S_t = s]\\<br>&amp;= \mathbb{E}[R_{t+1} + \gamma G_{t+1} |S_t = s]\\<br>&amp;= \mathbb{E}[R_{t+1} + \gamma v(S_{t+1})|S_t = s]\\<br>v(s) &amp;= \mathbb{E}[R_{t+1} + \gamma v(S_{t+1})|S_t = s]<br>\end{align*}<br>v(s)由两部分组成，一部分是immediate reward的期望(expectation)，$\mathbb{E}[R_{t+1}]$, 只与当前时刻state有关；另一部分是下一时刻state的value function的expectation。如果用s’表示s状态下一时刻的state，那么bellman equation可以写成：<br>$$v(s) = R_s + \gamma \sum_{s’ \epsilon S} P_{ss’}v(s’)$$<br>我们最终的目的是通过迭代使得t轮迭代时的v(s)和第t+1轮迭代时的v(s)相等。将其写成矩阵形式为：<br>$$v_t = R + \gamma P v_{t+1}$$<br>$$(v_1,v_2,…,v_n)^T = (R_1,R_2,…,R_n)^T + \gamma \begin{bmatrix}P_{11}&amp;P_{12}&amp;…&amp;P_{1n}\\P_{21}&amp;P_{22}&amp;…&amp;P_{2n}\\&amp;&amp;…&amp;\\P_{n1}&amp;P_{n2}&amp;…&amp;P_{nn}\end{bmatrix} (v_1,v_2,…,v_n)^T $$<br>MRP的Bellman方程组是线性的，可以直接求解:<br>\begin{align*}<br>v &amp;= R + \gamma Pv\\<br>(1-\gamma P) &amp;= R\\<br>v &amp;= (1 - \gamma P)^{-1} R<br>\end{align*}<br>可以直接解方程，但是复杂度为$O(n^3)$，对于大的MRP方程组不适用，可以通过迭代法求解，常用的迭代法有动态规划,蒙特卡洛算法和时序差分算法等求解(动态规划是迭代法吗？）</p><h2 id="马尔科夫决策过程-markov-decision-process">马尔科夫决策过程(markov decision process)</h2><p>马尔科夫决策过程，比markov reward process多了一个A,它也是一个tuple $\lt S,A,P,R,\gamma\gt$, 在MRP中奖励R仅仅和状态S相关，在MDP中奖励R和概率P对应的是某个状态S和某个动作A的组合。<br>\begin{align*}<br>P_{ss’}^a &amp;= P[S_{t+1} = s’ | S_t = s, A_t = a]\\<br>R_s^a &amp;= \mathbb{E}[R_{t+1} | S_t = s, A_t = a]<br>\end{align*}<br>这里的reward不仅仅与state相关，而是与tuple $\lt state，action\gt$相关。</p><h3 id="回报">回报</h3><p>MDP中的$G_t$和式子$(1)$的$G_t$是一样的，将$G_t$写成和后继时刻相关的形式如下：<br>\begin{align*}<br>G_t &amp;= R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \gamma^3 R_{t+4} + …\\<br>&amp;= R_{t+1} + \gamma (R_{t+2} + \gamma^1 R_{t+3} + \gamma^2 R_{t+4} + …)\\<br>&amp;= R_{t+1} + \gamma G_{t+1} \tag{2}<br>\end{align*}<br>这里引入$\gamma$之后，即使是在continuing情况下，只要$G_t$是非零常数，$G_t$也可以通过等比数列求和公式进行计算，即:<br>$$G_t = \sum_{k=1}^{\infty} \gamma^k = \frac{1}{1-\gamma} \tag{3}$$</p><h3 id="策略-policy">策略(policy)</h3><p>策略$\pi$的定义:给定状态时采取各个动作的概率分布。<br>$$\pi(a|s) = P[A_t = a | S_t = a] \tag{4}$$</p><h3 id="值函数-value-function-v2">值函数(value function)</h3><p>这里给出的是值函数的定义，就是这么定义的。<br>MDP的值函数有两种，状态值函数(state value function)和动作值函数(action value function), 这两种值函数的含义其实是一样的，也可以相互转换。具体来说, 值函数定义为给定一个policy $\pi$，得到的回报的期望(expected return)。<br>一个MDP的状态s对应的值函数(state value function) $v_{\pi}(s)$是从状态s开始采取策略$\pi$得到的回报的期望。<br>\begin{align*}<br>v_{\pi}(s) &amp;= \mathbb{E}_{\pi}[G_t|S_t = s]\\<br>&amp;=\mathbb{E}_{\pi}[\sum_{k=0}^{\infty} \gamma^{k} R_{t+k+1}|S_t=s] \tag{5}<br>\end{align*}<br>这里的$G_t$是式子(2)中的回报。<br>一个MDP过程中动作值函数(action value function) $q_{\pi}(s,a)$是从状态s开始,采取action a，采取策略$\pi$得到的回报的期望。<br>&lt;action value function $q_{\pi}(s,a)$ is the expected return starting from states, taking action a, and then following policy \pi.&gt;<br>\begin{align*}<br>q_{\pi}(s,a) &amp;= \mathbb{E}_{\pi}\left[G_t | S_t = s, A_t = a\right]\\<br>&amp;= \mathbb{E}_{\pi}\left[\sum_{k=0}^{\infty} \gamma^{k} R_{t+k+1}|S_t=s, A_t=a\right] \tag{6}<br>\end{align*}</p><h4 id="状态值函数-state-value-function">状态值函数(state value function)</h4><p>\begin{align*}<br>v_{\pi}(s) &amp;= \sum_{a \epsilon A} \pi(a|s) q_{\pi} (s,a) \tag{7}\\<br>v_{\pi}(s) &amp;= \sum_a \pi(a|s)\sum_{s’,r}p(s’,r|s,a) \left[r + \gamma v_{\pi}(s’) \right] \tag{8}\\<br>\end{align*}<br>式子$(7)$是$v(s)$和$q(s,a)$的关系，式子$(8)$是$v(s)$和它的后继状态$v(s’)$的关系。<br>式子$(8)$的推导如下：<br>\begin{align*}<br>v_{\pi}(s) &amp;= \mathbb{E}_{\pi}[G_t|S_t = s]\\<br>&amp;= \mathbb{E}_{\pi}\left[R_{t+1}+\gamma G_{t+1}|S_t = s\right]\\<br>&amp;= \sum_a \pi(a|s)\sum_{s’}\sum_rp(s’,r|s,a) \left[r + \gamma \mathbb{E}_{\pi}\left[G_{t+1}|S_{t+1}=s’\right]\right]\\<br>&amp;= \sum_a \pi(a|s)\sum_{s’,r}p(s’,r|s,a) \left[r + \gamma v_{\pi}(s’) \right]\\<br>\end{align*}</p><h4 id="动作值函数-action-value-function">动作值函数(action value function)</h4><p>\begin{align*}<br>q_{\pi}(s,a) &amp;= \sum_{s’}\sum_r p(s’,r|s,a)(r + \gamma  v_{\pi}(s’)) \tag{9}\\<br>q_{\pi}(s,a) &amp;= \sum_{s’}\sum_r p(s’,r|s,a)(r + \gamma  \sum_{a’}\pi(a’|s’)q(s’,a’)) \tag{10}\\<br>\end{align*}<br>式子$(9)$是$q(s,a)$和$v(s)$的关系，式子$(10)$是$q(s,a)$和它的后继状态$q(s’,a’)$的关系。<br>以上都是针对MDP来说的，在MDP中，给定policy $\pi$下，状态s下选择a的action value function，$q_{\pi}(s,a)$类似MRP里面的v(s)，而MDP中的v(s)是要考虑在state s下采率各个action后的情况。</p><h3 id="贝尔曼期望方程-bellmam-expectation-equation">贝尔曼期望方程(Bellmam expectation equation)</h3><p>\begin{align*}<br>v_{\pi}(s) &amp;= \mathbb{E}_{\pi}[R_{t+1} + \gamma v_{\pi}(S_{t+1})|S_t = s] \tag{11}\\<br>v_{\pi}(s) &amp;= \mathbb{E}_{\pi}\left[q_{\pi}(S_t,A_t)|S_t=s,A_t=a\right]\tag{12}\\<br>q_{\pi}(s,a)&amp;= \mathbb{E}_{\pi}\left[R_{t+1} + \gamma v_{\pi}(S_{t+1}) |S_t=s,A_t=a\right]\tag{13}\\<br>q_{\pi}(s,a) &amp;= \mathbb{E}_{\pi}[R_{t+1} + \gamma q_{\pi}(S_{t+1},A_{t+1}) | S_t = s, A_t = a] \tag{14}<br>\end{align*}</p><h4 id="矩阵形式">矩阵形式</h4><p>\begin{align*}<br>v_{\pi} &amp;= R^{\pi} + \gamma P^{\pi} v_{\pi}\\<br>v_{\pi} &amp;= (I-\gamma P^{\pi} )^{-1} R^{\pi}<br>\end{align*}</p><h2 id="最优策程的求解-how-to-find-optimal-policy">最优策程的求解(how to find optimal policy)</h2><h3 id="最优价值函数-optimal-value-function">最优价值函数(optimal value function)</h3><p>$v_{*} = \max_{\pi}v_{\pi}(s)$,从所有策略产生的state value function中，选取使得state s的价值最大的函数<br>$q_{*}(s,a) = \max_{\pi} q_{\pi}(s,a)$,从所有策略产生的action value function中，选取使$\lt s,a\gt$价值最大的函数<br>当我们得到了optimal value function，也就知道了每个state的最优价值，便认为这个MDP被解决了</p><h3 id="最优策略-optimal-policy">最优策略(optimal policy)</h3><p>对于每一个state s，在policy $\pi$下的value 大于在policy $\pi’$的value， 就称策略$\pi$优于策略$\pi’$， $\pi \ge \pi’$ if $v_{\pi}(s) \ge v_{\pi’}(s)$, 对于任意s都成立<br>对于任何MDP，都满足以下条件：</p><ol><li>都存在一个optimal policy，它比其他策略好或者至少相等；</li><li>所有的optimal policy的optimal value function是相同的；</li><li>所有的optimal policy 都有相同的 action value function.</li></ol><h3 id="寻找最优策略">寻找最优策略</h3><p>寻找optimal policy可以通过寻找optimal action value function来实现：<br>$${\pi}_{*}(a|s) =<br>\begin{cases}1, &amp;if\quad a = \arg\max\ q_{*}(s,a)\\0, &amp;otherwise\end{cases}$$</p><h3 id="贝尔曼最优方程-bellman-optimal-equation">贝尔曼最优方程(bellman optimal equation)</h3><p>*号表示最优的策略。</p><h4 id="最优状态值函数-state-value-function">最优状态值函数(state value function)</h4><p>\begin{align*}<br>v_{*}(s) &amp;= \max_a q_{*}(s,a)\\<br>&amp;= \max_a\mathbb{E}_{\pi_{*}}\left[G_t|S_t=s,A_t=a\right]\\<br>&amp;= \max_a\mathbb{E}_{\pi_{*}}\left[R_{t+1}+\gamma G_t|S_t=s,A_t=a\right]\\<br>&amp;= \max_a\mathbb{E}\left[R_{t+1} +\gamma v_{*}(S_{t+1})|S_t=s,A_t=a\right]\\<br>&amp;= \max_a \left[\sum_{s’,r} p(s’,r|s,a)(r+\gamma v_{*}(s’) )\right] \tag{15}\\<br>\end{align*}</p><h4 id="最优动作值函数-action-value-function">最优动作值函数(action value function)</h4><p>\begin{align*}<br>q_{*}(s,a) &amp;= \sum_{s’,r} p(s’,r|s,a) (r + \gamma v_{*}(s’))\\<br>&amp;= \sum_{s’,r} p(s’,r|s,a) (r + \gamma \max_{a’} q_{*}(s’,a’))\\<br>&amp;=\mathbb{E}\left[R_{t+1}+\gamma \max_{a’}q_{*}(S_{t+1},a’)|S_t=s,A_t=a \right]\tag{16}\\<br>\end{align*}</p><h3 id="贝尔曼最优方程的求解-solution-to-bellman-optimal-equation">贝尔曼最优方程的求解(solution to Bellman optimal equation)</h3><p>Bellman equation和Bellman optimal equation相比，一个是对于给定的策略，求其对应的value function,是对一个策略的估计，而bellman optimal equation是要寻找最优策略，通过对action value function进行贪心。<br>Bellman最优方程是非线性的，没有固定的解决方案，只能通过迭代法来解决，如Policy iteration，value iteration，Q-learning，Sarsa等。</p><h2 id="参考文献">参考文献</h2><p>1.<a href="http://incompleteideas.net/book/the-book-2nd.html" target="_blank" rel="noopener">http://incompleteideas.net/book/the-book-2nd.html</a><br>2.<a href="https://www.bilibili.com/video/av32149008/?p=2" target="_blank" rel="noopener">https://www.bilibili.com/video/av32149008/?p=2</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;马尔科夫过程-markov-process-马尔科夫链-markov-chain&quot;&gt;马尔科夫过程(markov process)、马尔科夫链(markov chain)&lt;/h2&gt;
&lt;p&gt;马尔科夫过程或者马尔科夫链(markov chain)是一个tuple $\l
      
    
    </summary>
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="MDP" scheme="http://mxxhcm.github.io/tags/MDP/"/>
    
      <category term="MRP" scheme="http://mxxhcm.github.io/tags/MRP/"/>
    
      <category term="Bellman Equation" scheme="http://mxxhcm.github.io/tags/Bellman-Equation/"/>
    
  </entry>
  
  <entry>
    <title>classfication</title>
    <link href="http://mxxhcm.github.io/2018/10/21/classfication/"/>
    <id>http://mxxhcm.github.io/2018/10/21/classfication/</id>
    <published>2018-10-21T10:47:44.000Z</published>
    <updated>2019-10-21T14:53:18.789Z</updated>
    
    <content type="html"><![CDATA[<h2 id="classfication">Classfication</h2><h2 id="lda">LDA</h2><h2 id="logistic-regression">Logistic Regression</h2><h3 id="logistic-function">Logistic function</h3><p>$$ S(x) = \frac{1}{1+e^{x} }$$<br>如下图所示：<br><img src="/2018/10/21/classfication/logistic_function.png" alt="logistic_func"><br>它的取值在$[0,1]$之间。<br>logistic regression的目标函数是：<br>$$h(x) = \frac{1}{1+e<sup>{-\theta</sup>T x} 3}$$<br>其中$x$是输入，$\theta$是要求的参数。</p><h3 id="思路">思路</h3><p>Logistic regression利用logistic function进行分类，给出一个输入，经过参数$\theta$的变换，输出一个$[0,1]$之间的值，如果大于$0.5$，把它分为一类，小于$0.5$，分为另一类。这个$0.5$只是一个例子，可以根据不同的需求选择不同的值。<br>$\theta^T x$相当于给出了一个非线性的决策边界。</p><h3 id="cost-function">Cost function</h3><p>$$J(\theta) = -\log L(\theta) = -\sum_{i=1}^m (y(i)\log h(x^{(i)}) + (1-y<sup>{(i)})\log(1-h(x</sup>{(i)} )) )$$<br>给出两种方式推导logistic regression的cost function</p><h4 id="maximum-likelyhood-estimation">Maximum likelyhood estimation</h4><p>通过极大似然估计推导得到的，当是两个类别的分类时，即$0$或者$1$，有：<br>$$P(y=1|x,\theta) = h(x)$$<br>$$P(y=0|x,\theta) = 1- h(x)$$<br>服从二项分布，写成一个式子是：<br>$$P(y|x,\theta) = h(x)^y (1-h(x))^{1-y}$$<br>其中$y$取值只有$0$和$1$。<br>有了$y$的表达式，我们就可以使用最大似然估计进行求解了：<br>$$L(\theta) = \prod_{i=1}^m (h(x<sup>{(i)})</sup>{y(i)}(1-h(x^{(i)} ))<sup>{(1-y</sup>{(i)})}$$<br>似然函数要求最大化，即求使得$m$个observation出现概率最大的$\theta$，<br>损失函数是用来衡量损失的，令损失函数取负的对数似然，然后最小化loss也就是最大化似然函数了：<br>$$J(\theta) = -\log L(\theta) = -\sum_{i=1}^m (y(i)\log h(x^{(i)}) + (1-y<sup>{(i)})\log(1-h(x</sup>{(i)} )) )$$</p><h4 id="cross-entropy">Cross-entropy</h4><p>对于$k$类问题，写出交叉熵公式如下所示：<br>$$J(\theta) = -\frac{1}{n}\left[\sum_{i=1}^m \sum_k y_k^{(i)} \log h(x_k^{(i)} ) \right]$$<br>当$k=2$时：<br>$$J(\theta) = -\frac{1}{n}\left[\sum_{i=1}^m  y^{(i)} \log h(x^{(i)} ) + (1-y^{(i)}) \log (1-h(x^{(i)} ))\right]$$</p><h3 id="梯度下降">梯度下降</h3><p>$$J(\theta) = -\log L(\theta) = -\sum_{i=1}^m \left[y(i)\log h(x^{(i)}) + (1-y<sup>{(i)})\log(1-h(x</sup>{(i)} )) \right]$$</p><p>\begin{align*}<br>\nabla J &amp; =  -\sum_{i=1}^m \left[ y(i)\frac{1}{h(x^{(i)})}\nabla h(x^{(i)}) - (1-y<sup>{(i)})\frac{1}{\log(1-h(x</sup>{(i)} ))}\nabla\log(1-h(x^{(i)} ))\right]<br>&amp;=-\sum_{i=1}^m  (h(x^{(i)}) - y^{(i)}) x^{(i)}<br>\end{align*}</p><h2 id="参考文献">参考文献</h2><p>1.<a href="https://blog.csdn.net/jk123vip/article/details/80591619" target="_blank" rel="noopener">https://blog.csdn.net/jk123vip/article/details/80591619</a><br>2.<a href="https://zhuanlan.zhihu.com/p/28408516" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/28408516</a><br>3.<a href="https://www.cnblogs.com/pinard/p/6029432.html" target="_blank" rel="noopener">https://www.cnblogs.com/pinard/p/6029432.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;classfication&quot;&gt;Classfication&lt;/h2&gt;
&lt;h2 id=&quot;lda&quot;&gt;LDA&lt;/h2&gt;
&lt;h2 id=&quot;logistic-regression&quot;&gt;Logistic Regression&lt;/h2&gt;
&lt;h3 id=&quot;logistic-funct
      
    
    </summary>
    
      <category term="机器学习" scheme="http://mxxhcm.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://mxxhcm.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="classfication" scheme="http://mxxhcm.github.io/tags/classfication/"/>
    
      <category term="分类" scheme="http://mxxhcm.github.io/tags/%E5%88%86%E7%B1%BB/"/>
    
  </entry>
  
</feed>

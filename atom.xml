<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>mxxhcm&#39;s blog</title>
  <icon>https://www.gravatar.com/avatar/e8e79984d2e37363d60a84f0f1e8cf0e</icon>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://mxxhcm.github.io/"/>
  <updated>2019-09-18T07:50:19.665Z</updated>
  <id>http://mxxhcm.github.io/</id>
  
  <author>
    <name>马晓鑫爱马荟荟</name>
    <email>mxxhcm@gmail.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Asymptotically Efficient 渐进有效性</title>
    <link href="http://mxxhcm.github.io/2019/09/18/asymptotically-efficient-%E6%B8%90%E8%BF%9B%E6%9C%89%E6%95%88%E6%80%A7/"/>
    <id>http://mxxhcm.github.io/2019/09/18/asymptotically-efficient-渐进有效性/</id>
    <published>2019-09-18T06:55:50.000Z</published>
    <updated>2019-09-18T07:50:19.665Z</updated>
    
    <content type="html"><![CDATA[<h2 id="无偏估计的方差下界-cramer-rao-bound">无偏估计的方差下界 cramer-rao bound</h2><p>理论上可以证明，任何无偏估计的方差都有一个下界，这个下界叫做cramer-rao bound。具体的证明好复杂，这里只是简单说一下。如果证明算法无偏估计量的方差的下界是cramer-rao bound，说明这个算法的下界已经没有优化了。。。这个下界其实很多时候不知道什么时候能取到，到时能给我们一定的信息，就像期望一样。</p><p>它的最简单形式是：任何无偏估计的方差至少大于fisher information的倒数。</p><h2 id="efficient">Efficient</h2><p>Efficient说的是在所有的无偏估计方法中，如果某种方法中无偏估计的方差等于cramer-rao bound，那么这个方法就是efficient。（应该是这样子吧。。。）</p><h2 id="asymptotically">Asymptotically</h2><p>在样本有效的情况下，统计量的方差不好计算。但是当样本不断增大时，方差会逐渐接近一个定值。用asymptotically修饰不断增大趋向于无穷的样本数量。</p><h2 id="asymptotically-efficient">Asymptotically  Efficient</h2><p>Asymptotically efficient指得是某种方法在小样本时可能不是efficient的，但是随着样本数量不断增加，变成了efficient的，这种方式就是asymptotically efficient的。</p><h2 id="参考文献">参考文献</h2><p>渐进有效性<br>1.<a href="https://www.zhihu.com/question/285834087/answer/446120288" target="_blank" rel="noopener">https://www.zhihu.com/question/285834087/answer/446120288</a><br>2.<a href="https://bbs.pinggu.org/thread-2139008-1-1.html" target="_blank" rel="noopener">https://bbs.pinggu.org/thread-2139008-1-1.html</a><br>3.<a href="https://www.zhihu.com/question/28908532/answer/254617423" target="_blank" rel="noopener">https://www.zhihu.com/question/28908532/answer/254617423</a><br>4.<a href="https://en.wikipedia.org/wiki/Efficiency_(statistics)#Asymptotic_efficiency" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Efficiency_(statistics)#Asymptotic_efficiency</a><br>5.<a href="https://cs.stackexchange.com/questions/69819/what-does-it-mean-by-saying-asymptotically-more-efficient" target="_blank" rel="noopener">https://cs.stackexchange.com/questions/69819/what-does-it-mean-by-saying-asymptotically-more-efficient</a><br>cramer-rao bound<br>6.<a href="https://www.zhihu.com/question/24710773/answer/117796031" target="_blank" rel="noopener">https://www.zhihu.com/question/24710773/answer/117796031</a><br>7.<a href="http://www2.math.ou.edu/~kmartin/stats/cramer-rao.pdf" target="_blank" rel="noopener">http://www2.math.ou.edu/~kmartin/stats/cramer-rao.pdf</a><br>8.<a href="https://www.zhihu.com/question/311561435/answer/607730638" target="_blank" rel="noopener">https://www.zhihu.com/question/311561435/answer/607730638</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;无偏估计的方差下界-cramer-rao-bound&quot;&gt;无偏估计的方差下界 cramer-rao bound&lt;/h2&gt;
&lt;p&gt;理论上可以证明，任何无偏估计的方差都有一个下界，这个下界叫做cramer-rao bound。具体的证明好复杂，这里只是简单说一下。如果证
      
    
    </summary>
    
      <category term="概率论" scheme="http://mxxhcm.github.io/categories/%E6%A6%82%E7%8E%87%E8%AE%BA/"/>
    
    
      <category term="渐进有效性" scheme="http://mxxhcm.github.io/tags/%E6%B8%90%E8%BF%9B%E6%9C%89%E6%95%88%E6%80%A7/"/>
    
      <category term="asymptotically efficient" scheme="http://mxxhcm.github.io/tags/asymptotically-efficient/"/>
    
      <category term="cramer-rao bound" scheme="http://mxxhcm.github.io/tags/cramer-rao-bound/"/>
    
  </entry>
  
  <entry>
    <title>c</title>
    <link href="http://mxxhcm.github.io/2019/09/17/c/"/>
    <id>http://mxxhcm.github.io/2019/09/17/c/</id>
    <published>2019-09-17T09:12:35.000Z</published>
    <updated>2019-09-17T09:14:19.428Z</updated>
    
    <content type="html"><![CDATA[<h2 id="获得数组长度">获得数组长度</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> a[] = &#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>&#125;;</span><br><span class="line">l = <span class="keyword">sizeof</span>(a)/<span class="keyword">sizeof</span>(<span class="keyword">int</span>);</span><br></pre></td></tr></table></figure><h2 id="printf输出格式">printf输出格式</h2><p>%d  有符号十进制整数(int)<br>%ld, %Ld    长整形数据(long)<br>%i  有符号十进制数，和%d一样<br>%u  无符号十进制整数(unsigned int)<br>%lu, %Lu    无符号十进制长整形数据(unsigned long)<br>%f  单精度浮点数(float)<br>%c  单字符(char)<br>%o  无符号八进制整数<br>%x(%X) 十六进制无符号整数</p><h2 id="c类型字符串数组">c类型字符串数组</h2><ol><li>导包</li></ol><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string.h&gt;</span></span></span><br></pre></td></tr></table></figure><ol start="2"><li>完整字符串复制</li></ol><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">strcpy</span>(des, src);</span><br></pre></td></tr></table></figure><ol start="3"><li>部分字符串复制</li></ol><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">strncpy</span>(des, src+n, len);</span><br></pre></td></tr></table></figure><ol start="4"><li>结束符</li></ol><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sub[len] = <span class="string">'\0'</span>;</span><br></pre></td></tr></table></figure><ol start="5"><li>new字符串数组</li></ol><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">char</span> *str = <span class="keyword">new</span> <span class="keyword">char</span>[<span class="number">100</span>];</span><br></pre></td></tr></table></figure><ol start="6"><li>delete字符串数组</li></ol><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">delete</span> []str;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;获得数组长度&quot;&gt;获得数组长度&lt;/h2&gt;
&lt;figure class=&quot;highlight c++&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;lin
      
    
    </summary>
    
      <category term="C/C++" scheme="http://mxxhcm.github.io/categories/C-C/"/>
    
    
      <category term="C" scheme="http://mxxhcm.github.io/tags/C/"/>
    
  </entry>
  
  <entry>
    <title>fisher information</title>
    <link href="http://mxxhcm.github.io/2019/09/16/fisher-information/"/>
    <id>http://mxxhcm.github.io/2019/09/16/fisher-information/</id>
    <published>2019-09-16T02:24:34.000Z</published>
    <updated>2019-09-19T01:45:18.837Z</updated>
    
    <content type="html"><![CDATA[<h2 id="fisher-information">Fisher information</h2><p>当$\theta$是标量的时候。</p><h3 id="最大似然估计">最大似然估计</h3><p>根据最大似然估计，有似然对数：<br>$$l = \log p(x|\theta)$$</p><h2 id="score-function">score function</h2><p>根据似然对数，定义一个score function：<br>$$s(\theta) = \nabla_{\theta} \log p(x|\theta) $$<br>即score是似然对数的一阶导（梯度），似然对数是标量，score function是似然对数对$\theta$的导数。</p><h3 id="score-function的期望">score function的期望</h3><p><strong>定理</strong> score function的期望是$0$<br>证明：<br>\begin{align*}<br>\mathbb{E}_{p(x|\theta)}\left[s(\theta)\right] &amp; = \mathbb{E}_{p(x|\theta)}\left[\nabla \log p(x|\theta)\right]\\<br>&amp;=\int \nabla \log p(x|\theta) p(x|\theta) dx\\<br>&amp;=\int \frac{\nabla p(x|\theta)}{p(x|\theta)} p(x|\theta) dx\\<br>&amp;=\int \nabla p(x|\theta) dx\\<br>&amp;=\nabla \int p(x|\theta) dx\\<br>&amp;=\nabla 1\\<br>&amp;= 0<br>\end{align*}<br>即似然对数梯度向量的期望为是$0$。</p><h3 id="第一种意义：score-function的方差">第一种意义：score function的方差</h3><p>用$I(\theta)$表示fisher information，它的定义就是score function（似然对数的一阶导）的方差：<br>$$I(\theta) = \mathbb{E} \left[ \left(\frac{\partial}{\partial \theta} \log f(\mathbf{X}; \theta) \right)^2 |\theta \right] = \int \left( \frac{\partial}{\partial \theta} \log f(\mathbf{X}; \theta) \right)^2 f(x;\theta)dx$$</p><p>随机变量的Fisher information总是大于等于$0$的，Fisher information不是某一个observation的函数。</p><h3 id="第二种意义：参数真实值处二阶导数期望的相反数">第二种意义：参数真实值处二阶导数期望的相反数</h3><p>$$I(\theta) =  - \mathbb{E}\left[ \frac{\partial^2 }{\partial \theta^2 } \log f(\mathbf{X}; \theta) |\theta \right] $$<br>Fisher informaction可以看成似然对数对参数估计的能力，在最大似然的估计值附近，fisher信息大代表着图像陡而尖，参数估计能力好；fisher信息小代表着图像宽而平，参数估计能力差。</p><h3 id="第三种意义：cramer-rao-bound的不正式推导">第三种意义：Cramer-Rao bound的不正式推导</h3><p>Fisher informaction的导数是$\theta$无偏估计值方差的下界。换句话说，$\theta$的精确度被似然对数的fisher information限制了。</p><h2 id="fisher-information-matirx">Fisher information Matirx</h2><p>当$\theta$是多维变量的时候。</p><h3 id="第一种意义：协方差矩阵">第一种意义：协方差矩阵</h3><p>$$I(\theta) = \mathbb{E}\left[s(\theta) s(\theta)^T\right]$$<br>根据协方差矩阵的定义：<br>$$\Sigma = \mathbb{E}\left[(X-\mathbb{E}(X))(X-\mathbb{E}(X))^T \right]$$<br>所以$s(\theta)$的协方差矩阵为：<br>$$\Sigma = \mathbb{E}_{p(x|\theta)} \left[(s(\theta)-0)(s(\theta) - 0)^T \right] = \mathbb{E}_{p(x|\theta)} \left[(s(\theta)s(\theta)^T \right] $$</p><h3 id="第二种意义：fisher-information-matrix和hessian-matrix">第二种意义：Fisher information matrix和Hessian matrix</h3><p>Fisher information matrix $F$等于似然对数的二阶导数（海塞矩阵），也是score function的一阶导，期望的负数。<br>$$I(\theta) =  - \mathbb{E}\left[ \frac{\partial^2 }{\partial \theta^2 } \log f(\mathbf{X}; \theta) |\theta \right] $$</p><p>证明：<br>\begin{align*}<br>\text{H}_{\log p(x \vert \theta)} &amp;= \text{J} \left( \nabla \log p(x \vert \theta) \right) \\<br>&amp;= \text{J} \left( \frac{\nabla p(x \vert \theta)}{p(x \vert \theta)} \right) \tag{log-derivative-trick}\\<br>&amp;= \frac{ \text{H}_{p(x \vert \theta)} , p(x \vert \theta) - \nabla p(x \vert \theta) , \nabla p(x \vert \theta)^{\text{T}}}{p(x \vert \theta) , p(x \vert \theta)} \tag{分数求导}\\<br>&amp;= \frac{\text{H}_{p(x \vert \theta)} , p(x \vert \theta)}{p(x \vert \theta) , p(x \vert \theta)} - \frac{\nabla p(x \vert \theta) , \nabla p(x \vert \theta)^{\text{T}}}{p(x \vert \theta) , p(x \vert \theta)} \\<br>&amp;= \frac{\text{H}_{p(x \vert \theta)}}{p(x \vert \theta)} - \left( \frac{\nabla p(x \vert \theta)}{p(x \vert \theta)} \right) \left( \frac{\nabla p(x \vert \theta)}{p(x \vert \theta)}\right)^{\text{T}} \\<br>\end{align*}<br>第一个等号是$\log p$的海塞矩阵（二阶导）等于$\nabla \log p$（一阶导）的雅克比矩阵。<br>对上式取期望，得到：<br>\begin{align*}<br>\mathop{\mathbb{E}}_{p(x \vert \theta)} \left[ \text{H}_{\log p(x \vert \theta)} \right] &amp;= \mathop{\mathbb{E}}_{p(x \vert \theta)} \left[ \frac{\text{H}_{p(x \vert \theta)}}{p(x \vert \theta)} - \left( \frac{\nabla p(x \vert \theta)}{p(x \vert \theta)} \right) \left( \frac{\nabla p(x \vert \theta)}{p(x \vert \theta)} \right)^{\text{T}} \right] \\<br>&amp;= \mathop{\mathbb{E}}_{p(x \vert \theta)} \left[ \frac{\text{H}_{p(x \vert \theta)}}{p(x \vert \theta)} \right] - \mathop{\mathbb{E}}_{p(x \vert \theta)} \left[ \left( \frac{\nabla p(x \vert \theta)}{p(x \vert \theta)} \right) \left( \frac{\nabla p(x \vert \theta)}{p(x \vert \theta)}\right)^{\text{T}} \right] \\<br>&amp;= \int \frac{\text{H}_{p(x \vert \theta)}}{p(x \vert \theta)} p(x \vert \theta) , \text{d}x , - \mathop{\mathbb{E}}_{p(x \vert \theta)} \left[ \nabla \log p(x \vert \theta) , \nabla \log p(x \vert \theta)^{\text{T}} \right] \\<br>&amp;= \text{H}_{\int p(x \vert \theta) , \text{d}x} , - \text{F} \\<br>&amp;= \text{H}_{1} - \text{F} \\<br>&amp;= -\text{F} \\<br>\end{align*}<br>最后得到：$\mathbf{F} = - \mathop{\mathbb{E}}_{p(x \vert \theta)} \left[ \mathbf{H}_{\log p(x|\theta)}\right] $</p><h2 id="参考文献">参考文献</h2><p>1.<a href="https://en.wikipedia.org/wiki/Fisher_information" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Fisher_information</a><br>2.<a href="https://math.stackexchange.com/a/265933" target="_blank" rel="noopener">https://math.stackexchange.com/a/265933</a><br>3.<a href="https://www.zhihu.com/question/26561604/answer/33275982" target="_blank" rel="noopener">https://www.zhihu.com/question/26561604/answer/33275982</a><br>4.<a href="https://wiseodd.github.io/techblog/2018/03/11/fisher-information/" target="_blank" rel="noopener">https://wiseodd.github.io/techblog/2018/03/11/fisher-information/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;fisher-information&quot;&gt;Fisher information&lt;/h2&gt;
&lt;p&gt;当$\theta$是标量的时候。&lt;/p&gt;
&lt;h3 id=&quot;最大似然估计&quot;&gt;最大似然估计&lt;/h3&gt;
&lt;p&gt;根据最大似然估计，有似然对数：&lt;br&gt;
$$l = \log p(
      
    
    </summary>
    
      <category term="概率论" scheme="http://mxxhcm.github.io/categories/%E6%A6%82%E7%8E%87%E8%AE%BA/"/>
    
    
      <category term="概率论" scheme="http://mxxhcm.github.io/tags/%E6%A6%82%E7%8E%87%E8%AE%BA/"/>
    
      <category term="最大似然估计" scheme="http://mxxhcm.github.io/tags/%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/"/>
    
      <category term="费雪信息" scheme="http://mxxhcm.github.io/tags/%E8%B4%B9%E9%9B%AA%E4%BF%A1%E6%81%AF/"/>
    
      <category term="fisher information" scheme="http://mxxhcm.github.io/tags/fisher-information/"/>
    
  </entry>
  
  <entry>
    <title>gym retro</title>
    <link href="http://mxxhcm.github.io/2019/09/15/gym-retro/"/>
    <id>http://mxxhcm.github.io/2019/09/15/gym-retro/</id>
    <published>2019-09-15T12:28:55.000Z</published>
    <updated>2019-09-17T15:04:16.810Z</updated>
    
    <content type="html"><![CDATA[<h2 id="gym-retro">Gym Retro</h2><h3 id="什么是gym-retro？">什么是Gym Retro？</h3><p>将不同平台的video games都转换成gym environments。可以使用统一的gym接口进行管理。</p><h3 id="包含哪些平台">包含哪些平台</h3><ul><li>Atari</li><li>NEC</li><li>Nintndo</li><li>Sega</li></ul><h3 id="包含哪些roms">包含哪些ROMs</h3><ul><li>the 128 sine-dot by Anthrox</li><li>Sega Tween by Ben Ryves</li><li>Happy 10! by Blind IO</li><li>512-Colour Test Demo by Chris Covell</li><li>Dekadrive by Dekadence</li><li>Automaton by Derek Ledbetter</li><li>Fire by dox</li><li>FamiCON intro by dr88</li><li>Airstriker by Electrokinesis</li><li>Lost Marbles by Vantage</li></ul><h2 id="示例">示例</h2><h3 id="安装gym-retro">安装gym retro</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip3 install gym-retro</span><br></pre></td></tr></table></figure><h3 id="创建gym-env">创建Gym Env</h3><p>下面代码创建一个gym环境</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> retro</span><br><span class="line">env = retro.make(game=<span class="string">'Airstriker-Genesis'</span>)</span><br></pre></td></tr></table></figure><p>retro中的environment是从gym.Env类继承而来的。</p><h3 id="默认rom">默认ROM</h3><p>Airstriker-Genesis的ROM是默认包含在gym-retro之中的，其他的一些ROMs需要手动添加。</p><h3 id="所有的games">所有的games</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> retro</span><br><span class="line">retro.data.list_games()</span><br></pre></td></tr></table></figure><p>上述代码会列出所有的游戏，包含那些默认没有集成的ROMS中的。</p><h3 id="手动添加roms">手动添加ROMs</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python3 -m retor.import /path/to/your/ROMs/directory</span><br></pre></td></tr></table></figure><p>上述代码将存放在某个路径下的ROMs拷贝到Gym Retro的集成目录中去。</p><h2 id="参考文献">参考文献</h2><p>1.<a href="https://retro.readthedocs.io/en/latest/" target="_blank" rel="noopener">https://retro.readthedocs.io/en/latest/</a><br>2.<a href="https://arxiv.org/pdf/1804.03720.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1804.03720.pdf</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;gym-retro&quot;&gt;Gym Retro&lt;/h2&gt;
&lt;h3 id=&quot;什么是gym-retro？&quot;&gt;什么是Gym Retro？&lt;/h3&gt;
&lt;p&gt;将不同平台的video games都转换成gym environments。可以使用统一的gym接口进行管理。&lt;/p&gt;
&lt;
      
    
    </summary>
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="gym-retro" scheme="http://mxxhcm.github.io/tags/gym-retro/"/>
    
      <category term="gym" scheme="http://mxxhcm.github.io/tags/gym/"/>
    
  </entry>
  
  <entry>
    <title>gradient, directional, derivative derivative, partial derivative</title>
    <link href="http://mxxhcm.github.io/2019/09/13/gradient-directional-derivative-derivative-partial-derivative/"/>
    <id>http://mxxhcm.github.io/2019/09/13/gradient-directional-derivative-derivative-partial-derivative/</id>
    <published>2019-09-13T01:56:12.000Z</published>
    <updated>2019-09-17T06:09:18.815Z</updated>
    
    <content type="html"><![CDATA[<h2 id="导数">导数</h2><p>导数表示函数在该点的变化率。<br>$$f’(x) = lim_{\Delta x\rightarrow 0}\frac{\Delta y}{\Delta x} = lim_{\Delta x\rightarrow 0} \frac{f(x+\Delta x) - f(x)}{ \Delta x}$$<br>更直接的来说，导数表示自变量无穷小时，函数值的变化与自变量变化的比值，几何意义是该点的切线。物理意义表示该时刻的瞬时变化率。</p><h2 id="偏导数">偏导数</h2><p>偏导数是多元函数沿着坐标轴的变化率。<br>在一元函数中，只有一个自变量，也就是只存在一个方向上的变化率，叫做导数。对于多元函数，有两个及以上的自变量。从导数到偏导数，相当于从曲线到了曲面，曲线上的一点只有一条切线，而曲面上的一点有无数条切线。我们把沿着坐标轴上的切线的斜率叫做偏导数。</p><h2 id="方向导数">方向导数</h2><p>多元函数沿着任意向量的变化率。</p><h2 id="梯度">梯度</h2><p>多元函数取最大变化率时的方向向量。<br>对于三元函数，计算公式：<br>$$\nabla f  = \frac{\partial f}{\partial x}i+\frac{\partial f}{\partial y}j + \frac{\partial f}{\partial z}j$$<br>梯度到底是行向量还是列向量？取决于使用什么layout。<br>梯度的公式是$\frac{\partial{y}{\partial\mathbf{x}}，<br>如果使用numerator layout，梯度是行向量；<br>如果使用denominator layout，梯度是列向量。</p><h2 id="全微分">全微分</h2><p>函数从$A$点到离它无穷近的$B$点的变化量。<br>对于二元函数，定义为<br>$$dz = \frac{\partial f}{\partial x}dx+\frac{\partial f}{\partial y}dy$$</p><h2 id="区别和联系">区别和联系</h2><p>导数，偏导数和方向导数都是个向量，它们其实都是一个概念。偏导数和导数都是方向导数的特殊情况。<br>梯度是个向量，它指向最陡峭的上升方向，这个时候方向导数最大。</p><h2 id="参考文献">参考文献</h2><p>1.<a href="https://www.zhihu.com/question/36301367/answer/142096153" target="_blank" rel="noopener">https://www.zhihu.com/question/36301367/answer/142096153</a><br>2.<a href="https://math.stackexchange.com/questions/661195/what-is-the-difference-between-the-gradient-and-the-directional-derivative" target="_blank" rel="noopener">https://math.stackexchange.com/questions/661195/what-is-the-difference-between-the-gradient-and-the-directional-derivative</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;导数&quot;&gt;导数&lt;/h2&gt;
&lt;p&gt;导数表示函数在该点的变化率。&lt;br&gt;
$$f’(x) = lim_{\Delta x\rightarrow 0}\frac{\Delta y}{\Delta x} = lim_{\Delta x\rightarrow 0} \frac
      
    
    </summary>
    
      <category term="高等数学" scheme="http://mxxhcm.github.io/categories/%E9%AB%98%E7%AD%89%E6%95%B0%E5%AD%A6/"/>
    
    
      <category term="梯度" scheme="http://mxxhcm.github.io/tags/%E6%A2%AF%E5%BA%A6/"/>
    
      <category term="导数" scheme="http://mxxhcm.github.io/tags/%E5%AF%BC%E6%95%B0/"/>
    
      <category term="偏导数" scheme="http://mxxhcm.github.io/tags/%E5%81%8F%E5%AF%BC%E6%95%B0/"/>
    
      <category term="方向导数" scheme="http://mxxhcm.github.io/tags/%E6%96%B9%E5%90%91%E5%AF%BC%E6%95%B0/"/>
    
      <category term="高等数学" scheme="http://mxxhcm.github.io/tags/%E9%AB%98%E7%AD%89%E6%95%B0%E5%AD%A6/"/>
    
  </entry>
  
  <entry>
    <title>位运算</title>
    <link href="http://mxxhcm.github.io/2019/09/13/bit_manipulation/"/>
    <id>http://mxxhcm.github.io/2019/09/13/bit_manipulation/</id>
    <published>2019-09-13T01:53:35.000Z</published>
    <updated>2019-09-16T14:46:04.153Z</updated>
    
    <content type="html"><![CDATA[<h2 id="按位异或">按位异或</h2><h3 id="定义">定义</h3><p>两个运算对象，相同为$0$，不同为$1$。</p><h3 id="示例">示例</h3><p>$12 ^{} 7 = 11$</p><p>$12 = 1100$<br>$7 = 0111$<br>按位异或得到<br>$1011 = 11$</p><h3 id="交换律">交换律</h3><p>$a$^$b$ = $b$^$a$</p><h3 id="结合律">结合律</h3><p>$a$ ^ $b$ ^ $c$ = $a$ ^ ($b$ ^ $c$)</p><h4 id="扩展">扩展</h4><p>$a$ ^ $b$ ^ $c$ ^ $d$ ^ $a$ ^ $b$ ^ $c$ ^ $d$ ^ $e$ = $a$ ^ $a$ ^ $b$ ^ $b$ ^ $c$ ^ $c$ ^ $d$ ^ $d$ ^ $e$</p><h4 id="示例-v2">示例</h4><p>为什么呢？<br>$1$ ^ $2$ ^ $3$ ^ $4$ ^ $1$ ^ $2$ ^ $3$ ^ $4$ ^ $5$ = $1$ ^ $1$ ^ $2$ ^ $2$ ^ $3$ ^ $3$ ^ $4$ ^ $4$ ^ $5$<br>就相当于<br>$1 = 0001$<br>$2 = 0010$<br>$3 = 0011$<br>$4 = 0100$<br>$1 = 0001$<br>$2 = 0010$<br>$3 = 0011$<br>$4 = 0100$<br>$5 = 0101$<br>分别对每一位异或，对于每一位来说，其实他们都是没有顺序的，只需要统计每一位有多少个$0$和$1$即可了，重复偶数次的值都相互抵消了，剩下的就是没有抵消的那些。</p><h3 id="应用">应用</h3><ul><li>任意两个相等的数亦或都为$0$。</li><li>$0$与任何数亦或都等于那个数。</li></ul><p>只要谨记这两条规则就好。</p><h2 id="移位">移位</h2><h3 id="左移位">左移位</h3><p>$1 \ll 2 $<br>相当于<br>$0001 \ll 2 = 0100 = 4$</p><h3 id="右移位">右移位</h3><p>$8 \gg 2$<br>相当于<br>$1000 \gg 2 = 0010 = 2$</p><h2 id="按位与">按位与</h2><h3 id="定义-v2">定义</h3><p>两个运算对象，相同为$0$，不同为$1$。</p><h3 id="示例-v3">示例</h3><p>十进制的<br>$0, 1, 2, 3, 4, 5, 6, 7, 8$<br>对应的二进制为<br>$0000, 0001, 0010, 0011, 0100, 0101, 0110, 0111, 1000$<br>判断十进制数中每一个二进制位是否是$1$。<br>$1$分别左移$i=0,1,2,3$位，然后与要判断的二进制数进行按位与，如果不是$0$，则说明第$i$位为$1$。</p><h3 id="应用-v2">应用</h3><p>判断一个数是不是$2$的幂，再进一步就是统计一个数二进制位为$1$的个数。<br>$2$的幂有一个特征，就是只有一个二进制是$1$，其余的所有位都是$0$。<br>而$2$的幂减去$1$会得到所有的二进制位都是$1$，他们按位与，得到所有的二进制位是$0$，即$(2$ ^ $n)$&amp;$(2$ ^ $n-1) = 0$，实际上，这个公式会消去二进制位最右边的一个$1$。</p><h2 id="移位和与">移位和与</h2><p>移位和与结合起来判断某一位二进制位是否是$1$。</p><h3 id="示例-v4">示例</h3><p>判断$13$从右数的的第$3$个二进制位是否为$1$。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span>(<span class="number">13</span> &gt;&gt; <span class="number">1</span> &amp; <span class="number">1</span>)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"true\n"</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;按位异或&quot;&gt;按位异或&lt;/h2&gt;
&lt;h3 id=&quot;定义&quot;&gt;定义&lt;/h3&gt;
&lt;p&gt;两个运算对象，相同为$0$，不同为$1$。&lt;/p&gt;
&lt;h3 id=&quot;示例&quot;&gt;示例&lt;/h3&gt;
&lt;p&gt;$12 ^{} 7 = 11$&lt;/p&gt;
&lt;p&gt;$12 = 1100$&lt;br&gt;
$7 = 
      
    
    </summary>
    
      <category term="算法" scheme="http://mxxhcm.github.io/categories/%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="按位与" scheme="http://mxxhcm.github.io/tags/%E6%8C%89%E4%BD%8D%E4%B8%8E/"/>
    
      <category term="按位或" scheme="http://mxxhcm.github.io/tags/%E6%8C%89%E4%BD%8D%E6%88%96/"/>
    
      <category term="按位异或" scheme="http://mxxhcm.github.io/tags/%E6%8C%89%E4%BD%8D%E5%BC%82%E6%88%96/"/>
    
      <category term="非" scheme="http://mxxhcm.github.io/tags/%E9%9D%9E/"/>
    
      <category term="移位" scheme="http://mxxhcm.github.io/tags/%E7%A7%BB%E4%BD%8D/"/>
    
      <category term="逻辑运算" scheme="http://mxxhcm.github.io/tags/%E9%80%BB%E8%BE%91%E8%BF%90%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>log derivative trick</title>
    <link href="http://mxxhcm.github.io/2019/09/12/log-derivative-trick/"/>
    <id>http://mxxhcm.github.io/2019/09/12/log-derivative-trick/</id>
    <published>2019-09-12T11:21:10.000Z</published>
    <updated>2019-09-13T03:01:21.054Z</updated>
    
    <content type="html"><![CDATA[<h2 id="什么是log-derivative-trick">什么是log derivative trick</h2><p>$$\nabla_{\theta} log p(\mathbf{x}; \theta) = \frac{\nabla_{\theta} p(\mathbf{x}; \theta)}{ p(\mathbf{x}; \theta)}$$</p><h2 id="为什么？">为什么？</h2><p>导数公式：<br>$$(log_a x)’ = \frac{1}{x ln\ a}$$<br>$$(ln\ x)’ = \frac{1}{x}$$<br>所以：<br>$$(ln\ p(\mathbf{x};\theta))’ = \frac{1}{p(\mathbf{x}; \theta)}$$<br>\begin{align*}<br>\nabla_{\theta} ln\ p(\mathbf{x};\theta) &amp;=\left[\frac{\partial ln\ p(\mathbf{x}; \theta)}{\partial \theta} \right]^T \\<br>&amp;= \left[\frac{\partial ln\ p(\mathbf{x}; \theta)}{\partial p(\mathbf{x};\theta)}\frac{\partial p(\mathbf{x};\theta)}{\partial \theta} \right]^T  \\<br>&amp;= \left[\frac{1}{ p(\mathbf{x};\theta)}\frac{\partial p(\mathbf{x};\theta)}{\partial \theta} \right]^T \\<br>&amp;= \frac{1}{ p(\mathbf{x};\theta)}\left[\frac{\partial p(\mathbf{x};\theta)}{\partial \theta} \right]^T \\<br>&amp;=\frac{\nabla_{\theta}p(\mathbf{x}; \theta)}{p(\mathbf{x}; \theta)}\\<br>\end{align*}</p><p>其实就是$\nabla log\ p = \frac{\nabla p}{p}$。</p><h2 id="参考文献">参考文献</h2><p>1.<a href="https://math.stackexchange.com/questions/2554749/whats-the-trick-in-log-derivative-trick" target="_blank" rel="noopener">https://math.stackexchange.com/questions/2554749/whats-the-trick-in-log-derivative-trick</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;什么是log-derivative-trick&quot;&gt;什么是log derivative trick&lt;/h2&gt;
&lt;p&gt;$$\nabla_{\theta} log p(\mathbf{x}; \theta) = \frac{\nabla_{\theta} p(\math
      
    
    </summary>
    
      <category term="高等数学" scheme="http://mxxhcm.github.io/categories/%E9%AB%98%E7%AD%89%E6%95%B0%E5%AD%A6/"/>
    
    
      <category term="高等数学" scheme="http://mxxhcm.github.io/tags/%E9%AB%98%E7%AD%89%E6%95%B0%E5%AD%A6/"/>
    
      <category term="log detivative trick" scheme="http://mxxhcm.github.io/tags/log-detivative-trick/"/>
    
  </entry>
  
  <entry>
    <title>Matrix calculus</title>
    <link href="http://mxxhcm.github.io/2019/09/12/matrix-calculus/"/>
    <id>http://mxxhcm.github.io/2019/09/12/matrix-calculus/</id>
    <published>2019-09-12T01:35:36.000Z</published>
    <updated>2019-09-17T07:48:02.465Z</updated>
    
    <content type="html"><![CDATA[<h2 id="符号声明">符号声明</h2><p>小写字母$x,y$是标量，小写加粗字母$\mathbf{x},\mathbf{y}$是向量，大写加粗$\mathbf{X},\mathbf{Y}$是矩阵。标量和向量都可以看成是矩阵，将vector看成$1\times n$或者$m\times 1$的矩阵，将scalar看成$1\times 1$的矩阵。$\mathbf{X}^T $表示矩阵$\mathbf{X}$的转置，$tr(\mathbf{X})$表示迹，即对角线元素之和。$det(\mathbf{X})$或者$\vert \mathbf{X}\vert$表示行列式。</p><h2 id="基础">基础</h2><h3 id="迹">迹</h3><p>$$ tr(\mathbf{A}) = tr(\mathbf{A}^T )$$<br>$$ tr(\mathbf{A}\mathbf{B}) = tr(\mathbf{B}\mathbf{A})$$<br>$$ tr(\mathbf{A}+\mathbf{B}) = tr(\mathbf{B})+tr(\mathbf{A})$$</p><h3 id="行列式">行列式</h3><p>…</p><h2 id="简介">简介</h2><p>矩阵微积分值得是使用矩阵或者向量表示因变量中每一个元素相对于自变量中每一个元素的导数。一般来说，自变量和因变量都可以是标量，向量和矩阵。</p><h3 id="示例">示例</h3><p>考虑向量梯度，给定三个自变量，一个因变量的函数：$f(x_1, x_2, x_3)$，向量的梯度是：<br>$$\nabla f= \frac{\partial f}{\partial x_1}\mathbf{i} + \frac{\partial f}{\partial x_2}\mathbf{j} + \frac{\partial f}{\partial x_3}\mathbf{k}$$<br>其中$\mathbf{i,j,k}$表示坐标轴正方向上的单位向量。这类问题可以看成标量$f$对向量$x$求导数，结果依然是一个向量（梯度）：<br>$$\nabla f=(\frac{\partial f }{\partial \mathbf{x}})^T = \left[\frac{\partial f}{\partial x_1} + \frac{\partial f}{\partial x_2} + \frac{\partial f}{\partial x_3}\right]^T $$</p><p>更复杂的情况是标量$f$对矩阵$\mathbf{X}$求导，叫做矩阵梯度(gradient matrix)。标量，向量，矩阵的组合总共有$9$中情况，其中六种情况可以用以下方式表示出来：</p><table><thead><tr><th style="text-align:center">种类</th><th style="text-align:center">标量</th><th style="text-align:center">向量</th><th style="text-align:center">矩阵</th></tr></thead><tbody><tr><td style="text-align:center">标量</td><td style="text-align:center">$\frac{\partial{y}}{\partial{x}}$</td><td style="text-align:center">$\frac{\partial{\mathbf{y}}}{\partial{x}}$</td><td style="text-align:center">$\frac{\partial{\mathbf{Y}}}{\partial{x}}$</td></tr><tr><td style="text-align:center">向量</td><td style="text-align:center">$\frac{\partial{y}}{\partial{x}}$</td><td style="text-align:center">$\frac{\partial{\mathbf{y}}}{\partial{x}}$</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">矩阵</td><td style="text-align:center">$\frac{\partial{y}}{\partial{\mathbf{X}}}$</td><td style="text-align:center"></td><td style="text-align:center"></td></tr></tbody></table><h3 id="用途">用途</h3><p>Matrix calculus通常用于优化，常常用在拉格朗日乘子法中。包括Kalman滤波，高斯混合分布的EM算法，梯度下降法的的导数。</p><h2 id="向量导数">向量导数</h2><p>向量可以看成只有一列的矩阵，最简单的矩阵导数应该是标量导数，然后是向量导数。这一节使用的是numerator layout（分子布局）。</p><h3 id="向量对标量求导">向量对标量求导</h3><h3 id="标量对向量求导">标量对向量求导</h3><h3 id="向量对向量求导">向量对向量求导</h3><h2 id="矩阵导数">矩阵导数</h2><h3 id="矩阵对标量求导">矩阵对标量求导</h3><h3 id="标量对矩阵求导">标量对矩阵求导</h3><h2 id="layout">Layout</h2><p>事实上，有两种定义矩阵导数的方式，这两种方法刚好差一个转置。这也是我们平常矩阵求导最容易迷惑的地方。<br>求向量对向量的导数时，即$\frac{\partial \mathbf{y}}{\partial \mathbf{x}}, \partial \mathbf{x} \in \mathbb{R}^n , \partial \mathbf{y} \in \mathbb{R}^m $，有两种方式表示，一种结果是$m\times n$的矩阵，一种是$n\times m$的矩阵。这就有以下的layout：</p><ol><li>Numetator layout，根据$\partial \mathbf{y}$和$\partial \mathbf{x}^T $进行布局。也叫Jacobian 公式，最后是一个$m\times n$的矩阵。</li><li>Denominator layout，根据$\partial \mathbf{y}^T $和$\partial \mathbf{x}$进行布局。也叫Hessian公式，最后是一个$n\times m$的矩阵，是numetator layout的转置。也有人把它叫做梯度，但是梯度通常指的是$\frac{\partial y}{\partial \mathbf{x}}$，即标量对向量求导，不需要考虑layout。</li></ol><p>在计算梯度$\frac{\partial y}{\partial \mathbf{x}}$和$\frac{\partial\mathbf{y}}{\partial x}$的时候，也会有冲突。</p><ol><li>采用分子layout，$\frac{\partial y}{\partial \mathbf{x}}$是行向量和$\frac{\partial\mathbf{y}}{\partial x}$是列向量。</li><li>采用分母layout，$\frac{\partial y}{\partial \mathbf{x}}$是列向量和$\frac{\partial\mathbf{y}}{\partial x}$是行向量。</li></ol><p>最后计算标量对矩阵求导$\frac{\partial y}{\partial \mathbf{X}}$和矩阵对标量求导$\frac{\partial\mathbf{Y}}{\partial x}$。</p><ol><li>采用分子layout，$\frac{\partial y}{\partial \mathbf{X}}$和$\mathbf{Y}$的shape一样，$\frac{\partial\mathbf{Y}}{\partial x}$和$\mathbf{X}^T $的shape一样。</li><li>采用分母layout，$\frac{\partial y}{\partial \mathbf{X}}$和$\mathbf{Y}$的shape一样，这里不用转置这是因为这样子好看。$\frac{\partial\mathbf{Y}}{\partial x}$和$\mathbf{X} $的shape一样。</li></ol><p>接下来的公式主要对$\frac{\partial \mathbf{y}}{\partial x}, \frac{\partial y}{\partial \mathbf{x}}, \frac{\partial \mathbf{y}}{\partial \mathbf{x}}, \frac{\partial \mathbf{Y}}{\partial x},\frac{\partial y}{\partial \mathbf{X}}$这种组合分别考虑。</p><table><thead><tr><th style="text-align:center">种类</th><th style="text-align:center">标量$y$</th><th style="text-align:center">$m\times 1$列向量$\mathbf{y}$</th><th style="text-align:center">$m\times n$矩阵$\mathbf{Y}$</th></tr></thead><tbody><tr><td style="text-align:center">标量$x$,numetator layout</td><td style="text-align:center">$\frac{\partial{y}}{\partial{x}}$: 标量</td><td style="text-align:center">$\frac{\partial{\mathbf{y}}}{\partial{x}}$: size为$m$的列向量</td><td style="text-align:center">$\frac{\partial{\mathbf{Y}}}{\partial{x}}$: $m\times n$的矩阵</td></tr><tr><td style="text-align:center">标量$x$,denominator layout</td><td style="text-align:center">$\frac{\partial{y}}{\partial{x}}$: 标量</td><td style="text-align:center">$\frac{\partial{\mathbf{y}}}{\partial{x}}$: size为$m$的行向量</td><td style="text-align:center">$\frac{\partial{\mathbf{Y}}}{\partial{x}}$: $m\times n$的矩阵</td></tr><tr><td style="text-align:center">$n\times 1$列向量$\mathbf{x}$,numetator layout</td><td style="text-align:center">$\frac{\partial{y}}{\partial{x}}$: size为$n$的行向量</td><td style="text-align:center">$\frac{\partial{\mathbf{y}}}{\partial{x}}$: $m\times n$的矩阵</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">$n\times 1$列向量$\mathbf{x}$,denominator layout</td><td style="text-align:center">$\frac{\partial{y}}{\partial{x}}$: size为$n$的列向量</td><td style="text-align:center">$\frac{\partial{\mathbf{y}}}{\partial{x}}$: $n\times m$的矩阵</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">$p\times q$矩阵$\mathbf{Y}$ numetator layout</td><td style="text-align:center">$\frac{\partial{y}}{\partial{\mathbf{X}}}$: $q\times p$的矩阵</td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">$p\times q$矩阵$\mathbf{Y}$ denominator layout</td><td style="text-align:center">$\frac{\partial{y}}{\partial{\mathbf{X}}}$: $p\times q$的矩阵</td><td style="text-align:center"></td><td style="text-align:center"></td></tr></tbody></table><h3 id="numerator-layout">numerator layout</h3><p>标量对向量求导：<br>$\frac{\partial y}{\partial \mathbf{x}} = \begin{bmatrix}\frac{\partial y}{\partial x_1} &amp; \cdots &amp;\frac{\partial y}{\partial x_n}\end{bmatrix}$<br>向量对标量求导：<br>$\frac{\partial \mathbf{y}}{\partial x} = \begin{bmatrix}\frac{\partial y_1}{\partial x} \\ \vdots \\ \frac{\partial y_m}{\partial x}\end{bmatrix}$<br>向量对向量求导：<br>$\frac{\partial \mathbf{y}}{\partial \mathbf{x}} = \begin{bmatrix}\frac{\partial y_1}{\partial x_1} &amp; \cdots &amp; \frac{\partial y_1}{\partial x_n}  \\ \vdots \\ \frac{\partial y_m}{\partial x_1} &amp; \cdots &amp; \frac{\partial y_m}{\partial x_n}\end{bmatrix}$<br>标量对矩阵求导：<br>$\frac{\partial y}{\partial \mathbf{X}} = \begin{bmatrix}\frac{\partial y}{\partial x_{11}} &amp; \cdots &amp; \frac{\partial y}{\partial x_{p1}}  \\ \vdots \\ \frac{\partial y}{\partial x_{1q}} &amp; \cdots &amp;\frac{\partial y}{\partial x_{pq}}\end{bmatrix}$</p><p>下列公式只有numerator layout，没有denominator-layout：<br>矩阵对标量求导：<br>$\frac{\partial \mathbf{Y}}{\partial x} = \begin{bmatrix}\frac{\partial y_{11}}{\partial x}&amp;\cdots \frac{\partial y_{1n}}{\partial x}  \\ \vdots \\ \frac{\partial y_{m1}}{\partial x} &amp; \cdots &amp; \frac{\partial y_{mn}}{\partial x}\end{bmatrix}$<br>矩阵微分：<br>$dx = \begin{bmatrix}dx_{11} &amp; \cdots &amp; dx_{1n} \\ \vdots \\ dx_{m1} &amp; \cdots &amp; dx_{mn}\end{bmatrix}$</p><h3 id="denominator-layout">denominator layout</h3><p>标量对向量求导：<br>$\frac{\partial y}{\partial \mathbf{x}} = \begin{bmatrix}\frac{\partial y}{\partial x_1} &amp; \cdots &amp;\frac{\partial y}{\partial x_n}\end{bmatrix}$<br>$\frac{\partial y}{\partial \mathbf{x}} = \begin{bmatrix}\frac{\partial y}{\partial x_1} \\ \cdots \\ \frac{\partial y}{\partial x_n}\end{bmatrix}$<br>向量对标量求导：<br>$\frac{\partial \mathbf{y}}{\partial x} = \begin{bmatrix}\frac{\partial y_1}{\partial x} &amp; \vdots &amp; \frac{\partial y_m}{\partial x}\end{bmatrix}$<br>向量对向量求导：<br>$\frac{\partial \mathbf{y}}{\partial \mathbf{x}} = \begin{bmatrix}\frac{\partial y_1}{\partial x_1} &amp; \cdots &amp; \frac{\partial y_m}{\partial x_1}  \\ \vdots \\ \frac{\partial y_1}{\partial x_n} &amp; \cdots &amp; \frac{\partial y_m}{\partial x_n}\end{bmatrix}$<br>标量对矩阵求导：<br>$\frac{\partial y}{\partial \mathbf{X}} = \begin{bmatrix}\frac{\partial y}{\partial x_{11}} &amp;\cdots &amp; \frac{\partial y}{\partial x_{1q}}  \\ \vdots \\ \frac{\partial y}{\partial x_{p1}} &amp; \cdots &amp; \frac{\partial y}{\partial x_{pq}}\end{bmatrix}$</p><h2 id="公式">公式</h2><h3 id="向量对向量求导公式">向量对向量求导公式</h3><h3 id="标量对向量求导公式">标量对向量求导公式</h3><h3 id="向量对标量求导公式">向量对标量求导公式</h3><h3 id="标量对矩阵求导公式">标量对矩阵求导公式</h3><h3 id="矩阵对标量求导公式">矩阵对标量求导公式</h3><h3 id="标量对标量求导公式">标量对标量求导公式</h3><h3 id="微分形式的公式">微分形式的公式</h3><p>通常来说使用微分形式然后转换成导数更简单。但是只有在使用numerator layout才起作用。</p><table><thead><tr><th style="text-align:center">表达式</th><th style="text-align:center">结果(numerator layout)</th></tr></thead><tbody><tr><td style="text-align:center">$d(\mathbf{A})  $</td><td style="text-align:center">$0$</td></tr><tr><td style="text-align:center">$d(a\mathbf{X})$</td><td style="text-align:center">$ad\mathbf{A}$</td></tr><tr><td style="text-align:center">$d(\mathbf{X}+\mathbf{Y})$</td><td style="text-align:center">$d\mathbf{X}+d\mathbf{Y}$</td></tr><tr><td style="text-align:center">$d(tr(\mathbf{X}))$</td><td style="text-align:center">$tr(d\mathbf{X})$</td></tr><tr><td style="text-align:center">$d(\mathbf{X}\mathbf{Y})$</td><td style="text-align:center">$(d\mathbf{X})\mathbf{Y}+\mathbf{X}(d\mathbf{Y})$</td></tr><tr><td style="text-align:center">$d(\mathbf{X}^{-1} ) $</td><td style="text-align:center">$- \mathbf{X}^{-1} (d\mathbf{X}) \mathbf{X}^{-1} $</td></tr><tr><td style="text-align:center">$d(\vert\mathbf{X} \vert)$</td><td style="text-align:center">$\vert\mathbf{X}\vert tr(\mathbf{X}^{-1} d\mathbf{X}) = tr(adj(\mathbf{X})d\mathbf{X})$</td></tr><tr><td style="text-align:center">$d(ln \vert\mathbf{X} \vert)$</td><td style="text-align:center">$tr(\mathbf{X}^{-1} d\mathbf{X})$</td></tr><tr><td style="text-align:center">$d(\mathbf{X}^T) $</td><td style="text-align:center">$(d\mathbf{X})^T $</td></tr><tr><td style="text-align:center">$d(\mathbf{X}^H ) $</td><td style="text-align:center">$(d\mathbf{X})^T $</td></tr></tbody></table><p>其中$\mathbf{A}$不是$\mathbf{X}$的函数，$a$不是$\mathbf{X}$的函数，上面的公式可以根据链式法则迭代使用。<br>上面的绝大部分公式可以使用$\mathbf{F}(\mathbf{X} + d\mathbf{X}) - \mathbf{F}(\mathbf{X})$计算，取线性部分可以得到，例如：<br>$$(\mathbf{X} + d\mathbf{X}) (\mathbf{Y} + d\mathbf{Y}) = \mathbf{X}\mathbf{Y} + (d\mathbf{X})\mathbf{Y} + \mathbf{X}d\mathbf{Y} + (d\mathbf{X})(d\mathbf{Y})$$<br>然后得到$d(\mathbf{X}\mathbf{Y})= (d\mathbf{X})\mathbf{Y}+\mathbf{X}(d\mathbf{Y})$。<br>计算$d\mathbf{X}^{-1} $，有<br>$$0 = d\mathbf{I} = d(\mathbf{X}^{-1} \mathbf{X}) = (d(\mathbf{X}^{-1}) \mathbf{X} + \mathbf{X}^{-1} d(\mathbf{X})$$<br>移项得$d(\mathbf{X}^{-1} ) = - \mathbf{X}^{-1} (d\mathbf{X}) \mathbf{X}^{-1} $<br>关于迹的公式，有：<br>$$tr(\mathbf{X} + d\mathbf{X}) - tr(\mathbf{X}) = tr(d\mathbf{X})$$</p><p>下面给出导数和微分之间转换的标准公式，我们的目标就是使用上面的公式将一些复杂的公式转换成下面的标准公式。</p><h4 id="微分和导数的转换">微分和导数的转换</h4><table><thead><tr><th style="text-align:center">标准微分公式</th><th style="text-align:center">等价的导数形式</th></tr></thead><tbody><tr><td style="text-align:center">$dy = a\ dx$</td><td style="text-align:center">$\frac{dy}{dx} = a$</td></tr><tr><td style="text-align:center">$dy = \mathbf{a}d\mathbf{x}$</td><td style="text-align:center">$\frac{dy}{d \mathbf{x}} = \mathbf{a}$</td></tr><tr><td style="text-align:center">$dy = tr(\mathbf{A}d\mathbf{A})$</td><td style="text-align:center">$\frac{dy}{d \mathbf{X}} = \mathbf{A}$</td></tr><tr><td style="text-align:center">$d\mathbf{y} = \mathbf{a}dx$</td><td style="text-align:center">$\frac{d\mathbf{y}}{d x} = \mathbf{a}$</td></tr><tr><td style="text-align:center">$d\mathbf{y} = \mathbf{A}d\mathbf{x}$</td><td style="text-align:center">$\frac{d\mathbf{y}}{d \mathbf{x}} = \mathbf{A}$</td></tr><tr><td style="text-align:center">$d\mathbf{Y} = \mathbf{A}dx$</td><td style="text-align:center">$\frac{d\mathbf{Y}}{dx} = \mathbf{A}$</td></tr></tbody></table><p>有一个很重要的公式是：<br>$$tr(\mathbf{A}\mathbf{B}) = tr(\mathbf{B}\mathbf{A})$$</p><h4 id="示例-v2">示例</h4><p>$$\frac{d}{d\mathbf{X}} tr(\mathbf{A}\mathbf{X}\mathbf{B}) = tr(\mathbf{A}\mathbf{B})$$<br>因为：<br>$$d tr(\mathbf{A}\mathbf{X}\mathbf{B}) = tr(d(\mathbf{A}\mathbf{X}\mathbf{B})) = tr(\mathbf{A}d(\mathbf{X})\mathbf{B}) =  tr(\mathbf{B}\mathbf{A}d(\mathbf{X})) $$<br>对应$\frac{d\mathbf{Y}}{dx} = \mathbf{A}$。</p><h5 id="二次型">二次型</h5><p>计算二次型$\mathbf{x}^T \mathbf{A}\mathbf{x}$的导数，因为$\mathbf{x}^T \mathbf{A}\mathbf{x}$是一个标量，所以可以套上一个$tr$操作：<br>$$\frac{d(\mathbf{x}^T \mathbf{A}\mathbf{x})}{d\mathbf{x}}= \mathbf{x}^T (\mathbf{A}^T + \mathbf{A})$$<br>推导：<br>\begin{align*}<br>d(\mathbf{x}^T \mathbf{A}\mathbf{x}) &amp;= tr(d(\mathbf{x}^T \mathbf{A}\mathbf{x})) \\<br>&amp;= tr(d(\mathbf{x}^T) \mathbf{A}\mathbf{x} + \mathbf{x}^T d(\mathbf{A}) \mathbf{x} + \mathbf{x}^T \mathbf{A}d(\mathbf{x})) \\<br>&amp;=  tr(\mathbf{x}^T \mathbf{A}^T  d(\mathbf{x}) + \mathbf{x}^T \mathbf{A}d(\mathbf{x})) \\<br>&amp;= tr(\mathbf{x}^T (\mathbf{A}^T + \mathbf{A})d(\mathbf{x}))\\<br>\end{align*}<br>满足$dy = \mathbf{a}d\mathbf{x}$。所以$\mathbf{x}^T \mathbf{A}\mathbf{x}$的导数是$\mathbf{x}^T (\mathbf{A}^T +\mathbf{A})$。</p><h2 id="参考文献">参考文献</h2><p>1.<a href="https://en.wikipedia.org/wiki/Matrix_calculus" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Matrix_calculus</a><br>2.<a href="https://zhuanlan.zhihu.com/p/24709748" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/24709748</a><br>3.<a href="https://zhuanlan.zhihu.com/p/24863977" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/24863977</a><br><a href="http://4.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf" target="_blank" rel="noopener">4.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf</a><br>5.<a href="http://www.iro.umontreal.ca/~pift6266/A06/refs/minka-matrix.pdf" target="_blank" rel="noopener">http://www.iro.umontreal.ca/~pift6266/A06/refs/minka-matrix.pdf</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;符号声明&quot;&gt;符号声明&lt;/h2&gt;
&lt;p&gt;小写字母$x,y$是标量，小写加粗字母$\mathbf{x},\mathbf{y}$是向量，大写加粗$\mathbf{X},\mathbf{Y}$是矩阵。标量和向量都可以看成是矩阵，将vector看成$1\times n$或者
      
    
    </summary>
    
      <category term="线性代数" scheme="http://mxxhcm.github.io/categories/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/"/>
    
    
      <category term="矩阵求导" scheme="http://mxxhcm.github.io/tags/%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC/"/>
    
      <category term="矩阵微积分" scheme="http://mxxhcm.github.io/tags/%E7%9F%A9%E9%98%B5%E5%BE%AE%E7%A7%AF%E5%88%86/"/>
    
  </entry>
  
  <entry>
    <title>雅克比矩阵和海塞矩阵</title>
    <link href="http://mxxhcm.github.io/2019/09/10/Jacobian-matrix-and-Hessian-matrix/"/>
    <id>http://mxxhcm.github.io/2019/09/10/Jacobian-matrix-and-Hessian-matrix/</id>
    <published>2019-09-10T11:22:54.000Z</published>
    <updated>2019-09-17T08:51:22.182Z</updated>
    
    <content type="html"><![CDATA[<h2 id="雅克比矩阵和海塞矩阵对比">雅克比矩阵和海塞矩阵对比</h2><ol><li>雅克比矩阵是一阶导数，海塞矩阵是二阶导数。</li><li>雅克比矩阵要求函数的输入是向量，输出也是向量，而海塞矩阵要求输入是向量，输出是标量。</li><li>雅克比矩阵不一定是方阵（当输入和输出的维度相等时是方阵），但是海塞矩阵一定是方阵。当函数的输出是标量的时候，雅克比矩阵退化成了梯度向量。</li><li>海塞矩阵可以看成梯度的雅克比矩阵。</li></ol><h2 id="雅克比矩阵">雅克比矩阵</h2><p>定义：$f:\mathbb{R}^n \rightarrow \mathbf{R}^m $，即一个函数的输入和输出都是向量，计算输出向量对于输入向量的偏导数（详情可见<a href="https://mxxhcm.github.io/2019/09/12/matrix-calculus/">矩阵求导</a>)，得到一个$m\times n$的矩阵（numerator layout），这就是雅克比矩阵。<br>$$J = \begin{bmatrix}\frac{\partial \mathbf{y}}{\partial x_1}\cdots &amp; \cdots &amp; \frac{\partial \mathbf{y}}{\partial x_n}\end{bmatrix} = \begin{bmatrix} \frac{\partial \mathbf{y_1}}{\partial x_1}\cdots &amp; \cdots &amp; \frac{\partial \mathbf{y_1}}{\partial x_n}\\ \cdots \\ \frac{\partial \mathbf{y_m}}{\partial x_1}\cdots &amp; \cdots &amp; \frac{\partial \mathbf{y_m}}{\partial x_n}\end{bmatirx}$$</p><h2 id="海塞矩阵">海塞矩阵</h2><p>定义：$f:\mathbb{R}^n \rightarrow \mathbf{R} $，即一个函数的输入是向量，输出是标量时，计算输出对于输入向量的二阶导数（详情可见<a href="https://mxxhcm.github.io/2019/09/12/matrix-calculus/">矩阵求导</a>)，得到一个$n\times n$的矩阵（numerator layout），这就是雅克比矩阵。<br>$$ H = \begin{bmatrix} \frac{\partial^2 \mathbf{y}}{\partial x_1^2 }\cdots &amp; \cdots &amp; \frac{\partial^2 \mathbf{y}}{\partial x_1 x_n}\\ \cdots \\ \frac{\partial^2 \mathbf{y}}{\partial x_nx_1}\cdots &amp; \cdots &amp; \frac{\partial^2 \mathbf{y}}{\partial x_nx_n}\end{bmatrix}$$</p><h2 id="参考文献">参考文献</h2><p>1.<a href="https://zhuanlan.zhihu.com/p/67521774" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/67521774</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;雅克比矩阵和海塞矩阵对比&quot;&gt;雅克比矩阵和海塞矩阵对比&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;雅克比矩阵是一阶导数，海塞矩阵是二阶导数。&lt;/li&gt;
&lt;li&gt;雅克比矩阵要求函数的输入是向量，输出也是向量，而海塞矩阵要求输入是向量，输出是标量。&lt;/li&gt;
&lt;li&gt;雅克比矩阵不一定
      
    
    </summary>
    
      <category term="线性代数" scheme="http://mxxhcm.github.io/categories/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/"/>
    
    
      <category term="线性代数" scheme="http://mxxhcm.github.io/tags/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/"/>
    
      <category term="雅克比矩阵" scheme="http://mxxhcm.github.io/tags/%E9%9B%85%E5%85%8B%E6%AF%94%E7%9F%A9%E9%98%B5/"/>
    
      <category term="海塞矩阵" scheme="http://mxxhcm.github.io/tags/%E6%B5%B7%E5%A1%9E%E7%9F%A9%E9%98%B5/"/>
    
  </entry>
  
  <entry>
    <title>排序专题</title>
    <link href="http://mxxhcm.github.io/2019/09/10/%E6%8E%92%E5%BA%8F%E4%B8%93%E9%A2%98/"/>
    <id>http://mxxhcm.github.io/2019/09/10/排序专题/</id>
    <published>2019-09-10T11:22:40.000Z</published>
    <updated>2019-09-10T11:22:40.892Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>trust region policy optimization</title>
    <link href="http://mxxhcm.github.io/2019/09/08/trust-region-policy-optimization/"/>
    <id>http://mxxhcm.github.io/2019/09/08/trust-region-policy-optimization/</id>
    <published>2019-09-08T06:24:37.000Z</published>
    <updated>2019-09-19T02:23:08.134Z</updated>
    
    <content type="html"><![CDATA[<h2 id="术语定义"><a href="#术语定义" class="headerlink" title="术语定义"></a>术语定义</h2><p>更多介绍可以点击查看<a href>reinforcement learning an introduction 第三章</a></p><ol><li>状态集合<br>$\mathcal{S}$是有限states set，包含所有state的可能取值</li><li>动作集合<br>$\mathcal{A}$是有限actions set，包含所有action的可能取值</li><li>转换概率矩阵或者状态转换函数<br>$P:\mathcal{S}\times \mathcal{A}\times \mathcal{S} \rightarrow \mathbb{R}$是transition probability distribution，或者写成$p(s_{t+1}|s_t,a_t)$</li><li>奖励函数<br>$R:\mathcal{S}\times \mathcal{A}\rightarrow \mathbb{R}$是reward function</li><li>折扣因子<br>$\gamma \in (0, 1)$</li><li>初始状态分布<br>$\rho_0$是初始状态$s_0$服从的distribution，$s_0\sim \rho_0$</li><li>带折扣因子的MDP<br>定义为tuple $\left(\mathcal{S},\mathcal{A},P,R,\rho_0, \gamma\right)$</li><li>随机策略<br>选择action，stochastic policy表示为：$\pi_\theta: \mathcal{S}\rightarrow P(\mathcal{A})$，其中$P(\mathcal{A})$是选择$\mathcal{A}$中每个action的概率，$\theta$表示policy的参数，$\pi_\theta(a_t|s_t)$是在$s_t$处取action $a_t$的概率</li><li>期望折扣回报<br>定义<script type="math/tex; mode=display">G_t = \sum_{k=t}^{\infty} \gamma^{k-t} R_{k+1}</script>为expected discounted returns，表示从$t$时刻开始的expected discounted return；<br>用<script type="math/tex; mode=display">\eta(\pi)= \mathbb{E}\_{s_0, a_0, \cdots\sim \pi}\left[\sum_{t=0}^{\infty} \gamma^t R_{t+1}\right]</script>表示$t=0$时policy $\pi$的expected discounted return，其中$s_0\sim\rho_0(s_0), a_t\sim\pi(a_t|s_t), s_{t+1}\sim P(s_{t+1}|s_t,a_t)$</li><li>状态值函数<br>state value function的定义是从$t$时刻的$s_t$开始的累计期望折扣奖励：<script type="math/tex; mode=display">V^{\pi} (s_t) = \mathbb{E}\_{a_{t}, s_{t+1},\cdots\sim \pi}\left[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \right]</script>或者有时候也定义成从$t=0$开始的expected return：<script type="math/tex; mode=display">V^{\pi} (s_0) = \mathbb{E}\_{\pi}\left[G_0|S_0=s_0;\pi\right]=\mathbb{E}\_{\pi}\left[\sum_{t=0}^{\infty} \gamma^t R_{t+1}|S_0=s_0;\pi \right]</script></li><li>动作值函数<br>action value function定义为从$t$时刻的$s_t, a_t$开始的累计期望折扣奖励：<script type="math/tex; mode=display">Q^{\pi} (s_t, a_t) = \mathbb{E}\_{s_{t+1}, a_{t+1},\cdots\sim\pi}\left[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \right]</script>或者有时候也定义为从$t=0$开始的return的期望：<script type="math/tex; mode=display">Q^{\pi} (s_0, a_0) = \mathbb{E}\_{\pi}\left[G_0|S_0=s_0,A_0=a_0;\pi\right]=\mathbb{E}\_{\pi}\left[\sum_{t=0}^{\infty} \gamma^t R_{t+1}|S_0=s_0,A_0=a_0;\pi \right]</script></li><li>优势函数<br>$A^{\pi} (s,a) = Q^{\pi}(s,a) -V^{\pi} (s)$，其中$a_t\sim \pi(a_t|s_t), s_{t+1}\sim P(s_{t+1}|s_t, a_t)$<br>$V^{\pi} (s)$可以看成状态$s$下所有$Q(s,a)$的期望，而$A^{\pi} (s,a)$可以看成当前的单个$Q(s,a)$是否要比$Q(s,a)$的期望要好，如果为正，说明这个$Q$比$Q$的期望要好，否则就不好。</li><li>目标函数<br>Agents的目标是找到一个policy，最大化从state $s_0$开始的expected return：$J(\pi)=\mathbb{E}_{\pi} \left[G_0|\pi\right]$，用$p(s\rightarrow s’,t,\pi)$表示从$s$经过$t$个timesteps到$s’$的概率，用<script type="math/tex; mode=display">\rho^{\pi} (s'):=\int_S \sum_{t=0}^{\infty} \gamma^{t} \rho_0(s_0)p(s_0\rightarrow s', t,\pi)ds_0</script>表示$s’$服从的概率分布，其中$\rho_0(s_0)$是初始状态$s_0$服从的概率分布。我们可以将performance objective表示成在state distribution $\rho^\pi $和policy $\pi_\theta$上的期望：<br>\begin{align*}<br>J(\pi_{\theta}) &amp;= \int_S \rho^{\pi} (s) \int_A \pi_{\theta}(s,a) R(s,a)dads\\\\<br>&amp;= \mathbb{E}_{s\sim \rho^{\pi} , a\sim \pi_{\theta}}\left[R(s,a)\right] \tag{1}\\\ <br>\end{align*}<br>其中$\rho^{\pi} (s)$可以理解为$\rho^{\pi} (s) = P(s_0 = s) +\gamma P(s_1=s) + \gamma^2 P(s_2 = s)+\cdots$，就是policy $\pi$下state $s$出现的概率。这里在每一个$t$处，$s_t=s$都是有一定概率发生的，也就是$\rho_{\pi}(s)$表示的东西。</li></ol><h2 id="A-natural-policy-gradient"><a href="#A-natural-policy-gradient" class="headerlink" title="A natural policy gradient"></a>A natural policy gradient</h2><p>论文名称：A Natural Policy Gradient<br>论文地址：<a href="https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf" target="_blank" rel="noopener">https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf</a></p><h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>作者基于参数空间的底层结构提出了natural gradient方法，找出下降最快方向。尽管gradient方法不能过大的改变参数，它还是能够朝着选择greedy optimal action而不是更好的action方向移动。基于兼容值函数的policy iteration，在每一个improvement step选择greedy optimal action。</p><h3 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h3><p>直接的policy gradient在解决大规模的MDPs时很有用，这种方法基于future reward的梯度在满足约束条件的一类polices中找一个$\pi$，但是这种方法是non covariant的，简单来说，就是左右两边的维度不一致。<br>这篇文章基于policy的底层参数结构定义了一个metric，提出了一个covariant gradient方法，通过将它和policy iteration联系起来，可以证明natural gradient朝着选择greedy optimal action的方向移动。通过在简单和复杂的MDP中进行测试，结果表明这种方法中没有出现严重的plateau phenomenon。</p><h3 id="A-Natural-Gradient"><a href="#A-Natural-Gradient" class="headerlink" title="A Natural Gradient"></a>A Natural Gradient</h3><p>定义average reward $\eta(\pi)$为：</p><script type="math/tex; mode=display">\eta(\pi) = \sum_{s,a}\rho^{\pi} (s) \pi(a;s) R(s, a)</script><p>这篇文章中定义了新的state action value和value function：</p><script type="math/tex; mode=display">Q^{\pi} (s,a) = \mathbb{E}\_{\pi}\left[\right]</script><p>Average reward的精确梯度是：</p><script type="math/tex; mode=display">\nabla\eta(\theta) = \sum_{s,a} \rho^{\pi} (s) \nabla \pi(a;s,\theta) Q^{\pi} (s,a)</script><p>在这使用$\eta(\theta)$代替了$\eta(\pi_{\theta})$。$\eta(\theta)$下降最快的方向定义为在$d\theta$的平方长度$\vert d\theta\vert^2 $ 等于一个常数时，使得$\eta(\theta+d\theta)$最小的$d\theta$的方向。平方长度的定义和一个正定矩阵$G(\theta)$有关，即：</p><script type="math/tex; mode=display">\vert\theta\vert^2 = \sum_{ij} G_{ij} (\theta)d\theta_i d\theta_j = d\theta^T G(\theta) d\theta</script><p>可以证明，最块的梯度下降方向是$G^{-1} \nabla \eta(\theta)$。标准的policy gradient假设$\mathbf{G}=\mathbf{I}$，所以最陡的下降方向是$\nabla\eta(\theta)$。作者的想法是选择一个其他的$\mathbf{G}$，新的metric不根据坐标轴的选择而变化，而是跟着坐标参数化的mainfold变化。根据新的metric定义natural gradient。<br>分布$\pi(a;s,\theta)$的fisher information是：</p><script type="math/tex; mode=display">\mathbf{F}_s(\theta) = \mathbb{E}\_{\pi(a;s,\theta) \left[\frac{\partial \log \pi(a;s,\theta)}{\partial \theta_i}\frac{\partial \log \pi(a;s,\theta)}{\partial \theta_j}\right]</script><p>显然$\mathbf{F}_s$是正定矩阵，可以证明，FIM是概率分布参数空间上的一个invariant metric。不论两个点的坐标怎么选择，它都能计算处相同的distance，所以说它是invariant。<br>当然，$\mathbf{F}_s$只用了单个的$s$，而在计算average reward时，使用的是一个分布，定义metric为：</p><script type="math/tex; mode=display">\mathbf{F}(\theta) = \mathbbf{E}\_{\rho^{pi} (s)} \left[\mathbb{F}_s (\theta)\right]</script><p>每一个$s$对应的单个$\mathbf{F}_s$都和MDP的transition model没有关系，期望操作引入了每一个transition model的参数。直观上来说，$\mathbf{F}_s$测量的是在$s$上的probability manifold的距离，$\mathbf{F}(\theta)$对它们进行了平均。对应的下降最快的方向是：</p><script type="math/tex; mode=display">\hat{\nabla}\eta(\theta) =\mathbf{F}(\theta)^{-1} \nabla\eta(\theta)</script><h3 id="The-Natural-Gradient-和-Policy-Iteration"><a href="#The-Natural-Gradient-和-Policy-Iteration" class="headerlink" title="The Natural Gradient 和 Policy Iteration"></a>The Natural Gradient 和 Policy Iteration</h3><p>使用$\omega$参数化的兼容性值函数$f^{\pi} (s,a;\omega)$近似$Q^{\pi} (s,a)$。</p><h4 id="兼容性值函数"><a href="#兼容性值函数" class="headerlink" title="兼容性值函数"></a>兼容性值函数</h4><p>定义：</p><script type="math/tex; mode=display">\(s,a)^{\pi} = \nabla \log \pi(a;s, \theta)\qquad f^{\pi} (s,a;\omega) = \omega^T \^{\pi} (s,a)</script><h2 id="Trust-Region-Policy-Optimization"><a href="#Trust-Region-Policy-Optimization" class="headerlink" title="Trust Region Policy Optimization"></a>Trust Region Policy Optimization</h2><p>作者提出了optimizing policies的一个迭代算法，理论上保证可以以non-trivial steps单调改善plicy。对经过理论验证的算法做一些近似，产生一个实用算法，叫做Trust Region Policy Optimization(TRPO)。这个算法和natural policy gradient很像，并且在大的非线性网络优化问题上有很高的效率。TRPO有两个变种，single-path方法应用在model-free环境中，vine方法，需要整个system能够能够从特定的states重启，通常在仿真环境中可用。<br>为什么要有TRPO？</p><ol><li>policy gradient计算的是expected rewards梯度的最大方向，然后朝着这个方向更新policy的参数。因为梯度使用的是一阶导数，梯度太大时容易fail，梯度太小的话更新太慢。</li><li>学习率很难选择，学习率固定，梯度大容易失败，梯度小更新太慢。</li><li>如何限制policy，防止它进行太大的move。然后如何将policy的改变转换到model parameter的改变上。</li><li>采样效率很低。对整个trajectory进行采样，但是仅仅用于一次policy update。在一个trajectory中的states是很像的，尤其是用pixels表示时。如果在每一个timestep都改进policy的话，会一直在某一个局部进行更新，训练会变得很不稳定。</li></ol><h2 id="Minorize-Maximization-MM算法"><a href="#Minorize-Maximization-MM算法" class="headerlink" title="Minorize-Maximization MM算法"></a>Minorize-Maximization MM算法</h2><p><img src="/2019/09/08/trust-region-policy-optimization/mm.jpeg" alt="mm"><br>如上图所示，通过迭代的最大化下界函数局部地逼近expected reward。更详细的来说，随机的初始化$\theta$，在当前$\theta$下，找到下界$M$最接近expected reward $\eta$的点，然后将$M$的最优点作为下一次的$\theta$。不断的迭代，直到收敛到optimal policy。这样做有一个条件，就是$M$要比$\eta$容易优化。比如$M$是二次函数：</p><script type="math/tex; mode=display">ax^2 + bx+c</script><p>用向量形式表示是：</p><script type="math/tex; mode=display">g\cdot(\theta- \theta_{old}) - \frac{\beta}{2} (\theta- \theta_{old})^T F(\theta - \theta_{old})</script><p>是一个convex function。<br>为什么MM算法会收敛到optimal policy，如果$M$是下界的话，它不会跨过红线$\eta$。假设新的$\eta$中的new policy更低，那么blue线一定会越过$\eta$，和$M$是下界冲突。</p><h2 id="Trust-region"><a href="#Trust-region" class="headerlink" title="Trust region"></a>Trust region</h2><p>有两种优化方法：line search和trust region。Gradient descent是line search方法。首先确定下降的方向，然后超这个方向移动一步。而trust region中，首先确定我们想要探索的step size，然后直到在trust region中的optimal point。用$\delta$表示初始的maximum step size，作为trust region的半径：</p><script type="math/tex; mode=display">max_{s\in \mathbb{R}^n} m_k(s), \qquad s.t. \vert s\vert \le \delta</script><p>$m$是原始目标函数$f$的近似，我们的目标是找到半径$\delta$范围$m$的最优点，迭代下去直到最高点。在运行时可以根据表面的曲率延伸或者压缩$\delta$控制学习的速度。如果在optimal point，$m$是$f$的一个poor approximator，收缩trust region。如果approximatation很好，就expand trust region。如果policy改变太多的话，可以收缩trust region。</p><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>每一次策略$\pi$的更新，都能使得$\eta(\pi)$单调递增。要是能将它写成old poliy $\pi$和new policy $\hat{\pi}$的关系式就好啦。这里就给出这样一个关系式！恩！就是！</p><script type="math/tex; mode=display">\eta(\hat{\pi}) = \eta(\pi) + \mathbb{E}\_{s_0, a_0, \cdots \sim \hat{\pi}} \left[\sum_{t=0}^{\infty} \gamma^t A^{\pi}(s_t,a_t)\right] \tag{1}</script><p>证明：<br>\begin{align*}<br>\mathbb{E}_{s_0, a_0,\cdots\sim \hat{\pi} }\left[\sum_{t=0}^{\infty} \gamma^t A^{\pi} (s_t,a_t) \right] &amp;=\mathbb{E}_{s_0, a_0,\cdots\sim \hat{\pi}}\left[\sum_{t=0}^{\infty} \gamma^t (Q^{\pi} (s_t,a_t) - V^{\pi} (s_t))\right]  \\\\<br>&amp;=\mathbb{E}_{s_0, a_0,\cdots\sim \hat{\pi}} \left[\sum_{t=0}^{\infty} \gamma^t ( R_{t+1} + \gamma V^{\pi} (s_{t+1}) -  V^{\pi} (s_t))\right]  \\\\<br>&amp;=\mathbb{E}_{s_0, a_0,\cdots\sim \hat{\pi}} \left[\sum_{t=0}^{\infty} \gamma^t R_{t+1} + \sum_{t=0}^{\infty} \gamma^t (\gamma V^{\pi} (s_{t+1}) -  V^{\pi} (s_t))\right]  \\\\<br>&amp;=\mathbb{E}_{s_0, a_0,\cdots\sim \hat{\pi}} \left[\sum_{t=0}^{\infty} \gamma^t R_{t+1} \right]+ \mathbb{E}_{s_0, a_0,\cdots\sim \hat{\pi}} \left[\sum_{t=0}^{\infty} \gamma^t (\gamma V^{\pi} (s_{t+1}) -  V^{\pi} (s_t))\right]  \\\\<br>&amp;=\eta(\hat{\pi}) + \mathbb{E}_{s_0, a_0,\cdots\sim \hat{\pi}} \left[ -  V^{\pi} (s_0))\right]  \\\\<br>&amp;=\eta(\hat{\pi}) - \mathbb{E}_{s_0, a_0,\cdots\sim \hat{\pi}} \left[ V^{\pi} (s_0))\right]  \\\\<br>&amp;=\eta(\hat{\pi}) - \eta(\pi)\\\\<br>\end{align*}<br>将new policy $\hat{\pi}$的期望回报表示为old policy $\pi$的期望回报加上另一项，只要保证这一项是非负的即可。其中$\mathbb{E}_{s_0, a_0,\cdots, \sim \hat{\pi}}\left[\cdots\right]$表示actions是从$a_t\sim\hat{\pi}(\cdot|s_t)$得到的。</p><h2 id="用求和代替期望"><a href="#用求和代替期望" class="headerlink" title="用求和代替期望"></a>用求和代替期望</h2><p>代入$s$的概率分布$\rho_{\pi}(s) = P(s_0 = s) +\gamma P(s_1=s) + \gamma^2 P(s_2 = s)+\cdots, s_0\sim \rho_0$，并将期望换成求和：<br>\begin{align*}<br>\eta(\hat{\pi}) &amp;= \eta(\pi) + \mathbb{E}_{s_0, a_0, \cdots \sim \hat{\pi}} \left[\sum_{t=0}^{\infty} \gamma^t A^{\pi}(s_t,a_t)\right]\\\\<br>&amp;=\eta(\pi) +\sum_{t=0}^{\infty} \sum_s P(s_t=s|\hat{\pi}) \sum_a \hat{\pi}(a|s)\gamma^t A^{\pi}(s,a)\\\\<br>&amp;=\eta(\pi) +\sum_s\sum_{t=0}^{\infty} \gamma^t P(s_t=s|\hat{\pi}) \sum_a \hat{\pi}(a|s)A^{\pi}(s,a)\\\\<br>&amp;=\eta(\pi) + \sum_s \rho_{\hat{\pi}}(s) \sum_a \hat{\pi}(a|s) A^{\pi} (s,a) \tag{2}\\\\<br>\end{align*}<br>从上面的推导可以看出来，任何从$\pi$到$\hat{\pi}$的更新，只要保证每个state $s$处的expected advantage是非负的，即$\sum_a \hat{\pi}(a|s) A_{\pi}(s,a)\ge 0$，就能说明$\hat{\pi}$要比$\pi$好，在$s$处，新的policy $\hat{\pi}$:</p><script type="math/tex; mode=display">\hat{\pi}(s) = arg\ max_a A^{\pi} (s,a) \tag{3}</script><p>直到所有$s$处的$A^{\pi} (s,a)$为非正停止。当然，在实际应用中，因为各种误差，可能会有一些state的expected advantage是负的。</p><h2 id="rho-pi-s-近似-rho-hat-pi-s-（第一次近似）"><a href="#rho-pi-s-近似-rho-hat-pi-s-（第一次近似）" class="headerlink" title="$\rho_{\pi}(s)$近似$\rho_{\hat{\pi}}(s)$（第一次近似）"></a>$\rho_{\pi}(s)$近似$\rho_{\hat{\pi}}(s)$（第一次近似）</h2><p>上式中包含$\rho_{\hat{\pi}}$，依赖于$\hat{\pi}$，很难直接优化，作者就进行了一个近似：</p><script type="math/tex; mode=display">L_{\pi} (\hat{\pi}) = \eta(\pi) + \sum_s\rho_{\pi}(s)\sum_a\hat{\pi}(a|s)A^{\pi} (s,a) \tag{4}</script><script type="math/tex; mode=display">\eta (\hat{\pi}) = \eta(\pi) + \sum_s\rho_{\hat{\pi}}(s)\sum_a\hat{\pi}(a|s)A^{\pi} (s,a)</script><p>在$L_{\pi}(\hat{\pi} )$中用$\rho_{\pi}(s)$代替$\rho_{\hat{\pi}}(s)$，从而忽略因为policy改变导致的state访问频率的改变。当$\pi(a|s)$可导时，用$\pi_{\theta}$表示policy，用$\theta$表示$\pi$的参数，则$L_{\pi}(\hat{\pi})$和$\eta(\hat{\pi})$的一阶导相等；当$\hat{\pi} = \pi$时，$L_{\pi}(\hat{\pi}) = \eta(\hat{\pi})$</p><script type="math/tex; mode=display">L_{\pi_{\theta_0}} (\pi_{\theta_0}) = \eta(\pi_{\theta_0}) \tag{5}</script><script type="math/tex; mode=display">\nabla_{\theta} L_{\pi_{\theta_0}}(\pi_{\theta})|\_{\theta=\theta_0} =\nabla_{\theta} \eta(\pi_{\theta})|\_{\theta=\theta_0}\tag{6}</script><p>证明：<br>第一个式子不需要证明，而第二个式子，左边为$\eta(\pi) + \sum_s\rho_{\pi}(s)\sum_a\hat{\pi}(a|s)A^{\pi} (s,a)$，右边为$\eta(\pi) + \sum_s\rho_{\hat{\pi}}(s)\sum_a\hat{\pi}(a|s)A^{\pi} (s,a)$，分别求它们关于$\theta$的导数。$\pi$是已知量，$\hat{\pi}$是关于$\theta$的函数，$\rho_{\hat{\pi}}$是通过样本得到的，不是关于$\hat{\pi}$的函数，最后相当于只有$\hat{\pi}(a|s)$是关于$\hat{\pi}$的函数，所以左右两边就一样了。。（！！！有疑问，就是为什么？$\rho_{\hat{\pi}}$到底是怎么求的，怎么证明）<br>也就是说当$\hat{\pi} = \pi$时，$L_{\pi}(\pi)$和$\eta(\pi)$是相等的，在$\pi$对应的参数$\theta$周围的无穷小范围内，可以近似认为它们依然相等。$\pi$的参数$\theta_{\pi}$进行足够小的step更新到达新的policy $\hat{\pi}$，相应参数为$\theta_{\hat{\pi}}$，在改进$L_{\pi}$同时也改进了$\eta$，但是这个足够小的step是多少是不知道的。</p><h2 id="conservative-policy-iteration"><a href="#conservative-policy-iteration" class="headerlink" title="conservative policy iteration"></a>conservative policy iteration</h2><p>为了求出这个step到底是多少，有人提出了conservative policy iteration算法，该算法提供了$\eta$提高的一个lower bound。用$\pi_{old}$表示current policy，用$\pi’$表示使得$L_{\pi_{old}}$取得最大值的policy，$\pi’ = arg min_{\pi’} L_{\pi_{old}}(\pi’)$，新的policy $\pi_{new}$定义为：</p><script type="math/tex; mode=display">\pi_{new}(a|s) = (1-\alpha) \pi_{old}(a|s)+\alpha\pi'(a|s) \tag{7}</script><p>可以证明，新的policy $\pi_{new}$和老的policy $\pi_{old}$之间存在以下关系：</p><script type="math/tex; mode=display">\eta(\pi_{new})\ge L_{\pi_{old}}(\pi_{new}) - \frac{2\epsilon \gamma}{(1-\gamma(1-\alpha))(1-\gamma)}\alpha^2 , \epsilon = max_s \vert\mathbb{E}\_{a\sim\pi'}\left[A^{\pi} (s,a)\right]\vert \tag{8}</script><p>证明：<br>进行缩放得到：</p><script type="math/tex; mode=display">\eta(\pi_{new})\ge L_{\pi_{old}}(\pi_{new}) - \frac{2\epsilon \gamma}{(1-\gamma)^2 }\alpha^2 \tag{9}</script><h2 id="通用随机策略单调增加的证明"><a href="#通用随机策略单调增加的证明" class="headerlink" title="通用随机策略单调增加的证明"></a>通用随机策略单调增加的证明</h2><p>从公式$9$我们可以看出来，改进右边就一定能改进真实的performance $\eta$。然而，这个bound只适用于通过公式$7$生成的混合policy，在实践中，这类policy很少用到，而且限制条件很多。所以我们想要的是一个适用于任何stochastic policy的lower bound，通过提升这个bound提升$\eta$。<br>作者使用$\pi$和$\hat{\pi}$之间的一个距离代替$\alpha$，将公式$8$扩展到了任意stochastic policy，而不仅仅是混合policy。这里使用的distance measure，叫做total variation divergence，对于离散的概率分布$p,q$来说，定义为：</p><script type="math/tex; mode=display">D_{TV}(p||q) = \frac{1}{2} \sum_i \vert p_i -q_i \vert \tag{10}</script><p>定义$D_{TV}^{max}(\pi, \hat{\pi})$为：</p><script type="math/tex; mode=display">D_{TV}^{max} (\pi, \hat{\pi}) = max_s D_{TV}(\pi(\cdot|s) || \hat{\pi}(\cdot|s))\tag{11}</script><p>让$\alpha = D_{TV}^{max}(\pi_{old}, \pi_{new})$，新的bound如下：</p><script type="math/tex; mode=display">\eta(\pi_{new})\ge L_{\pi_{old}}(\pi_{new}) - \frac{4\epsilon \gamma}{(1-\gamma)^2 }\alpha^2 , \qquad\epsilon = max_{s,a} \vert A^{\pi}(s,a)\vert \tag{12}</script><p>证明：<br>…</p><p>Total variation divergence和KL散度之间有这样一个关系：</p><script type="math/tex; mode=display">D_{TV}(p||q)^2 \le D_{KL}(p||q) \tag{13}</script><p>证明：<br>…<br>让</p><script type="math/tex; mode=display">D_{KL}^{max}(\pi, \hat{\pi}) = max_s D_{KL}(\pi(\cdot|s)||\hat{\pi}(\cdot|s)) \tag{14}</script><p>从公式$12$中可以直接得到：<br>\begin{align*}<br>\eta(\hat{\pi}) &amp;\ge L_{\pi}(\hat{\pi}) - \frac{4\epsilon \gamma}{(1-\gamma)^2 }\alpha^2 \\\\<br>&amp;\ge L_{\pi}(\hat{\pi}) - \frac{4\epsilon \gamma}{(1-\gamma)^2 }D_{KL}^{max}(\pi, \hat{\pi}) \\\\<br>&amp; \ge L_{\pi}(\hat{\pi}) - CD_{KL}^{max}(\pi, \hat{\pi}), C=\frac{4\epsilon \gamma}{(1-\gamma)^2} \tag{15}<br>\end{align*}<br>根据公式$12$，我们能生成一个单调非递减的sequence：$\eta(\pi_0)\le \eta(\pi_1) \le \eta(\pi_2) \le \cdots$，记$M_i(\pi) = L_{\pi_i}(\pi) - CD_{KL}^{max}(\pi_i, \pi)$，有：<br>因为：</p><script type="math/tex; mode=display">\eta(\pi_{i+1}) \ge M_i(\pi_{i+1})</script><script type="math/tex; mode=display">\eta(\pi_i) = M_i(\pi_i)</script><p>上面的第一个式子减去第二个式子得到：</p><script type="math/tex; mode=display">\eta(\pi_{i+1}) - \eta(\pi_i)\ge M_i(\pi_{i+1})-M_i(\pi_i) \tag{16}</script><p>在每一次迭代的时候，确保$M_i(\pi_{i+1}) - M_i(\pi_i)\ge 0$就能够保证$\eta$是非递减的，最大化$M_i$就能实现这个目标，$M_i$是miorize $\eta$的近似目标。这种算法是minorizaiton maximization的一种。</p><h2 id="参数化策略的优化（第二次近似）"><a href="#参数化策略的优化（第二次近似）" class="headerlink" title="参数化策略的优化（第二次近似）"></a>参数化策略的优化（第二次近似）</h2><p>前面几小节考虑的optimization问题时没有考虑$\pi$的参数化，并且假设所有的states都可以被evaluated。这一节介绍如何在有限的样本下和任意的参数化策略下，从理论基础推导出一个实用的算法。<br>用$\theta$表示参数化策略$\pi_{\theta}(a|s)$的参数$\theta$，将目标表示成$\theta$而不是$\pi$的函数，即用$\eta(\theta)$表示原来的$\eta(\pi_\theta)$，用$L_{\theta}(\hat{\theta})$表示$L_{\pi_{\theta}}(\pi_{\hat{\theta}})$，用$D_{KL}(\theta||\hat{\theta})$表示$D_{KL}(\pi_{\theta}||\pi_{\hat{\theta}})$。用$\theta_{old}$表示我们想要改进的policy参数。<br>上一小节我们得到$\eta(\theta) \ge L_{\theta_{old}}(\theta) - CD_{KL}^{max}(\theta_{old}, \theta)$，当$\theta = \theta_{old}$时取等。通过最大化等式右边，可以提高$\eta$的下界：</p><script type="math/tex; mode=display">maximize_{\theta}\left[L_{\theta_{old}}(\theta) - CD_{KL}^{max}(\theta_{old}, \theta)\right]\tag{17}</script><p>在实践中，如果使用上述理论中的penalty coefficient $C$，会导致steps size很小。一种方法是使用new policy 和old policy之间的KL散度进行约束，可以采取更大的steps，这个约束叫做trust region constraint:</p><script type="math/tex; mode=display">maxmize_{\theta} L_{\theta_{old}} (\theta),\qquad s.t. D_{KL}^{max}(\theta_{old},\theta) \le \delta \tag{18}</script><p>这样会在state space的每一个state都有一个KL散度约束。由于约束太多，这个问题还是不能解。这里使用average KL divergence进行近似:</p><script type="math/tex; mode=display">\bar{D}\_{KL}^{\rho}(\theta_1, \theta_2) = \mathbb{E}\_{s\sim \rho}\left[D_{KL}(\pi_{\theta_1}(\cdot|s) || \pi_{\theta_2}(\cdot|s))\right] \tag{19}</script><p>公式$15$变成：</p><script type="math/tex; mode=display">maxmize_{\theta} L_{\theta_{old}} (\theta), \qquad s.t. \bar{D}\_{KL}^{\rho_{\theta_{old}}}(\theta_{old},\theta) \le \delta \tag{20}</script><h2 id="目标函数和约束的采样估计（第三次近似）"><a href="#目标函数和约束的采样估计（第三次近似）" class="headerlink" title="目标函数和约束的采样估计（第三次近似）"></a>目标函数和约束的采样估计（第三次近似）</h2><p>上一节介绍的是关于policy parameter的有约束优化问题，约束条件为每一次policy更新时限制policy变化的大小，优化expected toral reward $\eta$的一个估计值。这一节使用Monte Carlo仿真近似目标和约束函数。<br>代入$L_{\theta_{old}}$的等式，得到：</p><script type="math/tex; mode=display">maxmize_{\theta}\sum_s \rho_{\theta_{old}}(s) \sum_a\pi_{\theta}(a|s)A_{\theta_{old}}(s,a), \qquad s.t. \bar{D}\_{KL}^{\rho_{\theta_{old}}}(\theta_{old},\theta) \le \delta \tag{21}</script><p>首先用期望$\frac{1}{1-\gamma}\mathbb{E}_{s\sim \rho_{\theta_{old}}}\left[\cdots\right]$代替目标函数中的$\sum_s\rho_{\theta_{old}}(s) \left[\cdots\right]$。接下来用$Q$值$Q_{\theta_{old}}$代替advantage $A_{\theta_{old}}$，结果多了一个常数项，不影响。最后使用importance smapling代替actions上的求和。使用$q$表示采样分布，$q$分布中单个的$s_n$对于loss函数的贡献在于：</p><script type="math/tex; mode=display">\sum_a \pi_{\theta}(a|s_n) A_{\theta_{old}}(s_n,a) = \mathbb{E}\_{a\sim q}\left[\frac{\pi_{\theta} (a|s_n) }{q(a|s_n)}A_{\theta_{old}}(s_n,a) \right]</script><p>上面的公式就是使用importance sampling代替求和。将$A$展开：<br>\begin{align*}<br>\sum_a \pi_{\theta}(a|s) A_{\theta_{old}}(s,a) &amp;= \sum_a \pi_{\theta}(a|s)\left( Q_{\theta_{old}}(s,a)  - V_{\theta_{old}}(s)\right)\\\\<br>&amp;= \sum_a \pi_{\theta}(a|s)Q_{\theta_{old}}(s,a)- \sum_a \pi_{\theta}(a|s)V_{\theta_{old}}(s)\\\\<br>&amp;= \sum_a \pi_{\theta}(a|s)Q_{\theta_{old}}(s,a)- V_{\theta_{old}}(s)\\\\<br>\end{align*}<br>将公式$17$的优化问题转化为：</p><script type="math/tex; mode=display">maxmize_{\theta} \mathbb{E}\_{s\sim\rho_{\theta_{old}}, a\sim q}\left[\frac{\pi_{\theta} (a|s) }{q(a|s)}Q_{\theta_{old}}(s,a)\right] \qquad s.t. \mathbb{E}\_{s\sim \rho_{\theta_{old}}}\left[D_{KL}(\pi_{\theta_{old}}(\cdot|s)||\pi_{\theta}(\cdot|s))\right]\le \delta \tag{22}</script><p>接下来要做的就是用采样代替期望，用经验估计代替$Q$值。接下来会介绍两种方法进行估计。</p><p>第一个叫做single path，通常用在policy gradient estimation，基于单个轨迹的采样。第二个叫做vine，构建一个rollout set，从rollout set的每一个state处执行多个actions。这种方法经常用在policy iteration方法上。</p><h3 id="Single-Path"><a href="#Single-Path" class="headerlink" title="Single Path"></a>Single Path</h3><p>采样$s_0\sim \rho_0$，模拟policy $\pi_{\theta_{old}}$一些timesteps生成一个trajectory $s_0, a_0, s_1, a_1, \cdots, s_{T-1}, a_{T-1}, s_T$，因此$q(a|s) = \pi_{\theta_{old}}(a|s)$。根据trajectory对每一个state action pair $(s_t,a_t)$计算$Q_{\theta_{old}}(s,a)$。</p><h3 id="Vine"><a href="#Vine" class="headerlink" title="Vine"></a>Vine</h3><p>采样$s_0\sim \rho_0$，模拟policy $\pi_{\theta_i}$生成一系列trajectories。在这些trajectories选择一个具有$N$个states的子集，表示为$s_1, c\dots, s_N$，这个集合称为rollout set。对于rollout set中的每一个state $s_n$，根据$a_{n,k}\sim q(\cdot|s_n)$采样$K$个actions。任何$q(\cdot|s_n)$都行，在实践中，$q(\cdot|s_n) = \pi_{\theta_i}(\cdot|s_n)$适用于contionous problems，像机器人运动；而均匀分布适用于离散任务，如Atari游戏。<br>对于$s_n$处的每一个action $a_{n,k}$，从$s_n$和$a_{n,k}$处进行rollout，估计$\hat{Q}_{\theta_i}(s_n, a_{n,k})$。在小的有限action spaces情况下，我们可以对从给定状态任何可能的action生成一个rollout，单个$s_n$对$L_{\theta_{old}}$的贡献如下：</p><script type="math/tex; mode=display">L_n(\theta) = \sum_{k=1}^K \pi_{\theta} (a_k|s_n) \hat{Q}(s_n, a_k)</script><p>其中action space是$\mathcal{A} = \{a_1, a_2,\cdots, a_K\}$。在大的连续state space中，可以使用importance sampling构建一个新的目标近似。从$s_n$处计算的$L_{\theta_{old}}$的self-normalized 估计是：</p><script type="math/tex; mode=display">L_n(\theta) = \frac{\sum_{k=1}^K \frac{\pi_{\theta}(a_{n,k}|s_n)}{\pi_{\theta_{old}}(a_{n,k}|s_n)}\hat{Q}(s_n, a_{n,k}}{\sum_{k=1}^K \frac{\pi_{\theta}(a_{n,k}|s_n)}{\pi_{\theta_{old}}(a_{n,k}|s_n)}}</script><p>假设在$s_n$处执行了$K$个actions $a_{n,1}, a_{n,2}, \cdots, a_{n,K}$。Self-normalized 估计去掉了$Q$值baseline的需要。在$s_n\sim \rho(\pi)$上做平均，可以得到$L_{\theta_{old}}$和它的gradient的估计。<br>Vine比single path好的地方在于，给定相同数量的$Q$样本，目标函数的局部估计有更低的方差，也就是vine能更好的估计advantage。Vine的缺点在于，需要执行更多steps的模拟计算相应的advantage。此外，vine方法需要对rollout set 中的每一个state都生成多个trajectories，这就需要整个system可以重置到任意的一个state，而single path算法不需要，可以直接应用在真实的system中。</p><h2 id="实用算法"><a href="#实用算法" class="headerlink" title="实用算法"></a>实用算法</h2><p>使用上面介绍的single path或者vine进行采样，给出两个算法。重复执行以下步骤：</p><ol><li>使用single path或者vine算法产生一系列state-action pairs，使用Monte Carlo估计相应的$Q$值；</li><li>利用样本计算公式$18$中目标函数和约束函数的估计值</li><li>求出有约束优化问题的近似解，更新policy参数$\theta$，使用共轭梯度和line search。</li></ol><p>对于$3$来说，计算KL散度的Hessian矩阵而不是协方差矩阵的梯度，生成Fisher information matrix。即使用$\frac{1}{N}\sum_{n=1}^N \frac{\partial^2}{\partial \theta_j}D_{KL}(\pi_{\theta_{old}}(\cdot|s_n)||\pi_{\theta}(\cdot|s_n))$近似$A_{ij}$而不是$\frac{1}{N}\sum_{n=1}^N \frac{\partial}{\partial \theta_i}log(\pi_{\theta}(a_n|s_n))\frac{\partial}{\partial \partial_j}log(\pi_{\theta}(a_n|s_n))$。<br>在前面介绍的理论和本节介绍的算法之间有以下关联：</p><ol><li>理论上验证了优化带有KL散度penalty的目标函数可以保证policy improvement是单调递增的。这个很大的penalty系数$C$会产生很小的step，所以我们想要减小这个系数。经验上来讲，很难选择一个鲁邦的penalty系数，所以我们使用一个KL散度上的一个hard constraint而不是一个penalty。</li><li>$D_{KL}^{max}(\theta_{old}, \theta)$是很难计算和估计的，所以将约束条件改成对期望$\bar{D}_{KL}(\theta_{old}, \theta)$进行约束。</li><li>本文的理论忽略了advantage function的近似误差。</li></ol><h2 id="和前面工作的联系"><a href="#和前面工作的联系" class="headerlink" title="和前面工作的联系"></a>和前面工作的联系</h2><p>本问的推导结果和一些之前的方法有联系，他们可以统一在policy update框架下。The natural policy gradient可以看成公式$16$的一个特例：使用$L$的一个linear approximation，和$\bar{D}_{KL}$的一个二次估计，就变成了下面的优化问题：</p><script type="math/tex; mode=display">maximize_{\theta} \left[\nabla_{\theta}L_{\theta_{old}}(\theta)|\_{\theta=\theta_{old}}\cdot (\theta-\theta_{old}) \right] \qquad s.t. \frac{1}{2}(\theta_{old}-\theta)^A(\theta_{old})(\theta_{old} - \theta)\le\delta</script><p>其中$A(\theta_{old})_{ij} = \frac{\partial}{\partial\theta_i}\frac{\partial}{\partial \theta_j}\mathbb{E}_{s\sim \rho_{\pi}}\left[D_{KL}(\pi(\cdot|s, \theta_{old})||\pi(\cdot|s, \theta))\right]_{\theta=\theta_{old}}$，更新公式是$\theta_{new} = \theta_{old}+\frac{1}{\lambda}A(\theta_{old})^{-1} \nabla_{\theta}L(\theta)|_{\theta=\theta_{old}}$，其中步长$\frac{1}{\lambda}$可以看成算法参数。这和trpo不同，在每一次更新都有constraint。尽管这个差别很小，实验表明它能改善在更大规模问题上算法的性能。<br>同样，也可以使用$l2$约束，推导出标准的policy gradient如下：</p><script type="math/tex; mode=display">maximize_{\theta} \left[\nabla_{\theta} L_{\theta_{old}}(\theta)|\_{\theta=\theta_{old}}\cdot (\theta- \theta_{old}) \qquad s.t. \frac{1}{2}\vert \theta-\theta_{old}\vert^2 \le \delta\right]</script><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>Trust Region Policy Optimization<br>1.<a href="http://joschu.net/docs/thesis.pdf" target="_blank" rel="noopener">http://joschu.net/docs/thesis.pdf</a><br>2.<a href="https://arxiv.org/pdf/1502.05477.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1502.05477.pdf</a><br>3.<a href="https://medium.com/@jonathan_hui/rl-trust-region-policy-optimization-trpo-explained-a6ee04eeeee9" target="_blank" rel="noopener">https://medium.com/@jonathan_hui/rl-trust-region-policy-optimization-trpo-explained-a6ee04eeeee9</a><br>4.<a href="https://medium.com/@jonathan_hui/rl-trust-region-policy-optimization-trpo-part-2-f51e3b2e373a" target="_blank" rel="noopener">https://medium.com/@jonathan_hui/rl-trust-region-policy-optimization-trpo-part-2-f51e3b2e373a</a><br>5.<a href="https://people.eecs.berkeley.edu/~pabbeel/cs287-fa09/readings/KakadeLangford-icml2002.pdf" target="_blank" rel="noopener">https://people.eecs.berkeley.edu/~pabbeel/cs287-fa09/readings/KakadeLangford-icml2002.pdf</a><br>6.<a href="https://drive.google.com/file/d/0BxXI_RttTZAhMVhsNk5VSXU0U3c/view" target="_blank" rel="noopener">https://drive.google.com/file/d/0BxXI_RttTZAhMVhsNk5VSXU0U3c/view</a><br>7.<a href="https://zhuanlan.zhihu.com/p/26308073" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/26308073</a><br>8.<a href="https://zhuanlan.zhihu.com/p/60257706" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/60257706</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;术语定义&quot;&gt;&lt;a href=&quot;#术语定义&quot; class=&quot;headerlink&quot; title=&quot;术语定义&quot;&gt;&lt;/a&gt;术语定义&lt;/h2&gt;&lt;p&gt;更多介绍可以点击查看&lt;a href&gt;reinforcement learning an introduction 第三章&lt;/
      
    
    </summary>
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="reinforcement learning" scheme="http://mxxhcm.github.io/tags/reinforcement-learning/"/>
    
      <category term="trpo" scheme="http://mxxhcm.github.io/tags/trpo/"/>
    
      <category term="trust region policy optimization" scheme="http://mxxhcm.github.io/tags/trust-region-policy-optimization/"/>
    
  </entry>
  
  <entry>
    <title>generalization in RL</title>
    <link href="http://mxxhcm.github.io/2019/09/05/generalization-in-RL/"/>
    <id>http://mxxhcm.github.io/2019/09/05/generalization-in-RL/</id>
    <published>2019-09-05T04:25:40.000Z</published>
    <updated>2019-09-16T07:53:27.835Z</updated>
    
    <content type="html"><![CDATA[<h2 id="gotta-learn-fast-a-new-benchmark-for-generalization-in-rl">Gotta Learn Fast: A New Benchmark for Generalization in RL</h2><h3 id="下载地址">下载地址</h3><p><a href="https://arxiv.org/pdf/1804.03720.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1804.03720.pdf</a></p><h3 id="摘要">摘要</h3><p>这篇文章基于Sonic the Hedgehog视频游戏提出了Gym Retro rl benchmark。这个benchmark主要用于评估RL领域的transfer learning和few-shot learning算法。</p><h3 id="简介">简介</h3><p>像ALE之类的RL benchmark并不是一个理想的测试相似任务之间泛化性能的benchmark。RL相当于在测试集上进行训练，然后展现出和训练环境相同环境下算法的最终性能。所以，这篇文章给出了一个benchmark，和监督学习一样，区分了train和test环境。<br>作者给出的benchmark设计了一个meta-learning的RL dataset，从单个任务分布中采样很多个相似的任务，可以作为few-shot RL算法的test bed。除了few-shot learning，这个benchmark还可以用来衡量cross-task generalization，train set和test set的设置可以用来在一些levels训练，然后迁移到另一些levels。<br>Gym Retro 和Retro Learning Environment以及Arcade Learning Environment都有关，但是，Gym Retro更灵活，更容易扩展，有相当多的environments。OpenAI想要做的是RL中的Omniglot和mini-ImageNet。<br>在之前RL领域的transfer learning中，主要有两种evaluation techniques，在人工设计的task上进行evaluation，或者在ALE上进行evaluation。人工设计的任务中，许多不同的算法难以进行比较，而ALE中，不同的任务之间差异太大，使用transfer learning不可能获得太大的improvements。</p><h3 id="sonic-benchmark">Sonic benchmark</h3><p>这一章详细介绍了Sonic benchmark，从technical details到高维的特征设计。</p><h4 id="gym-repo">Gym Repo</h4><p>Sonic benchmark的底层是Gym Retro，一个video games仿真RL environments项目。Gym Retro的核心是gym-retro python包，它将各种emulated games都封装成Gym environments，使用统一的接口进行操作。Gym-Retro中的每一个游戏都由一个ROM，一个或多个svae state，一个或多个scenarios以及一个data file组成。</p><ul><li>ROM，组成游戏的data和code，通过一个emulator进行加载。</li><li>Save state，游戏的在某个console状态的截图。</li><li>Data file，描述在console memory中各种各样的信息，比如包含分数在哪里存储。</li><li>Scenario，描述完成条件以及奖励函数。</li></ul><h4 id="the-sonic-video-game">The Sonic Video Game</h4><p>Sonic benchmark包含三个相似的游戏：Sonic The Hedgehog, Sonic The Hedgehog2,Sonic 3 Knuckles。这三个游戏的规则和操作都很像，当然还是有一些差别。通过使用这几个游戏获得尽可能多的environments作为datasets。<br>每一个sonic game被划分为zones，每一个zone又进一步划分为acts，相当于我们说的关卡的意思。整个游戏的规则和目的都是一样的，每一个zone都有独特的textures和objects，一个zone中的不同acts share textures和objects，但是空间布局不同。作者把(ROM, zone, act)称为一个level。</p><h4 id="games-and-levels">Games and Levels</h4><p>整个benchmark包含$3$个游戏的$58$个save states，每一个save states都有different level开始时的player。许多原始游戏中的acts没有被使用，因为它们只包含打boss或者和设计的reward function不兼容。<br>随机的选择包含超过一个act的zones，然后随机的从选中的zone中选择act。Test set包含绝大数在train sets中出现的objects和objects，但是layouts不同。<br>Test levels如下表所示：</p><table><thead><tr><th style="text-align:center">ROM</th><th style="text-align:center">one</th><th style="text-align:center">Act</th></tr></thead><tbody><tr><td style="text-align:center">Sonic The Hedgehog</td><td style="text-align:center">SpringYardZone</td><td style="text-align:center">1</td></tr><tr><td style="text-align:center">Sonic The Hedgehog</td><td style="text-align:center">GreenHillZone</td><td style="text-align:center">2</td></tr><tr><td style="text-align:center">Sonic The Hedgehog</td><td style="text-align:center">StarLightZone</td><td style="text-align:center">3</td></tr><tr><td style="text-align:center">Sonic The Hedgehog</td><td style="text-align:center">ScrapBrainZone</td><td style="text-align:center">1</td></tr><tr><td style="text-align:center">Sonic The Hedgehog 2</td><td style="text-align:center">MetropolisZone</td><td style="text-align:center">3</td></tr><tr><td style="text-align:center">Sonic The Hedgehog 2</td><td style="text-align:center">HillTopZone</td><td style="text-align:center">2</td></tr><tr><td style="text-align:center">Sonic The Hedgehog 2</td><td style="text-align:center">CasinoNightZone</td><td style="text-align:center">2</td></tr><tr><td style="text-align:center">Sonic 3 &amp; Knuckles</td><td style="text-align:center">LavaReefZone</td><td style="text-align:center">1</td></tr><tr><td style="text-align:center">Sonic 3 &amp; Knuckles</td><td style="text-align:center">FlyingBatteryZone</td><td style="text-align:center">2</td></tr><tr><td style="text-align:center">Sonic 3 &amp; Knuckles</td><td style="text-align:center">HydrocityZone</td><td style="text-align:center">1</td></tr><tr><td style="text-align:center">Sonic 3 &amp; Knuckles</td><td style="text-align:center">AngelIslandZone</td><td style="text-align:center">2</td></tr></tbody></table><h4 id="frame-skip">Frame Skip</h4><p>原生的gym-retro environments每隔$\frac{1}{60}$秒调用一次step()方法。在ALE环境的实践中，通常使用长为$4$的fram skip of $4$。使用timesteps作为测量游戏内时间的主要单位，则使用长为$4$的frame skip，一个timesteps代表一秒的$\frac{1}{15}$。而deterministic environments容易受到trivial scripted solutions的影响，作者使用了sticky fram skip，即每一个action有$0.25$的概率延迟一个frame，也就是说这个action aplly了$5$次。</p><h4 id="episode-boundaries">Episode Boundaries</h4><p>游戏中的experience被划分成episodes，对应于存活，在每一个episode的结束，environment重置为save states。每个episode在遇到以下任何一个条件时结束：</p><ul><li>成功的完成这个level，在这个benchmark中，完成一个level对应于通过这个level内一个确定的水平offset。</li><li>玩家失败。</li><li>当前episodes经过了4500个episodes，大概有$300$s，$5$分钟。</li></ul><p>如果满足以上任意一个条件，environment都应该被重置，agents不应该使用任何特殊的APIs告诉environments开始一个新的episode。这个benchmark取消了打boss这一环节，这个难度太大了。</p><h4 id="observations">Observations</h4><p>Gym-Retro环境在每一个timestep开始前产生一个observation，这个observation是一个三通道的图像，不同的游戏维度不同，对于Sonic，images是$320\times 224$大小的。</p><h4 id="actions">Actions</h4><h4 id="rewards">Rewards</h4><p>Reward由两部分组成：一个水平offset和一个完成任务的bonus，每个level的horizontal offset被归一化了，如果它通过了作为终点的那个horizontal offset，每个agent总的reward是$9000$。这样子便于不同长度的level之间进行比较。完成的bonus是$1000$，在$4500$个timesteps内线性减少为$0$，这是为了鼓励agents尽可能快的完成这个游戏。</p><h4 id="evaluation">Evaluation</h4><p>使用test set中所有levels的mean score作为metric。步骤入如下：</p><ol><li>训练时候，使用尽可能少的训练集训练</li><li>测试的时候，每一个test levels执行$100$万个timesteps。在test每一个level的时候都是分开的。</li><li>对每个level的$100$万个timesteps中所有episode的total rewards进行平均。</li><li>对所有test level的scores进行平均。</li></ol><p>这个$100$万是怎么选出来的？在无限个timestep的场景下，没有证明meta-learning或者transfer-learning是必要的，但是在有限个timestep的场景下，transfer learning对于得到好的performance是很必要的。</p><h3 id="baselines">Baselines</h3><p>作者给出了几个方法在benchmark上的效果，包含人类，没有使用训练集的一些方法以及使用joint training加上fine tunning的一些迁移学习方法。</p><ol><li>人类<br>有四个人，每个人先在训练集上玩两个小时，然后在每个test level上玩一个小时。</li><li>Rainbow<br>Rainbow是DQN的变种，加了各种各样的trick。<br>这里对Rainbow做了一些改变：设置$V_{max} = 200$；buffer size为$0.5M$；直接使用了rainbow中的参数初始化方式，没有系列化实验。</li><li>JERK</li><li>PPO<br>每一个PPO单独运行在一个test level上。action和observation space都和rainbow一样，相同的reward preprocessing。同时还使用了一个小的常数对reward进行缩放。CNN结构和dqn一样。</li><li>Joint PPO<br>在所有的training levels上训练一个policy，然后用它作为test levels的初值。<br>在训练过程中，训练一个policy，使用188个parallel的workers，每一个都对应training set中的一个level。在每一gradient step，所有的workers平均梯度，确保policy是在整个训练集上得到的。整个训练过程需要几百万个timesteps才收敛，除了训练的setting不同，其他参数和常规的PPO一样。</li><li>Joint Rainbow<br>没有joint training的Rainbow超过了PPO，但是joint Rainbow没有超过joint PPO。在整个训练集上训练单个Rainbow的时候，使用了$32$个GPUs。每一个GPU对应一个单个的worker，每一个worker有自己的replay buffer和$8$个环境。环境也是joint environments，在每一个episode开始的时候采样一个新的tranining level。<br>除了batch size和distributed worker，其他的超参数和常规的Rainbow一样。</li></ol><h3 id="结果">结果</h3><p>本文提出了一个benchmark，并用这个benchmark评估了几个baselines。结果表明最好的transfer结果没有比重头开始训练的结果好多少。而且离最好的的score（设计时是$9000$到$10000$）有一定距离。</p><h2 id="quantifying-generalization-in-reinforcement-learning">Quantifying Generalization in Reinforcement Learning</h2><h3 id="下载地址-v2">下载地址</h3><p><a href="https://arxiv.org/pdf/1812.02341.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1812.02341.pdf</a></p><h3 id="摘要-v2">摘要</h3><p>这篇文章研究的是drl中的overfitting问题。在绝大多数的RL benchmark中，训练和测试都习惯性的在相同环境中，这就让agent’s的泛化能力不够优秀。本文通过程序生成环境构建不同的训练集和测试集解决这个问题。本文设计了一个新的CoinRun环境用于RL的generalization。<br>作者实验证明更深的cnn能够改善generalization，监督学习中常用的一些方法，如$L2$正则化，dropout，data augmentation以及batch normalization。<br>这篇文章的贡献：</p><ol><li>transfer需要的训练环境数量要远远高于之前的工作中使用到的。</li><li>使用CoinRun benchmark提出了一个generalization metric。</li><li>找到了不同网络结构和正则化项中最好的那些设置。</li></ol><h3 id="简介-v2">简介</h3><p>尽管RL agents能解决很复杂的任务，将experience迁移到新环境，或者在不同的RL tasks之间进行泛化是很困难的。即使已经掌握了$10$ video game的agents，在初次遇到第$11$级时也会失败，agents在训练的时候只学习到了和这个环境相关的知识。<br>RL agents的训练其实是过拟合的，但是绝大多数的benchmark还是鼓励在相同的环境中进行train和evaluation。和Sonic Benchmark一样，作者认为分train和test集是必要的，同时作者认为量化agent的泛化能力也是很有用的。<br>Sonic Benchmark通过对环境划分训练集和测试集，解决了在测试集上训练的问题。而Farebrother认识到，没有划分训练集和测试集，使得RL训练中缺乏正则化手段，通过使用监督学习中的$L2$正则化和dropout，使得agents能够学习更多泛化特征。<br>Zhang等利用程序生成划分train和test环境，他们在gridworld mase上进行实验，得出了许多RL agents为什么过拟合的结论。通过让agents记住在训练集中具体的levels，以及stick actions，random starts，可以减少过拟合的发生。</p><h3 id="quantifying-generalization">Quantifying Generalization</h3><h4 id="coinrun">CoinRun</h4><p>CoinRun环境很简单，一个智能体，从最左边，一直往右走，收集在最右边的金币，中间或均匀或随机的散布着一些障碍，只有得到金币后有一个常数的reward。当agent死亡，或者采集到金币，或者$1000$个timesteps后自动结束。<br>CoinRun和Sonic很像，但是更简单，更容易泛化。每一个level都是从一个给定的seed中顺序生成的，能够给智能体相当多并且易于量化提供的训练数据。不同的level之间难度差异很大，所以levels的分布相当于agents的学习课程。</p><h4 id="coinrun泛化曲线">CoinRun泛化曲线</h4><p>使用CoinRun可以衡量agents从给定的training level中训练迁移到没有见过的test levels上的效果。随着training中使用的levels数量增多，我们期望在test set上的性能变好，即使是训练固定的timesteps时。test时，作者在test set上进行了zero-shot performance评估，即没有在test set上进行fine-tuning。<br>作者训练了$9$个agents 运行CoinRun，每一个都在具有不同levels数量的训练集上运行。训练过程中，每一个episode从相应的set中随机采样一个level。前$8$个agents使用的train sets中level的数量从$100$到$16000$，最后一个agent在无限个levels的训练集上。Level的seed是$2^32$，所以几乎不可能发生冲突。训练集包含$2M$个独一无二的levels，在test的时候仍然不会遇到已经遇到过的level。整个实验进行了$5$次，每次重新生成training sets。<br>首先使用nature-dqn中的三层CNN进行训练，然后使用$8$个workers的PPO总共训练了$256M$步。每个agents都训练相同的timesteps，然后在每个mini-batch的$8$个works上取gradients的均值。<br>最后的结果中，对$10000$个episodes的final performance进行平均，我们可以看出来，在$4000$个levels以内，过拟合很严重，即使是$1600$个training levels，过拟合也是很多的。当agents遇到的都是新的levels时，表现最好。</p><h3 id="evaluating-architectures">Evaluating Architectures</h3><p>比较Nature-CNN和IMPALA-CNN，结果证明IMPALA-CNN要好一些。为了评估generalization performance，可以直接在无限个levels的train set上训练agents，然后比较learning curves。在这个settings中，智能体不可能过拟合某一些levels的子集，每一个level都是新的。因为Impala-cnn的记过更好，所以作者尝试了更深的网络，发现效果更好，新的网络使用了$5$个residual blocks，每层的channels数量是原来的两倍。再进一步增加网络深度的时候，发现了returns逐渐变小，同时花费的时间也更多了。<br>当然，使用无限的training set并不会总是会带来性能的提升。选择好的超参数能够加快训练速度，但是并不一定会改善generalization的性能。通常来讲，在固定levels的训练集上训练，能产生一个更有用的metric。</p><h3 id="evaluating-regularization">Evaluating Regularization</h3><p>正则化在监督学习中有很重要的作用，因为监督学习更关心的是generalization。监督学习的数据集通常分为训练集和数据集，有很多正则化技巧可以用来减少generalization gap。但是因为RL中训练集和测试集通常是同一个，所以正则化技术就没什么效果。<br>现在既然RL中要衡量generalization，可能正则化技术就能起到作用。作者分别在CoinRun environment中试了$L2$正则化，dropout，data augmentation以及batch normalization。<br>这一节中，所有的agents都是在$500$个levels的CoinRun环境中进行的。我们可以看到有过拟合发生，所以就希望这个setting能够evaluating出不同正则化技术的效果。所有接下来实验的均值和方差都是runs$3$次得到的。使用的是有$3$个残差块的IMPALA-CNN。</p><h4 id="dropout和l2正则化">Dropout和L2正则化</h4><p>作者分别试了dropout为$p\in [0, 0.05, 0.10, 0.15, 0.20, 0.25]$，以及L2正则化权重$w\in [0, 0.5, 1.0, 1.5, 2.0, 2.5]\times 10^{-1} $，一次只试了一个。最终找到了$p=0.1$以及$w=10^{-4} $。使用$L2$正则化训练了$256M$ timesteps，使用dropout训练了$512M$ timesteps，dropout更难收敛，而且效果没有$L2$正则化好用。</p><h4 id="data-augmentation">Data Augmentation</h4><p>监督学习中，数据增强的手段主要用于图像，包括变换，旋转，亮度调整，锐化等等。不同的数据集可能需要使用不同的augmentations。这里作用使用的是Cutout的一个变形。对于每一个observation，可变大小的矩阵区别被masked掉，这些masked的区域给一个随机的颜色，这个和domain randomization非常相似，用于机器人从仿真到真实世界的transfer。</p><h4 id="batch-normalization">Batch Normalization</h4><p>在IMPALA-CNN架构中每层CNN之后使用batch normalization。</p><h4 id="stochasticity">Stochasticity</h4><p>这个随机性很有意思哦。作者考虑了两种方法，一种是改变环境的stochasticity，另一个是改变policy的stochasticity。首先，使用$\epsilon$-greedy action selection算法向环境中加入stochasticity，在每一个timestep中，有$\epsilon$的概率，使用random action代替policy的action。在之前的一些研究中，$\epsilon$-greedy用来鼓励exploration和防止overfitting。然后，通过改变PPO中的entropy bonus控制policy stochasticity。Baseline中使用的entropy bonus是$k_H = 0.01$。<br>同时增加训练时间步到$512M$个timesteps。单独向环境或者policy加入stochasticity都能改善generalization。</p><h4 id="combining-regularization-methods">Combining Regularization Methods</h4><p>作者尝试将data augmentation,batch normalization和L2结合起来，结果表明比任意单独的一种都要好，但是提升的大小并不是很大，可以说这三种方法解决的过拟合问题差不多。<br>由于某些未知的原因，将stochastic和正则化手段结合起来的效果不理想。</p><h2 id="参考文献">参考文献</h2><p>1.<a href="https://arxiv.org/pdf/1804.03720.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1804.03720.pdf</a><br>2.<a href="https://arxiv.org/pdf/1812.02341.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1812.02341.pdf</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;gotta-learn-fast-a-new-benchmark-for-generalization-in-rl&quot;&gt;Gotta Learn Fast: A New Benchmark for Generalization in RL&lt;/h2&gt;
&lt;h3 id=&quot;下
      
    
    </summary>
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="reinforcement learning" scheme="http://mxxhcm.github.io/tags/reinforcement-learning/"/>
    
  </entry>
  
  <entry>
    <title>convex optimization chapter 3 convex functions</title>
    <link href="http://mxxhcm.github.io/2019/09/04/convex-optimization-chapter-3-convex-functions/"/>
    <id>http://mxxhcm.github.io/2019/09/04/convex-optimization-chapter-3-convex-functions/</id>
    <published>2019-09-04T12:49:37.000Z</published>
    <updated>2019-09-04T13:44:30.488Z</updated>
    
    <content type="html"><![CDATA[<h2 id="凸函数-convex-functions">凸函数(convex functions)</h2><h2 id="参考文献">参考文献</h2><p>1.stephen boyd. Convex optimization</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;凸函数-convex-functions&quot;&gt;凸函数(convex functions)&lt;/h2&gt;
&lt;h2 id=&quot;参考文献&quot;&gt;参考文献&lt;/h2&gt;
&lt;p&gt;1.stephen boyd. Convex optimization&lt;/p&gt;

      
    
    </summary>
    
      <category term="凸优化" scheme="http://mxxhcm.github.io/categories/%E5%87%B8%E4%BC%98%E5%8C%96/"/>
    
    
      <category term="凸优化" scheme="http://mxxhcm.github.io/tags/%E5%87%B8%E4%BC%98%E5%8C%96/"/>
    
      <category term="convex optimization" scheme="http://mxxhcm.github.io/tags/convex-optimization/"/>
    
      <category term="convex functions" scheme="http://mxxhcm.github.io/tags/convex-functions/"/>
    
      <category term="凸函数" scheme="http://mxxhcm.github.io/tags/%E5%87%B8%E5%87%BD%E6%95%B0/"/>
    
  </entry>
  
  <entry>
    <title>c++常用函数</title>
    <link href="http://mxxhcm.github.io/2019/08/31/c-plus-plus/"/>
    <id>http://mxxhcm.github.io/2019/08/31/c-plus-plus/</id>
    <published>2019-08-31T03:31:32.000Z</published>
    <updated>2019-09-17T09:14:18.856Z</updated>
    
    <content type="html"><![CDATA[<h2 id="stl">STL</h2><h3 id="顺序容器">顺序容器</h3><h4 id="vector">vector</h4><ul><li>内部数据结构为数组，可以自动增长</li><li>在后端插入和删除，push_back()和pop_back()，时间复杂度为$O(1)$</li><li>在中间和前段插入和删除，insert()和erase()，时间和空间复杂度是$O(n)$</li><li>分配连续内存，</li><li>支持随机数组存取，查找的时间复杂度$O(1)$</li><li>支持[]访问</li><li>头文件vector</li></ul><h4 id="list">list</h4><ul><li>内部数据结构为双向环状链表</li><li>任意位置插入和删除的时间复杂度是$O(1)$</li><li>链式存储，非连续内存</li><li>不支持随机存取，查找的时间复杂度是$O(n)$</li><li>不支持[]访问</li><li>头文件list</li></ul><h4 id="deque">deque</h4><ul><li>vector和deque的结合，使用若干个内存片段进行链接。兼有vector和list的好处。</li><li>内部数据结构为数组</li><li>头文件deque</li></ul><h3 id="关联容器">关联容器</h3><h4 id="map-multimap">map, multimap</h4><p>封装了二叉树</p><h4 id="set-multiset">set, multiset</h4><p>封装了二叉树</p><h2 id="命令空间">命令空间</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">using</span> <span class="built_in">std</span>::<span class="built_in">cin</span>;</span><br><span class="line"><span class="keyword">using</span> <span class="built_in">std</span>::<span class="built_in">cout</span>;</span><br><span class="line"><span class="keyword">using</span> <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line"><span class="keyword">using</span> <span class="built_in">std</span>;:<span class="built_in">vector</span>;</span><br><span class="line"><span class="keyword">using</span> <span class="built_in">std</span>::<span class="built_in">string</span>;</span><br></pre></td></tr></table></figure><h2 id="字符串">字符串</h2><h3 id="string">string</h3><ol><li>导包</li></ol><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;string&gt;</span></span></span><br></pre></td></tr></table></figure><ol start="2"><li>string转字符串数组</li></ol><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">string</span>.c_str()</span><br></pre></td></tr></table></figure><h2 id="vector-v2">vector</h2><ol><li>初始化</li></ol><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//1.初始化一个size为0的vecotr</span></span><br><span class="line"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; a; </span><br><span class="line"></span><br><span class="line"><span class="comment">//2.初始化size为10，默认值为0的vector</span></span><br><span class="line"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; b(<span class="number">10</span>);  </span><br><span class="line"></span><br><span class="line"><span class="comment">//3.初始化size为10，默认值为1的vector</span></span><br><span class="line"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; c(<span class="number">10</span>, <span class="number">1</span>);   </span><br><span class="line"></span><br><span class="line"><span class="comment">//4.使用vector初始化vector</span></span><br><span class="line"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; d(a);   </span><br><span class="line"></span><br><span class="line"><span class="comment">//5.使用数组初始化vector</span></span><br><span class="line"><span class="keyword">int</span> e = &#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>&#125;;</span><br><span class="line"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; f(e, e+<span class="number">3</span>);</span><br></pre></td></tr></table></figure><ol start="2"><li>vector的读取方式</li></ol><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//[]和迭代器,at</span></span><br><span class="line"><span class="keyword">int</span> a = &#123; <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>&#125;;</span><br><span class="line"><span class="built_in">vector</span> &lt;<span class="keyword">int</span>&gt; b(a, a+ <span class="number">4</span>);</span><br><span class="line"><span class="comment">//1.[]</span></span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"%d\n"</span>, a[<span class="number">3</span>]);</span><br><span class="line"><span class="comment">//2.迭代器</span></span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">auto</span> it = b.begin(); it != b.end(); it++)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"%d\n"</span>, *it);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//3.at</span></span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"%d\n"</span>, a.at(<span class="number">3</span>));</span><br></pre></td></tr></table></figure><ol start="3"><li>向量长度</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vector.size()</span><br></pre></td></tr></table></figure><h2 id="vector的vector">vector的vector</h2><ol><li>声明</li></ol><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="built_in">std</span>::<span class="built_in">vector</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> len = <span class="number">100</span>;</span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">vector</span>&lt; <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &gt; a(len);</span><br></pre></td></tr></table></figure><ol start="2"><li>访问</li></ol><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="built_in">std</span>::<span class="built_in">vector</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> len = <span class="number">0</span>;</span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">vector</span>&lt; <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &gt; a(len);</span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; temp;</span><br><span class="line"></span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">vector</span>&lt; <span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &gt;::iterator it ;</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; len; i++)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> j =<span class="number">0</span>; j &lt; len; j++)</span><br><span class="line">    &#123;</span><br><span class="line">        temp.push_back(j);</span><br><span class="line">    &#125;</span><br><span class="line">    a.push_back(temp);</span><br><span class="line">    temp.clear();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="参考文献">参考文献</h2><p>1.<a href="https://blog.csdn.net/xiexievv/article/details/6831194" target="_blank" rel="noopener">https://blog.csdn.net/xiexievv/article/details/6831194</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;stl&quot;&gt;STL&lt;/h2&gt;
&lt;h3 id=&quot;顺序容器&quot;&gt;顺序容器&lt;/h3&gt;
&lt;h4 id=&quot;vector&quot;&gt;vector&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;内部数据结构为数组，可以自动增长&lt;/li&gt;
&lt;li&gt;在后端插入和删除，push_back()和pop_back()
      
    
    </summary>
    
      <category term="C/C++" scheme="http://mxxhcm.github.io/categories/C-C/"/>
    
    
      <category term="C++" scheme="http://mxxhcm.github.io/tags/C/"/>
    
  </entry>
  
  <entry>
    <title>gcc编译</title>
    <link href="http://mxxhcm.github.io/2019/08/30/gcc%E7%BC%96%E8%AF%91/"/>
    <id>http://mxxhcm.github.io/2019/08/30/gcc编译/</id>
    <published>2019-08-30T15:35:20.000Z</published>
    <updated>2019-09-02T05:10:19.898Z</updated>
    
    <content type="html"><![CDATA[<h2 id="快速入门">快速入门</h2><ol><li>保存c++源文件为helloworld.cpp</li></ol><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line"><span class="built_in">cout</span>&lt;&lt;<span class="string">"Hello World"</span>&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ol start="2"><li>使用g++ 编译</li></ol><blockquote><blockquote><blockquote><p>g++ -o hello_world hello_world.cpp</p></blockquote></blockquote></blockquote><ol start="3"><li>执行编译后的源文件<br>./hello_world</li></ol><h2 id="参考文献">参考文献</h2><p>1.<a href="https://legacy.gitbook.com/download/pdf/book/lexdene/gcc-five-minutes" target="_blank" rel="noopener">https://legacy.gitbook.com/download/pdf/book/lexdene/gcc-five-minutes</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;快速入门&quot;&gt;快速入门&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;保存c++源文件为helloworld.cpp&lt;/li&gt;
&lt;/ol&gt;
&lt;figure class=&quot;highlight c&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span cla
      
    
    </summary>
    
      <category term="工具" scheme="http://mxxhcm.github.io/categories/%E5%B7%A5%E5%85%B7/"/>
    
    
      <category term="工具" scheme="http://mxxhcm.github.io/tags/%E5%B7%A5%E5%85%B7/"/>
    
      <category term="gcc" scheme="http://mxxhcm.github.io/tags/gcc/"/>
    
      <category term="g++" scheme="http://mxxhcm.github.io/tags/g/"/>
    
  </entry>
  
  <entry>
    <title>dot product cross product and triple product</title>
    <link href="http://mxxhcm.github.io/2019/08/28/dot-product-cross-product-and-triple-product/"/>
    <id>http://mxxhcm.github.io/2019/08/28/dot-product-cross-product-and-triple-product/</id>
    <published>2019-08-28T09:26:35.000Z</published>
    <updated>2019-08-29T03:36:23.670Z</updated>
    
    <content type="html"><![CDATA[<p>先给出已知条件，两个列向量$a= (x_1,y_1, z_1), w=(x_2,y_2, z_2)$。</p><h2 id="点积-dot-product">点积(dot product)</h2><h3 id="定义">定义</h3><p>点积，又叫数量积，定义为$a\cdot b = x_1 x_2 + y_1y_2 + z_1z_2$。<br>另一种定义方式是$a\cdot b = \vert a \vert \vert b \vert cos &lt;a, b&gt;$`<br>这两种定义方式实际上是一样的：<br><img src="/2019/08/28/dot-product-cross-product-and-triple-product/dot_product.jpg" alt="dot_product"></p><h2 id="叉积-cross-product">叉积(cross product)</h2><h3 id="定义-v2">定义</h3><p>叉积，又叫向量积，定义为$\vert a\times b \vert = \vert a \vert \vert b \vert sin &lt;a, b&gt;$，最后得到一个方向和$a,b$都正交的向量，并且方向符合右手规则。向量积的大小等于$a,b$构成的平行四边形的面积。</p><h3 id="属性">属性</h3><ol><li>$a\times b, b\times a$是方向相反的</li><li>$a,b$的叉积垂直于$a$和$b$</li><li>任意向量和它自己的叉积为$0$。</li></ol><h2 id="混合积-trple-product">混合积(trple product)</h2><h3 id="定义-v3">定义</h3><p>混合积定义为$(a\times b) \cdot c$，它的绝对值等于以$a,b,c$三个向量构成的形状的体积。</p><h3 id="属性-v2">属性</h3><p>向量$a\times b$是一个向量，$a\times b$的大小相当于底面积，方向垂直于$a,b$所有的平面，再和$c$点乘，乘上了$c$投影到$a,b$点乘结果所在的直线上，相当于乘上了高。</p><h2 id="参考文献">参考文献</h2><p>1.MIT线性代数公开课</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;先给出已知条件，两个列向量$a= (x_1,y_1, z_1), w=(x_2,y_2, z_2)$。&lt;/p&gt;
&lt;h2 id=&quot;点积-dot-product&quot;&gt;点积(dot product)&lt;/h2&gt;
&lt;h3 id=&quot;定义&quot;&gt;定义&lt;/h3&gt;
&lt;p&gt;点积，又叫数量积，定义为$
      
    
    </summary>
    
      <category term="线性代数" scheme="http://mxxhcm.github.io/categories/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/"/>
    
    
      <category term="线性代数" scheme="http://mxxhcm.github.io/tags/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/"/>
    
      <category term="点积" scheme="http://mxxhcm.github.io/tags/%E7%82%B9%E7%A7%AF/"/>
    
      <category term="叉积" scheme="http://mxxhcm.github.io/tags/%E5%8F%89%E7%A7%AF/"/>
    
      <category term="混合积" scheme="http://mxxhcm.github.io/tags/%E6%B7%B7%E5%90%88%E7%A7%AF/"/>
    
  </entry>
  
  <entry>
    <title>eigenvalues and eigenvectors（特征值和特征向量）</title>
    <link href="http://mxxhcm.github.io/2019/08/28/linear-algebra-eigenvalues-and-eigenvectors/"/>
    <id>http://mxxhcm.github.io/2019/08/28/linear-algebra-eigenvalues-and-eigenvectors/</id>
    <published>2019-08-28T09:21:43.000Z</published>
    <updated>2019-09-09T08:53:24.996Z</updated>
    
    <content type="html"><![CDATA[<h2 id="特征值和特征向量">特征值和特征向量</h2><p>这里介绍的东西都是针对于方阵来说的。</p><h3 id="定义">定义</h3><p>$Ax=\lambda x $，满足该式子的$x$称为矩阵$A$的特征向量，相应的$\lambda$称为特征值。</p><h3 id="求解">求解</h3><p>将$Ax=\lambda x$进行移项，得到$(A-\lambda I) x =0$，其中$A-\lambda I$必须是sigular（即不可逆），如果$A - \lambda I$是非奇异矩阵，也就是说它的列向量相互独立，那么只有零解，无意义。令$det (A-\lambda I)=0$，求出相应的$\lambda$和$x$。</p><h3 id="属性">属性</h3><ol><li>$n$个特征值的乘积等于行列式。</li><li>$n$个特征值之和等于对角线元素之和。</li></ol><h2 id="迹">迹</h2><h3 id="定义-v2">定义</h3><p>主对角线元素之和叫做迹（trace）。<br>$$\lambda_1 +\cdots + \lambda_n = trace = a_{11} + \cdots + a_{nn}$$</p><h2 id="矩阵对角化">矩阵对角化</h2><h3 id="定义-v3">定义</h3><p>如果合适的使用矩阵$A$的特征向量，可以把$A$转换成一个对角矩阵。<br>假设$n\times n$的矩阵$A$有$n$个线性独立的特征向量$x_1,\cdots, x_n$，把它们当做列向量，构成一个新的矩阵$S=\left[x_1, \cdots, x_n\right]$。<br>$$AS = A\left[x_1, \cdots, x_n\right] = \left[\lambda_1 x_1, \cdots, \lambda_n x_n\right] = \left[x_1, \cdots, x_n\right] \begin{bmatrix} \lambda_1 &amp;\cdots &amp; 0 \\ \vdots&amp;\lambda_i &amp; \vdots\\ 0&amp; \cdots &amp; \lambda_n\end{bmatrix} = S\Lambda$$<br>即$AS = S\Lambda$，所以有$S^{-1} AS = \Lambda, A = S\Lambda S^{-1} $，这里我们假设$A$的$n$个特征向量都是线性无关的。$A, \Lambda$的特征值相同，特征向量不同。$A$的特征向量用来对角化$A$。</p><h3 id="属性-v2">属性</h3><p>如果一个矩阵有$n$个不同的实特征值，那么它一定可对角化。<br>如果存在重复的特征值，可能但不一定可对角化，单位矩阵就有重复特征值，但可对角化。</p><h2 id="可逆和对角化">可逆和对角化</h2><p>矩阵可逆和矩阵可对角化之间没有关联。<br>矩阵可逆和特征值是否为$0$有关，而矩阵可对角化与特征向量有关，是否有足够的线性无关的特征向量。</p><h2 id="矩阵的幂">矩阵的幂</h2><h3 id="矩阵幂">矩阵幂</h3><p>$A= S\Lambda S^{-1} $,<br>$A^2 = S\Lambda S^{-1}S\Lambda S^{-1} = S\Lambda^2 S^{-1} $,<br>$A^k = S\Lambda^k S^{-1}$<br>所以，$A^k $和$A$的特征向量相同，特征值是$\Lambda^k $。当$k\rightarrow \infty$时，如果所有的特征值$\lambda_i \lt 1$，那么$A^k \rightarrow 0$。</p><h3 id="以解方程组-u-k-1-au-k">以解方程组$u_{k+1} = Au_k$</h3><p>从给定的向量$u_0$开始，$u_1 = Au_0, u_2 = Au_1, u_k = A^k u_0$<br>假设$u_0 = c_1 x_1 + c_2 x_2 + \cdots + c_nx_n$，$x_1, \cdots, x_n$是一组正交基。<br>$Au_0 =  c_1 \lambda_1 x_1 + \cdots + c_n\lambda_n x_n$<br>$u_{100} = A^{100} u_0 = c_1 \lambda_1^{100} x_1 + \cdots + c_n \lambda_n^{100} x_n$<br>$u_{100} = A^{100} u_0 = \Lambda^{100} S c$</p><h2 id="微分方程">微分方程</h2><h2 id="指数矩阵">指数矩阵</h2><h2 id="markov-matrices">Markov Matrices</h2><h3 id="定义-v4">定义</h3><p>马尔科夫矩阵满足两个条件</p><ol><li>所有元素大于$0$</li><li>行向量之和为$1$</li></ol><h3 id="属性-v3">属性</h3><ol><li>$\lambda = 1$是一个特征值，对应的特征向量的所有分量大于等于$0$。可以直接验证，假设$A = \begin{bmatrix}a&amp;b\\c&amp;d\\ \end{bmatrix}, a + b = 1, c + d = 1$，$A-\lambda I =  \begin{bmatrix}a - 1&amp;b\\c&amp;d - 1\\ \end{bmatrix}$，所有元素加起来等于$0$，即$(A-I)(1, \cdots, 1)^T = 0$，所以这些向量线性相关，因为存在一组不全为$0$的系数使得他们的和为$0$。所以$A-I$是奇异矩阵，也就是说$1$是$A$的一个特征值。</li><li>所有其他的特征值小于$1$。</li></ol><h3 id="马尔科夫矩阵的幂">马尔科夫矩阵的幂</h3><p>$u_k = A^k u_0 = c_1 \lambda_1^k x_1 + c_2 \lambda_2^k x_2 + \cdots$<br>如果只有一个特征值为$1$，所有其他特征值都小于$1$，幂运算之后$\lambda^k \rightarrow 0, k\rightaroow \infty, \lambda_k \neq 1$。</p><h2 id="对称矩阵">对称矩阵</h2><h3 id="定义-v5">定义</h3><p>满足$A= A^T $的矩阵$A$被称为对称矩阵。</p><h3 id="属性-v4">属性</h3><ol><li>实对称矩阵的特征值都是实数<br>证明：由$Ax= \lambda x$，得到$A\bar{x} = \bar{\lambda} \bar{x}$，$\bar{x}$是$x$的共轭，转置得：<br>$$\bar{x}^T A^T = \bar{x}^T A = \bar{x}^T \bar{\lambda}$$<br>$Ax = \lambda x$的左边乘上$\bar{x}^T $，在$\bar{x}^T A = \bar{x}^T \lambda$的右边同时乘上$x$：<br>$$\bar{x}^T Ax = \bar{x}^T \lambda x = \bar{x}^T A x= \bar{x}^T \bar{\lambda} x$$<br>即$\bar{x}^T \lambda x = \bar{x}^T \bar{\lambda} x$，而$\bar{x}^T x= \vert x\vert \ge 0 $，如果$x\neq 0$，则$\lambda = \bar{\lambda}$，即$\lambda$的虚部为$0$，即特征值都是实数。</li><li>对称矩阵有单位正交的特征向量。<br>证明：假设$S = \left[v_1, \cdots, v_i, \cdots, v_n\right]$是矩阵$A$的特征向量矩阵，根据矩阵对角化公式：<br>$$A = S \Lambda S^{-1}  $$<br>而$A=A^T $，所以得到<br>$$S\Lambda S^{-1} = A = A^T = \left(S \Lambda S^{-1} \right)^T = S^{-T} \Lambda^T S^T = S^{-T} \Lambda S^T $$<br>可以得出$S^T = S^{-1} $，所以$S S^T = I$，即$v_i^T v_i = 1, v_i^T v_j = 0, \forall i\neq j$。</li><li>所有的对称矩阵都是可对角化的。</li></ol><h3 id="谱定理-spectral-theorem">谱定理（Spectral Theorem）</h3><p>对称矩阵的对角化可以从$A=S\Lambda S^{-1} $变成$A=Q\Lambda Q^{-1} =Q\Lambda Q^T $。<br>谱定理：每一个对称矩阵都有以下分解$A = Q\Lambda Q^T $，$\Lambda$是实特征值，$Q$是单位正交向量矩阵。<br>$$A = Q\Lambda Q^{-1} = Q\Lambda Q^T $$<br>$A$是对称的，$Q \Lambda Q^T $也是对称的。</p><h2 id="正定矩阵">正定矩阵</h2><p>正定矩阵，负定矩阵，半正定矩阵，半负定矩阵都是对于对称矩阵来说的。</p><h3 id="定义-v6">定义</h3><p>如果对于所有的非零向量$x$，$x^T Ax$都是大于$0$的，我们称矩阵$A$是正定矩阵。</p><h3 id="属性-v5">属性</h3><ol><li>所有的$n$个特征值都是正的</li><li>所有的$n$个左上行列式都是正的</li><li>所有的$n$个主元都是正的</li><li>对于任意$x\neq 0$，$x^T A x$大于$0$。</li><li>$A=R^T R$，$R$是一个具有$n$个独立column的矩阵。</li></ol><p>如果任意矩阵$A$拥有以上属性中的任意一个，那么它就有其他四个性质，或者说上面五个属性都可以用来判定矩阵是否为正定矩阵。</p><h3 id="半正定矩阵">半正定矩阵</h3><p>如果对于所有的非零向量$x$，$x^T Ax$都是大于等于$0$的，我们称矩阵$A$是半正定矩阵。</p><h3 id="属性-v6">属性</h3><p>对于任何矩阵$A$，$A^T A$和$A A^T $都是对称矩阵，并且它们一定是半正定矩阵。<br>假设$A = \begin{bmatrix} a&amp;b\\c&amp;d\end{bmatrix}$，如何判断$A^T A$是不是正定的？根据定义，判断$x^T (A^T A) x$的符号：<br>$$x^T (A^T A) x = x^T A^T Ax = (Ax)^T (Ax) = \vert Ax \vert $$<br>相当于计算向量$Ax$的模长，它一定是大于等于$0$的。<br>同理$A A^T $的二次型相当于计算$A^T x$的模长，大于等于$0$。</p><h2 id="参考文献">参考文献</h2><p>1.MIT线性代数公开课<br>2.<a href="http://maecourses.ucsd.edu/~mdeolive/mae280a/lecture11.pdf" target="_blank" rel="noopener">http://maecourses.ucsd.edu/~mdeolive/mae280a/lecture11.pdf</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;特征值和特征向量&quot;&gt;特征值和特征向量&lt;/h2&gt;
&lt;p&gt;这里介绍的东西都是针对于方阵来说的。&lt;/p&gt;
&lt;h3 id=&quot;定义&quot;&gt;定义&lt;/h3&gt;
&lt;p&gt;$Ax=\lambda x $，满足该式子的$x$称为矩阵$A$的特征向量，相应的$\lambda$称为特征值。&lt;/p
      
    
    </summary>
    
      <category term="线性代数" scheme="http://mxxhcm.github.io/categories/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/"/>
    
    
      <category term="线性代数" scheme="http://mxxhcm.github.io/tags/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/"/>
    
      <category term="特征值" scheme="http://mxxhcm.github.io/tags/%E7%89%B9%E5%BE%81%E5%80%BC/"/>
    
      <category term="特征向量" scheme="http://mxxhcm.github.io/tags/%E7%89%B9%E5%BE%81%E5%90%91%E9%87%8F/"/>
    
  </entry>
  
  <entry>
    <title>determinants（行列式）</title>
    <link href="http://mxxhcm.github.io/2019/08/28/linear-algebra-determinants/"/>
    <id>http://mxxhcm.github.io/2019/08/28/linear-algebra-determinants/</id>
    <published>2019-08-28T06:45:18.000Z</published>
    <updated>2019-09-09T08:53:24.996Z</updated>
    
    <content type="html"><![CDATA[<h2 id="行列式-determinants">行列式（Determinants）</h2><p>这一章介绍行列式相关知识，行列式的一些属性等等。</p><ul><li>矩阵不可逆，行列式为$0$。</li><li>主元的乘积是行列式。</li><li>交换任意两行和两列，行列式的符号改变。</li><li>行列式的绝对值等于这个矩阵描述的space的体积。</li><li>行列式的计算公式有三个：<ol><li>主元公式，就是所有主元的乘积</li><li>big formula</li><li>cofact formula</li></ol></li></ul><h2 id="定义以及性质">定义以及性质</h2><p>行列式用det表示，给出以下的几个属性：</p><ol><li>$n\times n$的单位矩阵$I$的行列式$det I = I$<br>$$\begin{vmatrix}1&amp;0\\0&amp;1 \end{vmatrix} = 1$$</li><li>交换任意两行，行列式符号取反。<br>$$\begin{vmatrix}a&amp;b\\c&amp;d \end{vmatrix} = - \begin{vmatrix}c&amp;d\\a&amp;b \end{vmatrix}$$</li><li>行列式是每一行的线性函数<br>$$\begin{vmatrix}ta&amp;tb\\c&amp;d \end{vmatrix} = t \begin{vmatrix}a&amp;b\\c&amp;d \end{vmatrix}$$<br>$$\begin{vmatrix}a+a’&amp;b+b’\\c&amp;d \end{vmatrix} = \begin{vmatrix}a’&amp;b’\\c&amp;d \end{vmatrix}+\begin{vmatrix}a&amp;b\\c&amp;d \end{vmatrix}$$</li></ol><p>以上的三个属性是行列式的性质，事实上，它们定义了行列式是什么，从这几个基本属性出发，我们能推导出更多的属性。</p><ol start="4"><li>如果$A$的两行相等，那么$det A=0$，交换两行，还是矩阵$A$，行列式变号，所以行列式只能为$0$。<br>假设$A = \begin{bmatrix}a&amp;b\\a&amp;b \end{bmatrix}$，$\begin{vmatrix}a&amp;b\\a&amp;b \end{vmatrix}= - \begin{vmatrix}a&amp;b\\a&amp;b \end{vmatrix}$</li><li>从某一行减去其他行的倍数，行列式不变<br>$$\begin{vmatrix}a&amp;b\\c- la&amp;d-lb \end{vmatrix}= \begin{vmatrix}a&amp;b\\c&amp;d \end{vmatrix} -l \begin{vmatrix}a&amp;b\\a&amp;b\end{vmatrix}   = \begin{vmatrix}a&amp;b\\c&amp;d \end{vmatrix}$$</li><li>某一行为$0$矩阵，行列式为$0$。<br>$$\begin{vmatrix}0&amp;0\\c&amp;d \end{vmatrix} = \begin{vmatrix}c&amp;d\\c&amp;d \end{vmatrix} = 0$$</li><li>如果$A$是三角矩阵，行列式的值等于对角元素乘积。<br>$$\begin{vmatrix}a&amp;b\\0&amp;d \end{vmatrix} = \begin{vmatrix}a&amp;0\\c&amp;d \end{vmatrix} = \begin{vmatrix}a&amp;0\\0&amp;d \end{vmatrix} = ad \begin{vmatrix}1&amp;0\\0&amp;1 \end{vmatrix} = ad$$</li><li>当且仅当$A$不可逆的时候，$det A\neq 0$<br>$det A = det U$，如果$A$不可逆，$U$中有零行，从$6$我们知道，行列式为$0$。如果$A$可逆，行列式的值等于主元的乘积。</li><li>矩阵$AB$的行列式等于矩阵$A$的行列式以及矩阵$B$的行列式。<br>$$\begin{vmatrix}a&amp;b\\c&amp;d \end{vmatrix}\begin{vmatrix}p&amp;q\\r&amp;s \end{vmatrix} = \begin{vmatrix}ap+br&amp;aq+bs\\cp+dr&amp;cq+ds \end{vmatrix}$$<br>证明：<br>对于$2\times 2$的情况，有$\begin{vmatrix}A\end{vmatrix}\begin{vmatrix}B\end{vmatrix} = (ad - bc) ( ps - qr) = (ap + br) (cq+ds) - (aq+bs)(cp+dr) \begin{vmatrix}AB \end{vmatrix}$<br>当$B$是$A^{-1} $的时候，有$det (A A^{-1}) = det (I) = 1 = det (A) det(A^{-1} )$，所以$det A^{-1} = \frac{1}{det A}$</li><li>$A^T$和$A$的行列式相同。<br>$PA=LU, A^T P^T = U^T L^T$，$det L = det L^T = 1, det U = det U^T $，$L$是对角线元素为$1$的对角矩阵，$U$是对角矩阵，$P$是置换矩阵，$P^T P = I$，$det P det P^T = 1$，则$det P = det P^T = 1$，这个为什么？我有点不明明白。最后有$det A = det A^T $。</li></ol><h2 id="行列式的计算">行列式的计算</h2><h3 id="主元公式">主元公式</h3><p>行列式等于主元的乘积。</p><h3 id="大公式">大公式</h3><p>$n=2$的情况下：<br>$$A= \begin{bmatrix} a &amp; b\\c&amp;d\\\end{bmatrix}$$<br>$$det A = \begin{vmatrix}a&amp;0\\c&amp;d\end{vmatrix}+\begin{vmatrix}0&amp;b\\c&amp;d\end{vmatrix} = \begin{vmatrix}a&amp;0\\c&amp;0\end{vmatrix}+\begin{vmatrix}a&amp;0\\0&amp;d\end{vmatrix}+\begin{vmatrix}0&amp;b\\c&amp;0\end{vmatrix} \begin{vmatrix}0&amp;b\\0&amp;d\end{vmatrix} = ad - bc$$<br>$n=3$的情况下，最后有六项不为$0$的取值，$3!= 3\times 2\times 1= 6$<br>在$n$的情况下，有$n!$个项，将它们加起来求和。</p><h3 id="代数余子式-cofactors">代数余子式（Cofactors）</h3><h4 id="定义">定义</h4><p>用$C$表示代数余子式，用$M_{ij}$表示划去$i$行，$j$列的子矩阵，<br>$$C_{ij} = (-1)^{i+j} det  M_{ij} $$</p><h4 id="计算行列式">计算行列式</h4><p>行列式可以沿着任意一行或者任意一列，利用代数余子式进行计算，<br>沿着第$i$行计算的公式如下：<br>$$ det A = \sum_{j=1}^n a_{ij} C_{ij}$$<br>沿着第$j$列计算的公式如下：<br>$$ det A = \sum_{i=1}^m a_{ij} C_{ij}$$<br>可以递归下去进行计算。</p><h2 id="参考文献">参考文献</h2><p>1.MIT线性代数公开课视频</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;行列式-determinants&quot;&gt;行列式（Determinants）&lt;/h2&gt;
&lt;p&gt;这一章介绍行列式相关知识，行列式的一些属性等等。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;矩阵不可逆，行列式为$0$。&lt;/li&gt;
&lt;li&gt;主元的乘积是行列式。&lt;/li&gt;
&lt;li&gt;交换任意两行
      
    
    </summary>
    
      <category term="线性代数" scheme="http://mxxhcm.github.io/categories/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/"/>
    
    
      <category term="线性代数" scheme="http://mxxhcm.github.io/tags/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/"/>
    
      <category term="行列式" scheme="http://mxxhcm.github.io/tags/%E8%A1%8C%E5%88%97%E5%BC%8F/"/>
    
      <category term="determinants" scheme="http://mxxhcm.github.io/tags/determinants/"/>
    
  </entry>
  
  <entry>
    <title>orthogonality（正交性）</title>
    <link href="http://mxxhcm.github.io/2019/08/27/linear-algebra-orthogonality/"/>
    <id>http://mxxhcm.github.io/2019/08/27/linear-algebra-orthogonality/</id>
    <published>2019-08-27T11:21:44.000Z</published>
    <updated>2019-09-09T08:53:24.996Z</updated>
    
    <content type="html"><![CDATA[<h2 id="正交性-orthogonality">正交性（Orthogonality）</h2><p>这一章主要介绍正交性相关的内容。包括正交向量，投影，正交子空间，正交基以及如果求一组正交基，最后介绍QR分解求线性方程组。</p><h2 id="正交向量-orthogonal-vectors">正交向量（Orthogonal vectors）</h2><p>给定向量$v,w$，如果$v^T w = 0$，那么这两个向量就是正交向量。</p><h2 id="正交子空间-orthogonal-subspaces">正交子空间（Orthogonal subspaces）</h2><p>如果对于$\forall v\in V, \forall w\in W$，都有$v^T w = 0$，那么我们称subspaces $V,W$是orthogonal subspaces。</p><h2 id="column-space-nullsapce-row-space-left-nullspace的正交性">Column space, nullsapce, row space, left nullspace的正交性</h2><ol><li><p>Row space和nullspace是正交的。<br>举个例子来证明吧，有$A= \begin{bmatrix}c1&amp;c2\end{bmatrix} = \begin{bmatrix}r1\\r2\end{bmatrix} = \begin{bmatrix}1&amp;1&amp;2&amp;4\\0&amp;0&amp;1&amp;3\end{bmatrix}$<br>因为row space是row vector的linear combination，即$c_1 r_1+c_2 r_2$，而nullspace是$Ax=0$的所有解，即$x_1 c_1+x_2c_2  = 0$，这里的$0$是向量，可以推出来$r_1x = 0, r_2x =0 $，所以$c_1 r_1 x =0, c_2 r_2x = 0$，也就是说row space中的任意vector和nullspace中的vector都正交。<br>使用数值方法证明：<br>$x$表示$Ax=0$中的$x$，$A^Ty$表示row space，那么有<br>$$x^T (A^T y) = (Ax)^T y = 0^T y = 0$$</p></li><li><p>Column space和nullspace是正交的。</p></li></ol><h2 id="正交补-orthogonal-complements">正交补（Orthogonal complements）</h2><h3 id="定义">定义</h3><p>如果一个subspace包含所有和subspace $V$正交的向量，称这个subspace是$V$的orthogonal complements（正交补）。</p><h3 id="示例">示例</h3><p>Nullspace是row space的正交补（$\mathbb{R}^n$上）。<br>Left nullspace是column space的正交补（$\mathbb{R}^m$上）。</p><h2 id="投影-projections">投影（Projections）</h2><p>如下图所示，左边是投影到一条直线上的结果，右边是投影到一个subspace上的结果<br><img src="/2019/08/27/linear-algebra-orthogonality/projection.jpg" alt="projection"></p><h2 id="a-t-a">$A^T A$</h2><p>$A^T A$是可逆的，当且仅当$A$有linear independent columns时<br>证明：<br>$A^TA$是一个$n\times n$的方阵，$A$的nullspace和$A^T A$的nullspace相等。<br>如果$Ax= 0$，那么$A^T Ax = 0$，所以$x$也在$A^T A$的nullspace中。如果$A^T Ax=0$，那么我们要证明$Ax=0$，在左右两边同乘$x^T $得$x^T A^T Ax=0$，则$(AX)^T AX =0$，所以$\vert Ax\vert^2 =0$。也即是说如果$A^T Ax=0$，那么$Ax$的长度为$0$，也就是$Ax=0$。<br>如果$A^T A$的columns是独立的，也就是说nullspace为空，所以$A$的columns也是独立的；同理，如果$A$的columns是独立的，那么$A^T $的columns也是独立的。</p><h2 id="最小二乘法-least-squares-approximations">最小二乘法（Least Squares Approximations）</h2><p>$Ax=b$无解的情况，通常是等式个数大于未知数的个数，即$m\gt n$，$b$不在$A$的column space内。我们的目标是想让$e=b-Ax$为$0$，当这个目标不能实现的时候，可以在方程左右两边同时乘上$A^T$，求出一个近似的$\hat{x}$：<br>$$A^TAx = A^Tb$$<br>如何推导出这个结果，有以下几种方法：</p><h3 id="最小化误差">最小化误差</h3><ol><li><p>几何上<br>$Ax=b$的最好近似是$A\bar{x} = p$，最小的可能误差是$e=b-p$，$b$上的点的投影都在$p$上，而$p$在$A$的column space上，从直线拟合的角度上来看，$\bar{x}$给出了最好的结果。</p></li><li><p>代数上<br>$b=p+e$，$e$在$A$的nullspace上，$Ax=b=p+e$我们解不出来，$A\bar{x} = p$我们可以解出来。</p></li><li><p>积分</p></li></ol><h3 id="直线拟合">直线拟合</h3><h3 id="抛物线拟合">抛物线拟合</h3><h2 id="正交基-orthogonal-bases">正交基（Orthogonal Bases）</h2><h3 id="定义-v2">定义</h3><p>一组向量$q_1, q_2, \cdots , q_n$如果满足以下条件：<br>$$q_i^T q_j\begin{cases}0, i\neq j \\1, i=j\end{cases}$$<br>我们称这一组向量是正交向量，由正交column vectors构成的矩阵用一个特殊字母$Q$表示。如果这组正交向量同时还是单位向量，我们叫它单位正交向量。如果columns仅仅正交，而不是单位向量的话，点乘仍然会得到一个对角矩阵，但是它的性质没有那么好。</p><h3 id="性质">性质</h3><ol><li>满足$Q^T Q=I$。</li><li>如果$Q$是方阵，那么$Q^T = Q^{-1}$，即转置等于逆。</li><li>如果$Q$是方阵的话，$QQ^T = Q^T Q= I$。</li><li>如果$Q$是rectangular的话，$QQ^T =I$不成立，而$Q^T Q =I$依然成立。</li></ol><h2 id="用-q-取代-a-进行正交投影">用$Q$取代$A$进行正交投影</h2><p>假设矩阵$A$的所有column vectors都是orthonormal的，$a$就变成了$q$，$A^T A$就变成了$Q^T Q=I$，所以$Ax=b$的解变成了$\bar{x} = Q^T b$，而投影矩阵变成了$P=QQ^T $。</p><h2 id="gram-schmidi正交化">Gram-Schmidi正交化</h2><p>Gram-Schmidt正交化过程就相当于是在不断的进行投影，这个方法的想法是从$n$个独立的column vector出发，构建$n$个正交向量，然后再单位化。拿$3$个过程举个例子。用$a,b,c$表示初始的$3$个独立向量，$A,B,C$表示三个正交向量，$q_1, q_2,q_3$表示三个正交单位向量。<br>第一个正交向量，直接对第一个向量单位化<br>$$A=a, q_1 = \frac{A}{\vert A\vert}$$<br>第二个正交向量，将第二个向量投影到第一个向量上，计算出一个和第二个向量正交的向量。<br>$$B=b-\frac{A^T B}{A^T A}A , q_2 = \frac{B}{\vert B\vert}$$<br>第三个正交向量，将第三个向量分别投影到第一个和第二个正交向量上，计算处第三个正交向量。<br>$$C=c - \frac{A^T C}{A^T A}A - \frac{B^T C}{B^T B}B , q_2 = \frac{C}{\vert C\vert}$$<br><img src="/2019/08/27/linear-algebra-orthogonality/gram_schmidi.jpg" alt="gram_schmidi"></p><h2 id="qr分解">QR分解</h2><p>假设一个矩阵$A$的列向量分别为$a,b,c$，最后经过一个三角矩阵$R$化简成一个正交矩阵$Q$，相应的列向量分别为$q_1,q_2,q_3$。<br>首先根据Gram-Schmidi计算处一组正交基$Q = \begin{bmatrix}q_1&amp;q_2&amp;q_3 \end{bmatrix}$。根据$A$，能直接计算出$Q$，那么如何得到$R$呢？我们假设$A=QR$，在$A$和$Q$已知的情况下，并且满足$Q^T Q = I$，我们可以左右两边同时乘上$Q^T $，就有$Q^T A = Q^T QR = R$，即$R=Q^T A$。</p><h2 id="参考文献">参考文献</h2><p>1.MIT线性代数公开课</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;正交性-orthogonality&quot;&gt;正交性（Orthogonality）&lt;/h2&gt;
&lt;p&gt;这一章主要介绍正交性相关的内容。包括正交向量，投影，正交子空间，正交基以及如果求一组正交基，最后介绍QR分解求线性方程组。&lt;/p&gt;
&lt;h2 id=&quot;正交向量-orthogo
      
    
    </summary>
    
      <category term="线性代数" scheme="http://mxxhcm.github.io/categories/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/"/>
    
    
      <category term="线性代数" scheme="http://mxxhcm.github.io/tags/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/"/>
    
      <category term="正交" scheme="http://mxxhcm.github.io/tags/%E6%AD%A3%E4%BA%A4/"/>
    
      <category term="orthogonality" scheme="http://mxxhcm.github.io/tags/orthogonality/"/>
    
  </entry>
  
  <entry>
    <title>vector spaces和subspaces（向量空间和子空间）</title>
    <link href="http://mxxhcm.github.io/2019/08/26/linear-algebra-vector-spaces%E5%92%8Csubspaces/"/>
    <id>http://mxxhcm.github.io/2019/08/26/linear-algebra-vector-spaces和subspaces/</id>
    <published>2019-08-26T11:17:41.000Z</published>
    <updated>2019-09-09T08:53:25.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="向量空间和子空间-vector-spaces-and-subspaces">向量空间和子空间（vector spaces and subspaces）</h2><p>这一件介绍和space相关的概念以及很多其他的基础知识。</p><h2 id="线性组合-linear-of-combinations">线性组合（Linear of Combinations）</h2><p>线性组合有两种：加法和数乘。</p><h3 id="定义">定义</h3><p>如果$v$和$w$是column vectors，$c,d$是标量，那么$cv+dw$是$v$和$w$的线性组合。</p><h2 id="向量空间-vector-spaces">向量空间（Vector Spaces）</h2><p>Vector spaces是向量的集合，通常表示为$\mathbb{R}^1 , \mathbb{R}^2 , \mathbb{R}^n $。$\mathbb{R}^5 $表示所有$5$维的column vectors。</p><h3 id="定义-v2">定义</h3><p>Space $\mathbb{R}^n $是所有$n$维column vectors $v$组成的space。</p><h2 id="子空间-subspaces">子空间（Subspaces）</h2><h3 id="定义-v3">定义</h3><p>某个vector space的subspace是满足以下条件的vectors的集合，如果$v$和$w$是subspace中的vectors，并且$c$是任意的salar，</p><ol><li>$v+w$还在subspace中</li><li>$cv$还在subspace</li></ol><p>也就是说subspace是对加法和数乘封闭的vectors set，所有的线性组合仍然还在这个subspace。</p><h3 id="例子">例子</h3><ol><li>所有的subspace都包括zero vector。</li><li>通过原点的直线都是subspace。</li><li>包含$v$和$w$的subspace一定得包含所有的线性组合$cv+dw$</li><li>给定两个subspace $S,T$<ol><li>$S\cup T$不是一个subspace</li><li>$S\cap T$是一个subspace，证明<br>假设$v,w$是$S\cap T$的，则$v,w\in S, v,w\in T$，$v+w\in S, v+w\in T, cv+dw \in S, cv+dw \in T$，所以$cv+dw \in S\cap T$</li></ol></li></ol><h2 id="列空间-column-space">列空间（Column Space）</h2><h3 id="创建矩阵的subspace">创建矩阵的subspace</h3><p>取矩阵$A$的column vectors，计算它们的所有线性组合，借得到了一个subspace</p><h3 id="定义-v4">定义</h3><p>给定矩阵$A$，$A$的所有column vectors的linear combinations组成的subspace称为column space，用$C(A)$表示。$C(A)$由$Ax$的所有可能取值构成。</p><h3 id="性质">性质</h3><ol><li>当且仅当$b$在$A$的column space中，$Ax=b$才有解。</li><li>假设$A$是$m\times n$矩阵，$A$的column space是$\mathbb{R}^m $的subspace。</li></ol><h2 id="零空间-nullspace">零空间（Nullspace）</h2><h3 id="定义-v5">定义</h3><p>矩阵$A$的nullspace是所有$Ax=0$的解构成的vector space，用$N(A)$表示。$N(A)$是$\mathbb{R}^n $的subspace，因为$x$是在$\mathbb{R}^n $中的$n$维向量，所以是$\mathbb{R}^n $的subspace。</p><h2 id="special-solution-主元-自由变量-special-solution-pivot-variables和free-variables-pivot-columns和free-columns">special solution，主元，自由变量（Special solution, Pivot variables和free variables, Pivot columns和free columns）</h2><h3 id="special-solution">special solution</h3><p>如果方程数量小于未知数数量，那么这个方程（组）有无穷多个解，为了表示这个方程组，指定special solution来表示它。<br>如方程组<br>\begin{cases}x_1+2x_2=0\\3x_1+6x_2 = 0 \end{cases}<br>上面的方程组其实是一个方程$x_1+2x_2=0$，两个未知数。随便的选择一个变量，让它的值为$1$，求出另一个$x$。比如令$x_2 = 1$，那么$x_1 = -2$。我们就称$(x_1=-2,x_2=1)$为一个special solution。<br>再给一个例子，$x+2y+3z=0$，有两个special solution，随机选择两个变量，分别让其中一个取$1$，剩余的另一个取$0$，求解出来最后的一个变量。</p><h3 id="主元-主元列-自由变量-自由列">主元，主元列，自由变量，自由列</h3><p>我们通常把选定的两个变量叫做free variables，其他的那些变量叫做pivot variables。比如第一个例子中，$x_2$是free variable，$x_1$是pivot variable。第二个例子中，$x_2, x_3$是free variables，$x_1$是pivot variables。主元所在的column叫做pivot column，free variables所在的column叫做free columns。</p><h2 id="秩-rank">秩（rank）</h2><p>矩阵$A$的秩（rank），用$r(A)$表示，它等于pivots的数量，等于column space的维度，等于row space的维度。</p><h2 id="消元法解-ax-0">消元法解$Ax=0$</h2><p>两个步骤：</p><ol><li>将矩阵$A$化为三交矩阵$U$</li><li>解$Ux=0$或者$Rx=0$</li></ol><h3 id="示例">示例</h3><p>矩阵$A= \begin{bmatrix}1&amp;1&amp;2&amp;3\\2&amp;2&amp;8&amp;10\\ 3&amp;3&amp;10&amp;13\end{bmatrix}$化成三角矩阵为：$U= \begin{bmatrix}1&amp;1&amp;2&amp;3\\0&amp;0&amp;4&amp;4\\ 0&amp;0&amp;0&amp;0\end{bmatrix}$，第一列和第三列是pivot columns，第二列和第四列是free columns，然后求出special solutions，再计算出通解。<br>对于每个free variabled都有一个special solution，$Ax=0$共有$r$个pivots，以及$n-r$个free variables，$A$的nullspace $N(A)$包含$n-r$个special solutions，$N(A)$具有如下的形式：<br>$$N = \begin{bmatrix} -F\\I\end{bmatrix}$$<br>其中$F$为free variables取特值的时候，pivtos的取值，$I$为free variables的取值。</p><h2 id="行简化阶梯形矩阵-thre-reduced-row-echelon-matrix">行简化阶梯形矩阵（Thre reduced row echelon matrix）</h2><p>行简化阶梯形矩阵是pivot colunmn恰好构成单位矩阵的矩阵，如：<br>$$U = \begin{bmatrix}1&amp;1&amp;0&amp;1\\0&amp;0&amp;1&amp;1\\0&amp;0&amp;0&amp;0\end{bmatrix}$$<br>所有的pivots都是$1$，主元所在列的其余位置都是$0$。<br>行简化阶梯形矩阵给出了很多有用信息：</p><ol><li>pivot columns</li><li>pivot rows</li><li>pivots是$1$</li><li>zero rows显示这一行是其他rows的lihear combination</li><li>free columns</li></ol><h2 id="ax-b-的通解">$Ax=b$的通解</h2><p>先求出particular solution，让所有的free variables取$0$，求解出pivots，得到一个solution，我们称它为particular solution。然后求出所有的special solutions，则$Ax=b$的通解可以表示为：<br>$$x = x_p + a x_{special_solution_1} + b x_{special_solution} + \cdots=x_p+x_n$$<br>即particular solution加上nullspace组成的新的vector sets。当没有free variables的时候，也就没有special solutions，nullspace为空。</p><!--当$b=0$的时候，我们可以求出通解$x$，当$b\neq 0$时，有点难。通过使用增广矩阵：$\left[A\ b\right]$，然后进行消元，得到$\left[R\ d\right]$，$R$是行间化阶梯形矩阵，$d$是$b$做了和$A$一样的变换后的结果。--><h2 id="列满秩">列满秩</h2><h3 id="定义-v6">定义</h3><p>对于$m\times n$的矩阵$A$，每一列都有一个pivot，rank $r=n$，matrix是瘦高的$(m\ge n)$，其实就相当于每一个column vector都用到了，没有多余的column vector。可以用以下的形式表示：<br>$$R = \begin{bmatrix}I\\0\end{bmatrix}=\begin{bmatrix}n\times n 单位矩阵\\m-n行零向量\end{bmatrix}$$</p><h3 id="属性">属性</h3><p>当$A$列满秩的时候，有以下结论：</p><ol><li>$A$的所有columns都是pivot columns</li><li>没有free variables，free columns和special solutions</li><li>Nullspace只有$x=0$</li><li>如果$Ax=b$有解，那么它只有一个解，或者一个解都没有</li></ol><h2 id="行满秩">行满秩</h2><h3 id="定义-v7">定义</h3><p>对于$m\times n$的矩阵$A$，如果$r=m$的话，$A$是一个矮胖的矩阵$(m\le n)$，每一行都有一个pivot。<br>$$R = \begin{bmatrix}I&amp;F\end{bmatrix}=\begin{bmatrix}m\times m 单位矩阵&amp;F\end{bmatrix}$$</p><h3 id="属性-v2">属性</h3><p>当$A$行满秩的时候，有以下结论：</p><ol><li>$A$的所有row都有pivots，$R$没有$0$向量</li><li>对于任何$b$，$Ax=b$都有解</li><li>column space就是整个$\mathbb{R}^m$</li><li>总共有$n-r= n-m$个special solutions。</li></ol><h2 id="秩和方程解个数之间的关系">秩和方程解个数之间的关系</h2><ol><li>$r=m, r=n$,可逆方阵，$Ax=b$有且只有一个解，$R=\begin{bmatrix}I\end{bmatrix}$</li><li>$r=m, r\lt n$,矮胖，$Ax=b$有无穷多个解，一个particular solution加上nullspace中的无穷个，$R=\begin{bmatrix}I&amp;F\end{bmatrix}$</li><li>$r\lt m, r=m$,瘦高，$Ax=b$没有或者只有一个解，如果$b$恰好在$A$的column space中有一个解，如果$b$恰好不在$A$的column space中无解，因为column vectors是相互独立的，所以$Ax=0$只有零解，$R=\begin{bmatrix}I\\0\end{bmatrix}$</li><li>$r\lt m, r\lt n$,并不满秩，$Ax=b$无解或者有无穷多个解，无解的情况是不在$A$的column space中，有解的情况是 在$A$的column space中，而在这部分中，又有无穷多个零解，所以要不无解要不无穷多个解。$R=\begin{bmatrix}I&amp;F\\0&amp;0\end{bmatrix}$</li></ol><h2 id="线性独立-linear-independence">线性独立（Linear independence）</h2><p>矩阵$A$的columns是linear independent的，当且仅当$Ax=0$的唯一解是$x=0$时。也就是说$A$的nullspace只有零向量的时候。</p><h3 id="定义-v8">定义</h3><p>给定一系列向量$v_1, \cdots, v_n$，$c_1 v_1 +\cdots+c_n v_n=0$当且仅当$c_1, \cdots, c_n=0$时候成立。</p><h2 id="生成-span">生成（Span）</h2><h3 id="定义-v9">定义</h3><p>使用一系列vectors生成space的过程就叫做span。</p><h2 id="行空间-row-space">行空间（Row Space）</h2><h3 id="定义-v10">定义</h3><p>使用矩阵的row vector生成的subspace就叫做row space，表示维$C(A<sup>T)$，它和$A</sup>T$的column space是相同的。</p><h2 id="基-basis">基（Basis）</h2><h3 id="定义-v11">定义</h3><p>生成space的最小vectors的independent vectors叫做这个space的一组basis，basis不是唯一的。</p><h3 id="示例-v2">示例</h3><p>矩阵的pivot columns是它的column space的一组basis。</p><h2 id="维度-dimension">维度（Dimension）</h2><h3 id="定义-v12">定义</h3><p>Space的dimension指的是每组basis中向量的个数。对于一个space，不同的baisis，它们的vectors不同，但是向量的个数都是相同的，这是space的属性。</p><h2 id="秩和维度的关系">秩和维度的关系</h2><p>和矩阵$A$相关的主要有四个subspace，分别是column space, nullspace, row space以及left nullspace。它们四个具有的属性如下所示：</p><ol><li>row space和column space的dimension都是$r$</li><li>nullspace和left nullspace的dimension是$n-r, m-r$，为什么是$n-r,m-r$，解$Ax=0$得到$x$是$n$维向量，也就是nullspace是$\mathbb{R}^n$的subspace，$A$的column space的dimension是$r$，free variables，free columns的个数就是$n-r$，special solutions的个数就是$n-r$，而nullspace的basis其实就是所有的special solutions，所以nullspace的dimension就是$n-r$，$m-r$同理。</li></ol><h2 id="a-和-r-的维度和基的关系">$A$和$R$的维度和基的关系</h2><p>这里的$A$是矩阵，$R$是行间化阶梯形矩阵</p><ol><li>$A$和$R$的row space相同，dimension相同，为$r$，basis相同</li><li>$A$和$R$的column space不同，dimension相同，也为$r$，basis不同</li><li>$A$和$R$的nullspace相同，dimension相同，为$n-r$,basis相同</li><li>$A$和$R$的left nullspace不同，dimension相同，为$m-r$</li></ol><p>Row space和nullspace都是$\mathbb{R}^n $的subspace，它们的dimension加起来等于n，但是这两个subspace加起来并不是$\mathbb{R}^n $。Row space是对$r$个$n$维pivot row vectors进行linear combination，而nullspace是对$n-r$个$n$维的解向量$x$进行linear combination，这里虽然都出现了$n$，但是第一个$n$是row vector的长度$n$，而第二个$n$是解向量的$n$。<br>同理，可以得column space和left nullspace都是$\mathbb{R}^m$的subspace。</p><h2 id="参考文献">参考文献</h2><p>1.MIT线性代数公开课</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;向量空间和子空间-vector-spaces-and-subspaces&quot;&gt;向量空间和子空间（vector spaces and subspaces）&lt;/h2&gt;
&lt;p&gt;这一件介绍和space相关的概念以及很多其他的基础知识。&lt;/p&gt;
&lt;h2 id=&quot;线性组合-li
      
    
    </summary>
    
      <category term="线性代数" scheme="http://mxxhcm.github.io/categories/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/"/>
    
    
      <category term="线性代数" scheme="http://mxxhcm.github.io/tags/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/"/>
    
      <category term="spaces" scheme="http://mxxhcm.github.io/tags/spaces/"/>
    
      <category term="subspaces" scheme="http://mxxhcm.github.io/tags/subspaces/"/>
    
  </entry>
  
  <entry>
    <title>手动bp</title>
    <link href="http://mxxhcm.github.io/2019/08/24/%E6%89%8B%E5%8A%A8bp/"/>
    <id>http://mxxhcm.github.io/2019/08/24/手动bp/</id>
    <published>2019-08-24T11:58:18.000Z</published>
    <updated>2019-08-24T12:00:43.683Z</updated>
    
    <content type="html"><![CDATA[<h2 id="参考文献">参考文献</h2><p>0.<a href="https://datascience.stackexchange.com/questions/27506/back-propagation-in-cnn" target="_blank" rel="noopener">https://datascience.stackexchange.com/questions/27506/back-propagation-in-cnn</a><br>1.<a href="https://medium.com/@14prakash/back-propagation-is-very-simple-who-made-it-complicated-97b794c97e5c" target="_blank" rel="noopener">https://medium.com/@14prakash/back-propagation-is-very-simple-who-made-it-complicated-97b794c97e5c</a><br>2.<a href="https://medium.com/@pavisj/convolutions-and-backpropagations-46026a8f5d2c" target="_blank" rel="noopener">https://medium.com/@pavisj/convolutions-and-backpropagations-46026a8f5d2c</a><br>3.<a href="https://becominghuman.ai/back-propagation-in-convolutional-neural-networks-intuition-and-code-714ef1c38199" target="_blank" rel="noopener">https://becominghuman.ai/back-propagation-in-convolutional-neural-networks-intuition-and-code-714ef1c38199</a><br>4.<a href="http://www.robots.ox.ac.uk/~vgg/practicals/cnn/" target="_blank" rel="noopener">http://www.robots.ox.ac.uk/~vgg/practicals/cnn/</a><br>5.<a href="https://www.researchgate.net/post/How_backpropagation_works_for_learning_filters_in_CNN" target="_blank" rel="noopener">https://www.researchgate.net/post/How_backpropagation_works_for_learning_filters_in_CNN</a><br>6.<a href="https://github.com/ivallesp/awesome-optimizers" target="_blank" rel="noopener">https://github.com/ivallesp/awesome-optimizers</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;参考文献&quot;&gt;参考文献&lt;/h2&gt;
&lt;p&gt;0.&lt;a href=&quot;https://datascience.stackexchange.com/questions/27506/back-propagation-in-cnn&quot; target=&quot;_blank&quot; rel=&quot;no
      
    
    </summary>
    
      <category term="机器学习" scheme="http://mxxhcm.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://mxxhcm.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="bp" scheme="http://mxxhcm.github.io/tags/bp/"/>
    
  </entry>
  
  <entry>
    <title>花书第二章</title>
    <link href="http://mxxhcm.github.io/2019/08/22/Deep-Learning-%E7%AC%AC%E4%BA%8C%E7%AB%A0/"/>
    <id>http://mxxhcm.github.io/2019/08/22/Deep-Learning-第二章/</id>
    <published>2019-08-22T08:44:26.000Z</published>
    <updated>2019-08-29T04:01:23.762Z</updated>
    
    <content type="html"><![CDATA[<h2 id="线性代数">线性代数</h2><p>这一章介绍一些线性代数的知识</p><h2 id="标量-向量-矩阵和张量">标量，向量，矩阵和张量</h2><p>标量，单个的数，用不加粗小写斜体字母表示。<br>向量，有序的一维数组，用加粗小写斜体字母表示，向量的每一个元素加下标访问。一般一个$n$维的向量表示为：<br>$$\mathbf{x} = \begin{bmatrix}x_1\\ \cdots x_n\end{bmatrix} = (x_1,\cdots, x_n)^T$$<br>矩阵：二维数组，用加粗大写斜体字母表示，向量的每个元素需要使用两个索引才能定位。<br>张量：多维数组，用加粗大写非斜体字母表示，访问一个$k$维的张量，需要使用$k$个索引才能进行定位。</p><h2 id="向量和矩阵乘法">向量和矩阵乘法</h2><h3 id="矩阵乘法">矩阵乘法</h3><p>矩阵乘法要求两个矩阵的shape满足第一个矩阵的第二维和第二个矩阵的第一维相等。Shape为$m\times n$的矩阵$A$和shape维$n\times p$的矩阵$B$，使用矩阵相乘得到一个$m\times p$的矩阵$C$，计算公式如下：<br>$$C_{i,j} = \sum_{k=1}^n A_{i,k}B_{k,j}$$</p><h3 id="hadamard乘法-element-wise乘法">Hadamard乘法(element-wise乘法)</h3><p>Hadamrad乘法要求两个矩阵的shape必须相同。Shape为$m\times n$的矩阵$A$和shape维$m\times n$的矩阵$B$，使用矩阵相乘得到一个$m\times n$的矩阵$C$，计算公式如下：<br>$$C_{i,j} = A_{i,j}B_{i,j}$$</p><h3 id="向量点乘">向量点乘</h3><p>向量点乘要求两个向量$x$和$y$的维度相同，点乘的结果可以看成$1\times n$的矩阵和$n\times 1$的矩阵进行矩阵乘法的结果。</p><h3 id="矩阵乘向量">矩阵乘向量</h3><p>矩阵乘向量需要满足矩阵的第二维和向量的维度一致，得到新向量的维度和原来的向量维度一样：<br>$$Ax= b$$</p><h3 id="其他定律">其他定律</h3><p>矩阵乘法有分配率，结合律，但是没有交换律。<br>转置的一个公式<br>$$(AB)^T = B^T A^T $$</p><h2 id="单位矩阵和可逆矩阵">单位矩阵和可逆矩阵</h2><p>除了对角线上为$1$所有其他位置都为$0$的矩阵被称为单位矩阵，用$I_n$表示。矩阵$A$的可逆矩阵被表示为$A^{-1}$，定位为：<br>$$A^{-1}A= I_n$$<br>如果矩阵$A$的逆存在，那么$Ax=b$的解就是$x = A^{-1}b$。</p><h2 id="线性独立和线性生成子空间-span">线性独立和线性生成子空间（span）</h2><p>如果$Ax=b$有且只有一个解，那么$A^{-1}$一定存在。但是有时候会没有解或者无穷多个解。不可能存在大于一个但是小于无穷多个解的情况，如果$x,y$是$Ax=b$的解，那么<br>$\alpha x + (1 - \alpha) y$也一定是$Ax=b$的解。<br><strong>线性组合</strong>：对于向量集中的每个向量乘上一个系数再相加得到的结果就是一个线性组合。最简单的形式：$ax+by$就是一个线性组合，它的结果就是一个span（线性生成子空间）。$Ax$也是一个线性组合，它有一个特殊的名字，叫做$A$的column space，如果$b$在$A$的colum中，那么这个方程组有且只有一个解。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;线性代数&quot;&gt;线性代数&lt;/h2&gt;
&lt;p&gt;这一章介绍一些线性代数的知识&lt;/p&gt;
&lt;h2 id=&quot;标量-向量-矩阵和张量&quot;&gt;标量，向量，矩阵和张量&lt;/h2&gt;
&lt;p&gt;标量，单个的数，用不加粗小写斜体字母表示。&lt;br&gt;
向量，有序的一维数组，用加粗小写斜体字母表示，向量的每
      
    
    </summary>
    
      <category term="深度学习" scheme="http://mxxhcm.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="Deep Learning" scheme="http://mxxhcm.github.io/tags/Deep-Learning/"/>
    
      <category term="深度学习" scheme="http://mxxhcm.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="线性代数" scheme="http://mxxhcm.github.io/tags/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/"/>
    
  </entry>
  
  <entry>
    <title>均值、方差、矩、协方差、独立和相关</title>
    <link href="http://mxxhcm.github.io/2019/08/08/mean-variance-covariance-dependent-independent/"/>
    <id>http://mxxhcm.github.io/2019/08/08/mean-variance-covariance-dependent-independent/</id>
    <published>2019-08-08T11:01:15.000Z</published>
    <updated>2019-09-17T12:45:11.101Z</updated>
    
    <content type="html"><![CDATA[<h2 id="均值-期望">均值（期望）</h2><h3 id="简单介绍">简单介绍</h3><p>在概率论和统计学中，一个离散型随机变量的期望值是试验中每次可能的结果乘以相应概率的加和。<br>换句话说，期望值是变量取值的加权平均。期望并不一定包含于变量的值域内。</p><h3 id="定义">定义</h3><p>如果$X$是概率空间$(\Omega, F,P)$中的随机变量，它的期望值$\mathbb{E}(X)$定义为：<br>$$\mathbb{E}(X) = \int_{\Omega} XdP$$<br>如果$X$是离散的随机变量，取值$x_1,x_2,\cdots$的概率分别为$p_1, p_2, \cdots, p_1+p_2+\cdots = 1$，它的期望定义为：<br>$$\mathbb{E}(X) = \sum_i x_ip_i$$</p><h3 id="性质">性质</h3><ul><li>$\mathbb{E}(cX) = c\mathbb{E}(X)$</li><li>$\mathbb{E}(X+c) = \mathbb{E}(X) + c$</li><li>$\mathbb{E}(X+Y) = \mathbb{E}(X) + \mathbb{E}(Y)$</li><li>$\mathbb{E}(XY) = \mathbb{E}(X)\mathbb{E}(Y)$，（$X,Y$独立）</li><li>$\mathbb{E}(aX+bY) = a\mathbb{E}(X) + b\mathbb{E}(Y)$</li></ul><h2 id="方差">方差</h2><h3 id="简单介绍-v2">简单介绍</h3><p>方差描述的是一个随机变量的离散程度。</p><h3 id="定义-v2">定义</h3><p>假设$X$是服从分布$F$的随机变量，用$\mu=\mathbb{E}(X)$表示随机变量$X$的期望。<br>随机变量$X$的方差为：<br>$$Var(X) = \mathbb{E}\left[(X-\mu)^2\right]$$</p><h3 id="性质-v2">性质</h3><ol><li>方差总是大于等于$0$的<br>$$Var(X)\ge 0$$</li><li>常数的方差为$0$<br>$$Var ( c ) = 0$$</li><li>随机变量$X$加上一个常数$c$，它的方差不变<br>\begin{align*}<br>Var(X+c) &amp;= \mathbb{E}\left[(X+c - \mathbb{E}(X+c))^2\right]\\<br>&amp;= \mathbb{E}\left[(X-\mathbb{E}(X))^2\right]\\<br>&amp;=Var(X)<br>\end{align*}</li><li>随机变量$X$乘上一个常数$c$，它的方差变为原来的$c^2$倍。<br>\begin{align*}<br>Var(cX) &amp;= \mathbb{E}\left[(cX - \mathbb{E}(cX))^2\right]\\<br>&amp;= \mathbb{E}\left[c^2 X^2 - 2c^2 X \mathbb{E}(X) + (\mathbb{E}(cX))^2\right]\\<br>&amp; = c^2 \mathbb{E}\left[X^2 - 2X \mathbb{E}(X) + \mathbb{E}(X)\cdot \mathbb{E}(X)\right]\\<br>&amp;=c^2 Var(x)<br>\end{align*}</li></ol><h2 id="期望和方差关系">期望和方差关系</h2><p>\begin{align*}<br>Var(X) &amp;= \mathbb{E}\left[(X-\mathbb{E}\left[X\right])^2\right]\\<br>&amp;= \mathbb{E}\left[(X^2 - 2X \mathbb{E} \cdot \left[X\right]+\mathbb{E}\left[X\right] \cdot \mathbb{E}\left[X\right] )\right]\\<br>&amp;= \mathbb{E}\left[(X^2) \right] - 2\mathbb{E}\left[(X \cdot \mathbb{E}\left[ X \right]) \right]+\mathbb{E}\left[(\mathbb{E}\left[X \right] \cdot \mathbb{E}\left[ X \right])\right]\\<br>&amp;= \mathbb{E}    \left[(X^2) \right] -     \mathbb{E}\left[(\mathbb{E}\left[X \right] \cdot \mathbb{E}\left[ X \right])\right]\\<br>&amp;= \mathbb{E}    \left[(X^2) \right] -  \mathbb{E}\left[X \right] \cdot \mathbb{E}\left[ X \right]\\<br>\end{align*}</p><h2 id="均值和样本均值">均值和样本均值</h2><p>样本均值是均值的无偏估计。<br>假设用随机变量$X$表示所有男性的身高，得到一组样本值为$x_1, x_2, \cdots, x_n$，样本均值的计算方式如下：<br>$$\bar{X} = \frac{x_1 + \cdots +x_n}{n}$$<br>$\bar{X}$是对$\mu$的一个估计，真实的$\mu$我们不知道，但是我们能根据多个样本计算出来多个样本均值，这些样本均值的均值会收敛于均值。所以叫它无偏估计。</p><h2 id="方差和样本方差">方差和样本方差</h2><p>样本方差的分母如果是$n$的话，是方差的有偏估计，如果分母是$n-1$的话，是方差的无偏估计。<br>随机变量$X$方差的计算公式为：<br>$$Var(X) = \mathbb{E}\left[(X-\mu)^2\right]$$<br>但是我们往往不知道$X$的真实分布，所以经常用样本方差$S^2$来估计真实方差$Var(X)$：<br>$$S^2 = \frac{\sum_{i=1}^n (X_i - \mu)^2}{n}$$<br>而均值$\mu$往往也是不知道的，用样本均值$\bar{X}$代替均值$\mu$，得到<br>$$S^2 = \frac{1}{n-1}\sum_{i=1}^n (X_i - \bar{X})^2$$<br>为什么是$\frac{1}{n-1}$呢？<br>这里先给出几个辅助计算：</p><ol><li>样本均值的方差是样本方差的$\frac{1}{n}$<br>\begin{align*}<br>Var(\bar{X}) &amp;= Var(\frac{\sum_{i=1}^n X_i}{n})\\<br>&amp; = \frac{1}{n^2} Var(\sum_{i=1}^n X_i)\\<br>&amp; =  \frac{1}{n^2} \sum_{i=1}^n Var(X_i)\\<br>&amp; = \frac{n\sigma^2 }{n^2 }\\<br>&amp;=\frac{\sigma^2 }{n}<br>\end{align*}</li></ol><p>如果使用$\frac{1}{n-1}\sum_{i=1}^n (X_i - \bar{X})^2$进行计算<br>\begin{align*}<br>\mathbb{E}\left[\frac{1}{n}\sum_{i=1}^n \left(X_i-\bar{X}\right)^2\right]&amp;= \mathbb{E}\left[\frac{1}{n}\sum_{i=1}^n \left(X_i-\bar{X}+\mu-\mu\right)^2 \right]\\<br>&amp;= \mathbb{E}\left[\frac{1}{n}\sum_{i=1}^n \left(X_i-\mu - (\bar{X} - \mu)\right)^2 \right]\\<br>&amp;= \mathbb{E}\left[\frac{1}{n}\sum_{i=1}^n \left((X_i-\mu)^2 - 2(X_i-\mu)(\bar{X} - \mu) + (\bar{X} - \mu)^2 \right)\right]\\<br>&amp;= \mathbb{E}\left[\frac{1}{n}\sum_{i=1}^n (X_i-\mu)^2 - \frac{2}{n}\sum_{i=1}^n (X_i-\mu)(\bar{X} - \mu) + \frac{1}{n}\sum_{i=1}^n (\bar{X} - \mu)^2 \right]\\<br>&amp;= \mathbb{E}\left[\frac{1}{n}\sum_{i=1}^n (X_i-\mu)^2 - 2(\bar{X}-\mu)(\bar{X} - \mu) + \frac{1}{n}\sum_{i=1}^n (\bar{X} - \mu)^2 \right]\\<br>&amp;= \mathbb{E}\left[\frac{1}{n}\sum_{i=1}^n (X_i-\mu)^2 - 2(\bar{X}-\mu)^2 +  (\bar{X} - \mu)^2 \right]\\<br>&amp;= \mathbb{E}\left[\frac{1}{n}\sum_{i=1}^n (X_i-\mu)^2 - (\bar{X}-\mu)^2\right]\\<br>&amp;= \mathbb{E}\left[\frac{1}{n}\sum_{i=1}^n (X_i-\mu)^2 \right]-\mathbb{E}\left[ (\bar{X}-\mu)^2\right]\\<br>&amp;= Var(X) - Var(\bar{X})\\<br>&amp;= \sigma^2 - \frac{\sigma^2}{n}\\<br>&amp;= \frac{n-1}{n}\sigma^2<br>\end{align*}<br>最后得到$\mathbb{E}\left[\frac{1}{n}\sum_{i=1}^n \left(X_i-\bar{X}\right)^2  \right] = \frac{n-1}{n}\sigma^2 $，少算了$\frac{1}{n}\sigma^2 $，<br>所以，如果取$\mathbb{E}\left[S^2 \right] =\mathbb{E}\left[\frac{1}{n-1}\sum_{i=1}^n \left(X_i-\bar{X}\right)^2 \right]$，最后能得到$\mathbb{E}\left[S^2 \right] =\sigma^2$，是无偏估计。</p><h2 id="矩">矩</h2><p>设$X$为随机变量，$c$为常数，$k$为正整数，称$\mathbb{E}\left[(X-c)^k\right] = \int_{-\infty}^{\infty} (X-c)^k f(X) dX $ 为$X$关于$c$的$k$阶矩。</p><ol><li>$c=0, \alpha_k=\mathbb{E}\left[X^k\right] = \int_{-\infty}^{\infty} X^k f(X) dX  $称为$X$的$k$阶原点矩。</li><li>$c=\mathbb{E}\left[X\right],\mu_k=\mathbb{E}\left[(X-\mathbb{E}(X))^k \right] = \int_{-\infty}^{\infty}( X^k - \mathbb{E}(X))^k f(X)dX $称为$X$的$k$阶中心矩。</li></ol><p>一阶原点矩为期望，任意随机变量的一阶中心距为$0$，二阶中心矩为方差。三阶中心矩表示偏度，任何对称分布的偏度为$0$，分布尾部在左侧比较长具有负偏度，分布尾部在右侧较长具有正偏度。四阶中心距表示峰度，俗称方差的方差。</p><h2 id="协方差">协方差</h2><p>方差和标准差通常是用来描述一维随机变量的特性。那么对于多维随机变量来说，怎么衡量它们之间的关系呢？引入了协方差衡量两维随机变量之间的关系。</p><h3 id="定义-v3">定义</h3><p>$\mathbb{E}\left[(X-\mathbb{E}(X))(Y-\mathbb{E}(Y)\right]$称为$X,Y$的协方差，记为$Cov(X,Y)$。<br>当$X=Y$时，其实就是方差的定义。</p><h3 id="属性">属性</h3><ol><li>\begin{align*}<br>Cov(c_1X+c_2, c_3Y + c_4) &amp;= \mathbb{E}\left[(c_1X +c_2 - \mathbb{E}(c_1X+c_2))(c_3Y +c_4 - \mathbb{E}(c_3Y+c_4))\right]\\<br>&amp;= \mathbb{E}\left[(c_1X - c_1\mathbb{E}\left[X\right])(c_3Y - c_3\mathbb{E}\left[Y\right])\right]\\<br>&amp;= c_1c_3\mathbb{E}\left[(X - \mathbb{E}\left[X\right])(Y - \mathbb{E}\left[Y\right])\right]\\<br>&amp;= c_1c_3Cov(X,Y)\\<br>\end{align*}</li><li>\begin{align*}<br>Cov(X,Y) &amp;= \mathbb{E}\left[(X-\mathbb{E}\left[X\right])(Y-\mathbb{E}\left[Y\right])\right]\\<br>&amp;= \mathbb{E}\left[XY - \mathbb{E}\left[Y\right]X - \mathbb{E}\left[X\right]Y + \mathbb{E}\left[X\right]\mathbb{E}\left[Y\right]\right]\\<br>&amp;= \mathbb{E}\left[XY\right] - \mathbb{E}\left[X\right]\mathbb{E}\left[Y\right]\\<br>\end{align*}</li></ol><h3 id="独立与协方差之间的关系">独立与协方差之间的关系</h3><p>若$X,Y$独立，则$Cov(X,Y) = 0$，因为独立变量有$\mathbb{E}(XY) = \mathbb{E}(X)\mathbb{E}(Y)$，所以：<br>$$\mathbb{E}(XY) - \mathbb{E}(X)\mathbb{E}(Y) = Cov(X,Y) = 0$$</p><h3 id="协方差矩阵">协方差矩阵</h3><p>用一个矩阵表示多维随机变量之间的关系，比如三个维度的随机变量。可以两两求出它们之间的协方差，因为协方差是对称的，所以这个矩阵是对称矩阵，对角线元素是方差。<br>协方差矩阵一般记为$\Sigma$，计算公式如下：<br>$$\sigma = \mathbb{E}\left[(\mathbf{X} - \mathbb{E}\left[\mathbf{X}\right])(\mathbf{X} - \mathbb{E}\left[\mathbf{X}\right])^T \right]$$<br>这里的$\mathbf{X}$是多维的随机向量，而方差和期望中的$\mathbf{X}$是一维的随机变量。</p><h2 id="相关系数">相关系数</h2><h3 id="定义-v4">定义</h3><p>称$Cov(X,Y)/(\sigma_1,\sigma_2)$为$X,Y$的相关系数，记为$Corr(X,Y)$。</p><h3 id="性质-v3">性质</h3><ol><li>若$X,Y$独立，则$Covv(X,Y) = 0$</li><li>$\Vert Corr(X,Y)\Vert \le 1$，等号当且仅当$X$和$Y$有严格线性关系时取到</li><li>当$Corr(X,Y)= 0$或者$Cov(X,Y) = 0$时，称$X,Y$不相关。</li></ol><h2 id="独立和相关">独立和相关</h2><p>$X,Y$独立能够推出他们不相关，因为$Corr(X,Y) = 0$，满足不相关的定义。而$Corr(X,Y) = 0$不一定能推出$X,Y$独立。<br>即独立一定相关，但是相关不一定独立。有一个特例是正太分布，独立和相关互为充要条件。</p><h2 id="参考文献">参考文献</h2><p>1.<a href="https://zh.wikipedia.org/wiki/%E6%9C%9F%E6%9C%9B%E5%80%BC" target="_blank" rel="noopener">https://zh.wikipedia.org/wiki/期望值</a><br>2.<a href="https://zh.wikipedia.org/wiki/%E6%96%B9%E5%B7%AE" target="_blank" rel="noopener">https://zh.wikipedia.org/wiki/方差</a><br>3.<a href="https://www.zhihu.com/question/22983179/answer/404391738" target="_blank" rel="noopener">https://www.zhihu.com/question/22983179/answer/404391738</a><br>4.<a href="https://www.matongxue.com/madocs/607.html" target="_blank" rel="noopener">https://www.matongxue.com/madocs/607.html</a><br>5.<a href="https://math.stackexchange.com/a/2113753/629287" target="_blank" rel="noopener">https://math.stackexchange.com/a/2113753/629287</a><br>6.<a href="https://blog.csdn.net/yangdashi888/article/details/52397990" target="_blank" rel="noopener">https://blog.csdn.net/yangdashi888/article/details/52397990</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;均值-期望&quot;&gt;均值（期望）&lt;/h2&gt;
&lt;h3 id=&quot;简单介绍&quot;&gt;简单介绍&lt;/h3&gt;
&lt;p&gt;在概率论和统计学中，一个离散型随机变量的期望值是试验中每次可能的结果乘以相应概率的加和。&lt;br&gt;
换句话说，期望值是变量取值的加权平均。期望并不一定包含于变量的值域内。&lt;/
      
    
    </summary>
    
      <category term="概率论" scheme="http://mxxhcm.github.io/categories/%E6%A6%82%E7%8E%87%E8%AE%BA/"/>
    
    
      <category term="概率论" scheme="http://mxxhcm.github.io/tags/%E6%A6%82%E7%8E%87%E8%AE%BA/"/>
    
      <category term="均值" scheme="http://mxxhcm.github.io/tags/%E5%9D%87%E5%80%BC/"/>
    
      <category term="方差" scheme="http://mxxhcm.github.io/tags/%E6%96%B9%E5%B7%AE/"/>
    
      <category term="样本均值" scheme="http://mxxhcm.github.io/tags/%E6%A0%B7%E6%9C%AC%E5%9D%87%E5%80%BC/"/>
    
      <category term="样本方差" scheme="http://mxxhcm.github.io/tags/%E6%A0%B7%E6%9C%AC%E6%96%B9%E5%B7%AE/"/>
    
  </entry>
  
  <entry>
    <title>reinforcement learning an introduction 第8章笔记</title>
    <link href="http://mxxhcm.github.io/2019/08/07/reinforcement-learning-an-introduction-%E7%AC%AC8%E7%AB%A0%E7%AC%94%E8%AE%B0/"/>
    <id>http://mxxhcm.github.io/2019/08/07/reinforcement-learning-an-introduction-第8章笔记/</id>
    <published>2019-08-07T08:32:52.000Z</published>
    <updated>2019-08-20T13:05:53.387Z</updated>
    
    <content type="html"><![CDATA[<h2 id="planning-and-learning-with-tabular-methods">Planning and Learning with Tabular Methods</h2><p>前面几章，介绍了MC算法，TD算法，再用$n$-step TD框架把它们统一了起来，此外，这些算法都属于model free的方法。这一章要介绍model based方法，model based方法主要用于planning，而model free的方法主要用于learning。</p><h2 id="models和planning">Models和Planning</h2><h3 id="model">Model</h3><h4 id="定义">定义</h4><p>Environment的model是agents用来预测environment会如何对agent的action做出响应的任何东西。给定一个state和一个action，model可以预测下一个state和action。如果model是stochastic的话，有多个可能的next states和rewards，每一个都有一定概率。Distribution models计算所有可能性以及相应概率，sample models根据概率进行采样，只计算采样的结果。</p><h4 id="用处">用处</h4><p>Model可以用来模仿或者仿真。给定一个start state和action，sample model产生一个transition，distribution model产生所有可能的transitions，并使用概率进行加权。给定start state和一个policy，sample model产生一个episode，distribution model产生所有可能的episodes以及相应的概率。Model被用来模拟environment或者产生simulated experience。</p><h4 id="示例">示例</h4><p>DP中使用的$p(s’, r|s,a)$就是distribution model，第五章中blackjack例子中使用的模型是sample model。Distribution model可以用来产生samples，但是sample model要比distribution models好获得。考虑投掷很多骰子的和，distribution models计算所有可能值和相应的概率，而sample model只计算根据概率产生的一个样本。</p><h3 id="planning">Planning</h3><h4 id="定义-v2">定义</h4><p>Planning的定义是给定model，不断的与environment交互生成或者改进poilcy的过程。有两种planning的方法，state space在state space中寻找一个optimal policy或者optimal path，这本书中介绍的方法都是这类。Plan-space planning在plans space中search。Plan-sapce 方法很难高效的应用到stochastic sequential decision problems，在这本书中不做过多介绍。<br>这一章要介绍的state-space planning方法具有相同的结构，这个结构在learing和planning中都有。基本的想法是：计算value funtions，通过应用simulated experience的update以及backup操作计算value functions。</p><h4 id="示例-v2">示例</h4><p>如DP方法，扫描整个state space，然后生成每一个state可能的transitions的distribution，用于计算update target，更新state’s estimated value。这一章介绍的其他方法，也满足这个结构，只不过计算target的方式顺序以及长度不同而已。</p><h4 id="planning和learning的关系">planning和learning的关系</h4><p>将planning方法表示成这种形式主要是为了强调planning方法和learning方法之间的联系。learning和planning的重点都是使用backup update op更新estimations of value functions。区别在于plannning使用了model生成的simulate experience，而learning使用environment生成的real experience。同时perfomance measure以及experience的灵活性也不同，但是由于相同的结构，许多learning的方法可以直接应用到planning上去，使用simulated experience代替real experience即可。<br>给出一个利用sample model和Q-learning结合起来的planning算法：</p><h3 id="learning算法示例">learning算法示例</h3><p>Random-sample one-step tabular Q-planning<br>Loop forever<br>$\qquad 1.$随机选择初始state $S\in \mathbb{S}, A\in \mathbb{A}$<br>$\qquad 2.$将$S,A$发送给sample model，得到next state和reward $S’, R$<br>$\qquad 3.$应用one-step Q-learning更新公式：<br>$\qquad\qquad Q(S,A) \leftarrow Q(S,A) + \alpha \left[R+\gamma mx_a Q(S’, a) - Q(S, A)\right]$</p><h2 id="dyna">Dyna</h2><h3 id="简介">简介</h3><p>Online的planning更新需要不断的与environment交互，从交互中获得的information可能会改变model，以及与environment的交互，所以可能model也需要不断的学习。这一节主要介绍Dyna-Q，将online planning需要的内容都整合了起来，Dyna算法中包含了planning, acting以及learning。<br>Planning中real experience至少有两个作用，一个是改进model，叫做model-learning，另一个是使用learning的方法直接改进value function和policy，叫做direct reinforcement learning。相应的关系如下图所示：<br><img src="/2019/08/07/reinforcement-learning-an-introduction-第8章笔记/model_learning.png" alt="model_learning"><br>如图所示，experience可以直接改进value function，也可以通过model间接改进value fucntion，叫做indirect reinforcement learning。Direct learning和indirect learning各有优势，indirect方法能够充分利用有限的experience，得到一个更好的policy；direct方法更简单，不会受到model bias的影响。</p><h3 id="dyna-q-简介">Dyna-$Q$简介</h3><p>Dyna-$Q$包含planning, acting, model-learning和direct RL。planning是random-sample one-step tabular Q-planning；direct RL就是one-step tabular Q-learning，model-learning是table-based并且假设environment是deterministic，对于每一个transition $S_t,A_t\rightarrow R_{t+1}, S_{t+1}$，model用表格的形式记录下$S_t,A_t$的prediction值是$R_{t+1}, S_{t+1}$。如下图是Dyna算法的整体框架图（Dyna-Q是一个示例）。<br><img src="/2019/08/07/reinforcement-learning-an-introduction-第8章笔记/dyna.png" alt="dyna_q"><br>左边使用real experience进行direct RL，右边是model-based的方法，从real experience中学习出model，然后利用model生成simulated experience。search control指的是从model生成的simulated experiences中选择指定starting state和actions的experience。最后，planning使用simulated experience更新value function。从概念上来讲，Dyna agents中planning, acting, model-learning以及direct RL几乎同时发生。但是在实现的时候，还是需要串行的进行。Dyna-$Q$中，计算量主要集中在planning上。具体的算法如下：</p><h3 id="tabular-dyna-q">Tabular Dyna-Q</h3><p>初始化$Q(s,a), Model(s,a), s\in \mathbb{S}, a\in \mathbb{A}$<br>Loop forever<br>$\qquad (a)S\leftarrow$ current state<br>$\qquad (b)A\leftarrow \epsilon$-greedy$(S,Q)$<br>$\qquad ( c )$采取action $A$，得到下一时刻的state $S’$和reward $R$<br>$\qquad (d)Q(S,A) \leftarrow Q(S,A) + \alpha\left[R+\gamma max_a Q(S’, a) -Q(S,A)\right]$<br>$\qquad (e)Model(S,A)\leftarrow R,S’$（假设deterministic environment)<br>$\qquad (f)$Loop repeat $n$ 次<br>$\qquad\qquad S\leftarrow$ 任意之前的观测状态<br>$\qquad\qquad A\leftarrow$ 在$S$处采取的任意action $A$<br>$\qquad\qquad R,S’\leftarrow Model(S,A)$<br>$\qquad\qquad Q(S,A) \leftarrow Q(S,A) + \alpha\left[R+\gamma max_a Q(S’, a) -Q(S,A)\right]$</p><p>其中$Model(s,a)$表示预测$(s,a)$的next state。(d)是direct RL，(e)是model-learning，(f)是planning。如果忽略了(e,f)，就是one-step tabular Q-learning。</p><h2 id="如果model出错">如果model出错</h2><p>前面给的例子很简单，model是不会错的。但是如果environment是stochastic的，或者samples很少的话，或者function approximation效果不好，或者environment刚刚改变，新的behaviour还没有被观测到，model就可能会出错。当model是错的话，就可能会产生suboptimal policy。在一些情况下，suboptimal会发现并且纠正model的error。当模型预测的结果比真实的结果好的时候就会发生这种情况。这里给出两个例子。一个environment变坏，一个environment变好。<br>第一个例子，有一个迷宫，如图所示。<br><img src="/2019/08/07/reinforcement-learning-an-introduction-第8章笔记/blocking_maze.png" alt="blocking mase"><br>刚开始，路在右边，1000步之后，右边的路被堵上了，左边有一条新的路。<br><img src="/2019/08/07/reinforcement-learning-an-introduction-第8章笔记/shortcut_maze.png" alt="blocking mase"><br>第二个例子，刚开始路在左边，3000步之后，右边有一条新的路，左边的路也被保留。<br>这又是一个exploration和exploitation问题。在planning中，exploration意味着尝试那些让model变得更好的actions，而exploitation意味着给定当前的model，选择optimal action。我们想要agents能够explore environment的变化，但是不影响performance。</p><h3 id="启发式搜索">启发式搜索</h3><p>Dyna-Q+ 使用了一个简单有效的heuristics，agent记录每一个state-action pair 在real environment中从上次使用到现在经历了多少个time steps，累计的时间步越多，说明这个pair改变的可能性越大，model不对的可能性越大。为了鼓励使用很久没有用的action，这里加了一个bonus reward，如果一个transition的reward是$r$，这个transition已经有$\tau$步没有试过，在planning进行update的时候，用一个新的reward $r + k\sqrt{\tau}$，$k$很小。不过新添加的bonus会有一定的cost，而且会对value function造成影响，但是在上面的两个例子中，这个cost比performance的提升要好。</p><h2 id="优先级">优先级</h2><p>在Dyna进行planning时，所有的state-action pair被选中的概率是一样的，显然是不合理的，planning的效率太低，可以有效的集中在某些state-action pair中。举个例子，在dyna_maze例子中，在第二个episode开始的时候，只有goal state前的那个state会产生正的reward，其余的都仍然是$0$，这里意味着很多updates都是无意义的。从一个value为$0$的state转移到另一个value为$0$的state，这个updates是没有意义的。只有那些在goal前面的state才会被更新，或者，那些value不为$0$的state的前面的state的value的更新才有意义。在planning过程中，有用的更新变多了，但是离effcient还差得多。在真实应用中，states可能相当大，这种没有重点的更新是非常低效的。。</p><h3 id="backward-focusing">backward focusing</h3><p>Dyna maze的例子给了我们一个提示，从goal state backward的进行更新。这里的goal state不是一个具体的goal state，指的是抽象的goal state。一般来说，包括goal state以及value改变的state。假设model的value都是正确的，如果agent发现了environment的一个变化以及相应state value的变化，首先更新这个state的predecessor states value，然后一直往前利用value改变的state进行更新就行了。这种想法叫做backward focusing。对于那些低效的state，不更新就是了。<br>Backward过程很快，会有很多state-action被更新，但并不是所有的pair是等价的。有的state value变化很大，有的变化很小。在stochastic环境中，对transiton probability变化的估计也会导致change大小的变化和紧急性的变化。所以，根据紧急性对不同的pair排一个优先级，然后根据这个优先级进行更新。用一个queue记录如果更新某个state-action pair的话，它的estimated value会变多少，根据这个值的大小排优先级。队头的元素取出来进行更新以后，计算它的predecessor pair变化大小。如果这个大小大于某个阈值，就使用新的优先级，把它加入队列，如果queue中有这个pair了，queue中保存大的优先级。完整的算法如下所示：</p><h3 id="deteriministic-environment下的优先级">deteriministic environment下的优先级</h3><p>初始化$Q(s,a), Model(s,a), \forall s, a$，置$PQueue$为空<br>Loop forever<br>$\qquad(a) S\leftarrow$ currnet state<br>$\qquad(b) A\leftarrow policy(S,Q)$<br>$\qquad( c )$采取action $A$；得到下一个reward $R$和state $S’$<br>$\qquad(d)Model(S,A) \leftarrow R, S’$<br>$\qquad(e) P\leftarrow |R+\gamma max_aQ(S’,a) - Q(S,A)|$<br>$\qquad$(f)如果$P\gt \theta$，将$S,A$以$P$的优先级插入$PQueue$<br>$\qquad$(g) Loop repeat $n$次，当$PQueue$非空的时候<br>$\qquad S, A\leftarrow fisrt(PQueue)$<br>$\qquad R,S’\leftarrow Model(S,A)$<br>$\qquad Q(S,A) \leftarrow Q(S,A) +\alpha \left[R+\gamma max_a Q(S’,a) -Q(S,A)\right]$<br>$\qquad$Loop for all 预计能到$S$的$\bar{S}, \bar{A} $<br>$\qquad\qquad \bar{R} \leftarrow $predicted reward for $\bar{S}, \bar{A}, S$<br>$\qquad\qquad P\leftarrow |\bar{R}+\gamma max_aQ(S,a) -Q(\bar{S}, \bar{A})$<br>$\qquad\qquad$如果$P\gt \theta$，将$\bar{S},\bar{A}$以$P$的优先级插入到$PQueue$</p><p>推广到stochastic environment上，用model记录state-action pair experience的次数，以及next state，计算出概率，然后进行expected update。Expected update会计算许多低概率transition上，浪费资源，所以可以使用sample update。<br>Sample update相对于expected update的好处，相当于将完整的backup分解成单个更小的transition。这个idea是从van Seijen和Sutton(2013)的一篇论文中得到的，叫做&quot;small backups&quot;，使用单个的transition进行更新，像sample update，但是使用的是这个transition的概率，而不是sampling的$1$。</p><h2 id="expected-updates和sample-updates">Expected updates和Sample updates</h2><p>这本书介绍了不同的value function updates，就one-step方法来说，主要有三个binary dimensions。第一个是估计state values还是action values；第二个是估计optimal policy还是random policy的value，对应四种组合:$q_{*}, v_{*}, q_{\pi}, v_{\pi}$；第三个是使用expected updates还是sample updates，也是这节的内容。总共有八种组合，其中七种对应具体的算法，如下图所示：<br><img src="/2019/08/07/reinforcement-learning-an-introduction-第8章笔记/seven_backup.png" alt="seven_bakcup"><br>这些算法都可以用来planning，之前介绍的Dyna-$Q$使用的是$q_{*}$ sample update，也可以用$q_{*}$ expected update，还可以用$q_{\pi}$ sample updates。</p><h3 id="简介和比较">简介和比较</h3><p>Expected update对于每一个$(s,a)$ pair，考虑所有可能的next state和next action $(s’,a’)$，需要distributions model进行精确计算；而sample update仅仅需要sample model，考虑一个next state，会有采样误差。所以expected upadte一定要比sample update好，但是需要的计算量也大。当环境是deteriministic的话，expected udpaet和sample update其实是一样的，只有在stochastic环境下才有区别。<br>假设每一个state有$b$个next state，expected upadte要比sample update的计算量大$b$倍。如果有足够的时间进行完全的expected update，进行一次完全的expected update一定比进行$b$次sample update好，因为虽然计算次数相等，但是sample update有sampling error；如果没有足够的时间的话，在计算次数小于$b$次的时候，sample update要比expected update好，sample update至少进行了一部分improvement，而sample update只有完全进行$b$次计算后才会得到正确的value function。而当state-action pairs很多的情况下，进行完全的expected update是不可能的，sample update是一个可行的方案。<br>给定一个state-action pair，到底是进行一些（小于$b$次）expected update好呢，还是进行$b$次sample updates好呢？如下图所示，展示了expected update和sample update在不同的$b$下，estimation error关于计算次数的函数。<br><img src="/2019/08/07/reinforcement-learning-an-introduction-第8章笔记/fig_8_7.png" alt="fig_8_7"><br>在这个例子中，所有$b$个后继状态出现的可能性相等，开始的error为$1$。假设所有的next state value都是正确的，expected update完成之后，error从$1$变成了$0$。而sample updates根据$\sqrt{\frac{b-1}{bt}}$减少error。对于$b$很大的值来说，进行$b$的很小比例次数的更新，error就会下降很快。</p><h3 id="sample-updates的好处">Sample updates的好处</h3><p>上图中，sample update的结果可能要比实际结果差一些。在实际问题中，后继状态的value会被它们自身更新。他们的estimate value会更精确，从后继状态进行backup也会更精确。</p><h3 id="结论">结论</h3><p>在stochastic环境下，如果每个state处可能的next state数量非常多，并且有很多个states，那么sample update可能要比expected update好。</p><h2 id="trajectory-sampling">Trajectory Sampling</h2><p>这里比较两种distributing updates。<br>DP进行update的时候，每一次扫描整个state spaces，很多state其实是没有用的，没有focus，所有的states地位一样。原则上，只要确保收敛，任何distributed方法都行，然而在实际上常用的是exhaustive sweep。<br>第二种是根据一些distributions从state space或者state action space中进行采样。Dyna-$Q$使用均匀分布采样，和exhaustive sweep问题一样，没有focus。使用on-policy distribution是一种很不错的想法，根据当前的policy不断的与model交互，产生一个trajectory。Sample state transitions以及reward是model给出来的，sample action是当前的policy给出来的。这种方法叫做generating experience和update trajectory sampling。</p><h3 id="原因">原因</h3><p>如果on-policy的distribution是已知的，可以根据这个distribution对所有的states进行加权，但是这和exhaustive sweeps的计算量差不多。或者从distribution中采样state-action paris，这比simulating trajectories好在哪里呢？事实上我们不知道distribution，当policy改变，计算distribution的计算量和policy evaluation相等。</p><h3 id="分析">分析</h3><p>使用on-policy distribution可以去掉很多我们不感兴趣的内容，或者一遍又一遍的进行无用的重复更新。通过一个小例子分析它的效果。使用one-step expected updates，公式如下：<br>$$Q(s,a) \leftarrow \sum_{s’,r} \hat{p}(s’,r|s,a)\left[r+\gamma max_{a’}Q(s’,a’)\right]$$<br>使用均匀分布的时候，对所有的state-action pair进行in place的expected update更新；使用on-policy的时候，使用当前$\epsilon$-greedy policy（$\epsilon=0.1$）直接生成episodes，对episodes中出现的state-action pair进行expected update。也就是说一个是随机选的，一个是on-policy中出现的，选择的方式不一样，但是都是进行expected update。<br>简单介绍一下environment。每个state都有两个action，等可能性的到达$b$个next states，$b$对于所有state-action pair都是一样的，所有的transition都有$0.1$的概率到达terminal state。Expected reward服从均值为$0$,方差为$1$的高斯分布。<br>在planning过程中的任何一步都可以停止，根据当前估计的action value，计算greedy policy $\hat{\pi}$下start state的value $v_{\hat{\pi}}(s_0)$，也就是说告诉我们使用greedy重新开始一个新的episodes，agent的表现会怎么样。（假设model是正确的）。<br><img src="/2019/08/07/reinforcement-learning-an-introduction-第8章笔记/fig_8_8.png" alt="fig_8_8"><br>上半部分是进行了$200$次sample任务，$1000$个states，$b$分别为$1,3,10$的结果。在图中根据on-policy 采样更新的方法，一开始很快，时间一长，就慢下来了。当$b$越小的时候，效果越好，越快。另一个实验中表明，随着states数量的增加，on-policy distribution采样的效果也在变好。<br>使用on-policy distribtion进行采样，能够帮助我们关注start state的后继状态。如果states很多，并且$b$很小，这个效果很好并且会持续很久。在长时间的计算中，使用on-policy distribution采样可能会有副作用，因为一直在更新那些value已经正确的states。对它们进行采样没用了，还不如采样一些其他的states。这就是为什么在长时间的运行中使用均匀分布进行采样的效果更好。</p><h2 id="real-time-dp">Real-time DP</h2><p>Real time DP是DP的on-policy trajectory sampling版本。RLDP和上一节介绍的on-policy expected update的区别在update的方式不同，上一届实际上使用的是state-action pair，而这一节DP用的是state，它的更新公式是：<br>$$v_{k+1}(s) = \max_a \sum_{s’,r}p(s’,r|s,a) \left[r+\gamma v_k(s’)\right]$$<br>一个是更新action value function，一个是更新state valut function。</p><h3 id="irrelevant-states">irrelevant states</h3><p>如果trajectories可以仅仅从指定的states开始，我们感兴趣的仅仅是给定policy下states的value，on-policy trajectory sampling可以完全跳过给定policy下不能到达的states。这些states跟prediction问题无关。对于control问题，目标是找到optimal policy而不是evaluating一个给定的policy，可能存在无论从哪个start states开始都无法到达的states，所以就没有必要给出这些无关states的action。我们需要的是一个optimal partial policy，对于relevant states，给出optimal policy，而对于irrelevant states，给出任意的actions。<br>找到这样一个policy，按道理来说需要访问所有的state-action pairs无数次，包括那些无关状态。<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.137.1955&amp;rep=rep1&amp;type=pdf" target="_blank" rel="noopener">Korf</a>证明了在满足以下条件的时候，能够在很少访问relevant states甚至不访问的时候，找到这样一个policy。</p><ol><li>goal state的初始value为$0$</li><li>存在至少一个policy保证从任何start state都能到达goal state</li><li>所有到达的非goal states的reward是严格为负的</li><li>所有states的初始value要大于等于optimal values（可以设置为$0$，一定比负值大）</li></ol><p>如果满足这些条件，RTDP一定会收敛到relevant states的optimal policy。</p><h2 id="决策时进行planning">决策时进行planning</h2><h3 id="backgroud-planning">backgroud planning</h3><p>进行planning至少有两种方法。第一种是已经介绍的DP和Dyna这些算法，使用planning基于从model得到的simulated experience不断的改进policy和value function。通过比较某一个state处不同state-action pairs value值的大小选择action。在action被选择之前，planning更新所有的$Q$值。这里planning的结果被很多个states用来选择action。这种planning叫做background planning。</p><h3 id="decision-time-planning">decision-time planning</h3><p>第二种方法是使用planning输出单个state的action，遇到一个新的state $S_t$，输出是单个的action $A_t$，然后再下一个时间步根据$S_{t+1}$继续计算$A_{t+1}$。最简单的一个例子是当只有states value可以使用的时候，通过比较model预测每一个action能够到达的后继state的value（也就是使用after state value）选择相应的action，。这种方法叫做decision-time planning。<br>事实上，decision-time planning和background planning的流程是一样的，都是使用simulated experience到backup values再到policy。只不过decision-time planning只是对当前可访问的单个state和action的value和policy进行planning。在许多decision-time planning的过程中，用于选择当前state对应的action时，使用到的value和policy用过以后都被丢弃了，这并不会造成太大的计算损失，因为在大部分任务中很多states在短时间内都不会被再次访问到。</p><h3 id="应用场景">应用场景</h3><p>decision-time planning适用于不需要快速实时相应的场景，比如各种下棋。如果需要快速响应的，那么最好使用backgroud planning计算一个policy可以快速应用到新的states。</p><h2 id="启发式搜索-v2">启发式搜索</h2><p>AI中经典的state-space方法都是decision-time planning方法，统称为heuristic search。在启发式搜索中，会使用树进行搜索，近似的value function应用到叶子节点，进行backup到根节点（当前state），相应的backup diagram和optimal的expected updates类似。根据计算的backed-up值，选择相应的action之后，丢弃这些值。<br>传统的启发式搜索并不保存backup value，它更像是多步的greedy policy，目的是更好的选择actions。如果我们有一个perfect model以及imperfect action value function，搜索的越深结果越好。如果search沿着所有可能的路一直到episode结束，那么imperfect value function的结果被抵消了，选出来的action也是optimal。如果搜索的深度$k$足够深，$\gamma^k$接近于$0$,那么找到的action也是近乎于最优的。当然，搜索的深度越深，需要的计算资源就越多。<br>启发式搜索的的updates集中在current state，它的search tree集中在接下来可能的states和actions，这也是它的结果为什么很好的原因。在某些特殊的情况下，我们可以将具体的启发式搜索算法构建出一个tree，自底向上的执行one-step update，如下图所示：<br><img src="/2019/08/07/reinforcement-learning-an-introduction-第8章笔记/heuristic_search_tree.png" alt="heuristic_search_tree"><br>如果update是有序的，并且使用tabular representation，整个updates可以看成深搜，所有的state-space搜索可以看成很多个one-step updates的组合。我们得出了一个结论，搜索深度越深，性能越好的原因不是multistep updates的使用，因为它实际上使用的是多个one-step update。真正的原因是更新都集中在current state downstream的states和actions上，所有的计算都集中在candidate actions相关的states和actions上。</p><h2 id="rollout算法">Rollout算法</h2><h3 id="什么是rollout算法">什么是Rollout算法</h3><p>Rollout算法是将Monte Carlo Control应用到从current state开始的simulate trajectories上的decision-time planning算法。Rollout算法根据给定的policy，这个policy叫做rolloutpolicy，从当前state可能采取的所有action开始生成很多simulated trajectories，对得到的returns进行平均估计aciton values。当action value估计的足够精确的时候，选择最大的那个action执行。<br>和MC Control的区别在于，MC Control的目的是估计整个action value function $q_{\pi}$或者$q_{*}$，而Rollout算法的目的是对于每一个current state，在一个给定policy下估计每一个可能的action的value。Rollout是decision-time planning算法，计算完相应的estimate action value之后，就丢弃它们。</p><h3 id="rollout算法做了什么">Rollout算法做了什么</h3><p>Rollout算法和policy iteration差不多。在policy improvement theorem理论中，如果在一个state处采取新的action，它的value要比原来的value高，那么就说这个新的policy要比老的policy好。Rollout算法在每个current state处，估计不同的state action value，然后选择最好的，其实就相当于one-step的policy iteration，或者更像on-step的asynchronous value iteration。<br>也就是说，rollout算法的目的是改进rollout policy，而不是寻找最优的policy。Rollout算法非常有效，但是它的效果也取决于rollout policy，roloout policy越好，最后算法生成的policy就越好。</p><h3 id="如何选择好的rollout-policy">如何选择好的rollout policy</h3><p>更好的rollout policy也就需要更多的资源，因为是decision-time算法，时间约束一定要满足，rollout算法的计算时间取决于每一个decision需要选择的action数量，sample trajectories的长度，rollout policy做决策的事件，以及足够的sample trajectories的数量。接下来给出几种方法去权衡这些影响因素：<br>第一个方法，MC trials都是独立的，所以可以使用多个分开的处理器运行多个trials。第二个方法是在simulated trajectories结束之前截断，通过一个分类评估函数对truncated returns进行修正。第三个方法是剪枝，剪掉那些不可能是最优的actions，或者那些和当前最优结果没啥差别的acitons。</p><h3 id="rollout算法和learning算法的关系">rollout算法和learning算法的关系</h3><p>Rollout算法并不是learning算法，因为它没有保存values和polices。但是rollout算法具有rl很多好的特征。作为MC Control的应用，他们使用sample trajectories，避免了DP的exhausstive sweeps，同时不需要使用distributin models，使用sample models。最后，rollout算法还使用了policy improvement property，即选择当前estimate action values最大的action。</p><h2 id="monte-carlo-tree-search">Monte Carlo Tree Search</h2><p>MCTS是decision-time planning算法，实际上，MCTS是一个rollout算法，它在上一节介绍的rollout算法上，加上了acucumulating value estimates的均值。MCTS是AlphaGO的基础算法。<br>每到达一个新的state，MCTS根据state选择action，到达新的state，再选择action，持续下去。在大多数情况下，simulated trajectories使用rollout policy生成actions。当model和rollout policy都不需要大量计算的时候，可以在短时间内生成大量simulated trajectories。只保留在接下来的几步内最后可能访问到的state-action pairs的子集，形成一棵树，如下图所示。任意simulated trajectory都会经过这棵树，并且从叶子节点退出。在tree的外边，使用rollout policy选择actions，在tree的内部使用一个新的policy，称为tree policy，平衡exploration和exploitation。Tree policy可以使用$\epsilon$-greedy算法。<br><img src="/2019/08/07/reinforcement-learning-an-introduction-第8章笔记/mcts.png" alt="mcts"><br>MCTS总共有四个部分，一直在迭代进行，第一步是Selection，根据tree policy生成一个episode的前半部分；第二步是expansion，从选中的叶子节点处的上一个节点探索其他没有探索过的节点；第三步是simulation，从选中节点，或者第二步中增加的节点处，使用rollout policy生成一个episode的后半部分；第四步是backup，从第一二三步得到的episode进行backup。这四步一直迭代下去，等到资源耗尽，或者没有time的时候，就退出，然后根据生成的tree中的信息选择相应的aciton，比如可以选择root node处action value最大的action，也可以选择最经常访问的action。<br>MCTS是一种rollout算法，所以它拥有online，incremental，sample-based和policy improvement等优点。同时，它保存了tree边上的estimate action value，并且使用sample update进行更新。优势是让trivals的初始部分集中在之前simulated的high-return trajectories的公共部分。然后不断的expanding这个tree，高效增长相关的action value table，通过这样子，MCTS避免了全局近似value funciton，同时又能够利用过去的experience指定exploraion。</p><h2 id="参考文献">参考文献</h2><p>1.《reinforcement learning an introduction》第二版</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;planning-and-learning-with-tabular-methods&quot;&gt;Planning and Learning with Tabular Methods&lt;/h2&gt;
&lt;p&gt;前面几章，介绍了MC算法，TD算法，再用$n$-step TD框架把它们统
      
    
    </summary>
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="model based" scheme="http://mxxhcm.github.io/tags/model-based/"/>
    
  </entry>
  
  <entry>
    <title>python selenium</title>
    <link href="http://mxxhcm.github.io/2019/08/06/python-selenium%E5%AE%89%E8%A3%85/"/>
    <id>http://mxxhcm.github.io/2019/08/06/python-selenium安装/</id>
    <published>2019-08-06T03:24:09.000Z</published>
    <updated>2019-08-06T07:18:18.592Z</updated>
    
    <content type="html"><![CDATA[<h2 id="ubuntu-安装chrome-driver">ubuntu 安装chrome driver</h2><ol><li>下载chrome driver<br><a href="http://chromedriver.chromium.org/downloads" target="_blank" rel="noopener">http://chromedriver.chromium.org/downloads</a><br>根据自己的操作系统和chrome下载相应的chrome driver</li><li>解压<br>将解压后的chromedriver放置在PATH环境变量中的任意目录即可，我是放置在了/usr/local/bin</li></ol><h2 id="参考文献">参考文献</h2><p>1.<a href="https://www.jianshu.com/p/dd848e40c7ad" target="_blank" rel="noopener">https://www.jianshu.com/p/dd848e40c7ad</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;ubuntu-安装chrome-driver&quot;&gt;ubuntu 安装chrome driver&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;下载chrome driver&lt;br&gt;
&lt;a href=&quot;http://chromedriver.chromium.org/downloads
      
    
    </summary>
    
      <category term="python" scheme="http://mxxhcm.github.io/categories/python/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="selenium" scheme="http://mxxhcm.github.io/tags/selenium/"/>
    
  </entry>
  
  <entry>
    <title>python-spider</title>
    <link href="http://mxxhcm.github.io/2019/08/06/python-spider/"/>
    <id>http://mxxhcm.github.io/2019/08/06/python-spider/</id>
    <published>2019-08-06T02:53:56.000Z</published>
    <updated>2019-08-09T08:37:45.484Z</updated>
    
    <content type="html"><![CDATA[<h2 id="requests">requests</h2><p>获取网页<br>response = requests.post(url, headers)<br>返回的response包含有网页返回的内容。<br>response.text以文字方式访问。</p><h2 id="beautiful-soup">beautiful soup</h2><p>soup.find_all()</p><h2 id="selenium">selenium</h2><ol><li>xpath查找<br>查找class为&quot;wos-style-checkbox&quot;，type为&quot;checkbox&quot;的任意elements。可以把*号换成input，就变成查找满足上述条件的input elemetns。</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">driver.find_elements_by_xpath(<span class="string">"//*[@type='checkbox'][@class='wos-style-checkbox']"</span>)</span><br></pre></td></tr></table></figure><ol start="2"><li>访问元素的内容<br>element.text</li><li>复选框选中与取消选中</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">checkbox = driver.find_element_by_id(<span class="string">"checkbox"</span>)</span><br><span class="line"><span class="keyword">if</span> checkbox.is_selected():  <span class="comment">#如果被选中</span></span><br><span class="line">    checkbox.click()    <span class="comment"># 再点击一次，就变成了取消选中</span></span><br><span class="line"><span class="keyword">else</span>:   <span class="comment"># 如果没有被选中</span></span><br><span class="line">    checkbox.click()    <span class="comment"># 再点击一次，就变成了选中</span></span><br></pre></td></tr></table></figure><ol start="4"><li>提交表单</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">keywords = driver.find_element_by_id(<span class="string">"input"</span>)</span><br><span class="line"><span class="comment"># 清空表单默认值</span></span><br><span class="line">keywords.clear()</span><br><span class="line"><span class="comment"># 提交表单</span></span><br><span class="line">keywords.send_keys(values)</span><br></pre></td></tr></table></figure><ol start="5"><li>输完表单内容需要回车<br>直接在表单内容中添加\n即可。</li><li>下拉框<br>使用Select对象</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">from selenium.webdriver.support.ui import Select</span><br><span class="line">from selenium import webdriver</span><br><span class="line">s = Select(driver.find_element_by_id(&quot;databases&quot;))</span><br><span class="line"></span><br><span class="line">s.select_by_value(&quot;value&quot;)</span><br><span class="line">s.select_by_index(index)</span><br><span class="line">s.select_by_visible_text(&quot;visible_text&quot;)</span><br></pre></td></tr></table></figure><ol start="7"><li>模拟鼠标操作</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">from selenium.webdriver.common.action_chains import ActionChains</span><br><span class="line"></span><br><span class="line"># 定位element</span><br><span class="line">arrow = driver.find_element_by_id(&quot;next&quot;)</span><br><span class="line"># 单击</span><br><span class="line">ActionChains(driver).click(arrow).perform()</span><br><span class="line"># 双击</span><br><span class="line">ActionChains(driver).double_click(arrow).perform()</span><br></pre></td></tr></table></figure><ol start="8"><li>display:none和visible: hidden<br>display:none表示完全不可见，不占据页面空间，而visible: hidden仅仅隐藏了element的显示效果，仍然占据页面空间，并且是可以被定位到，但是无法被访问（如selenium的click,clear以及send_keys等，会报错ElementNotVisibleException’: Message: Element is not currently visible and so may not be interacted with）。</li></ol><h2 id="参考文献">参考文献</h2><p>1.<a href="https://www.w3schools.com/xml/xml_xpath.asp" target="_blank" rel="noopener">https://www.w3schools.com/xml/xml_xpath.asp</a><br>2.<a href="https://devhints.io/xpath" target="_blank" rel="noopener">https://devhints.io/xpath</a><br>3.<a href="https://zhuanlan.zhihu.com/p/31604356" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/31604356</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;requests&quot;&gt;requests&lt;/h2&gt;
&lt;p&gt;获取网页&lt;br&gt;
response = requests.post(url, headers)&lt;br&gt;
返回的response包含有网页返回的内容。&lt;br&gt;
response.text以文字方式访问。&lt;/p&gt;

      
    
    </summary>
    
      <category term="python" scheme="http://mxxhcm.github.io/categories/python/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="spider" scheme="http://mxxhcm.github.io/tags/spider/"/>
    
  </entry>
  
  <entry>
    <title>Monte Carlo Markov Chain</title>
    <link href="http://mxxhcm.github.io/2019/08/01/Monte-Carlo-Markov-Chain/"/>
    <id>http://mxxhcm.github.io/2019/08/01/Monte-Carlo-Markov-Chain/</id>
    <published>2019-08-01T12:07:38.000Z</published>
    <updated>2019-09-04T02:01:36.794Z</updated>
    
    <content type="html"><![CDATA[<h2 id="概述">概述</h2><p>Monte Carlo方法在很多地方都出现过，但是它具体到底是干什么的，之前从来没有仔细了解过，这次正好趁着这个机会好好学习一下。<br>统计模拟中有一个重要的问题就是给定一个概率分布$p(x)$，生成它的样本。对于一些简单的分布，可以使用均匀分布产生的样本进行样本。但是对于一些复杂的样本，单单使用均匀分布就不行了，需要使用更加复杂的随机模拟方法。Markov Chain Monte Carlo就是一种随机模拟方法(simple simulation)，我们常见的gibbs sampling也是。通过多次模拟，产生多组实验数据，进行积分啊什么的。</p><h2 id="markov-matrices">Markov Matrices</h2><h3 id="定义">定义</h3><p>马尔科夫矩阵满足两个条件</p><ol><li>所有元素大于$0$</li><li>行向量之和为$1$</li></ol><h3 id="属性">属性</h3><ol><li>$\lambda = 1$是一个特征值，对应的特征向量的所有分量大于等于$0$。可以直接验证，假设$A = \begin{bmatrix}a&amp;b\\c&amp;d\\ \end{bmatrix}, a + b = 1, c + d = 1$，$A-\lambda I =  \begin{bmatrix}a - 1&amp;b\\c&amp;d - 1\\ \end{bmatrix}$，所有元素加起来等于$0$，即$(A-I)(1, \cdots, 1)^T = 0$，所以这些向量线性相关，因为存在一组不全为$0$的系数使得他们的和为$0$。所以$A-I$是奇异矩阵，也就是说$1$是$A$的一个特征值。</li><li>所有其他的特征值小于$1$。</li></ol><h3 id="马尔科夫矩阵的幂">马尔科夫矩阵的幂</h3><p>$u_{k+1}=Au_k$，其中$A$是马尔科夫矩阵。我们能得到<br>$u_k = A^k u_0 = c_1 \lambda_1^k x_1 + c_2 \lambda_2^k x_2 + \cdots$<br>如果只有一个特征值为$1$，所有其他特征值都小于$1$，幂运算之后$\lambda^k \rightarrow 0, k\rightarrow \infty, \lambda_k \neq 1$。即能得到一个稳态。</p><h2 id="markov-property">Markov Property</h2><p>简单的来说，就是下一时刻的状态只取决于当前状态，跟之前所有时刻的状态无关，即：<br>$$P(X_{t+1} = k |X_t=k_t,\cdots, X_1 = k_1) = P(X_{t+1}=k |X_t=k_t)$$</p><h2 id="markov-chain-process">Markov Chain(Process)</h2><p>如果一个Chain(process)是Markov的，就叫它Markov Chain(Process)。</p><h2 id="stationary-distribution">Stationary Distribution</h2><p>为什么要用Markov Chain呢？因为它有一个很好的性质，叫做stationary distribution。简单的来说，就是不论初始状态是什么，经过很多步之后，都会达到一个stable state。举个例子，股票有牛市和熊市，还有波动状态，它们的转换关系如下所示：<br><img src="/2019/08/01/Monte-Carlo-Markov-Chain/markov_transition.png" alt="markov_transition"><br>状态转义矩阵矩阵如下表所示：</p><table><thead><tr><th style="text-align:center"></th><th style="text-align:center">牛市</th><th style="text-align:center">熊市</th><th style="text-align:center">波动</th></tr></thead><tbody><tr><td style="text-align:center">牛市</td><td style="text-align:center">0.9</td><td style="text-align:center">0.075</td><td style="text-align:center">0.025</td></tr><tr><td style="text-align:center">熊市</td><td style="text-align:center">0.15</td><td style="text-align:center">0.8</td><td style="text-align:center">0.05</td></tr><tr><td style="text-align:center">波动</td><td style="text-align:center">0.25</td><td style="text-align:center">0.25</td><td style="text-align:center">0.5</td></tr></tbody></table><p>假如从熊市开始，初始state是[牛市，熊市，波动]，用数值表示就是$(0, 1, 0)^T$。根据当前时刻计算下一时刻的公式为：<br>$$s_{t+1} = s_t Q$$<br>相应结果为$s_{t+1} = (0.15, 0.8, 0.05)^T$<br>$t+2$时刻的计算公式为：<br>$$s_{t+2} = s_t Q^2$$<br>这样一直计算下去，可以到达一个state:<br>$$sQ = s$$<br>对于这个例子来说，就是$s = (0.625, 0.3125, 0.0625)^T$，不管从什么初始状态开始，最后都会到达这个$s$状态。<br>那么这个stationary distribution有什么用呢？它能够给出一个process在任意时刻某个state出现的概率，比如牛市出现的概率是$62.5%$，熊市出现的概率是$31.25%$。</p><h3 id="马尔科夫链的稳定性">马尔科夫链的稳定性</h3><p>如果一个非周期性马尔科夫连有转移矩阵$P$，并且它的任意两个状态都是连通的，那么$lim_{n\rightarrow \infty} P_{ij}^m$存在，且与$i$无关，记为$lim_{n\rightarrow \infty }P_{ij}^n = \pi(j)$，即矩阵$P^n$的所有第$j$列都是$\pi(j)$，与$P$的初始值无关。那么有：</p><h2 id="monte-carlov-markov-chain">Monte Carlov Markov Chain</h2><p>首先考虑，给定一个beta分布，如何进行采样？MCMC提供了从任意概率分布中采样的方法，尤其是当我们计算后验概率时极为有用。在贝叶斯公式中，如下图所示，我们需要从后验分布中进行采样，但是后验分布并不是那么好计算的，因为牵扯到$p(D)$的计算，根据全概率公式，需要进行积分，而beta分布的积分并不好解。<br><img src="/2019/08/01/Monte-Carlo-Markov-Chain/bayesian.png" alt="bayesian"></p><p>Wikipedia上MCMC的定义：MCMC是一类方法的统称，MCMC方法构建一个Markov chain，这个Markov chain的stationary distribution是我们的目标distribution，然后进行采样。经过很多步之后的一个state可以看成是我们目标distribution的一个样本。简单解释以下就是，给定一个概率分布$p(x)$，我们想要生成这个概率分布的一些样本。因为马尔科夫链能够收敛到stationary distribution，我们的想法就是构造一个转移矩阵为$P$的马尔科夫连，使得该马尔科夫的stationry distribution是$p(x)$，那么不管我们从任何初始状态$x_0$出发，得到一个马尔科夫链$x_0,x_1,\cdots, x_t,x_{t+1}, \cdots$，如果马尔科夫链在第$n$步已经到了stationary distribution，那么$t+1$后的states都可以看成$p(x)$的样本。</p><h2 id="示例">示例</h2><p>我们从Beta分布中进行采样，Beta分布的公式如下所示：<br>$$f(\theta;\alpha, \beta) = \frac{1}{B(\alpha, \beta)}\theta^{\alpha-1}(1-\theta)^{\beta-1},\alpha\gt 0,\beta\gt 0, x\in \left[0,1\right]$$<br>其中，$\frac{1}{B(\alpha, \beta)} = \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}$，是$\alpha,\beta$的函数，这里我们假设$\alpha,\beta$是固定的。<br>MCMC方法能够创建一个Markov chain，它的stationary distribution是Beta distribution。</p><p>定义$s=(s_1,s_2,\cdots, s_M)$是desired stationary distribution。我们的目标是创建一个Markov Chain，它的stationary distribution是desitred stationary distribution。随机初始化一个具有$M$个states的Markov Chain，transition matrix 是$P$，$p_{ij}$代表从state $i$到$j$的概率。<br>随机初始化的Markov Chain肯定有一个stationary distribution，但是不是我们想要的哪个。我们的目标就是改变这个Markov Chain让它的stationary distribution是我们想要的。为了实现这个目的：</p><ol><li>随机选择一个初始state $i$</li><li>根据$P$的第$i$行随机选择一个新的proposal state</li><li>计算一个measure称作Acceptance Probabiliby：<br>$$a_{ij} = min(\frac{s_j p_{ji}}{s_ip_{ij}},1)$$</li><li>投掷一枚正面向上概率为$a_{ij}$的硬币，如果正面向上，accept这个proposal，移动到下一个state，否则，reject 这个proposal，留在当前state。</li><li>重复下去</li></ol><p>经过很长时间以后，这个chain会收敛，我们可以使用这个chain的states作为从任意distribution的采样。不同的分布，使用的acceptance probability不同而已，其实就是根据给出的分布计算一个接收概率而已。</p><h3 id="从beta分布中采样">从beta分布中采样</h3><p>Beta分布是定义在$[0,1]$上的连续分布，它有$[0,1]$上的无穷多个states。假设具有$[0,1]$上无限个states的markov chain的transition matrix $P$是一个对称矩阵，即$p_{ij}=p_{ji}$，acceptance probability中的$p_{ij}$项就可以消掉。<br>接下来我们要做的是：</p><ol><li>从均匀分布$(0,1)$中随机选择一个初始state $i$</li><li>根据$P$的第$i$行值随机选择一个新的proposal state。</li><li>计算Acceptance Probability：<br>$$a_{ij} = min(\frac{s_j p_{ji}}{s_ip_{ij}},1)$$<br>可以化简为：<br>$$a_{ij} = min(\frac{s_j}{s_i},1)$$<br>其中$s_i = Ci^{\alpha-1} (1-i)^{\beta-1},s_j = Cj^{\alpha-1} (1-j)^{\beta-1}$，</li><li>投掷一枚正面向上概率为$a_{ij}$的硬币，如果正面向上，accept这个proposal，移动到下一个state，否则，reject 这个proposal，留在当前state。</li><li>重复下去</li></ol><h2 id="参考文献">参考文献</h2><p>1.<a href="https://stats.stackexchange.com/questions/165/how-would-you-explain-markov-chain-monte-carlo-mcmc-to-a-layperson" target="_blank" rel="noopener">https://stats.stackexchange.com/questions/165/how-would-you-explain-markov-chain-monte-carlo-mcmc-to-a-layperson</a><br>2.<a href="https://towardsdatascience.com/mcmc-intuition-for-everyone-5ae79fff22b1" target="_blank" rel="noopener">https://towardsdatascience.com/mcmc-intuition-for-everyone-5ae79fff22b1</a><br>3.<a href="http://bloglxm.oss-cn-beijing.aliyuncs.com/lda-LDA%E6%95%B0%E5%AD%A6%E5%85%AB%E5%8D%A6.pdf" target="_blank" rel="noopener">http://bloglxm.oss-cn-beijing.aliyuncs.com/lda-LDA数学八卦.pdf</a><br>4.<a href="https://jeremykun.com/2015/04/06/markov-chain-monte-carlo-without-all-the-bullshit/" target="_blank" rel="noopener">https://jeremykun.com/2015/04/06/markov-chain-monte-carlo-without-all-the-bullshit/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;概述&quot;&gt;概述&lt;/h2&gt;
&lt;p&gt;Monte Carlo方法在很多地方都出现过，但是它具体到底是干什么的，之前从来没有仔细了解过，这次正好趁着这个机会好好学习一下。&lt;br&gt;
统计模拟中有一个重要的问题就是给定一个概率分布$p(x)$，生成它的样本。对于一些简单的分布，
      
    
    </summary>
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="蒙特卡洛" scheme="http://mxxhcm.github.io/tags/%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B/"/>
    
      <category term="MCMC" scheme="http://mxxhcm.github.io/tags/MCMC/"/>
    
      <category term="概率论" scheme="http://mxxhcm.github.io/tags/%E6%A6%82%E7%8E%87%E8%AE%BA/"/>
    
  </entry>
  
  <entry>
    <title>各类分布</title>
    <link href="http://mxxhcm.github.io/2019/07/31/%E5%90%84%E7%B1%BB%E5%88%86%E5%B8%83/"/>
    <id>http://mxxhcm.github.io/2019/07/31/各类分布/</id>
    <published>2019-07-31T03:25:10.000Z</published>
    <updated>2019-09-09T07:37:11.339Z</updated>
    
    <content type="html"><![CDATA[<h2 id="常见的离散分布和连续分布">常见的离散分布和连续分布</h2><p>常见的离散分布有伯努利分布，二项分布，泊松分布，几何分布，多项分布。</p><h2 id="伯努利分布">伯努利分布</h2><h3 id="定义">定义</h3><p>伯努利实验是单次随机试验，只有成功和失败两种结果。它服从的分布叫做伯努利分布，伯努利分布也叫两点分布或者$0-1$分布。</p><h3 id="概率密度函数">概率密度函数</h3><h3 id="期望和方差">期望和方差</h3><h2 id="二项分布">二项分布</h2><h3 id="定义-v2">定义</h3><p>如果某个试验是伯努利试验，事件A在一次试验中发生的概率是$p$，现在独立的重复该试验$n$次，用$X$表示事件A在发生的次数，$X=i$的所有的可能取值为$0,1,\cdots,n$。$X$所服从的分布就叫二项分布，它的计算公式如下：<br>$$P(X=i|\theta)= \begin{pmatrix}n\\ i\end{pmatrix}p^i(1-p)^{n-i}$$<br>二项分布有两个条件，一个是独立的进行$n$次试验，相互之间互不干扰，一个是同分布，即所有试验中A发生的概率都是相等的。</p><h3 id="概率密度函数-v2">概率密度函数</h3><h3 id="期望和方差-v2">期望和方差</h3><h2 id="几何分布和负二项分布">几何分布和负二项分布</h2><h3 id="定义-v3">定义</h3><h3 id="概率密度函数-v3">概率密度函数</h3><h3 id="期望和方差-v3">期望和方差</h3><h2 id="多项分布">多项分布</h2><h3 id="定义-v4">定义</h3><p>如果把事件A的取值推广到多个，而不是两个，独立进行$n$次实验，求$X=i$的所有可能的取值就是一个多项分布。比如掷骰子$100$次，出现$50$次$1$，$30$次$2$和$20$次$3$的概率就是一个多项分布。<br>多项分布的计算公式如下：<br>$$P(x_1,x_2,\cdots, x_k; n, p_1,p_2, \cdots,p_k) = \frac{n!}{x_1!\cdots x_k!}p_1^{x_1}\cdots p_k^{x_k},\sum_{i=1}^{k}p_i = 1$$</p><h3 id="概率密度函数-v4">概率密度函数</h3><h3 id="期望和方差-v4">期望和方差</h3><h2 id="泊松分布">泊松分布</h2><h3 id="定义-v5">定义</h3><h3 id="概率密度函数-v5">概率密度函数</h3><h3 id="期望和方差-v5">期望和方差</h3><h2 id="gamma函数">Gamma函数</h2><p>Gamma函数的定义：<br>$$\Gamma(x) = \int_0^{\infty}t^{x-1}e^{-t}dt$$<br>通过分部积分，可以推导出来这个函数有如下的性质：<br>$$\Gamma(x+1) = x\Gamma(x)$$<br>可以证明，$\Gamma(x)$函数是阶乘在实数集合上的扩展：<br>$$\Gamma(n) = (n-1)!$$<br>只不过这里是$(n-1)!$，而不是$n!$。事实上，如果把$t^{x-1}$替换成$t^x$就能得到$\Gamma(n) = n!$，但是欧拉不知道为什么，还是使用了$t^{x-1}$。</p><h2 id="beta分布">Beta分布</h2><h3 id="定义-v6">定义</h3><p>我们常用的贝叶斯估计，是将先验概率转化为了后验概率。。比如说抛硬币，我们通常假设硬币是公平的，也就是说正面向上的概率是$0.5$，这个先验概率就是$0.5$，然后根据观测到的事件将先验概率转化为后验概率。<br>这里的先验分布我们取得是一个值，如果不取一个值，而是选择一个分布呢？比如Beta分布：<br>$$f(\theta;\alpha, \beta) = \frac{1}{B(\alpha, \beta)}\theta^{\alpha-1}(1-\theta)^{\beta-1},\alpha\gt 0,\beta\gt 0, x\in \left[0,1\right]$$<br>其中，$\frac{1}{B(\alpha, \beta)} = \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}$</p><p>然后使用贝叶斯推断，得到一个后验概率分布。Beta分布有一个神奇的特点就是使用贝叶斯推断得到的后验概率也是一个Beta分布，这个特点叫做共轭分布，接下来就会介绍。</p><h3 id="特点">特点</h3><p>Beta分布可以表示各种各种的曲线，从而也能表示各种各样的先验分布。</p><h3 id="均值和方差">均值和方差</h3><p>Beta分布的均值是$\frac{\alpha}{\alpha+\beta}$。</p><h2 id="共轭先验分布">共轭先验分布</h2><p>在贝叶斯概率理论中，如果后验概率和先验概率服从相同类型的分布，那么先验分布和后验分布被叫做共轭分布。同时，先验分布叫做似然函数的共轭先验分布。<br>Beta分布就是一个共轭分布，但是有一个前提，就是数据符合二项分布的时候，参数的先验和后验都可以是Beta分布，也称Beta分布是二项分布的共轭先验分布，似然函数是二项分布。</p><h3 id="证明beta分布是二项分布的共轭先验分布">证明Beta分布是二项分布的共轭先验分布</h3><p>已知条件：先验是Beta分布，数据服从二项分布<br>根据贝叶斯公式，后验概率的计算公式如下：<br>$$P(\theta|data) = \frac{P(data|\theta)P(\theta)}{P(data)} \propto P(data|\theta)P(\theta)$$<br>而$P(data|\theta)$可以根据似然函数的定义来计算$L(\theta|data) = P(data|\theta)$，又因为数据服从二项分布，有$P(data|\theta) \propto \theta^z(1-\theta)^{n-z}$，而Beta分布有$P(\theta) = Beta(\theta;\alpha,\beta) \propto \theta^{(\alpha-1)}(1-\theta)^{\beta-1}$，而$P(data)$与$\theta$无关，所以<br>$$P(\theta|data) \propto P(data|\theta)P(\theta) = \theta^{z+\alpha-1}(1-\theta)^{\beta+n-z-1}$$<br>也就是说后验分布服从Beta$(\alpha+z, \beta+n-z)$，得证。</p><h2 id="狄利克雷分布">狄利克雷分布</h2><h3 id="定义-v7">定义</h3><h2 id="参考文献">参考文献</h2><p>1.<a href="http://bloglxm.oss-cn-beijing.aliyuncs.com/lda-LDA%E6%95%B0%E5%AD%A6%E5%85%AB%E5%8D%A6.pdf" target="_blank" rel="noopener">http://bloglxm.oss-cn-beijing.aliyuncs.com/lda-LDA数学八卦.pdf</a><br>2.<a href="https://zhuanlan.zhihu.com/p/49267988" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/49267988</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;常见的离散分布和连续分布&quot;&gt;常见的离散分布和连续分布&lt;/h2&gt;
&lt;p&gt;常见的离散分布有伯努利分布，二项分布，泊松分布，几何分布，多项分布。&lt;/p&gt;
&lt;h2 id=&quot;伯努利分布&quot;&gt;伯努利分布&lt;/h2&gt;
&lt;h3 id=&quot;定义&quot;&gt;定义&lt;/h3&gt;
&lt;p&gt;伯努利实验是单次随
      
    
    </summary>
    
      <category term="概率论" scheme="http://mxxhcm.github.io/categories/%E6%A6%82%E7%8E%87%E8%AE%BA/"/>
    
    
      <category term="概率论" scheme="http://mxxhcm.github.io/tags/%E6%A6%82%E7%8E%87%E8%AE%BA/"/>
    
      <category term="二项分布" scheme="http://mxxhcm.github.io/tags/%E4%BA%8C%E9%A1%B9%E5%88%86%E5%B8%83/"/>
    
      <category term="几何分布" scheme="http://mxxhcm.github.io/tags/%E5%87%A0%E4%BD%95%E5%88%86%E5%B8%83/"/>
    
      <category term="伯努利分布" scheme="http://mxxhcm.github.io/tags/%E4%BC%AF%E5%8A%AA%E5%88%A9%E5%88%86%E5%B8%83/"/>
    
      <category term="泊松分布" scheme="http://mxxhcm.github.io/tags/%E6%B3%8A%E6%9D%BE%E5%88%86%E5%B8%83/"/>
    
      <category term="beta分布" scheme="http://mxxhcm.github.io/tags/beta%E5%88%86%E5%B8%83/"/>
    
  </entry>
  
  <entry>
    <title>概率论基础</title>
    <link href="http://mxxhcm.github.io/2019/07/31/%E6%A6%82%E7%8E%87%E8%AE%BA%E5%9F%BA%E7%A1%80/"/>
    <id>http://mxxhcm.github.io/2019/07/31/概率论基础/</id>
    <published>2019-07-31T03:24:55.000Z</published>
    <updated>2019-08-04T04:35:47.957Z</updated>
    
    <content type="html"><![CDATA[<h2 id="先验-后验-似然-条件概率">先验，后验，似然，条件概率</h2><h3 id="先验和后验">先验和后验</h3><p>假设的概率，或者给出的概率。<br>比如给出一个硬币，求随机投掷一次正面向上的概率。。在不知道任何其他条件的情况下，我们假设它是公平的，这就是一个先验。然后随机投掷了十次，发现十次正面都是向上的，显然，这个硬币不是公平的，根据这个事件我们可以修改这个硬币正面向上的概率，这时候得到的概率就是后延概率。</p><h3 id="似然">似然</h3><p>用$\theta$表示一个随机过程的参数，观测到的事件是$O$。$P(O|\theta)$是这件事发生的概率。然而，在现实生活中，我们常常是不知道$\theta$的，我们只能根据观测到的$O$，计算$P(\theta|O)$。而估计$P(\theta|O)$最常用的方法就是最大似然估计，即寻找使得当前观测$O$出现概率最大化的参数。似然的定义是给定观测$O$关于未知参数$\theta$的函数：<br>$$L(\theta|O) = P(O|\theta)$$<br>$L(\theta|O)$被称为似然，左面是似然，右面是概率密度（函数）。这两个函数从定义上来说是完全不同的对象，前者是$\theta$的函数，后者是$O$的函数，这里的等号意思是函数值相等，而不是两个函数本身就是同一个函数。</p><h3 id="条件概率">条件概率</h3><p>似然就是一种条件概率。</p><h2 id="参考文献">参考文献</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;先验-后验-似然-条件概率&quot;&gt;先验，后验，似然，条件概率&lt;/h2&gt;
&lt;h3 id=&quot;先验和后验&quot;&gt;先验和后验&lt;/h3&gt;
&lt;p&gt;假设的概率，或者给出的概率。&lt;br&gt;
比如给出一个硬币，求随机投掷一次正面向上的概率。。在不知道任何其他条件的情况下，我们假设它是公平的，
      
    
    </summary>
    
      <category term="概率论" scheme="http://mxxhcm.github.io/categories/%E6%A6%82%E7%8E%87%E8%AE%BA/"/>
    
    
      <category term="概率论" scheme="http://mxxhcm.github.io/tags/%E6%A6%82%E7%8E%87%E8%AE%BA/"/>
    
      <category term="先验" scheme="http://mxxhcm.github.io/tags/%E5%85%88%E9%AA%8C/"/>
    
      <category term="后验" scheme="http://mxxhcm.github.io/tags/%E5%90%8E%E9%AA%8C/"/>
    
      <category term="似然" scheme="http://mxxhcm.github.io/tags/%E4%BC%BC%E7%84%B6/"/>
    
  </entry>
  
  <entry>
    <title>reinforcement learning an introduction 第7章笔记</title>
    <link href="http://mxxhcm.github.io/2019/07/30/reinforcement-learning-an-introduction-%E7%AC%AC7%E7%AB%A0%E7%AC%94%E8%AE%B0/"/>
    <id>http://mxxhcm.github.io/2019/07/30/reinforcement-learning-an-introduction-第7章笔记/</id>
    <published>2019-07-30T01:59:50.000Z</published>
    <updated>2019-08-16T05:26:23.116Z</updated>
    
    <content type="html"><![CDATA[<h2 id="n-step-bootstrapping">n-step Bootstrapping</h2><p>这章要介绍的n-step TD方法，将MC和one-step TD用一个统一的框架整合了起来。在一个任务中，两种方法可以无缝切换。n-step方法生成了一系列方法，MC在一头，one-step在另一头。最好的方法往往不是MC也不是one-step TD。One-step TD每隔一个timestep进行bootstrapping，在许多需要及时更新变化的问题，这种方法很有用，但是一般情况下，经历了长时间的稳定变化之后，bootstrap的效果会更好。N-step TD就是将one-step TD方法中time interval的one改成了n。N-step方法的idea和eligibility traces很像，eligibility traces同时使用多个不同的time intervals进行bootstarp。</p><h2 id="n-step-td-prediction">n-step TD Prediction</h2><p>对于使用采样进行policy evaluation的方法来说，不断使用policy $\pi$生成sample episodes，然后估计$v_{\pi}$。MC方法用的是每一个episode中每个state的return进行更新，即无穷步reward之和。TD方法使用每一个state执行完某一个action之后的下一个reward加上下一个state的估计值进行更新。N-step方法使用的是每一个state接下来n步的reward之和加上第n步之后的state的估计值。相应的backup diagram如下所示：<br><img src="/2019/07/30/reinforcement-learning-an-introduction-第7章笔记/n_step_backup_diagram.png" alt="n_step_diagram"></p><p>n-step方法还是属于TD方法，因为n-step的更新还是基于时间维度上不同estimate的估计进行的。</p><blockquote><p>n-step updates are still TD methods because they still chagne an erlier estimate based on how it differs from a later estimate. Now the later estimate is not one step later, but n steps later。</p></blockquote><p>正式的，假设$S_t$的estimated value是基于state-reward sequence $S_t, R_{t+1}, S_{t+1}, R_{t+2}, \cdots, R_T,S_T$得到的。<br>MC方法更新基于completer return：<br>$$G_T = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots + \gamma^{T-t-1}R_T \tag{1}$$<br>one-step TD更新基于one-step return：<br>$$G_{t:t+1} = R_{t+1}+ \gamma V_t(S_{t+1}) \tag{2}$$<br>two-step TD更新基于two-step return：<br>$$G_{t:t+2} = R_{t+1}+ \gamma V_t(S_{t+1}) + \gamma^2 V_{t+1}(S_{t+2}) \tag{3}$$<br>n-step TD更新基于n-step return：<br>$$G_{t:t+n} = R_{t+1}+ \gamma V_t(S_{t+1}) + \gamma^2 V_{t+1}(S_{t+2}) +\cdots + \gamma^{n-1} R_{t+n} + \gamma^n V_{t+n-1}(S_{t+n}), n\ge1, 0\le t\le T-n \tag{4}$$<br>所有的n-step方法都可以看成使用n steps的rewards之和加上$V_{t+n-1}(S_{t+n})$近似其余项的rewards近似return。如果$t+n \ge T$时，$T$步以后的reward以及estimated value当做$0$，相当于定义$t+n \ge T$时，$G_{t:t+1} = G_t$。<br>当$n &gt; 1$时，只有在$t+n$时刻之后，$R_{t+n},S_{t+n}$也是已知的，所以不能使用real time的算法。使用$t+n-1$时刻的$V$近似估计$V_{t+n-1}(S_{t+n})$，将n-step return当做n-step TD的target，得到的更新公式如下：<br>$$V_{t+n}(S_t) = V_{t+n-1} (S_t) + \gamma(G_{t:t+n} - V_{t+n-1}(S_{t}))\tag{5}$$<br>当更新$V_{t+n}(S_t)$时，所有其他states的$V$不变，用公式来表示是$V_{t+n}(s) = V_{t+n-1}(s), \forall s\neq S_t$。在每个episode的前$n-1$步中，不进行任何更新，为了弥补这几步，在每个episode结束以后，即到达terminal state之后，仍然继续进行更新，直到下一个episode开始之前，依然进行updates。完整的n-step TD算法如下：</p><h3 id="n-step-td-prediction伪代码">n-step TD prediction伪代码</h3><p>n-step TD估计$V\approx v_{\pi}$<br>输入：一个policy $\pi$<br>算法参数：step size $\alpha \in (0, 1]$，正整数$n$<br>随机初始化$V(s), \forall s\in S$<br>Loop for each episode<br>$\qquad$初始化 $S_0 \neq terminal$<br>$\qquad$$T\leftarrow \infty$<br>$\qquad$Loop for $t=0, 1, 2, \cdots:$<br>$\qquad$ IF $t\lt T$, THEN<br>$\qquad\qquad$ 根据$\pi(\cdot|S_t)$执行action<br>$\qquad\qquad$ 接收并记录$R_{t+1}, S_{t+1}$<br>$\qquad\qquad$ 如果$S_{t+1}$是terminal ，更新$T\leftarrow t+1$<br>$\qquad$ END IF<br>$\qquad$ $\tau \leftarrow t - n + 1, \tau$是当前更新的时间<br>$\qquad$ If $\tau \ge 0$, then<br>$\qquad\qquad$ $G\leftarrow \sum_{i=\tau+1}^{min(\tau+n, T)} \gamma^{i-\tau -1} R_i$<br>$\qquad\qquad$ 如果$\tau+n \lt T$，那么$G\leftarrow G+ \gamma^n V(S_{\tau+n})$<br>$\qquad\qquad$ $V(S_{\tau}) \leftarrow V(S_{\tau}) +\alpha [G-V(S_{\tau})]$<br>$\qquad$ End if<br>Until $\tau = T-1$<br>n-step return有一个重要的属性叫做error reduction property，在最坏的情况下，n-step returns的期望也是一个比$V_{t+n-1}$更好的估计：<br>$$max_{s}|\mathbb{E}_{\pi}\left[G_{t:t+n}|S_t = s\right]- v_{\pi}(s)| \le \gamma^n max_s | V_{t+n-1}(s)-v_{\pi}(s)| \tag{6}$$<br>所以所有的n-step TD方法都可以收敛到真实值，MC和one-step TD是其中的一种特殊情况。</p><h2 id="n-step-sarsa">n-step Sarsa</h2><p>介绍完了prediction算法，接下来要介绍的就是control算法了。因为TD方法是model-free的，所以，还是要估计action value function。上一章介绍了one-step Sarsa，这一章自然介绍一下n-step Sarsa。n-step Sarsa的backup图如下所示：<br><img src="/2019/07/30/reinforcement-learning-an-introduction-第7章笔记/n_step_sarsa.png" alt="n_step_sarsa"><br>就是将n-step TD的state换成state-action就行了。定义action value的n-step returns如下：<br>$$G_{t:t+n} = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+1} + \cdots + \gamma^{n-1} R_{t+n} + \gamma^n Q_{t+n-1}(S_{t+n},A_{t+n}), n\ge 1, 0\le t\le T-n\tag{7}$$<br>如果$t+n\ge T$，那么$G_{t:t+n} = G_t$。<br>完整的$n$-step Sarsa如下所示：</p><h3 id="n-step-sarsa算法伪代码">n-step Sarsa算法伪代码</h3><p>n-step Sarsa算法，估计$Q\approx q_{*}$<br>随机初始化$Q(s,a),\forall s\in S, a\in A$<br>初始化$\pi$是相对于$Q$的$\epsilon$-greedy policy，或者是一个给定的不变policy<br>算法参数：step size $\alpha \in (0,1], \epsilon \gt 0$，一个正整数$n$<br>Loop for each episode<br>$\qquad$初始化$S_0\neq$ terminal<br>$\qquad$ 选择action $A_0= \pi(\cdot| S_0)$<br>$\qquad$ $T\leftarrow \infty$<br>$\qquad$ Loop for $t=0,1,2,\cdots$<br>$\qquad\qquad$ If $t\lt T$,then:<br>$\qquad\qquad\qquad$ 采取action $A_t$，<br>$\qquad\qquad\qquad$ 接收rewared $R_{t+1}$以及下一个state $S_{t+1}$<br>$\qquad\qquad$ 如果$S_{t+1}$是terminal，那么<br>$\qquad\qquad$ $T\leftarrow t+1$<br>$\qquad\qquad$ 否则选择$A_{t+1} = \pi(\cdot|S_{t+1})$<br>$\qquad\qquad$End if<br>$\qquad\qquad$ $\tau \leftarrow t-n+1$<br>$\qquad\qquad$ If $\tau \ge 0$<br>$\qquad\qquad\qquad$ $G\leftarrow \sum_{i=\tau +1}^{min(\tau+n, T)} \gamma^{i-\tau -1} R_i$<br>$\qquad\qquad\qquad$ If $\tau+n \le T$,then<br>$\qquad\qquad\qquad$ $G\leftarrow G+\gamma^n Q(S_{\tau+n}, A_{\tau+n})$<br>$\qquad\qquad\qquad$ $Q(S_{\tau}, Q_{\tau}) \leftarrow Q(S_{\tau}, Q_{\tau}) + \alpha \left[G-Q(S_{\tau}, Q_{\tau})\right]$<br>$\qquad\qquad$End if<br>$\qquad$ Until $\tau =T-1$</p><h3 id="n-step-expected-sarsa">n-step Expected Sarsa</h3><p>n-step Expected Sarsa的n-step return定义为：<br>$$G_{t:t+n} = R_{t+1} +\gamma R_{t+1} + \cdots +\gamma^{n-1} R_{t+n} + \gamma^n \bar{V}_{t+n-1}(S_{t+n}) \tag{8}$$<br>其中$\bar{V}<em>t(s) = \sum</em>{a}\pi(a|s) Q_t(s,a), \forall s\in S$<br>如果$s$是terminal，它的期望是$0$。</p><h2 id="n-step-off-policy-learning">n-step Off-policy Learning</h2><p>Off-policy算法使用behaviour policy采样的内容得到target policy的value function，但是需要使用他们在不同policy下采取某个action的relavtive probability。在$n$-step方法中，我们感兴趣的只有对应的$n$个actions，所以最简单的off-policy $n$-step TD算法，$t$时刻的更新可以使用importance sampling ratio $\rho_{t:t+n-1}$：<br>$$V_{t+n}(S_t) = V_{t+n-1} S_t + \alpha \rho_{t:t+n-1}\left[G_{t:t+n} - V_{t+n-1}(S_t)\right], 0\le t\le T \tag{9}$$<br>$\rho_{t:t+n-}$计算的是从$A_t$到$A_{t+n-1}$这$n$个action在behaviour policy和target policy下的relative probability，计算公式如下：<br>$$\rho_{t:h} = \prod_{k=t}^{min(h, T-1)} \frac{\pi(A_k|S_k)}{b(A_k|S_k)} \tag{10}$$<br>如果$\pi(A_k|S_k) = 0$，那么对应的$n$-step return对应的权重就应该是$0$，如果policy $pi$下选中某个action的概率很大，那么对应的return就应该比$b$下的权重大一些。因为在$b$下出现的概率下，很少出现，所以权重就该大一些啊。如果是on-policy的话，importance sampling ratio一直是$1$，所以公式$9$可以将on-policy和off-policy的$n$-step TD概括起来。同样的，$n$-step的Sarsa的更新公式也可以换成：<br>$$Q_{t+n}(S_t,A_t) = Q_{t+n-1}(S_t,A_t) + \alpha \rho_{t+1:t+n}\left[G_{t:t+n} - Q_{t+n-1} (S_t,A_t)\right], 0\le t\le T \tag{11}$$<br>公式$11$中的importance sampling ratio要比公式$9$中计算的晚一步。这是因为我们是在更新一个state-action pair，我们并不关心有多大的概率选中这个action，我们现在已经选中了它，importance sampling只是用于后续actions的选择。这个解释也让我理解了为什么Q-learning和Sarsa为什么没有使用importance sampling。完整的伪代码如下。</p><h3 id="off-policy-n-step-sarsa-估计-q">Off-policy $n$-step Sarsa 估计$Q$</h3><p>输入：任意的behaviour policy $b$, $b(a|s)\gt 0, \forall s\in S, a\in A$<br>随机初始化$Q(s,a), \forall s\in S, a\in A$<br>初始化$\pi$是相对于$Q$的greedy policy<br>算法参数：步长$\alpha \in (0,1]$，正整数$n$<br>Loop for each episode<br>$\qquad$初始化$S_0\neq terminal$<br>$\qquad$选择并存储$A_0 \sim b(\cdot|S_0)$<br>$\qquad T\leftarrow \infty$<br>$\qquad$Loop for $t=0,1,2,\cdots$<br>$\qquad\qquad$IF $t\lt T$,then<br>$\qquad\qquad\qquad$执行action $A_t$，接收$R_{t+1}, S_{t+1}$<br>$\qquad\qquad\qquad$如果$S_{t+1}$是terminal，那么$T\leftarrow t+1$<br>$\qquad\qquad\qquad$否则选择并记录$A_{t+1} \sim b(\cdot| S_{t+1})$<br>$\qquad\qquad$END IF<br>$\qquad\qquad \tau \leftarrow t-n +1$  (加$1$表示下标是从$0$开始的)<br>$\qquad\qquad$ IF $\tau \ge 0$<br>$\qquad\qquad\qquad \rho \leftarrow \prod_{i=\tau+1}^{min(\tau+n,T)} \frac{\pi(A_i|S_i)}{b(A_i|S_i)}$ （计算$\rho_{\tau+1:\tau+n}$，这里是不是写成了Expected Sarsa公式）<br>$\qquad\qquad\qquad G\leftarrow \sum_{i=\tau+1}^{min(\tau+n, T)}\gamma^{i-\tau -1}R_i$ （计算$n$个reward, $R_{\tau+1}+\cdots+R_{\tau+n}$）<br>$\qquad\qquad\qquad$如果$\tau+n \lt T$，$G\leftarrow G + \gamma^n Q(S_{\tau+n},A_{\tau+n})$ （因为没有$Q(S_T,A_T)$）<br>$\qquad\qquad\qquad Q(S_{\tau}, A_{\tau}) \leftarrow Q(S_{\tau}, A_{\tau})+\alpha \rho \left[G-Q(S_{\tau}, A_{\tau})\right]$（计算$Q(S_{\tau+n},A_{\tau+n})$）<br>$\qquad\qquad\qquad$确保$\pi$是相对于$Q$的greedy policy<br>$\qquad\qquad$ END IF<br>$\qquad$Until $\tau = T-1$<br>上面介绍的算法是$n$-step的Sarsa，计算importance ratio使用的$\rho_{t+1:t+n}$，$n$-step的Expected Sarsa计算importance ratio使用的是$\rho_{t+1:t+n-1}$，因为在估计$\bar{V}_{t+n-1}$时候，考虑到了last state中所有的actions。</p><h2 id="n-step-tree-backup算法">$n$-step Tree Backup算法</h2><p>这一章介绍的是不适用importance sampling的off-policy算法，叫做tree-backup algorithm。如下图所示，是一个$3$-step的tree-backup diaqgram。<br>沿着中间这条路走，有三个sample states和rewards以及两个sample actions。在旁边的是没有被选中的actions。对于这些没有选中的actions，因为没有采样，所以使用bootstrap计算target value。因为它的backup diagram有点像tree，所以就叫做tree backup upadte。或者更精确的说，backup update使用的是所用tree的叶子结点action value的估计值进行计算的。非叶子节点的action对应的是采样的action，不参与更新，所有叶子节点对于target的贡献都有一个权重正比于它在target policy下发生的概率。在$S_{t+1}$处的除了$A_{t+1}$之外所有action权重是$\pi(a|S_{t+1})$，$A_{t+1}$一点贡献没有；在$S_{t+2}$处所有没有被选中的action的权重是$\pi(A_{t+1}|S_{t+1})\pi(a’|S_{t+2})$；在$S_{t+3}$处所有没有被选中的action的权重是$\pi(A_{t+1}|S_{t+1})\pi(A_{t+2}|S_{t+2})\pi(a’’|S_{t+3})$<br>one-step Tree backup 算法其实就是Expected Sarsa：<br>$$G_{t:t+1} = R_{t+1} + \gamma \sum_a\pi(a|S_{t+1})Q(a, S_{t+1}), t\lt T-1 \tag{12}$$<br>two-step Tree backup算法return计算公式如下：<br>\begin{align*}<br>G_{t:t+2} &amp;= R_{t+1} + \gamma \sum_{a\neq A_{t+1}}\pi(a|S_{t+1})Q(a, S_{t+1}), t\lt T-1$ + \gamma\pi(A_{t+1}|S_{t+1})(R_{t+2} + \sum_{a} \pi(a|S_{t+2})Q(a, S_{t+2})\\<br>&amp;=R_{t+1} + \gamma \sum_{a\neq A_{t+1}}\pi(a|S_{t+1})Q(a, S_{t+1}), t\lt T-1$ + \gamma\pi(A_{t+1}|S_{t+1})G_{t+1:t+2}, t \lt T-2<br>\end{align*}<br>下面的形式给出了tree-backup的递归形式如下：<br>$$G_{t:t+n} = R_{t+1} + \gamma \sum_{a\neq A_{t+1}}\pi(a|S_{t+1})Q(a,S_{t+1}) + \gamma\pi(A_{t+1}|S_{t+1}) G_{t+1:t+n}, n\ge 2, t\lt T-1, \tag{13}$$<br>当$n=1$时除了$G_{T-1:t+n} = R_T$，其他的和式子$12$一样。使用这个新的target代替$n$-step Sarsa中的target就可以得到$n$-step tree backup 算法。<br>完整的算法如下所示：</p><h3 id="n-step-tree-backup-算法">$n$-step Tree Backup 算法</h3><p>随机初始化$Q(s,a),\forall s\in S, a\in A$<br>初始化$\pi$是相对于$Q$的$\epsilon$-greedy policy，或者是一个给定的不变policy<br>算法参数：step size $\alpha \in (0,1], \epsilon \gt 0$，一个正整数$n$<br>Loop for each episode<br>$\qquad$初始化$S_0\neq$ terminal<br>$\qquad$ 根据$S_0$随机选择action $A_0$<br>$\qquad$ $T\leftarrow \infty$<br>$\qquad$ Loop for $t=0,1,2,\cdots$<br>$\qquad\qquad$ If $t\lt T$,then:<br>$\qquad\qquad\qquad$ 采取action $A_t$，<br>$\qquad\qquad\qquad$ 接收rewared $R_{t+1}$以及下一个state $S_{t+1}$<br>$\qquad\qquad$ 如果$S_{t+1}$是terminal，那么<br>$\qquad\qquad$ $T\leftarrow t+1$<br>$\qquad\qquad$ 根据$S_{t+1}$随机选择action $A_{t+1}$<br>$\qquad\qquad$End IF<br>$\qquad\qquad$ $\tau \leftarrow t-n+1$<br>$\qquad\qquad\qquad$ IF $\tau+n\ge T$ then<br>$\qquad\qquad\qquad\qquad G\leftarrow R_T$<br>$\qquad\qquad\qquad$ ELSE<br>$\qquad\qquad\qquad\qquad G\leftarrow R_{t+1}+\gamma \sum_a\pi(a|S_{t+1})Q(a, S_{t+1})$<br>$\qquad\qquad\qquad$ END IF<br>$\qquad\qquad\qquad$Loop for $k = min(t, T-1)$ down $\tau+1$<br>$\qquad\qquad\qquad G\leftarrow R_k+\gamma^n\sum_{a\neq A_k}\pi(a|S_k)Q(a, S_k) + \gamma \pi(A_k|S_k) G$<br>$\qquad\qquad\qquad$ $Q(S_{\tau}, Q_{\tau}) \leftarrow Q(S_{\tau}, Q_{\tau}) + \alpha \left[G-Q(S_{\tau}, Q_{\tau})\right]$<br>$\qquad\qquad$End if<br>$\qquad$ Until $\tau =T-1$</p><h2 id="n-step-q-sigma">$n$-step $Q(\sigma)$</h2><p>现在已经讲了$n$-step Sarsa，$n$-step Expected Sarsa，$n$-step Tree Backup，$4$-step的一个backup diagram如下所示。它们其实有很多共同的特性，可以用一个框架把它们统一起来。<br>具体的算法就不细说了。</p><h2 id="参考文献">参考文献</h2><p>1.《reinforcement learning an introduction》第二版</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;n-step-bootstrapping&quot;&gt;n-step Bootstrapping&lt;/h2&gt;
&lt;p&gt;这章要介绍的n-step TD方法，将MC和one-step TD用一个统一的框架整合了起来。在一个任务中，两种方法可以无缝切换。n-step方法生成了一系列方法
      
    
    </summary>
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="n-steps" scheme="http://mxxhcm.github.io/tags/n-steps/"/>
    
  </entry>
  
  <entry>
    <title>maddpg</title>
    <link href="http://mxxhcm.github.io/2019/07/16/maddpg/"/>
    <id>http://mxxhcm.github.io/2019/07/16/maddpg/</id>
    <published>2019-07-16T02:31:58.000Z</published>
    <updated>2019-07-16T02:31:58.091Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>policy gradient</title>
    <link href="http://mxxhcm.github.io/2019/07/16/policy-gradient/"/>
    <id>http://mxxhcm.github.io/2019/07/16/policy-gradient/</id>
    <published>2019-07-16T02:31:55.000Z</published>
    <updated>2019-09-08T07:08:48.129Z</updated>
    
    <content type="html"><![CDATA[<h2 id="术语定义">术语定义</h2><p>更多介绍可以点击查看<a href>reinforcement learning an introduction 第三章</a></p><ol><li>状态集合<br>$\mathcal{S}$是有限states set，包含所有state的可能取值</li><li>动作集合<br>$\mathcal{A}$是有限actions set，包含所有action的可能取值</li><li>转换概率矩阵或者状态转换函数<br>$P:\mathcal{S}\times \mathcal{A}\times \mathcal{S} \rightarrow \mathbb{R}$是transition probability distribution，或者写成$p(s_{t+1}|s_t,a_t)$</li><li>奖励函数<br>$R:\mathcal{S}\times \mathcal{A}\rightarrow \mathbb{R}$是reward function</li><li>折扣因子<br>$\gamma \in (0, 1)$</li><li>初始状态分布<br>$\rho_0$是初始状态$s_0$服从的distribution，$s_0\sim \rho_0$</li><li>带折扣因子的MDP<br>定义为tuple $\left(\mathcal{S},\mathcal{A},P,R,\rho_0, \gamma\right)$</li><li>随机策略<br>选择action，stochastic policy表示为：$\pi_\theta: \mathcal{S}\rightarrow P(\mathcal{A})$，其中$P(\mathcal{A})$是选择$\mathcal{A}$中每个action的概率，$\theta$表示policy的参数，$\pi_\theta(a_t|s_t)$是在$s_t$处取action $a_t$的概率</li><li>期望折扣回报<br>定义$r_t = \sum_{k=t}^{\infty} \gamma^{k-t} R_{k+1}$为expected discounted returns，表示从$t$时刻开始的expected discounted return，<br>用$\eta(\pi)= \mathbb{E}_{s_0, a_0, \cdots\sim \pi}\left[\sum_{t=0}^{\infty} \gamma^t R_{t+1}\right]$表示$t=0$时policy $\pi$的expected discounted return，其中$s_0\sim\rho_0(s_0), a_t\sim\pi(a_t|s_t), s_{t+1}\sim P(s_{t+1}|s_t,a_t)$</li><li>状态值函数<br>state value function的定义是从$t$时刻的$s_t$开始的累计期望折扣奖励：<br>$$V^{\pi} (s_t) = \mathbb{E}_{a_{t}, s_{t+1},\cdots\sim \pi}\left[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \right]$$<br>或者有时候也定义成从$t=0$开始的expected return：<br>$$V^{\pi} (s) = \mathbb{E}_{\pi}\left[r_0|S_0=s;\pi\right]=\mathbb{E}_{\pi}\left[\sum_{t=0}^{\infty} \gamma^t R_{t+1}|S_0=s;\pi \right]$$</li><li>动作值函数<br>action value function定义为从$t$时刻的$s_t, a_t$开始的累计期望折扣奖励：<br>$$Q^{\pi} (s_t, a_t) = \mathbb{E}_{s_{t+1}, a_{t+1},\cdots\sim\pi}\left[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \right]$$<br>或者有时候也定义为从$t=0$开始的return的期望：<br>$$Q^{\pi} (s_0, a_0) = \mathbb{E}_{\pi}\left[r_0|S_0=s,A_0=a;\pi\right]=\mathbb{E}_{\pi}\left[\sum_{t=0}^{\infty} \gamma^t R_{t+1}|S_0=s,A_0=a;\pi \right]$$</li><li>优势函数<br>$A^{\pi} (s,a) = Q^{\pi}(s,a) -V^{\pi} (s)$，其中$a_t\sim \pi(a_t|s_t), s_{t+1}\sim P(s_{t+1}|s_t, a_t)$<br>$V^{\pi} (s)$可以看成状态$s$下所有$Q(s,a)$的期望，而$A^{\pi} (s,a)$可以看成当前的单个$Q(s,a)$是否要比$Q(s,a)$的期望要好，如果为正，说明这个$Q$比$Q$的期望要好，否则就不好。</li><li>目标函数<br>Agents的目标是找到一个policy，最大化从state $s_0$开始的expected return：$J(\pi)=\mathbb{E}_{\pi} \left[r_0|\pi\right]$，用$p(s\rightarrow s’,t,\pi)$表示从$s$经过$t$个timesteps到$s’$的概率，用<br>$$\rho^{\pi} (s’):=\int_S \sum_{t=0}^{\infty} \gamma^{t} \rho_0(s_0)p(s_0\rightarrow s’, t,\pi)ds_0$$<br>表示$s’$服从的概率分布，其中$\rho_0(s_0)$是初始状态$s_0$服从的概率分布。我们可以将performance objective表示成在state distribution $\rho^\pi $和policy $\pi_\theta$上的期望：<br>\begin{align*}<br>J(\pi_{\theta}) &amp;= \int_S \rho^{\pi} (s) \int_A \pi_{\theta}(s,a) r(s,a)dads\\<br>&amp;= \mathbb{E}_{s\sim \rho^{\pi} , a\sim \pi_{\theta}}\left[r(s,a)\right]\\ \tag{1}<br>\end{align*}<br>其中$\rho^{\pi} (s)$可以理解为$\rho^{\pi} (s) = P(s_0 = s) +\gamma P(s_1=s) + \gamma^2 P(s_2 = s)+\cdots$，就是policy $\pi$下state $s$出现的概率。<br>这里在每一个$t$处，$s_t=s$都是有一定概率发生的，也就是$\rho_{\pi}(s)$表示的东西。</li></ol><h2 id="policy-gradient">policy gradient</h2><h3 id="abstract">Abstract</h3><p>强化学习有三种常用的方法，第一种是基于值函数的，第二种是policy gradient，第三种是derivative-free的方法，即不利用导数的方法。基于值函数的方法在理论上证明是很难的。这篇论文提出了policy gradient的方法，直接用函数去表示策略，根据expected reward对策略参数的梯度进行更新，REINFORCE和actor-critic都是policy gradient的方法。<br>本文的贡献主要有两个，第一个是给出估计的action-value function或者advantage函数，梯度可以表示成experience的估计，第二个是证明了任意可导的函数表示的policy通过policy iteration都可以收敛到locl optimal policy。</p><h3 id="introduction">Introduction</h3><h4 id="值函数方法的缺点">值函数方法的缺点</h4><p>基于值函数的方法，在估计出值函数之后，每次通过greedy算法选择action。这种方法有两个缺点。</p><ul><li>基于值函数的方法会找到一个deterministic的策略，但是很多时候optimal policy可能是stochastic的。</li><li>某个action的估计值函数稍微改变一点就可能导致这个动作被选中或者不被选中，这种不连续是保证值函数收敛的一大障碍。</li></ul><h4 id="本文的工作">本文的工作</h4><h5 id="用函数表示stochastic-policy">用函数表示stochastic policy</h5><p>本文提出的policy gradient用函数表示stochastic policy。比如用神经网络表示的一个policy，输入是state，输出是每个action选择的概率，神经网络的参数是policy的参数。用$\mathbf{\theta}$表示policy参数，用$J$表示该策略的performance measure。然后参数$\mathbf{\theta}$的更新正比于以下梯度：<br>$$\nabla\mathbf{\theta} \approx \alpha \frac{\partial J}{\partial \mathbf{\theta}} \tag{1}$$<br>这里$\alpha$是步长，按照(1)式子进行更新，$\theta$可以确保收敛到J的局部最优值对应的local optimal policy。这里$\mathbf{\theta}$的微小改变只能造成policy和state分布的微小改变。</p><h5 id="使用值函数辅助学习policy">使用值函数辅助学习policy</h5><p>本文证明了通过使用满足特定属性的辅助近似值函数，使用experience可以得到(1)的一个无偏估计。另一个方法REINFORCE也找到了(1)的一个无偏估计，但是没有使用辅助的值函数。REINFORCE的学习速度要比使用值函数的方法慢很多。此外学习一个值函数，并用它取减少方差对快速学习是很重要的。</p><h5 id="证明policy-iteration收敛性">证明policy iteration收敛性</h5><p>本文还提出了一种方法证明基于actor-critic和policy-iteration架构方法的收敛性。在这篇文章中，他们只证明了使用通用函数逼近的policy iteration可以收敛到local optimal policy。</p><h3 id="policy-gradient-therorem">Policy Gradient Therorem</h3><p>智能体的在每一步的action由policy决定：$\pi(s,a,\mathbf{\theta})=Pr\left[a_t=a|s_t=s,\mathbf{\theta}\right],\forall s\in S, \forall a\in A,\mathbf{\theta}\in \mathbb{R}^l $。假设$\pi$是可导的，即，$\frac{\partial\pi(s,a)}{\partial\mathbf{\theta}}$存在。为了方便，通常把$\pi(s,a,\mathbf{\theta})$简写为$\pi(s,a)$。<br>这里有两种方式定义智能体的objective，一种是average reward，一种是从指定状态获得的长期奖励。</p><h4 id="average-reward-平均奖励">Average Reward(平均奖励)</h4><p>平均奖励是，策略根据每一步的长期期望奖励$\rho(\pi)$进行排名<br>$$\rho(\pi) = lim_{n\rightarrow \infty}\frac{1}{n}\mathbb{E}\left[r_1+r_2+\cdots+r_n|\pi\right] = \sum_s d^{\pi} (s) \sum_a\pi(s,a)R_s^a .$$<br>其中$d^{\pi} (s) = lim_{t\rightarrow \infty} Pr\left[s_t=s|s_0,\pi\right]$是我们假设的策略$\pi$下的固定分布，对于所有的策略都是独立于$s_0$的。这里，我想了一天都没有想明白，为什么？？？第一个等号，我可以理解，这里$r_n$表示的是在时间步$n$的immediate reward，所以第一个等号表示的是在策略$\pi$下$n$个时间步的imediate reward平均值的期望。<br>而第二个等号中，$d{\pi}(s)$是从初始状态$s_0$经过$t$步之后所有state $s$可能取值的概率，第一个求和号对$s$求和，就相当于一个离散积分，求的是$s$的期望；然后对$a$的求和，也相当于一个离散积分，求的是关于$a$的期望，所以第二个等式后面求的其实就是$R(s,a)$的期望。<br>state-action value定义为：<br>$$Q^{\pi} (s,a) = \sum_{t=1}^{\infty} \mathbb{E}\left[r_t - \rho(\pi)|s_0=s,a_0=a,\pi\right], \forall s\in S, a\in A.$$</p><h4 id="long-term-accumated-reward-from-designated-state-从指定状态开始的累计奖励">Long-term Accumated Reward from Designated State(从指定状态开始的累计奖励)</h4><p>这种情况是指定一个开始状态$s_0$，然后我们只关心从这个状态得到的长期reward。<br>$$\rho(\pi) = \mathbb{E}\left[\sum_{t=1}^{\infty} \gamma{t-1}|s_0,\pi\right],$$<br>$$Q^{\pi} (s,a) = \mathbb{E}\left[\sum_{k=1}^{\infty} r_{t+k}|s_t=s,a_t=a,\pi\right].$$<br>其中$\gamma\in[0,1]$是折扣因子，只有在episodic任务中才允许取$\gamma=1$。这里，我们定义$d^{\pi} (s)$是从开始状态$s_0$执行策略$\pi$遇到的状态的折扣权重之和：<br>$d^{\pi} (s) = \sum_{t=1}^{\infty} \gamma^t Pr\left[s_t = s|s_0,\pi\right].$<br>这里的$d^{\pi} $是从$s_0$开始，到$t=\infty$之间的任意时刻所有能到达state $s$的折扣概率之和。</p><h4 id="policy-gradient-theorem">Policy Gradient Theorem</h4><p>对于任何MDP，不论是平均奖励还是指定初始状态的形式，都有：<br>$$\frac{\partial J}{\partial \mathbf{\theta}} = \sum_a d^{\pi} (s)\sum_a\frac{\pi(s,a)}{\partial\mathbf{\theta}}Q^{\pi} (s,a), \tag{2}$$<br>证明：<br>平均奖励<br>\begin{align*}<br>\nabla v_{\pi}(s) &amp;= \nabla \left[ \sum_a \pi(a|s)q_{\pi}(s,a)\right], \forall s\in S \\<br>&amp;= \sum_a \left[\nabla\pi(a|s)q_{\pi}(s,a)\right], \\<br>&amp;= \sum_a \left[\nabla\pi(a|s)q_{\pi}(s,a) + \pi(a|s)\nabla q_{\pi}(s,a)\right] \\<br>&amp;= \sum_a\left[\nabla\pi(a|s)q_{\pi}(s,a) + \pi(a|s)\nabla \left[r-\rho(\pi)+\sum_{s’,r}p(s’,r|s,a)v_{\pi}(s’)\right]\right] \\<br>&amp;= \sum_a\left[\nabla\pi(a|s)q_{\pi}(s,a) + \pi(a|s)\left[-\nabla \rho(\pi)+\nabla \sum_{s’,r}p(s’,r|s,a)v_{\pi}(s’)\right]\right], \nabla r = 0\\<br>\end{align*}<br>而由$\sum_s\pi(s,a)=1$，我们得到：<br>$$\nabla v_{\pi}(s)=\sum_a\left[\nabla\pi(a|s)q_{\pi}(s,a) + \nabla \sum_{s’,r}p(s’,r|s,a)v_{\pi}(s’)\right] - \nabla v_{\pi}(s)$$<br>同时在上式两边对$d^{\pi} $进行求和，得到：<br>$$\sum_sd^{\pi} (s)\nabla v_{\pi}(s)=\sum_sd{\pi}(s)\sum_a\left[\nabla\pi(a|s)q_{\pi}(s,a) + \sum_sd{\pi}(s)\nabla \sum_{s’,r}p(s’,r|s,a)v_{\pi}(s’)\right] - \sum_sd^{\pi} (s)\nabla v_{\pi}(s)$$<br>因为$d^{\pi} $是稳定的，<br>$$\sum_sd^{\pi} (s)\nabla v_{\pi}(s)=\sum_sd{\pi}(s)\sum_a\left[\nabla\pi(a|s)q_{\pi}(s,a) + \sum_sd{\pi}(s)\nabla \sum_{s’,r}p(s’,r|s,a)v_{\pi}(s’)\right] - \sum_sd^{\pi} (s)\nabla v_{\pi}(s)$$<br>那么:<br>\begin{align*}<br>\end{align*}<br>指定初始状态$s_0$:<br>\begin{align*}<br>\nabla v_{\pi}(s) &amp;= \nabla \left[ \sum_a \pi(a|s)q_{\pi}(s,a)\right], \forall s\in S \\<br>&amp;= \sum_a \left[\nabla\pi(a|s)q_{\pi}(s,a)\right], \forall s\in S \\<br>&amp;= \sum_a\left[\nabla\pi(a|s)q_{\pi}(s,a) + \pi(a|s)\nabla q_{\pi}(s,a)\right] \\<br>&amp;= \sum_a\left[\nabla\pi(a|s)q_{\pi}(s,a) + \pi(a|s)\nabla \sum_{s’,r}p(s’,r|s,a)(r+\gamma v_{\pi}(s’))\right] \\<br>&amp;= \sum_a\left[\nabla\pi(a|s)q_{\pi}(s,a) + \pi(a|s) \nabla \sum_{s’,r}p(s’,r|s,a)r + \pi(a|s)\nabla \sum_{s’,r}p(s’,r|s,a)\gamma v_{\pi}(s’))\right] \\<br>&amp;= \sum_a\left[\nabla\pi(a|s)q_{\pi}(s,a) + 0 + \pi(a|s)\sum_{s’}\gamma p(s’|s,a)\nabla v_{\pi}(s’) \right] \\<br>&amp;= \sum_a\left[\nabla\pi(a|s)q_{\pi}(s,a) + 0 + \pi(a|s)\sum_{s’}\gamma p(s’|s,a)\right]\\<br>&amp;\ \ \ \ \ \ \ \ \sum_{a’}\left[\nabla\pi(a’|s’)q_{\pi}(s’,a’) + \pi(a’|s’)\sum_{s’’}\gamma p(s’’|s’,a’)\nabla v_{\pi}(s’’))] \right],  展开\\<br>&amp;= \sum_{x\in S}\sum_{k=0}^{\infty} Pr(s\rightarrow x, k,\pi)\sum_a\nabla\pi(a|x)q_{\pi}(x,a)<br>\end{align*}<br>第(5)式使用了$v_{\pi}(s) = \sum_a\pi(a|s)q(s,a)$进行展开。第(6)式将梯度符号放进求和里面。第(7)步使用product rule对q(s,a)求导。第(8)步利用$q_{\pi}(s, a) =\sum_{s’,r}p(s’,r|s,a)(r+v_{\pi}(s’)$ 对$q_{\pi}(s,a)$进行展开。第(9)步将(8)式进行分解。第(10)步对式(9)进行计算，因为$\sum_{s’,r}p(s’,r|s,a)r$是一个定制，求偏导之后为$0$。第(11)步对生成的$v_{\pi}(s’)$重复(5)-(10)步骤，得到式子(11)。如果对式子(11)中的$v_{\pi}(s)$一直展开，就得到了式子(12)。式子(12)中的$Pr(s\rightarrow x, k, \pi)$是在策略$\pi$下从state $s$经过$k$步转换到state $x$的概率，这里我有一个问题，就是为什么，$k$可以取到$\infty$，后来想了想，因为对第(11)步进行展开以后，可能会有重复的state，重复的意思就是从状态$s$开始，可能会多次到达某一个状态$x$，$k$就能取很多次，大不了$k=\infty$的概率为$0$就是了。</p><p>所以，对于$v_{\pi}(s_0)$，就有：<br>\begin{align*}<br>\nabla J(\mathbf{\theta}) &amp;= \nabla_{v_{\pi}}(s_0)\\<br>&amp;= \sum_{s\in S}( \sum_{k=0}^{\infty} Pr(s_0\rightarrow s,k,\pi) ) \sum_a\nabla_{\pi}(a|s)q_{\pi}(s,a)\\<br>&amp;=\sum_{s\in S}\eta(s)\sum_a \nabla_{\pi}(a|s)q_{\pi}(s,a)\\<br>&amp;=\sum_{s’\in S}\eta(s’)\sum_s\frac{\eta(s)}{\sum_{s’}\eta(s’)}\sum_a \nabla_{\pi}(a|s)q_{\pi}(s,a)\\<br>&amp;=\sum_{s’\in S}\eta(s’)\sum_s\mu(s)\sum_a \nabla_{\pi}(a|s)q_{\pi}(s,a)\\<br>&amp;\propto \sum_{s\in S}\mu(s)\sum_a\nabla\pi(a|s)q_{\pi}(s,a)<br>\end{align*}</p><p>从式子(2)可以看出来，这个梯度和$\frac{\partial d^{\pi} (s)}{\partial\mathbf{\theta}}$无关：即策略改变对于状态分布没有影响，这对于使用采样来估计梯度是很方便的。这里有点不明白，举个例子来说，如果$s$是从服从$\pi$的分布中采样的，那么$\sum_a\frac{\pi(s,a)}{\partial\mathbf{\theta}}Q{\pi}(s,a)$就是$\frac{\partial{\rho}}{\partial\mathbf{\theta}}$的一个无边估计。通常$Q{\pi}(s,a)$也是不知道的，需要去估计。一种方法是使用真实的returns，即$R_t = \sum_{k=1}^{\infty} r_{t+k}-\rho(\pi)$或者$R_t = \sum_{k=1}^{\infty} \gamma^{k-1} r_{t+k}-\rho(\pi)$（在指定初始状态条件下）。这就是REINFROCE方法，$\nabla\mathbf{\theta}\propto\frac{\partial\pi(s_t,a_t)}{\partial\mathbf{\theta}}R_t\frac{1}{\pi(s_t,a_t)}$,$\frac{1}{\pi(s_t,a_t)}$纠正了被$\pi$偏爱的action的oversampling）。</p><h3 id="policy-gradient-with-approximation-使用近似的策略梯度">Policy Gradient with Approximation(使用近似的策略梯度)</h3><p>如果$Q^{\pi} $也用一个学习的函数来近似，然后我们希望用近似的函数代替式子(2)中的$Q^{\pi} $，并大致给出梯度的方向。<br>用$f_w:S\times A \rightarrow R$表示$Q^{\pi} $的估计值。在策略$\pi$下，更新$w$的值:$\nabla w_t\propto \frac{\partial}{\partial w}\left[\hat{Q^{\pi} }(s_t,a_t) - f_w(s_t,a_t)\right]2 \propto \left[\hat{Q^{\pi} }(s_t,a_t) - f_w(s_t,a_t)\right]\frac{\partial f_w(s_t,a_t)}{\partial w}$，其中$\hat{Q^{\pi} }(s_t,a_t)$是$Q^{\pi} (s_t,a_t)$的一个无偏估计，可能是$R_t$，当这样一个过程收敛到local optimum，那么：<br>$$\sum_sd^{\pi} (s)\sum_a\pi(s,a)\left[Q^{\pi} (s,a) -f_w(s,a)\right]\frac{\partial f_w(s,a)}{\partial w}  = 0\tag{3}$$</p><h4 id="policy-gradient-with-approximation-theorem">Policy Gradient with Approximation Theorem</h4><p>如果$f_w$满足式子(3)，并且在某种意义上与policy parameterization兼容：<br>$$\frac{\partial f_w(s,a)}{\partial w} = \frac{\partial \pi(s,a)}{\partial \mathbf{\theta}}\frac{1}{\pi(s,a)}\tag{4}$$<br>那么有：<br>$$\frac{}{} = \sum_sd^{\pi} (s)\sum_a\frac{\partial \pi(s,a)}{\partial \mathbf{\theta}}f_w(s,a)\tag{5}$$</p><p>证明：<br>将(4)代入(3)得到：<br>\begin{align*}<br>&amp;\sum_sd^{\pi} (s)\sum_a\pi(s,a)\left[Q{\pi}(s,a) -f_w(s,a)\right]\frac{\partial f_w(s,a)}{\partial w} = 0\\<br>&amp;\sum_sd^{\pi} (s)\sum_a\pi(s,a)\left[Q{\pi}(s,a) -f_w(s,a)\right]\frac{\partial \pi(s,a)}{\partial \mathbf{\theta}}\frac{1}{\pi(s,a)}= 0\\<br>&amp;\sum_sd^{\pi} (s)\sum_a\left[Q{\pi}(s,a) -f_w(s,a)\right]\frac{\partial \pi(s,a)}{\partial \mathbf{\theta}}= 0 \tag{6}\\<br>\end{align*}<br>从这个式子中，我们能够从式子(6)中得到</p><h3 id="convergence-of-policy-iteration-with-function-approximation-使用函数近似的策略迭代的收敛性">Convergence of Policy Iteration with Function Approximation(使用函数近似的策略迭代的收敛性)</h3><h3 id="policy-iteration-with-function-apprpximation-theorem">Policy Iteration with Function Apprpximation Theorem</h3><p>用$\pi$和$f_w$表示策略和值函数的任意可导函数，并且满足式子(4)中的条件，Z</p><h2 id="deterministic-policy-gradient">deterministic policy gradient</h2><p>论文名称：Deterministic policy gradient algorithms<br>论文地址：</p><h3 id="摘要">摘要</h3><p>本文提出了deterministic policy gradient 算法。Deterministic实际上是action value function的expected gradient，这让deterministic policy gradient比stochastic policy gradient要更efficiently。为了保证足够的exploration，作者引入了off-policy的actor-critic算法学习determinitic target policy。<br>本文的contributions，dpg比spg要好，尤其是high dimensional tasks，此外，dpg不需要消耗更多的计算资源。还有就是对于一些应用，有可导的policy，但是没有办法加noist，这种情况下dpg更合适。</p><h3 id="stochastic-policy-gradient-vs-deterministic-policy-gradiet">stochastic policy gradient vs deterministic policy gradiet</h3><h4 id="sotchastic-policy-gradient">(sotchastic) policy gradient</h4><p>Policy gradient的basic idea是用参数化的policy distribution $\pi_\theta(a|s) = \mathbb{P}\left[a|s,\theta\right]$表示policy，这个policy $\pi$在state $s$处根据$\theta$表示的policy随机的选择action $a$。Policy gradient通常对policy 进行采样，然后调整policy的参数$\theta$朝着使cumulative reward更大的方向移动。</p><h4 id="deterministic-policy-gradient-v2">deterministic policy gradient</h4><p>一般来说，用$a=\mu_\theta(s)$表示deterministic policy。很自然能想到使用和stochastic policy gradient相同的方法能不能得到应用于deterministic policy gradient，即朝着使得cumulative reward更大的方向更新policy的参数。之前的一些工作认为deterministic policy gradient是不存在的，这篇文章证明了deterministic policy gradient是存在的，并且有一种非常简单的形式，可以从action value function的梯度中获得。而且在某些情况下，deterministic policy gradient可以看成stochastic policy gradient的特殊情况。</p><h4 id="spg-vs-dpg">spg vs dpg</h4><p>spg和dpg的第一个显著区别就是积分的space是不同的。Spg中policy gradient是在action和state spaces上进行积分的，而dpg的policy gradient仅仅在state space上进行积分。因此，计算spg需要更多samples，尤其是action spaces维度很高的情况下。<br>使用stochastic polic目的y是充分的explore整个state和action space。而在使用deterministic policy时，为了确保能够持续的进行explore，就需要使用off-policy的算法了，behaviour policy使用stochastic policy进行采样，target policy是deterministic policy。作者使用deterministic policy gradient推导出了一个off-policy的actor-critic方法，使用可导的function approximators估计action values，然后利用这个function的梯度更新policy的参数，同时为了确保policy gradient没有bias，使用而来compatible function。</p><h3 id="stochastic-policy-gradient-theorem">stochastic policy gradient theorem</h3><p>spg的基本想法就是调整policy的参数朝着$J$的梯度方向移动。<br>对$J(\pi_{\theta})$对$\theta$求导，得到：<br>\begin{align*}<br>\nabla_{\theta} J(\pi_{\theta})&amp;=\int_S\rho\pi(s) \int_A\nabla_\theta\pi_\theta (a|s)Q^{\pi} (s,a) dads \\<br>&amp;=\mathbb{E}_{s\sim \rho\pi, a\sim \pi_\theta}\left[\nabla_\theta log\pi_\theta(a|s)Q^{\pi} (s,a)\right] \tag{2}<br>\end{align*}<br>这就是policy gradient，很简单。state distribution $\rho\pi(s)$取决于policy parameters，但是policy gradient不依赖于state distribution的gradient。<br>这个理论有很重要的实用价值，因为它将performance gradient的计算变成了一个期望。然后可以通过sampling估计这个期望。这个方法中需要使用$Q\pi(s,a)$，估计$Q$不同方法就是不同的算法，最简单的使用sample return $ r_t^{\gamma} $估计$Q^{\pi} (s_t,a_t)$，就是REINFORCE算法。</p><h3 id="stochastic-actor-critic-算法">stochastic actor-critic 算法</h3><p>actor-critic是一个基于policy gradient theorem的结构。actor-critic包含两个组件。一个acotr通过stochastic gradient ascent调整stochastic policy $\pi_\theta(s)$的参数$\theta$，同时使用一个action-value function $Qw(s,a)$近似$Q\pi(s,a)$, $w$是function approximation的参数。Critic一般使用policy evaluation方法进行学习，比如使用td和mc等估计action value function $Q^w (s,a)\approx Q^\pi (s,a)$。一般来说，使用$Q^w (s,a)$代替真实的$Q^\pi (s,a)$会引入bias，但是，如果function approximator是compatible，即满足以下两个条件：</p><ol><li>$Qw(s,a) = \nabla_\theta log\pi_\theta(a|s)^T w$</li><li>参数$w$最小化mse: $\epsilon2(w)=\mathbb{E}_{s\sim \rho^\pi ,a\sim \pi_\theta}\left[(Q^w (s.a)-Q^\pi (s,a))^2 \right]$，这样就没有bias了，即：<br>$$\nabla_\theta J(\pi_\theta)=\mathbb{E}_{s\sim \rho\pi, a\sim \pi_\theta}\left[\nabla_\theta log\pi_\theta(a|s)Q^w (s,a)\right] \tag{3}$$</li></ol><p>直观上来说，条件1说的是compatible function approximators是stochastic policy梯度$\nabla_\theta log\pi_\theta(a|s)$的线性features，条件2需要满足$Qw(s,a)$是$Q^\pi (s,a)$的linear regression soulution。在实际应用中，条件2会放宽，使用如TD之类policy evaluation算法更efficiently的估计value function。事实上，如果条件1和2都满足的话，整个算法等价于没有使用critic，和REINFORCE算法很像。</p><h3 id="off-policy-actor-critic">off-policy actor critic</h3><p>在off-policy设置中，performance objective通常改成target policy的value function在behaviour policy的state distribution上进行平均，用$\beta(a|s)$表示behaviour policy：<br>\begin{align*}<br>J_\beta(\pi_\theta) &amp;= \int_S\rho\beta(s) V^\pi (s)ds\\<br>&amp;=\int_S\int_A\rho\beta(s)\pi_\theta(a|s)Q^\pi (s,a)dads<br>\end{align*}<br>对其求导和近似，得到：<br>\begin{align*}<br>\nabla_\theta J_\beta(\pi_\theta) &amp;\approx \int_S\int_A\rho\beta(s)\nabla_\theta\pi_\theta(a|s) Q^\pi (s,a)dads\tag{4} \\<br>&amp;=\mathbb{E}_{s\sim \rho\beta, a\sim \beta}\left[\frac{\pi_\theta(a|s)}{\beta_\theta(a|s)}\nabla_\theta log\pi_\theta(a|s) Q^\pi (s,a) \right]\tag{5}<br>\end{align*}<br>这个近似去掉了一项：$\nabla_\theta Q\pi(s,a)$，有人说这样子可以保留局部最小值。。Off-policy actor-critic算法使用behaviour policy $\beta$生成trajectories，critic off-policy的从那些trajectories中估计state value function $V^v (s)\approx v^\pi (s)$。actor使用sgd从这些trajectories中off policy的更新policy paramters $\theta$，同时使用TD-error估计$Q^\pi (s,a)$。actor和critic都是用importance sampling ratio $\frac{\pi_\theta(a|s)}{\beta_\theta(a|s)}$。</p><h3 id="gradients-of-deterministic-policies">Gradients of deterministic policies</h3><p>这一节主要介绍的是deterministic policy gradient theorem。首先给出直观上的解释，然后给定formal的证明。</p><h3 id="action-value-gradients">action value gradients</h3><p>绝大多数的model-free rl算法都属于GPI，迭代的policy evaluation和policy improvement。在contious action spaces中，greedy policy improvement是不可行的，因为在每一步都需要计算一个全局最大值。所以，一个简单的想法是使用让policy朝着$Q$的gradient方向移动，而不是全局最大化$Q$。具体而言，美誉每一个访问到的state $s$，policy parameters $\theta^{k+1} $的更新正比于$\nabla_{\theta} Q^{ {\mu}^k } (s, \mu_{\theta}(s) )$。如果每一个state给出一个不同的方向，如果使用state distribution $\rho^{\mu} (s)$求期望，最终的方向可能会被平均了：<br>$$\theta^{k+1} = \theta^k + \alpha \mathbb{E}_{s\sim \rho^{ {\mu}^k } }\left[\nabla_\theta Q^{ {\mu}^k } (s, \mu_{\theta}(s))\right] \tag{6}$$<br>通过使用chain rule，我们可以看到policy improvement可以被分解成action-value对于action的gradient和policy相对于policy parameters的gradient：<br>$$\theta{k+1} = \theta^k + \alpha \mathbb{E}_{s\sim \rho^{ {\mu}^k } }\left[\nabla_{\theta}\mu_{\theta}(s)\nabla_a Q^{ {\mu}^k } (s,a)|_{a=\mu_\theta(s_0)}\right] \tag{7}$$<br>按照惯例来说，$\nabla_{\theta}\mu_{\theta}(s)$是一个jacobian matrix，每一列是policy的$dth$ action对于$\theta$的gradient $\nabla_\theta\left[\mu_\theta(s)\right]_d$。然而，如果改变了policy，访问不同的states，state distribution　$\rho\mu$也会改变。最终不考虑distribution的变化的话，这个方法是保证收敛的。但是这里给给出的证明，和sgd一样，不需要计算state distributiond的gradient。</p><h3 id="deterministic-policy-gradient-theorem">deterministic policy gradient theorem</h3><p>deterministic policy定义为：$\mu_\theta: S\rightarrow A, \theta \in \mathbb{R}n$。定义performance objective $J(\mu_\theta) =\mathbb{E}\left[r_1^\gamma |\mu\right]$，定义概率分布维$p(s\rightarrow s’, t,u)$以及state distribution $\rho^\mu (s)$和stochastic case一样。将performance objective写成expectation如下：<br>\begin{align*}<br>J(\mu_\theta) &amp; = \int_S\rho\mu(s) r(s,\mu_\theta(s)) ds\\<br>&amp;= \mathbb{E}_{s\sim \rho\mu}\left[r(s, \mu_\theta(s))\right] \tag{8}<br>\end{align*}<br>给出deterministic policy gradient theorem：<br>假设MDP满足以下条件，即$\nabla_\theta\mu_\theta(s)$和$\nabla_a Q\mu(s,a)$存在，那么deterministic policy gradient存在，<br>\begin{align*}<br>\nabla_\theta J(\mu_\theta) &amp;= \int_S\rho\mu(s) \nabla_\theta\mu_\theta(s)\nabla_aQ^\mu (s,a)|_{a=\mu_\theta(s)}ds\\<br>&amp;=\mathbb{E}_{s\sim \rho\mu}\left[\nabla_\theta\mu_\theta(s)\nabla_aQ^\mu (s,a)|_{a=\mu_\theta(s)}\right] \tag{9}<br>\end{align*}</p><h3 id="spg的limit">spg的limit</h3><p>dpg theorem看起来和spg theorem很不像，事实上，对于一大类stochastic polices来说，dpg事实上是spg的一个特殊情况。如果使用deterministic policy $\mu_\theta:S\rightarrow A$和variance parameter $\sigma$表示某些stochastic policy $\pi_{\mu_{\theta,\sigma}}$，比如$\sigma = 0$时，$\pi_{\mu_{\theta, 0}} \equiv \mu_\theta$，当$\sigma \rightarrow 0$时，stochastic policy gradient收敛于deterministic policy gradient。<br>考虑一个stochastic policy $\pi_{\mu_{\theta,\sigma}}$让$\pi_{\mu_{\theta,\sigma}}(s,a)=v_\sigma(\mu_\theta(s),a)$，其中$\sigma$是控制方差的参数，并且$v_\sigma$满足条件B.1，以及MDP满足条件A.1和A.2，那么<br>$$\lim_{\sigma\rightarrow 0}\nabla_\theta J(\pi_{\mu_{\theta, \sigma}}) = \nabla_\theta J(\mu_\theta) \tag{10} $$<br>其中左边的gradient是标准spg的gradient，右边是dpg的gradient。<br>这就说明spg的很多方法同样也是适用于dpg的。</p><h3 id="deterministic-actor-critic">deterministic actor-critic</h3><p>接下来使用dpg theorem推到on-policy和off-policy的actor-critic算法。从最简单的on-policy更新，使用Sarsa critic开始，然后考虑off-policy算法，使用Q-learning critic描述核心思想。这些算法在实践中可能会有收敛问题，因为function approximator引入的biases问题，以及off-policy引入的不稳定。然后介绍使用compatiable function approximation的方法以及gradient td learning。</p><h3 id="on-policy-deterministic-actor-critic">on-policy deterministic actor-critic</h3><h3 id="off-policy-deterministic-actor-critic">off-policy deterministic actor-critic</h3><h3 id="compatible-function-approximation">compatible function approximation</h3><h2 id="ddpg">ddpg</h2><p>论文名称：CONTINUOUS CONTROL WITH DEEP REINFORCEMENT LEARNING<br>论文地址：<a href="https://arxiv.org/pdf/1509.02971.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1509.02971.pdf</a></p><h3 id="摘要-v2">摘要</h3><p>本文将DQN的思路推广到continuous action domain上。DQN是离散空间，DDPG是连续空间。</p><h3 id="简介">简介</h3><p>强化学习的目标是学习一个policy最大化$J=\mathbb{E}_{r_i,s_i\sim E, a_i\sim \pi}\left[R_1\right]$的expected return。<br>简要回顾以下action-value的定义，它的定义是从状态s开始,采取action a，采取策略$\pi$得到的回报的期望。<br>$$Q{\pi}(s_t,a_t) = \mathbb{E}_{r_{i\ge t}, s_{i \gt t}\sim E,a_{i\gt t}\sim \pi}\left[R_t|s_t,a_t\right] \tag{1}$$<br>（注意，这里$R$的下标和reinforcement learning an introduction中的定义不一样，但是这个无所谓，只要在用的时候保持统一就好了。）<br>许多rl方法使用bellman方程递归的更新Q:<br>$$Q{\pi}(s_t,a_t) = \mathbb{E}_{r_t,s_{t+1}\sim E}\left[r(s_t,a_t) + \gamma\mathbb{E}_{a_{t+1}\sim\pi}\left[Q^{\pi} (s_{t+1},a_{t+1})\right]\right]\tag{2}$$<br>如果target policy是deterministic的话，用$\mu$表示，那么就可以去掉式子里面的期望，action是deterministic的而不是服从一个概率分布：<br>$$Q{\mu}(s_t,a_t) = \mathbb{E}_{r_t,s_{t+1}\sim E}\left[r(s_t,a_t) + \gamma Q^{\mu} (s_{t+1},\mu(s_{t+1}))\right] \tag{3}$$<br>而第一个期望只和environment相关。这就意味着可以使用off-policy方法学习$Q{\mu}$。<br>在DQN中，作者使用replay buffer和target network缓解了non-linear funnction approximator不稳定的问题，作者在这篇文章将它们推广到了DDPG上面。</p><h3 id="ddpg-v2">DDPG</h3><p>直接将Q-learning推广到continuous action space是不可行的，因为action是continuous的，对其进行max等greedy操作是不可行的。这种优化方法只适合trival action spaces的情况。所以这里使用的是DPG(deterministic policy gradient)，将其推广到non-linear case，DPG是一种actor-critic的方法。<br>DPG使用一个参数化的actor function $\mu(s|\theta{\mu})$作为当前的policy，它将一个states直接mapping到一个specific action。$Q(s,a)$作为critic使用Q-learning中的Bellman公式进行更新。Actor的更新直接应用chain rule到$J$的expected reutrn ，更新actor的参数如下：<br>\begin{align*}<br>\nabla_{\theta{\mu}} &amp;\approx \mathbb{E}_{s_t\sim \rho^{\beta} }\left[\nabla_{\theta^{\mu} }Q(s,a|\theta^Q )|_{s=s_t, a= \mu(s_t|\theta^{\mu} )}\right]\\<br>&amp;= \mathbb{E}_{s_t\sim \rho{\beta}}\left[\frac{\partial Q(s,a|\theta^Q )}{\partial\theta^{\mu} }|_{s=s_t, a= \mu(s_t|\theta^{\mu} )}\right]\\<br>&amp;= \mathbb{E}_{s_t\sim \rho{\beta}}\left[\frac{\partial Q(s,a|\theta^Q )}{\partial a}|_{s=s_t, a= \mu(s_t)}\frac{\partial \mu(s_t|\theta^{\mu} )}{\partial\theta^{\mu} }|_{s=s_t}\right]\\<br>&amp;= \mathbb{E}_{s_t\sim \rho{\beta}}\left[\nabla_a Q(s,a|\theta^Q )|_{s=s_t, a= \mu(s_t)} \nabla_{\theta_{\mu}} \mu(s|\theta_{\mu})|_{s=s_t}\right]\\ \tag{4}<br>\end{align*}<br>中间的两行是我自己加的，不知道对不对，DPG论文中有证明，还没有看到，等到读完以后再说补充把。</p><h4 id="contributions">Contributions</h4><p>本文的几个改进：</p><ol><li>使用replay buffer，</li><li>使用target network解决不稳定的问题。</li><li>使用了batch-normalization。</li><li>exploration。off policy的一个优势就是target policy和behaviour policy可以不同。本文使用的behaviour policy $\mu’$ 添加了一个从noise process $N$中采样的noise：<br>$$\mu(s_t) = \mu(s_t|\theta_t{\mu}) + N \tag{5}$$</li></ol><h4 id="算法">算法</h4><p>算法1 DDPG<br>随机初始化critic 网络$Q(s,a |\theta Q)$，和actor网络$\mu(s|\theta^{\mu} )$的权重$\theta^Q $和$\theta^{\mu} $<br>初始化target networks　$Q’$和$\mu’$的权重$\theta{Q’}\leftarrow \theta^Q ,\theta^{\mu’} \leftarrow \theta^{\mu} $<br>初始化replay buffer $R$<br><strong>for</strong> episode = 1, M <strong>do</strong><br>初始化一个随机process $N$用于exploration<br>receive initial observation state $s_1$<br>for $t=1, T$ do<br>根据behaviour policy选择action $a_t = \mu(s_t| \theta{\mu}) + N_t$<br>执行action $a_t$，得到$r_t$和$s_{t+1}$<br>将transition $s_t, a_t, r_t, s_{t+1}$存到$R$<br>从$R$中采样$N$个transition $s_i, a_i, r_i, s_{i+1}$<br>设置target value $y_i = r_i + \gamma Q’(s_{i+1}, \mu’(s_{i+1}|\theta{\mu’})|\theta^{Q’} )$<br>使用$L = \frac{1}{N}\sum_i(y_i-Q(s_i,a_i|\theta Q))^2 $更新critic<br>使用sampled policy gradient 更新acotr:<br>$$\nabla_{\theta{\mu}}\approx \frac{1}{N}\sum_i\nabla_a Q(s,a|\theta^Q )|_{s=s_i, a=\mu(s_i)}\nabla_{\theta^{\mu} }\mu(s|\theta^{\mu} )|_{s_i}$$<br>更新target networks:<br>$$\theta’\leftarrow \tau \theta + (1-\tau) \theta’$$<br>end for<br>end for</p><h3 id="实验">实验</h3><p>所有任务中，都使用了low-dimensional state和high-dimensional renderings。在DQN中，为了让问题在high dimensional environment中fully observable，使用了action repeats。在agent的每一个timestep中，进行$3$个timesteps的仿真，包含repeating action以及rendering。因此agent的observation包含$9$个feature maps（RGB，每一个有3个renderings），可以让agent推理不同frames之间的differences。frames进行下采样，得到$64\times 64$的像素矩阵，然后$8$位的RGB值转化为$[0,1]$之间的float points。<br>在训练的时候，周期性的进行test，test时候的不需要exploration noise。实验表明，去掉不同的组件，即contribution中的几点之后，结果都会比原来差。没有使用target network的话，结果尤其差。<br>作者使用了两个baselines normalized scores，第一个是naive policy，在action space中均匀的采样action得到的mean return，第二个是iLQG。normalized之后，naive policy的mean score是0，iLQG的mean score是$1$。DDPG能够学习到好的policy，在某些任务上甚至比iLQG还要好。</p><h2 id="参考文献">参考文献</h2><p>Policy Gradient<br>1.<a href="https://arxiv.org/pdf/1509.02971.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1509.02971.pdf</a><br>2.<a href="https://medium.com/@jonathan_hui/rl-policy-gradients-explained-9b13b688b146" target="_blank" rel="noopener">https://medium.com/@jonathan_hui/rl-policy-gradients-explained-9b13b688b146</a><br>3.<a href="https://medium.com/@jonathan_hui/rl-policy-gradients-explained-advanced-topic-20c2b81a9a8b" target="_blank" rel="noopener">https://medium.com/@jonathan_hui/rl-policy-gradients-explained-advanced-topic-20c2b81a9a8b</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;术语定义&quot;&gt;术语定义&lt;/h2&gt;
&lt;p&gt;更多介绍可以点击查看&lt;a href&gt;reinforcement learning an introduction 第三章&lt;/a&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;状态集合&lt;br&gt;
$\mathcal{S}$是有限states set
      
    
    </summary>
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="http://mxxhcm.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="pg" scheme="http://mxxhcm.github.io/tags/pg/"/>
    
      <category term="dpg" scheme="http://mxxhcm.github.io/tags/dpg/"/>
    
      <category term="ddpg" scheme="http://mxxhcm.github.io/tags/ddpg/"/>
    
      <category term="maddpg" scheme="http://mxxhcm.github.io/tags/maddpg/"/>
    
      <category term="policy gradient" scheme="http://mxxhcm.github.io/tags/policy-gradient/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow distributed tensorflow</title>
    <link href="http://mxxhcm.github.io/2019/07/13/tensorflow-distributed-tensorflow/"/>
    <id>http://mxxhcm.github.io/2019/07/13/tensorflow-distributed-tensorflow/</id>
    <published>2019-07-13T13:11:06.000Z</published>
    <updated>2019-07-13T14:01:56.649Z</updated>
    
    <content type="html"><![CDATA[<h2 id="distributed-tensorflow">Distributed Tensorflow</h2><p>这篇文章主要介绍如何创建cluster of tensorflow serves，并且将一个computation graph分发到这个cluster上。</p><h2 id="cluster和server">Cluster和Server</h2><h3 id="简介">简介</h3><p>tensorflow中，一个cluster是一系列参与tensorflow graph distriuted execution的tasks。每个task和一个tensorflow server相关联，这个server可能包含一个&quot;master&quot;用来创建session，或者&quot;worker&quot;用来执行图中的op。一个cluster可以被分成更多jobs，每一个job包含一个或者更多个tasks。<br>为了创建一个cluster，需要给每一个task指定一个server。每一个task都运行在不同的devices上。在每一个task上，做以下事情：</p><ol><li>创建一个tf.train.ClusterSpec描述这个cluster的所有tasks，这对于所有的task都是一样的。</li><li>创建一个tf.train.Server，需要的参数是tr.train.ClusterSpec，识别local task使用job name和task index。</li></ol><h3 id="创建一个tf-train-clusterspec">创建一个tf.train.ClusterSpec</h3><p>下面的表格中给出了两个cluster的示例。传入的参数是一个dictionary，key是job的name，value是该job的所有可用devices。第二列对应的task的scope name。</p><table><thead><tr><th>tf.train.ClusterSpec construction</th><th>Available tasks</th></tr></thead><tbody><tr><td>tf.train.ClusterSpec({“local”: [“localhost:2222”, “localhost:2223”]})</td><td>/job:local/task:0/job:local/task:1</td></tr><tr><td>tf.train.ClusterSpec({ “worker”: [ “<a href="http://worker0.example.com:2222" target="_blank" rel="noopener">worker0.example.com:2222</a>”, “<a href="http://worker1.example.com:2222" target="_blank" rel="noopener">worker1.example.com:2222</a>”, “<a href="http://worker2.example.com:2222" target="_blank" rel="noopener">worker2.example.com:2222</a>”], “ps”: [“<a href="http://ps0.example.com:2222" target="_blank" rel="noopener">ps0.example.com:2222</a>”, “<a href="http://ps1.example.com:2222" target="_blank" rel="noopener">ps1.example.com:2222</a>”]})</td><td>/job:worker/task:0    /job:worker/task:1  /job:worker/task:2  /job:ps/task:0  /job:ps/task:1</td></tr></tbody></table><h3 id="为每一个task创建一个tf-train-server-instance">为每一个task创建一个tf.train.Server instance</h3><p>tf.train.Server对象包含local devices的集合，和其他tasks的connections</p><h2 id="参考文献">参考文献</h2><p>1.<a href="https://github.com/tensorflow/examples/blob/master/community/en/docs/deploy/distributed.md" target="_blank" rel="noopener">https://github.com/tensorflow/examples/blob/master/community/en/docs/deploy/distributed.md</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;distributed-tensorflow&quot;&gt;Distributed Tensorflow&lt;/h2&gt;
&lt;p&gt;这篇文章主要介绍如何创建cluster of tensorflow serves，并且将一个computation graph分发到这个cluster上。
      
    
    </summary>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/categories/tensorflow/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/tags/tensorflow/"/>
    
      <category term="distributed tensorflow" scheme="http://mxxhcm.github.io/tags/distributed-tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tensosrflow Coordinator</title>
    <link href="http://mxxhcm.github.io/2019/07/13/tensorflow-Coordinator/"/>
    <id>http://mxxhcm.github.io/2019/07/13/tensorflow-Coordinator/</id>
    <published>2019-07-13T12:17:13.000Z</published>
    <updated>2019-07-13T12:29:31.073Z</updated>
    
    <content type="html"><![CDATA[<h2 id="coordinator">Coordinator</h2><h3 id="简介">简介</h3><p>这个类用来协调多个thread同时工作，同时停止。常用的方法有：</p><ul><li>should_stop()：如果满足thread停止条件的话，返回True</li><li>request_stop()：请求thread停止，调用该方法后，should_stop()返回True</li><li>join(<list of threads>)：等待所有的threads停止。</list></li></ul><h3 id="理解">理解</h3><p>其实我觉得这个类的作用好像没有那么大，或者我没有用到这种场景。反正就是满足thread的终止条件时，调用request_stop()函数，让should_stop()返回True。</p><h2 id="代码示例">代码示例</h2><p><a href="https://github.com/mxxhcm/code/blob/master/tf/ops/tf_train_Coordinator.py" target="_blank" rel="noopener">代码地址</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"></span><br><span class="line">n = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add</span><span class="params">(index)</span>:</span></span><br><span class="line">    <span class="keyword">global</span> n</span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">not</span> coord.should_stop():</span><br><span class="line">    <span class="comment">#while True:</span></span><br><span class="line">        <span class="keyword">if</span> n &gt; <span class="number">10</span>:</span><br><span class="line">            print(index, <span class="string">" done"</span>)</span><br><span class="line">            coord.request_stop()</span><br><span class="line">            <span class="comment">#break</span></span><br><span class="line">        print(<span class="string">"before A, thread "</span>, index, n)</span><br><span class="line">        n += <span class="number">1</span></span><br><span class="line">        print(<span class="string">"after A, thread "</span>, index, n)</span><br><span class="line">        time.sleep(random.random())</span><br><span class="line">        print(<span class="string">"before B, thread "</span>, index, n)</span><br><span class="line">        n += <span class="number">1</span></span><br><span class="line">        print(<span class="string">"after B, thread "</span>, index, n)</span><br><span class="line">    </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">        coord = tf.train.Coordinator()</span><br><span class="line"></span><br><span class="line">        jobs = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">            jobs.append(threading.Thread(target=add, args=(i,)))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> jobs:</span><br><span class="line">            j.start()</span><br><span class="line"></span><br><span class="line">        coord.join(jobs)</span><br><span class="line">        print(<span class="string">"Hello World!"</span>)</span><br></pre></td></tr></table></figure><h2 id="参考文献">参考文献</h2><p>1.<a href="https://wiki.jikexueyuan.com/project/tensorflow-zh/how_tos/threading_and_queues.html" target="_blank" rel="noopener">https://wiki.jikexueyuan.com/project/tensorflow-zh/how_tos/threading_and_queues.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;coordinator&quot;&gt;Coordinator&lt;/h2&gt;
&lt;h3 id=&quot;简介&quot;&gt;简介&lt;/h3&gt;
&lt;p&gt;这个类用来协调多个thread同时工作，同时停止。常用的方法有：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;should_stop()：如果满足thread停止条件的话，返回
      
    
    </summary>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/categories/tensorflow/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/tags/tensorflow/"/>
    
      <category term="Coortinator" scheme="http://mxxhcm.github.io/tags/Coortinator/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow one_hot</title>
    <link href="http://mxxhcm.github.io/2019/07/13/tensorflow-one-hot/"/>
    <id>http://mxxhcm.github.io/2019/07/13/tensorflow-one-hot/</id>
    <published>2019-07-13T04:08:55.000Z</published>
    <updated>2019-07-13T04:16:19.791Z</updated>
    
    <content type="html"><![CDATA[<h2 id="tf-one-hot">tf.one_hot</h2><h3 id="一句话介绍">一句话介绍</h3><p>返回一个one-hot tensor</p><h3 id="api">API</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">tf.one_hot(</span><br><span class="line">    indices,    <span class="comment"># 每一个one-hot向量不为0的维度</span></span><br><span class="line">    depth,  <span class="comment"># 指定每一个one-hot向量的维度</span></span><br><span class="line">    on_value=<span class="literal">None</span>,  <span class="comment"># indices上取该值</span></span><br><span class="line">    off_value=<span class="literal">None</span>,     <span class="comment">#其他地方取该值</span></span><br><span class="line">    axis=<span class="literal">None</span>,</span><br><span class="line">    dtype=<span class="literal">None</span>,</span><br><span class="line">    name=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="代码示例">代码示例</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">indices = [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line">depth = <span class="number">5</span></span><br><span class="line"></span><br><span class="line">result = tf.one_hot(indices, depth)</span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line">print(sess.run(result))</span><br><span class="line"><span class="comment"># [[0. 1. 0. 0. 0.]</span></span><br><span class="line"><span class="comment">#  [0. 1. 0. 0. 0.]</span></span><br><span class="line"><span class="comment">#  [0. 1. 0. 0. 0.]]</span></span><br></pre></td></tr></table></figure><h2 id="参考文献">参考文献</h2><p>1.<a href="https://www.tensorflow.org/api_docs/python/tf/one_hot" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/one_hot</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;tf-one-hot&quot;&gt;tf.one_hot&lt;/h2&gt;
&lt;h3 id=&quot;一句话介绍&quot;&gt;一句话介绍&lt;/h3&gt;
&lt;p&gt;返回一个one-hot tensor&lt;/p&gt;
&lt;h3 id=&quot;api&quot;&gt;API&lt;/h3&gt;
&lt;figure class=&quot;highlight pytho
      
    
    </summary>
    
      <category term="python" scheme="http://mxxhcm.github.io/categories/python/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/tags/tensorflow/"/>
    
      <category term="one-hot" scheme="http://mxxhcm.github.io/tags/one-hot/"/>
    
  </entry>
  
  <entry>
    <title>python slice</title>
    <link href="http://mxxhcm.github.io/2019/07/13/python-slice/"/>
    <id>http://mxxhcm.github.io/2019/07/13/python-slice/</id>
    <published>2019-07-13T02:30:16.000Z</published>
    <updated>2019-07-13T02:43:00.402Z</updated>
    
    <content type="html"><![CDATA[<h2 id="python-slice-index">python slice index</h2><p>±–±--±–±--±–±--+<br>| P | y | t | h | o | n |<br>±–±--±–±--±–±--+<br>0   1   2   3   4   5   6<br>-6  -5  -4  -3  -2  -1</p><h2 id="start和stop">start和stop</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a[start:stop]   # 从start到stop-1的所有items</span><br><span class="line">a[start:]   # 从start到array结尾的所有items</span><br><span class="line">a[:stop]   # 从开始到stop-1的所有items</span><br><span class="line">a[:]   # 整个array的copy</span><br></pre></td></tr></table></figure><h2 id="step">step</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a[start:stop:step]   # 从start，每次加上step，不超过stop，step默认为1</span><br></pre></td></tr></table></figure><h2 id="负的start和stop">负的start和stop</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a[-1]   # array的最后一个item</span><br><span class="line">a[-2:]   # array的最后两个items</span><br><span class="line">a[:-2]   # 从开始到倒数第三个的所有items</span><br></pre></td></tr></table></figure><h2 id="负的step">负的step</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a[::-1]   # 所有元素，逆序</span><br><span class="line">a[1::-1]    # 前两个元素，逆序</span><br><span class="line">a[:-3:-1]   # 后两个元素，逆序</span><br><span class="line">a[-3::-1]   # 除了最后两个元素，逆序</span><br></pre></td></tr></table></figure><p>这里加一些自己的理解，其实就是倒着数而已，包含第一个:前面的，不包含两个:之间的。</p><h2 id="参考文献">参考文献</h2><p>1.<a href="https://stackoverflow.com/a/509297/8939281" target="_blank" rel="noopener">https://stackoverflow.com/a/509297/8939281</a><br>2.<a href="https://stackoverflow.com/a/509295/8939281" target="_blank" rel="noopener">https://stackoverflow.com/a/509295/8939281</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;python-slice-index&quot;&gt;python slice index&lt;/h2&gt;
&lt;p&gt;±–±--±–±--±–±--+&lt;br&gt;
| P | y | t | h | o | n |&lt;br&gt;
±–±--±–±--±–±--+&lt;br&gt;
0   1   2   3
      
    
    </summary>
    
      <category term="python" scheme="http://mxxhcm.github.io/categories/python/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="slice" scheme="http://mxxhcm.github.io/tags/slice/"/>
    
  </entry>
  
  <entry>
    <title>numpy expand_dims and newaxis</title>
    <link href="http://mxxhcm.github.io/2019/07/12/numpy-expand-dims-and-newaxis/"/>
    <id>http://mxxhcm.github.io/2019/07/12/numpy-expand-dims-and-newaxis/</id>
    <published>2019-07-12T12:08:59.000Z</published>
    <updated>2019-07-12T12:37:12.175Z</updated>
    
    <content type="html"><![CDATA[<h2 id="expand-dims">expand_dims</h2><p>在数组的某个维度增加一个为1的维度。</p><h3 id="代码示例">代码示例</h3><p><a href>代码地址</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">a = np.ones([<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">print(a.shape)</span><br><span class="line"></span><br><span class="line">a1 = np.expand_dims(a, <span class="number">0</span>)</span><br><span class="line">print(a1.shape)</span><br><span class="line"></span><br><span class="line">a2 = np.expand_dims(a, <span class="number">1</span>)</span><br><span class="line">print(a2.shape)</span><br><span class="line"></span><br><span class="line">a3 = np.expand_dims(a, <span class="number">2</span>)</span><br><span class="line">print(a3.shape)</span><br><span class="line"></span><br><span class="line">a4 = np.expand_dims(a, <span class="number">3</span>)</span><br><span class="line">print(a4.shape)</span><br></pre></td></tr></table></figure><h2 id="newaxis">newaxis</h2><h3 id="简介">简介</h3><p>newaxis就是None的一个alias</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>np.newaxis <span class="keyword">is</span> <span class="literal">None</span></span><br><span class="line"><span class="literal">True</span></span><br></pre></td></tr></table></figure><h3 id="用途">用途</h3><p>常用于slicing。用于给数组增加一个维度。</p><h3 id="代码示例-v2">代码示例</h3><p><a href>代码地址</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">a = np.ones((<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line">print(a.shape)</span><br><span class="line">(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">a1 = a[np.newaxis]</span><br><span class="line">print(a1.shape)</span><br><span class="line">(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">a2 = a[:, np.newaxis]</span><br><span class="line">print(a2.shape)</span><br><span class="line">(<span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">a3 = a[:, :, np.newaxis]</span><br><span class="line">print(a3.shape)</span><br><span class="line">(<span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">a4 = a[:, :, :, np.newaxis]</span><br><span class="line">print(a4.shape)</span><br><span class="line">(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">a5 = a[:, :, np.newaxis, np.newaxis, :]</span><br><span class="line">print(a5.shape)</span><br><span class="line">(<span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">4</span>)</span><br></pre></td></tr></table></figure><h2 id="参考文献">参考文献</h2><p>1.<a href="https://stackoverflow.com/questions/29241056/how-does-numpy-newaxis-work-and-when-to-use-it" target="_blank" rel="noopener">https://stackoverflow.com/questions/29241056/how-does-numpy-newaxis-work-and-when-to-use-it</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;expand-dims&quot;&gt;expand_dims&lt;/h2&gt;
&lt;p&gt;在数组的某个维度增加一个为1的维度。&lt;/p&gt;
&lt;h3 id=&quot;代码示例&quot;&gt;代码示例&lt;/h3&gt;
&lt;p&gt;&lt;a href&gt;代码地址&lt;/a&gt;&lt;/p&gt;
&lt;figure class=&quot;highlight pyt
      
    
    </summary>
    
      <category term="python" scheme="http://mxxhcm.github.io/categories/python/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="numpy" scheme="http://mxxhcm.github.io/tags/numpy/"/>
    
      <category term="expand_dims" scheme="http://mxxhcm.github.io/tags/expand-dims/"/>
    
      <category term="newaxis" scheme="http://mxxhcm.github.io/tags/newaxis/"/>
    
  </entry>
  
  <entry>
    <title>windows 文件和文件夹操作命令</title>
    <link href="http://mxxhcm.github.io/2019/07/05/windows-%E6%96%87%E4%BB%B6%E5%92%8C%E6%96%87%E4%BB%B6%E5%A4%B9%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4/"/>
    <id>http://mxxhcm.github.io/2019/07/05/windows-文件和文件夹操作命令/</id>
    <published>2019-07-05T03:14:17.000Z</published>
    <updated>2019-07-05T03:18:48.838Z</updated>
    
    <content type="html"><![CDATA[<h2 id="文件和文件夹查看">文件和文件夹查看</h2><ol><li>查看文件和文件夹<br>dir</li><li>只查看文件夹<br>dir /ad<br>dir /a:d</li><li>只查看文件</li></ol><h2 id="文件和文件夹删除">文件和文件夹删除</h2><ol><li>删除文件<br>del file_name</li><li>删除目录<br>rd dir_name</li></ol><h2 id="参考文献">参考文献</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;文件和文件夹查看&quot;&gt;文件和文件夹查看&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;查看文件和文件夹&lt;br&gt;
dir&lt;/li&gt;
&lt;li&gt;只查看文件夹&lt;br&gt;
dir /ad&lt;br&gt;
dir /a:d&lt;/li&gt;
&lt;li&gt;只查看文件&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;文件和文件夹删
      
    
    </summary>
    
      <category term="windows" scheme="http://mxxhcm.github.io/categories/windows/"/>
    
    
      <category term="windows" scheme="http://mxxhcm.github.io/tags/windows/"/>
    
  </entry>
  
  <entry>
    <title>常见单词解释</title>
    <link href="http://mxxhcm.github.io/2019/06/29/%E5%B8%B8%E8%A7%81%E5%8D%95%E8%AF%8D%E8%A7%A3%E9%87%8A/"/>
    <id>http://mxxhcm.github.io/2019/06/29/常见单词解释/</id>
    <published>2019-06-29T06:59:41.000Z</published>
    <updated>2019-07-07T14:24:23.590Z</updated>
    
    <content type="html"><![CDATA[<h2 id="non-trivial">non-trivial</h2><p>在数学上有专门的翻译，非平凡的。<br>trivial是无足轻重的，不重要的，所以non-trivial是有一定规模的，复杂的。</p><h2 id="whitened">whitened</h2><p>图片变化，指的是将输入经过线性变换成均值为0和单位方差的数据。。我是在batch_normalization论文中遇到的。</p><h2 id="saturating-and-non-saturating">saturating and non-saturating</h2><p>出现在ReLU论文中，用来形容激活函数，saturating指的是输出挤压在一个区间内。</p><h2 id="参考文献">参考文献</h2><p>1.<a href="https://www.zhihu.com/question/20681622" target="_blank" rel="noopener">https://www.zhihu.com/question/20681622</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;non-trivial&quot;&gt;non-trivial&lt;/h2&gt;
&lt;p&gt;在数学上有专门的翻译，非平凡的。&lt;br&gt;
trivial是无足轻重的，不重要的，所以non-trivial是有一定规模的，复杂的。&lt;/p&gt;
&lt;h2 id=&quot;whitened&quot;&gt;whitened&lt;/h
      
    
    </summary>
    
    
      <category term="工具" scheme="http://mxxhcm.github.io/tags/%E5%B7%A5%E5%85%B7/"/>
    
      <category term="单词" scheme="http://mxxhcm.github.io/tags/%E5%8D%95%E8%AF%8D/"/>
    
  </entry>
  
  <entry>
    <title>linux init系统</title>
    <link href="http://mxxhcm.github.io/2019/06/23/linux-init%E7%B3%BB%E7%BB%9F/"/>
    <id>http://mxxhcm.github.io/2019/06/23/linux-init系统/</id>
    <published>2019-06-23T08:35:02.000Z</published>
    <updated>2019-06-23T12:41:34.401Z</updated>
    
    <content type="html"><![CDATA[<h2 id="linux-init系统">Linux init系统</h2><p>这里会介绍下面三种init系统</p><ul><li>SysVInit(initd)</li><li>Upstart</li><li>SystemD</li></ul><p>Linux的启动从BIOS开始，bootloader载入内核，进行内核初始化。内核初始化的最后一步是启动pid为$1$的init进程。这个进程是系统的第一个进程，它负责产生其他所有用户进程。<br>Init系统能够定义、管理和控制init进程的行为。它负责组织和运行许多独立的或相关的始化job(因此被称为init系统)，从而让计算机系统进入某种用户规定的run level。<br>大多数Linux发行版的init 系统是和 System V 相兼容的，称为 SysVInit。Ubuntu和RHEL采用upstart替代了SysVInit。而 Fedora 从版本15开始使用SystemD的新init系统。ubuntu-16.10之后开始不再使用SysVInt管理系统，改用SystemD。</p><h2 id="sysvinit-initd">SysVInit(initd)</h2><p>SysVInit的最大缺点是主要依赖于 Shell 脚本，启动太慢。</p><h3 id="runlevel">runlevel</h3><p>SysVInit检查/etc/inittab文件中是否含有’initdefault’ 项，然后进入默认run level。如果没有默认的run level，用户手动决定进入哪个run level。SysVInit通常会有8种run-level，0到6和S或者s。但 0，1，6 run level的操作是公认的： 0代表关机, 1代表单用户模式, 6代表重启，其他run level跟发行版有关。/etc/inittab 文件中还定义了各种run level需要执行的初始化工作。</p><h3 id="sysvinit初始化顺序">SysVInit初始化顺序</h3><p>SysVInit用脚本，文件命名规则和软链接来实现不同的runlevel。首先，SysVInit 需要读取/etc/inittab 文件，获得以下配置信息：</p><ul><li>系统需要进入的 runlevel</li><li>捕获组合键的定义</li><li>定义电源 fail/restore 脚本</li><li>启动 getty 和虚拟控制台</li></ul><p>得到配置信息后，SysVInit 顺序地执行以下初始化步骤将系统初始化为相应的runlevel X。</p><ul><li>/etc/rc.d/rc.sysinit</li><li>/etc/rc.d/rc 和/etc/rc.d/rcX.d/ (X 代表运行级别 0-6)</li><li>/etc/rc.d/rc.local</li><li>X Display Manager（如果需要的话）</li></ul><h4 id="rc-sysinit初始化">rc.sysinit初始化</h4><p>首先运行rc.sysinit完成以下任务。</p><ul><li>激活 udev 和 selinux</li><li>设置定义在/etc/sysctl.conf 中的内核参数</li><li>设置系统时钟</li><li>加载 keymaps</li><li>使能交换分区</li><li>设置主机名(hostname)</li><li>根分区检查和 remount</li><li>激活 RAID 和 LVM 设备</li><li>开启磁盘配额</li><li>检查并挂载所有文件系统</li><li>清除过期的 locks 和 PID 文件</li></ul><h4 id="etc-rd-c-rc初始化">/etc/rd.c/rc初始化</h4><p>接下来运行/etc/rc.d/rc脚本。根据不同的runlevel，rc脚本打开对应该runlevel的rcX.d目录，以S开头的脚本是启动时运行的脚本，S后面的数字定义了这些脚本的执行顺序。</p><h4 id="etc-rc-d-rc-local初始化">/etc/rc.d/rc.local初始化</h4><p>rc.local是自定义脚本存放目录。</p><h3 id="sysvinit关闭系统">SysVInit关闭系统</h3><p>关闭顺序的控制也是依靠/etc/rc.d/rcX.d/目录下所有脚本的命名规则来控制的，所有以K开头的脚本都将在关闭系统时调用，K后的数字规定了执行顺序。这些脚本负责安全地停止service或者其他的关闭job。</p><h3 id="sysvinit工具">SysVInit工具</h3><p>SysVInit包含了一系列启动，运行和关闭所有其他程序的工具和命令。</p><ul><li>halt  停止系统。</li><li>init  SysVInit本身的init进程，pid=1，是所有用户进程的父进程。最主要的作用是在启动过程中使用/etc/inittab文件创建所有其他初始化进程。</li><li>killall5  SystemV的killall 命令。向除自己的会话(session)进程之外的其它进程发出信号，所以不能杀死当前使用的 shell。</li><li>last  回溯/var/log/wtmp 文件(或者-f 选项指定的文件)，显示自从这个文件建立以来，所有用户的登录情况。</li><li>lastb 作用和 last 差不多，默认情况下使用/var/log/btmp 文件，显示所有失败登录企图。</li><li>mesg  控制其它用户对用户终端的访问。</li><li>pidof 找出程序的进程识别号(pid)，输出到标准输出设备。</li><li>poweroff  等于 shutdown -h –p，或者 telinit 0。关闭系统并切断电源。</li><li>reboot    等于 shutdown –r 或者 telinit 6。重启系统。</li><li>runlevel  读取系统的登录记录文件(一般是/var/run/utmp)把以前和当前的run level输出到标准输出设备。</li><li>shutdown  以一种安全的方式终止系统，所有正在登录的用户都会收到系统将要终止通知，并且不准新的登录。</li><li>sulogin   当系统进入单用户模式时，被init调用。当接收到启动加载程序传递的-b 选项时，init 也会调用 sulogin。</li><li>telinit   实际是init的一个连接，用来向init传送单字符参数和信号。</li><li>utmpdump  以一种用户友好的格式向标准输出设备显示/var/run/utmp 文件的内容。</li><li>wall  向所有有信息权限的登录用户发送消息。</li></ul><h2 id="upstart">Upstart</h2><p>Ubuntu使用upstart init系统，没有/etc/inittab文件。Upstart解决了热插拔以及网络共享盘的挂载问题。在/etc/fstab 中，可以指定系统自动挂载一个网络盘，比如 NFS等，SysVInit 分析/etc/fstab 挂载文件系统这个步骤是在网络启动之前。可是如果网络没有启动，NFS或者iSCSI服务都不可访问，当然也无法进行挂载操作。SysVInit采用netdev的方式来解决这个问题，即/etc/fstab发现netdev属性挂载点的时候，不尝试挂载它，在网络初始化之后，有专门的netfs service来挂载所有这些网络盘。<br>UpStart 解决了之前提到的 SysVInit 的缺点。采用event驱动模型，UpStart 可以：</p><ul><li>更快地启动系统。采用event驱动机制加快了系统启动时间。SysVInit 运行时是同步阻塞的。一个脚本运行的时候，后续脚本必须等待。这意味着所有的初始化步骤都是串行执行的，而实际上很多service彼此并不相关，完全可以并行启动，从而减小系统的启动时间。</li><li>当新硬件被发现时动态启动相应的service，Ubuntu 开发了基于event机制的UpStart，比如 U 盘插入 USB 接口后，udev得到内核通知，发现该设备，这是一个新的event。UpStart感知到该event之后触发相应的等待任务，比如处理/etc/fstab中新的挂载点。采用这种event驱动的模式，upstart 完美地解决了即插即用设备带来的新问题。</li><li>硬件被拔除时动态停止相应的service</li></ul><h3 id="upstart-job和event">Upstart job和event</h3><p>Job是一个job unit，用来完成一件工作，比如启动一个后台service，或者运行一个配置命令。每个 Job 都等待一个或多个event，一旦event发生，upstart 就触发该job完成相应的工作。</p><h4 id="job">Job</h4><p>Job是一个job的unit，一个task或者一个service。可以理解为SysVInit中的一个service脚本。有三种类型的job：</p><ul><li>task job，task job 代表在一定时间内会执行完毕的任务，比如删除一个文件；</li><li>service job，service job 代表后台service进程，比如 apache httpd。这里进程一般不会退出，一旦开始运行就成为一个后台deamon，由 init 进程管理，如果这类进程退出，由 init 进程重新启动，它们只能由 init 进程发送信号停止。它们的停止一般也是由于所依赖的停止event而触发的，不过 upstart 也提供命令行工具，让管理人员手动停止某个service；</li><li>abstract job，abstract job 仅由 upstart 内部使用，仅对理解 upstart 内部机理有所帮助。我们不用关心它。</li></ul><h5 id="system-job-and-session-job">system job and session job</h5><p>还可以根据Upstart初始化的范围对job进行分类。系统的初始化任务叫做system job，比如挂载文件系统的任务就是一个system job；用户会话的初始化service就叫做 session job。</p><h5 id="job-生命周期">Job 生命周期</h5><p>Upstart为每个job都维护一个生命周期。一般来说，job有开始，运行和结束。以及其他更精细的状态，比如开始有开始之前(pre-start)，即将开始(starting)和已经开始了(started)几种不同的状态，这样可以更加精确地描述job的当前状态。详细的状态如下表所示：</p><table><thead><tr><th>状态名</th><th>含义</th></tr></thead><tbody><tr><td>Waiting</td><td>初始状态</td></tr><tr><td>Starting</td><td>Job 即将开始</td></tr><tr><td>pre-start</td><td>执行 pre-start 段，即任务开始前应该完成的job</td></tr><tr><td>Spawned</td><td>准备执行 script 或者 exec 段</td></tr><tr><td>post-start</td><td>执行 post-start 动作</td></tr><tr><td>Running</td><td>interim state set after post-start section processed denoting job is running (But it may have no associated PID!)</td></tr><tr><td>pre-stop</td><td>执行 pre-stop 段</td></tr><tr><td>Stopping</td><td>interim state set after pre-stop section processed</td></tr><tr><td>Killed</td><td>任务即将被停止</td></tr><tr><td>post-stop</td><td>执行 post-stop 段</td></tr></tbody></table><p>当job的状态即将发生变化的时候，init 进程会发出相应的event（event），如下图是job的状态机。其中只有四个状态Starting，Started，Stopping，Stopped会引起init进程发送相应的event，而其它的状态变化不会发出event。<br><img src="/2019/06/23/linux-init系统/" alt></p><h4 id="event">Event</h4><p>Event在upstart中以通知消息的形式具体存在。一旦某个event发生了，Upstart 就向整个系统发送一个消息。Event 可以分为三类: signal，methods 或者 hooks。</p><ul><li>Signals，Signal event是非阻塞的，异步的。发送一个信号之后控制权立即返回。</li><li>Methods，Methods event是阻塞的，同步的。</li><li>Hooks，Hooks event是阻塞的，同步的。它介于 Signals 和 Methods 之间，调用发出 Hooks event的进程必须等待event完成才可以得到控制权，但不检查event是否成功。</li></ul><h5 id="event示例">Event示例</h5><ol><li>系统上电启动，init 进程会发送&quot;start&quot;event</li><li>根文件系统可写时，相应 job 会发送文件系统就绪的event</li><li>一个块设备被发现并初始化完成，发送相应的event</li><li>某个文件系统被挂载，发送相应的event</li><li>类似 atd 和 cron，可以在某个时间点，或者周期的时间点发送event</li><li>另外一个 job 开始或结束时，发送相应的event</li><li>一个磁盘文件被修改时，可以发出相应的event</li><li>一个网络设备被发现时，可以发出相应的event</li><li>缺省路由被添加或删除时，可以发出相应的event</li></ol><p>不同的 Linux 发行版对 upstart 有不同的定制和实现，实现和支持的event也有所不同，可以用man 7 upstart-events来查看event列表。</p><h4 id="job-和-event-的相互协作">Job 和 Event 的相互协作</h4><p>Upstart是由event触发job运行的一个系统，每一个程序的运行都由其依赖的event发生而触发的。系统初始化时，init进程开始运行，init进程自身会发出不同的event，这些event会触发一些job运行。每个job运行过程中会释放不同的event，这些event又将触发新的job运行。如此反复，直到整个系统正常运行起来。</p><h4 id="job配置文件">job配置文件</h4><p>每一个Job都是由一个job配置文件（Job Configuration File）定义的。job配置文件存放在/etc/init下面，是以.conf 作为文件后缀的文件。</p><h5 id="示例">示例</h5><p>~$:cat /etc/init/anacron.conf</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># anacron - anac(h)ronistic cron</span><br><span class="line">#</span><br><span class="line"># anacron executes commands at specific periods, but does not assume that</span><br><span class="line"># the machine is running continuously</span><br><span class="line"></span><br><span class="line">description&quot;anac(h)ronistic cron&quot;</span><br><span class="line"></span><br><span class="line">start on runlevel [2345]</span><br><span class="line">stop on runlevel [!2345]</span><br><span class="line"></span><br><span class="line">expect fork</span><br><span class="line">normal exit 0</span><br><span class="line"></span><br><span class="line">exec anacron -s</span><br></pre></td></tr></table></figure><h5 id="常见的sec">常见的sec</h5><p><strong>“expect” Stanza</strong><br>为了启动，停止，重启和查询某个系统service。Upstart 需要跟踪该service所对应的进程。部分service为了将自己变成daemon，会采用fork调用， UpStart必须采用fork出来的进程号作为service的 PID。但是，UpStart 本身无法判断service进程是否fork了，所以需要指定expect告诉 UpStart 进程是否fork了。&quot;expect fork&quot;表示进程只会 fork 一次；&quot;expect daemonize&quot;表示进程会 fork 两次。</p><p><strong>“exec” Stanza 和&quot;script&quot; Stanza</strong><br>&quot;exec&quot;关键字配置job需要运行的命令，&quot;script&quot;关键字定义需要运行的脚本。</p><h5 id="示例-v2">示例</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># mountall.conf</span><br><span class="line">description “Mount filesystems on boot”</span><br><span class="line">start on startup</span><br><span class="line">stop on starting rcS</span><br><span class="line">...</span><br><span class="line">script</span><br><span class="line">  . /etc/default/rcS</span><br><span class="line">  [ -f /forcefsck ] &amp;&amp; force_fsck=”--force-fsck”</span><br><span class="line">  [ “$FSCKFIX”=”yes” ] &amp;&amp; fsck_fix=”--fsck-fix”</span><br><span class="line">    </span><br><span class="line">  ...</span><br><span class="line">   </span><br><span class="line">  exec mountall –daemon $force_fsck $fsck_fix</span><br><span class="line">end script</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>该job在系统启动时运行，负责挂载所有的文件系统。该job需要执行复杂的脚本，由&quot;script&quot;关键字定义；在脚本中，使用了 exec 来执行 mountall 命令。</p><p><strong>“start on” Stanza 和&quot;stop on&quot; Stanza</strong><br>&quot;start on&quot;定义了触发job的所有event。&quot;start on&quot;的语法很简单，如下所示：</p><p>start on EVENT [[KEY=]VALUE]… [and|or…]<br>EVENT 表示event的名字，可以在 start on 中指定多个event，表示该job的开始需要依赖多个event发生。多个event之间可以用 and 或者 or 组合，&quot;表示全部都必须发生&quot;或者&quot;其中之一发生即可&quot;等不同的依赖条件。除了event发生之外，job的启动还可以依赖特定的条件，因此在 start on 的 EVENT 之后，可以用 KEY=VALUE 来表示额外的条件，一般是某个环境变量(KEY)和特定值(VALUE)进行比较。如果只有一个变量，或者变量的顺序已知，则 KEY 可以省略。<br>&quot;stop on&quot;定义job在什么情况下需要停止。</p><h5 id="示例-v3">示例</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#dbus.conf</span><br><span class="line">description     “D-Bus system message bus”</span><br><span class="line"> </span><br><span class="line">start on local-filesystems</span><br><span class="line">stop on deconfiguring-networking</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>D-Bus 是一个系统消息service，上面的配置文件表明当系统发出 local-filesystems event时启动 D-Bus；当系统发出 deconfiguring-networking event时，停止 D-Bus service。</p><h3 id="session初始化">Session初始化</h3><p>Session 就是一个用户会话，即用户从远程或者本地登入系统开始job，直到用户退出，整个过程构成一个会话。用户往往会为自己的会话做一个定制，如添加特定的命令别名等等。这些job都属于对特定会话的初始化操作，被称为 Session Init。在字符模式下，会话初始化相对简单。用户登录后只能启动一个 Shell，通过 shell 命令使用系统。各种 shell 程序都支持一个自动运行的启动脚本，比如~/.bashrc。用户在这些脚本中加入需要运行的定制化命令。在图形界面下，用户登录后看到的并不是一个 shell 提示符，而是一个桌面。一个完整的桌面环境包括 window manager，panel以及其它一些定义在/usr/share/gnome-session/sessions/下面的基本组件，此外还有一些辅助的应用程序，，比如 system monitors，panel applets，NetworkManager，Bluetooth，printers 等。当用户登录之后，这些组件都需要被初始化。目前启动各种图形组件和应用的job由 gnome-session 完成。过程如下：<br>以 Ubuntu 为例，当用户登录 Ubuntu 图形界面后，显示管理器(Display Manager)lightDM 启动 Xsession。Xsession 接着启动 gnome-session，gnome-session 负责其它的初始化job，然后就开始了一个 desktop session。如下所示，是传统的desktop session 启动过程：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">init</span><br><span class="line"> |- lightdm</span><br><span class="line"> |   |- Xorg</span><br><span class="line"> |   |- lightdm ---session-child</span><br><span class="line"> |        |- gnome-session --session=ubuntu</span><br><span class="line"> |             |- compiz</span><br><span class="line"> |             |- gwibber</span><br><span class="line"> |             |- nautilus</span><br><span class="line"> |             |- nm-applet</span><br><span class="line"> |             :</span><br><span class="line"> |             :</span><br><span class="line"> |</span><br><span class="line"> |- dbus-daemon --session</span><br><span class="line"> |</span><br><span class="line"> :</span><br><span class="line"> :</span><br></pre></td></tr></table></figure><p>但是事实上一些应用和组件并不需要在会话初始化过程中启动，而是需要在需要它们的时候才启动。比如 Network Manager，一天之内用户很少切换网络设备，所以大部分时间 Network Manager service仅仅是在浪费系统资源。UpStart的基于event的按需启动的模式就可以很好地解决这些问题。下面给出了采用UpStart之后的会话初始化过程：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">init</span><br><span class="line"> |- lightdm</span><br><span class="line"> |   |- Xorg</span><br><span class="line"> |   |- lightdm ---session-child</span><br><span class="line"> |        |- session-init # &lt;-- upstart running as normal user</span><br><span class="line"> |             |- dbus-daemon --session</span><br><span class="line"> |             |- gnome-session --session=ubuntu</span><br><span class="line"> |             |- compiz</span><br><span class="line"> |             |- gwibber</span><br><span class="line"> |             |- nautilus</span><br><span class="line"> |             |- nm-applet</span><br><span class="line"> |             :</span><br><span class="line"> |             :</span><br><span class="line"> :</span><br><span class="line"> :</span><br></pre></td></tr></table></figure><h3 id="upstart-v2">UpStart</h3><h4 id="upstart系统中的run-level">Upstart系统中的run level</h4><p>Upstart 的运作完全是基于job和event的。Job的状态变化和运行会引起event，进而触发其它job和event。而SysVInit是基于运行级别的，因为历史的原因，Linux 上的多数软件还是采用传统的 SysVInit 脚本启动方式，所以UpStart还是必须模拟老的SysVInit的run level，以便和多数现有软件兼容。</p><h4 id="系统启动过程">系统启动过程</h4><p>下图描述了 UpStart 的启动过程。<br><img src="/2019/06/23/linux-init系统/startup.png" alt="upstart startup"><br>系统上电后运行GRUB载入内核。内核执行硬件初始化和内核自身初始化。在内核初始化的最后，内核将启动pid为1的Upstart init 进程。Upstart 进程执行一些自身的初始化job后，立即发出&quot;startup&quot; event。上图中用红色方框加红色箭头表示event，可以在左上方看到&quot;startup&quot;event。<br>所有依赖于&quot;startup&quot;event的job被触发，其中最重要的是mountall。mountall jog负责挂载系统中需要使用的文件系统，完成相应job后，mountall任务会发出以下event：local-filesystem，virtual-filesystem，all-swaps，<br>其中 virtual-filesystem event触发 udev 任务开始job。任务 udev 触发 upstart-udev-bridge 的job。Upstart-udev-bridge 会发出 net-device-up IFACE=lo event，表示本地回环 IP 网络已经准备就绪。同时，任务 mountall 继续执行，最终会发出 filesystem event。此时，任务 rc-sysinit 会被触发，因为 rc-sysinit 的 start on条件如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start on filesystem and net-device-up IFACE=lo</span><br></pre></td></tr></table></figure><p>任务rc-sysinit调用telinit。Telinit任务会发出 runlevel event，触发执行/etc/init/rc.conf。rc.conf 执行/etc/rc$.d/目录下的所有脚本，和 SysVInit 非常类似。</p><h3 id="upstart注意的事项">Upstart注意的事项</h3><ol><li>fork次数，都通过fork 两次的技巧将自己变成后台service程序需要指定expect stanza。</li><li>fork后即可用，后台程序在完成第二次fork的时候，必须保证service已经可用。因为UpStart通过派生计数来决定service是否处于就绪状态。</li><li>遵守 SIGHUP 的要求，UpStart 会给daemon发送SIGHUP信号，此时，UpStart 希望该deamon做以下这些响应job：</li></ol><ul><li>完成所有必要的重新初始化job，比如重新读取配置文件。这是因为 UpStart 的命令&quot;initctl reload&quot;被设计为可以让service在不重启的情况下更新配置。</li><li>deamon必须继续使用现有的 PID，即收到 SIGHUP 时不能调用 fork。如果service必须在这里调用 fork，则等同于派生两次，参考上面的规则一的处理。这个规则保证了 UpStart 可以继续使用 PID 管理本service。</li></ul><ol start="4"><li>收到 SIGTEM 即 shutdown。</li></ol><ul><li>当收到 SIGTERM 信号后，UpStart 希望deamon进程立即干净地退出，释放所有资源。如果一个进程在收到 SIGTERM 信号后不退出，Upstart 将对其发送 SIGKILL 信号。</li></ul><ol start="5"><li>initctl list 来查看所有job的概况，用 initctl stop 停止一个正在运行的job；用 initctl start 开始一个job；还可以用 initctl status 来查看一个job的状态；initctl restart 重启一个job；initctl reload 可以让一个正在运行的service重新载入配置文件。这些命令和传统的 service 命令十分相似。<br>~$:initctl list</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">alsa-mixer-save stop/waiting</span><br><span class="line">avahi-daemon start/running, process 690</span><br><span class="line">mountall-net stop/waiting</span><br><span class="line">rc stop/waiting</span><br><span class="line">rsyslog start/running, process 482</span><br><span class="line">screen-cleanup stop/waiting</span><br><span class="line">tty4 start/running, process 859</span><br><span class="line">udev start/running, process 334</span><br><span class="line">upstart-udev-bridge start/running, process 304</span><br><span class="line">ureadahead-other stop/waiting</span><br></pre></td></tr></table></figure><p>第一列是job名，比如 rsyslog。第二列是job的目标；第三列是job的状态。<br>UpStart 还提供了一些快捷命令来简化 initctl。比如 reload，restart，start，stop 等等。启动一个service可以简单地调用<br>~$:start job<br>这和执行 initctl start job是一样的效果。<br>一些命令是为了兼容其它系统(主要是 SysVInit)，比如显示 runlevel 用/sbin/runlevel 命令：<br>~$:runlevel<br>&gt; N 2</p><p>在 Upstart 系统中，需要修改/etc/init/rc-sysinti.conf 中的 DEFAULT_RUNLEVEL 这个参数，以便修改默认启动运行级别。这一点和 SysVInit 的习惯有所不同，大家需要格外留意。</p><h2 id="systemd">SystemD</h2><p>SystemD和SysVInit 和 LSB init scripts 兼容，以及更快的启动速度，SystemD 提供了比 UpStart 更激进的并行启动能力。<br>为了减少系统启动时间，SystemD 的目标是尽可能启动更少的进程和尽可能将更多进程并行启动，同样地，UpStart 也试图实现这两个目标。UpStart 采用event驱动机制，当service需要的时候才通过event触发启动；不相干的service可以并行启动。下面的图形演示了 UpStart 相对于 SysVInit 在并发启动这个方面的改进：<br><img src="/2019/06/23/linux-init系统/SystemD.png" alt="systemd"><br>假设有 7 个不同的启动项目， 比如 JobA、Job B 等等。在 SysVInit 中，每一个启动项目都由一个独立的脚本负责，它们由 SysVInit 顺序地，串行地调用。因此总的启动时间为 T1+T2+T3+T4+T5+T6+T7。其中一些任务有依赖关系，比如 A,B,C,D。而Job E和F却和A,B,C,D无关。这种情况下，UpStart 能够并发地运行任务{E，F，(A,B,C,D)}，使得总的启动时间减少为 T1+T2+T3。但是在 UpStart 中，有依赖关系的service还是必须先后启动。<br>让我们例举一些例子， Avahi service需要 D-Bus 提供的功能，因此 Avahi 的启动依赖于 D-Bus，UpStart 中，Avahi 必须等到 D-Bus 启动就绪之后才开始启动。类似的，livirtd 和 X11 都需要 HAL service先启动，而所有这些service都需要 syslog service记录日志，因此它们都必须等待 syslog service先启动起来。然而 httpd 和他们都没有关系，因此 httpd 可以和 Avahi 等service并发启动。<br>SystemD 能够更进一步提高并发性，即便对于那些 UpStart 认为存在相互依赖而必须串行的service，比如 Avahi 和 D-Bus 也可以并发启动。从而实现如下图所示的并发启动过程：<br><img src="/2019/06/23/linux-init系统/SystemD_para.jpg" alt="systemD_para"><br>所有的任务都同时并发执行，总的启动时间被进一步降低为 T1。</p><ul><li>SystemD 提供按需启动能力。</li><li>SystemD采用Cgroup 特性跟踪和管理进程的生命周期。</li></ul><p><img src="/2019/06/23/linux-init系统/find_pid.jpg" alt="find_pid"><br>如果 UpStart 找错了，将 p1作为service进程的 Pid，那么停止service的时候，UpStart 会试图杀死 p1进程，而真正的 p1进程则继续执行。换句话说该service就失去控制了。还有更加特殊的情况。比如，一个 CGI 程序会fork两次，从而脱离了和 Apache 的父子关系。当 Apache 进程被停止后，该 CGI 程序还在继续运行。而我们希望service停止后，所有由它所启动的相关进程也被停止。为了处理这类问题，UpStart 通过 strace 来跟踪 fork、exit 等系统调用，但是这种方法很笨拙，且缺乏可扩展性。SystemD 则利用了 Linux 内核的特性即 CGroup 来完成跟踪的任务。当停止service时，通过查询 CGroup，SystemD 可以确保找到所有的相关进程，从而干净地停止service。CGroup 已经出现了很久，它主要用来实现系统资源配额管理。CGroup 提供了类似文件系统的接口，使用方便。当进程创建子进程时，子进程会继承父进程的 CGroup。因此无论service如何启动新的子进程，所有的这些相关进程都会属于同一个 CGroup，SystemD 只需要简单地遍历指定的 CGroup 即可正确地找到所有的相关进程，将它们一一停止即可。</p><ul><li>启动挂载点和自动挂载的管理。</li><li>实现事务性依赖关系管理。</li><li>能够对系统进行快照和恢复。</li><li>日志service。SystemD Journal 的优点如下：</li></ul><ol><li>简单性：代码少，依赖少，抽象开销最小。</li><li>零维护：日志是除错和监控系统的核心功能，因此它自己不能再产生问题。举例说，自动管理磁盘空间，避免由于日志的不断产生而将磁盘空间耗尽。</li><li>移植性：日志 文件应该在所有类型的 Linux 系统上可用，无论它使用的何种 CPU 或者字节序。</li><li>性能：添加和浏览 日志 非常快。</li><li>最小资源占用：日志 数据文件需要较小。</li><li>统一化：各种不同的日志存储技术应该统一起来，将所有的可记录event保存在同一个数据存储中。</li><li>扩展性：日志的适用范围很广，从嵌入式设备到超级计算机集群都可以满足需求。</li><li>安全性：日志 文件是可以验证的，让无法检测的修改不再可能。</li></ol><h3 id="unit">Unit</h3><p>一个service可以认为是一个unit；一个挂载点是一个unit；一个交换分区的配置是一个unit；等等，总共有12种unit</p><ul><li>service ：系统服务</li><li>socket ：进程间通信的socket</li><li>device ：硬件设备</li><li>mount ：文件系统的挂在</li><li>automount ：自动挂载点</li><li>swap：swap文件</li><li>target ：unit分组统一的控制。比如图形化模式的所有unit组合为一个target。 (例如：multi-user.target 相当于在传统使用 SysV 的系统中运行run level 5)</li><li>timer：定时器</li><li>snapshot ：快照，它保存了系统当前的运行状态。</li></ul><h3 id="依赖关系">依赖关系</h3><p>虽然 SystemD 将大量的启动job解除了依赖，使得它们可以并发启动。但还是存在有些任务，它们之间存在天生的依赖，不能用&quot;套接字激活&quot;(socket activation)、D-Bus activation 和 autofs 三大方法来解除依赖（三大方法详情见后续描述）。比如：挂载必须等待挂载点在文件系统中被创建；挂载也必须等待相应的物理设备就绪。为了解决这类依赖问题，SystemD 的配置单元之间可以彼此定义依赖关系。<br>SystemD 用配置单元定义文件中的关键字来描述配置单元之间的依赖关系。比如：unit A 依赖 unit B，可以在 unit B 的定义中用&quot;require A&quot;来表示。这样 SystemD 就会保证先启动 A 再启动 B。</p><h3 id="systemd-事务">SystemD 事务</h3><p>SystemD 能保证事务完整性。SystemD 的事务概念和数据库中的有所不同，主要是为了保证多个依赖的配置单元之间没有环形引用。比如 unit A、B、C，假如它们的依赖关系为:<br><img src="/2019/06/23/linux-init系统/recurrent_dependency.jpg" alt="dependency"><br>存在循环依赖，那么 SystemD 将无法启动任意一个service。此时 SystemD 将会尝试解决这个问题，因为配置单元之间的依赖关系有两种：required 是强依赖；want 则是弱依赖，SystemD 将去掉 wants 关键字指定的依赖看看是否能打破循环。如果无法修复，SystemD 会报错。<br>SystemD 能够自动检测和修复这类配置错误，极大地减轻了管理员的排错负担。</p><h3 id="target-和运行级别">Target 和运行级别</h3><p>SystemD 用目标（target）替代了运行级别的概念，提供了更大的灵活性，如您可以继承一个已有的目标，并添加其它service，来创建自己的目标。下表列举了 SystemD 下的目标和常见 runlevel 的对应关系：<br>SysVInit 运行级别|SystemD 目标|备注<br>0|runlevel0.target, poweroff.target|关闭系统。<br>1, s, single|runlevel1.target, rescue.target|单用户模式。<br>2, 4|runlevel2.target, runlevel4.target, multi-user.target|用户定义/域特定运行级别。默认等同于 3。<br>3|runlevel3.target, multi-user.target|多用户，非图形化。用户可以通过多个控制台或网络登录。<br>5|runlevel5.target, graphical.target|多用户，图形化。通常为所有运行级别 3 的service外加图形化登录。<br>6|runlevel6.target, reboot.target|重启<br>emergency|emergency.target|紧急Shell</p><h3 id="systemd-的并发启动原理">SystemD 的并发启动原理</h3><p>并发启动原理之一：解决 socket 依赖<br>并发启动原理之二：解决 D-Bus 依赖<br>并发启动原理之三：解决文件系统依赖</p><h3 id="systemd-的使用">SystemD 的使用</h3><ul><li>后台service进程代码不需要执行两次fork来实现后台deamon，只需要实现service本身的主循环即可。</li><li>不要调用 setsid()，交给 SystemD 处理</li><li>不再需要维护 pid 文件。</li><li>SystemD 提供了日志功能，service进程只需要输出到 stderr 即可，无需使用 syslog。</li><li>处理信号 SIGTERM，这个信号的唯一正确作用就是停止当前service，不要做其他的事情。</li><li>SIGHUP 信号的作用是重启service。</li><li>需要套接字的service，不要自己创建套接字，让 SystemD 传入套接字。</li><li>使用 sd_notify()函数通知 SystemD service自己的状态改变。一般地，当service初始化结束，进入service就绪状态时，可以调用它。</li><li>Unit 文件的编写，详细可点击查看<a href></a>。</li></ul><h3 id="systemctl-工具">systemctl 工具</h3><h3 id="systemctl-工具-v2">systemctl 工具</h3><p>~$:systemctl list-units     # 列出正在运行的 Unit<br>~$:systemctl list-units --all   # 列出所有Unit，包括没有找到配置文件的或者启动失败的<br>~$:systemctl list-units --all --state=inactive      # 列出所有没有运行的 Unit<br>~$:systemctl list-units --failed    # 列出所有加载失败的 Unit<br>~$:systemctl list-units --type=service  # 列出所有正在运行的、类型为 service 的 Unit<br>~$:systemctl list-unit-files    # 列出所有配置文件<br>~$:systemctl list-unit-files --type=service    # 列出指定类型的配置文件<br>~$:systemctl start foo.service# 用来启动一个service (并不会重启现有的)<br>~$:systemctl stop foo.service# 用来停止一个service (并不会重启现有的)。<br>~$:systemctl restart foo.service# 用来停止并启动一个service。<br>~$:systemctl reload foo.service    # 当支持时，重新装载配置文件而不中断等待操作。<br>~$:systemctl condrestart foo.service    # 如果service正在运行那么重启它。<br>~$:systemctl status foo.service    # 汇报service是否正在运行。<br>~$:systemctl list-unit-files --type=service     # 用来列出可以启动或停止的service列表。<br>~$:systemctl enable foo.service     # 在下次启动时或满足其他触发条件时设置service为启用<br>~$:systemctl disable foo.service# 在下次启动时或满足其他触发条件时设置service为禁用<br>~$:systemctl is-enabled foo.service     # 用来检查一个service在当前环境下被配置为启用还是禁用。<br>~$:systemctl list-unit-files --type=service     # 输出在各个运行级别下service的启用和禁用情况<br>~$:systemctl daemon-reload    # 创建新service文件或者变更设置时使用。<br>~$:systemctl isolate multi-user.target (OR systemctl isolate runlevel3.target OR telinit 3)    # 改变至多用户运行级别<br>~$:ls /etc/SystemD/system/*.wants/foo.service      # 用来列出该service在哪些运行级别下启用和禁用。<br>~$:systemctl reboot     # 重启机器<br>~$:systemctl poweroff   # 关机<br>~$:systemctl suspend    # 待机<br>~$:systemctl hibernate  # 休眠<br>~$:systemctl hybrid-sleep   # 混合休眠模式（同时休眠到硬盘并待机）</p><p>一般只有管理员才可以关机。正常情况下系统不应该允许 SSH 远程登录的用户执行关机命令。否则其他用户正在job，一个用户把系统关了就不好了。使用logind解决这个问题。logind 不是 pid-1 的 init 进程。它的作用和 UpStart 的 session init 类似，但功能要丰富很多，它能够管理几乎所有用户会话(session)相关的事情。logind 不仅是 ConsoleKit 的替代，它可以：</p><ul><li>维护，跟踪会话和用户登录情况。</li><li>Logind 也负责统计用户会话是否长时间没有操作，可以执行休眠/关机等相应操作。</li><li>为用户会话的所有进程创建 CGroup。这不仅方便统计所有用户会话的相关进程，也可以实现会话级别的系统资源控制。</li><li>负责电源管理的组合键处理，比如用户按下电源键，将系统切换至睡眠状态。</li><li>多席位(multi-seat) 管理。</li></ul><h2 id="sysvinit-upstart-systemd比较">SysVInit，Upstart，SystemD比较</h2><p>SysVInit比较简单。Service开发人员只需要编写启动和停止脚本，将 service 添加/删除到某个 runlevel 时，只需要执行一些创建/删除软连接文件的基本操作。<br>其次，SysVInit 的另一个优点是确定的执行顺序：脚本严格按照启动数字的大小顺序执行，一个执行完毕再执行下一个，这非常有益于错误排查。UpStart 和 SystemD 支持并发启动，导致没有人可以确定地了解具体的启动顺序，排错不易。但是串行地执行脚本导致 SysVInit 运行效率较慢。<br>可以看到，UpStart 的设计比 SysVInit 更加先进。<br>SystemD 的最大特点有两个：并发启动能力强，极大地提高了系统启动速度；用 CGroup 统计跟踪子进程，干净可靠。<br>此外，和其前任不同的地方在于，SystemD 已经不仅仅是一个初始化系统了。SystemD 出色地替代了 SysVInit 的所有功能，但它并未就此自满。因为 init 进程是系统所有进程的父进程这样的特殊性，SystemD 非常适合提供曾经由其他service提供的功能，比如定时任务 (以前由 crond 完成)；会话管理 (以前由 ConsoleKit/PolKit 等管理) 。</p><h2 id="参考文献">参考文献</h2><p>1.<a href="https://www.ibm.com/developerworks/cn/linux/1407_liuming_init1/index.html" target="_blank" rel="noopener">https://www.ibm.com/developerworks/cn/linux/1407_liuming_init1/index.html</a><br>2.<a href="https://www.ibm.com/developerworks/cn/linux/1407_liuming_init2/index.html?ca=drs-" target="_blank" rel="noopener">https://www.ibm.com/developerworks/cn/linux/1407_liuming_init2/index.html?ca=drs-</a><br>3.<a href="https://www.ibm.com/developerworks/cn/linux/1407_liuming_init3/index.html?ca=drs-" target="_blank" rel="noopener">https://www.ibm.com/developerworks/cn/linux/1407_liuming_init3/index.html?ca=drs-</a><br>4.<a href="http://www.ruanyifeng.com/blog/2016/03/systemd-tutorial-commands.html" target="_blank" rel="noopener">http://www.ruanyifeng.com/blog/2016/03/systemd-tutorial-commands.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;linux-init系统&quot;&gt;Linux init系统&lt;/h2&gt;
&lt;p&gt;这里会介绍下面三种init系统&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;SysVInit(initd)&lt;/li&gt;
&lt;li&gt;Upstart&lt;/li&gt;
&lt;li&gt;SystemD&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Linu
      
    
    </summary>
    
      <category term="linux" scheme="http://mxxhcm.github.io/categories/linux/"/>
    
    
      <category term="linux" scheme="http://mxxhcm.github.io/tags/linux/"/>
    
      <category term="init" scheme="http://mxxhcm.github.io/tags/init/"/>
    
      <category term="Upstart" scheme="http://mxxhcm.github.io/tags/Upstart/"/>
    
      <category term="SystemD" scheme="http://mxxhcm.github.io/tags/SystemD/"/>
    
      <category term="SysVInit" scheme="http://mxxhcm.github.io/tags/SysVInit/"/>
    
  </entry>
  
  <entry>
    <title>linux screen capture</title>
    <link href="http://mxxhcm.github.io/2019/06/22/linux-screen-capture/"/>
    <id>http://mxxhcm.github.io/2019/06/22/linux-screen-capture/</id>
    <published>2019-06-22T13:48:27.000Z</published>
    <updated>2019-06-22T15:04:49.104Z</updated>
    
    <content type="html"><![CDATA[<h2 id="常用的截屏工具">常用的截屏工具</h2><ul><li>screenshot</li><li>scrot</li></ul><p>screentshot是系统自带的截屏工具，可以部分截取，直接按win键搜索可运行。<br>scrot需要安装，这个工具需要从终端运行，截图默认保存在当前目录。</p><h2 id="screenshot">screenshot</h2><h2 id="scrot">scrot</h2><p>scrot [options] [filename]</p><h3 id="参数介绍">参数介绍</h3><p>scrot [-vusbcm]<br>不加参数，直接截取整个screen<br>-v  查看scort版本<br>-u  截取当前鼠标焦点所在window<br>-s  执行命令之后，点击鼠标截取相对应的window。<br>-b  包含当前window的边界<br>–delay [NUM]   延长NUM秒之后截图<br>-c  设置delay的时候显示数字延迟<br>–quliaty [NUM] 指定screenshot image的质量，从$1-100$<br>–thumb [NUM]   缩略图，是原始大小的百分之多少。。。</p><h2 id="参考文献">参考文献</h2><p>1.<a href="https://www.howtoforge.com/tutorial/how-to-take-screenshots-in-linux-with-scrot/" target="_blank" rel="noopener">https://www.howtoforge.com/tutorial/how-to-take-screenshots-in-linux-with-scrot/</a><br>2.<a href="http://awesomescreenshot.com/" target="_blank" rel="noopener">http://awesomescreenshot.com/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;常用的截屏工具&quot;&gt;常用的截屏工具&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;screenshot&lt;/li&gt;
&lt;li&gt;scrot&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;screentshot是系统自带的截屏工具，可以部分截取，直接按win键搜索可运行。&lt;br&gt;
scrot需要安装，这个工具
      
    
    </summary>
    
      <category term="linux" scheme="http://mxxhcm.github.io/categories/linux/"/>
    
    
      <category term="linux" scheme="http://mxxhcm.github.io/tags/linux/"/>
    
      <category term="scrot" scheme="http://mxxhcm.github.io/tags/scrot/"/>
    
      <category term="screenshot" scheme="http://mxxhcm.github.io/tags/screenshot/"/>
    
  </entry>
  
  <entry>
    <title>python 指定长度list初始化</title>
    <link href="http://mxxhcm.github.io/2019/06/18/python-%E6%8C%87%E5%AE%9A%E9%95%BF%E5%BA%A6list%E5%88%9D%E5%A7%8B%E5%8C%96/"/>
    <id>http://mxxhcm.github.io/2019/06/18/python-指定长度list初始化/</id>
    <published>2019-06-18T07:26:32.000Z</published>
    <updated>2019-06-18T07:35:43.725Z</updated>
    
    <content type="html"><![CDATA[<h2 id="指定长度list初始化">指定长度list初始化</h2><p>想要找到初始化指定长度list最快的方法。<br>方法一：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">length = <span class="number">10</span></span><br><span class="line">array = [[]] * length</span><br></pre></td></tr></table></figure><p>方法二</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">length = <span class="number">10</span></span><br><span class="line">array = [[] <span class="keyword">for</span> _ <span class="keyword">in</span> range(length)]</span><br></pre></td></tr></table></figure><p>事实上，只有第二种方法是对的。第一种方法中，arrary中的10个[]都指向了同一个对象。。</p><h2 id="示例">示例</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">length = <span class="number">10</span></span><br><span class="line">v1 = [[]]*length</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(length):</span><br><span class="line">    v1[i].append(i)</span><br><span class="line">print(v1)</span><br><span class="line"><span class="comment"># [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]]</span></span><br><span class="line"></span><br><span class="line">v2 = [[] <span class="keyword">for</span> _ <span class="keyword">in</span> range(length)]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(length):</span><br><span class="line">    v2[i].append(i)</span><br><span class="line"></span><br><span class="line">print(v2)</span><br><span class="line"><span class="comment"># [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9]]</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;指定长度list初始化&quot;&gt;指定长度list初始化&lt;/h2&gt;
&lt;p&gt;想要找到初始化指定长度list最快的方法。&lt;br&gt;
方法一：&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;
      
    
    </summary>
    
      <category term="python" scheme="http://mxxhcm.github.io/categories/python/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="list" scheme="http://mxxhcm.github.io/tags/list/"/>
    
  </entry>
  
  <entry>
    <title>linux pipe and pipe command</title>
    <link href="http://mxxhcm.github.io/2019/06/17/linux-pipe-and-pipe-command/"/>
    <id>http://mxxhcm.github.io/2019/06/17/linux-pipe-and-pipe-command/</id>
    <published>2019-06-17T11:59:30.000Z</published>
    <updated>2019-06-26T09:09:55.226Z</updated>
    
    <content type="html"><![CDATA[<h2 id="pipe">PIPE</h2><p>管道命令仅会处理stdout并不会处理stderrout，管道命令必须要能接受前一个命令传回来的数据成为stdinput</p><h2 id="head-tail">head tail</h2><h2 id="cut">cut</h2><h3 id="参数说明">参数说明</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cut -d &apos;分隔字符&apos; -f (fields)fields为数字</span><br><span class="line">cut -c 字符范围</span><br></pre></td></tr></table></figure><h3 id="示例">示例</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cut -d &apos;:&apos; -f 2,3</span><br><span class="line">echo $PATH | cut -d &apos;:&apos; -f 2,4</span><br><span class="line">cut -c 20-30</span><br><span class="line">export | cut -c 12-</span><br><span class="line">不过cut 对于多个空格当做分隔字符的处理做的不够好</span><br></pre></td></tr></table></figure><h2 id="grep">grep</h2><h3 id="参数说明-v2">参数说明</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">grep [-aincv] [--color=auto] &apos;关键字&apos; filename</span><br><span class="line">-a 将binary文件以text的方式查找数据</span><br><span class="line">-i 忽略大小写</span><br><span class="line">-c 计算查找到的字符串的个数</span><br><span class="line">-n 顺便输出行号</span><br><span class="line">-v 反向选择</span><br><span class="line">grep -n  &apos;^$&apos; regular_express</span><br></pre></td></tr></table></figure><h3 id="示例-v2">示例</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">grep &apos;^the&apos; file</span><br><span class="line">grep &apos;[^[:lower:]]&apos; file</span><br><span class="line">grep &apos;\.$&apos; file</span><br><span class="line">grep &apos;^[^a-zA-Z]&apos; file</span><br><span class="line">grep &apos;go\&#123;2,3\&#125;g&apos; file</span><br><span class="line">对比</span><br><span class="line">ls -l /etc/a*</span><br><span class="line">grep -n &apos;^a.*/&apos;</span><br></pre></td></tr></table></figure><h2 id="dmesg">dmesg</h2><p>dmesg 查看内核信息<br>dmesg | grep -n A3 B2 ‘eth’<br>A --after  B --before</p><h2 id="last">last</h2><p>last | grep ‘mxx’ | cut -d ‘’ -f 1</p><h2 id="sort-wc-uniq">sort,wc,uniq</h2><h3 id="参数说明-v3">参数说明</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">sort 排序</span><br><span class="line">[-fbMnrutk] [file]</span><br><span class="line">-f 不区分大小写</span><br><span class="line">-u uniq</span><br><span class="line">-t 分隔符</span><br><span class="line">-k 以第几个字段进行排序</span><br><span class="line">-n 以数字进行排序(默认是以字母)</span><br><span class="line">-m 反向排序</span><br></pre></td></tr></table></figure><h3 id="示例-v3">示例</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sort -t &apos;:&apos; -k 3 -n /etc/passwd</span><br><span class="line">cat /etc/passwd | sort -t &apos;:&apos; -k 3 -n</span><br></pre></td></tr></table></figure><h2 id="uniq">uniq</h2><p>uniq 去重<br>[-il] [file]<br>-i 不区分大小写<br>-c 进行计数</p><pre><code>last | cut -d ' ' -f 1 | sort | uniq首先需要排序　才能去重</code></pre><p>last | cut -d ’ ’ -f 1 | sort | uniq -c</p><h2 id="tee">tee</h2><p>tee双重重定向将一份数据可以同时传到文件内以及屏幕中<br>last | tee last.list | sort</p><h2 id="tr">tr</h2><p>tr 删除一段文字或者对文字内容进行替换(如删除dos中的换行符^M)<br>[-ds]<br>-d 删除信息中的某个字串<br>-s 替换重复字符</p><pre><code>last | tr '[a-z]'  '[A-Z]'echo $PATH -d ':/'cat /root/passwd | tr -d '\r' &gt; passwd.linux</code></pre><h2 id="col">col</h2><p>col 简单处理<br>[-xb]<br>-x 将tab键换成空格键</p><pre><code>cat  manpath.config | col -x | cat -A | more</code></pre><h2 id="join">join</h2><p>join 将两个文件中具有相同数据的一行相加<br>join [-ti12] file1 file2<br>-i 大小写忽视<br>join -t ‘:’ passwd shadow<br>join -t ‘:’ -1 4 passwd -2 3 group</p><h2 id="paste">paste</h2><p>paste直接将两行粘在一起，默认并以tab键分开<br>-d后面可以加分隔字符默认以tab分隔<br>-表示来自standard input的数据的意思<br>paste shadow passwd<br>cat shadow | paste passwd - | head -n 3</p><h2 id="expand">expand</h2><p>expand将tab键换成空格默认是8个空格<br>-t 参数可以自行设定空格数<br>nl file | expand -t 6 - | cat -A</p><h2 id="split">split</h2><p>split [-bl] file PREFIX<br>-b后面加文件欲切割成的文件大小<br>-l以行数来切割<br>split -b 1M /etc/termcap termcap<br>ls -l termcap*</p><pre><code>cat termcap* &gt;&gt; termcapbackls -l / | spilt -l 10 -lsrootwc -l lsroot</code></pre><h2 id="xargs">xargs</h2><p>xargs产生某个命令的参数<br>[-pne0]<br>-p 执行每个命令询问用户<br>-e 是EOF的意思，后面可接一个字符，当xargs遇到这个字符，便会停止操作<br>-n 后面接次数，每次command命令执行时，要使用几个参数<br>-用来代替stdout以及stdin<br>tar -cvf - /home | tar xvf -</p><h2 id="sed-工具">sed 工具</h2><p>[-in]<br>-i直接修改文件内容<br>-n静默<br>-e 直接在shell下编辑<br>-c replace<br>-a append<br>-p print</p><p>nl file | sed '2,3d’<br>nl file | sed '$a add a test’<br>nl file | sed -n '5,7p’<br>nl file | sed '2,5c jkadfk<br>&gt;fdasf<br>&gt;asfddf '<br>nl file | sed ‘s/s_place/s_replace/g’<br>nl file | sed ‘/^$/d’</p><h2 id="egrep-扩展正则表达式">egrep 扩展正则表达式</h2><p>egrep -n ‘<sup>$|</sup>#’ file<br>egrep -n ‘go?d’ file　0个或者一个?之前的字符<br>egrep -n ‘go+d’ file　一个及以上+之前的字符<br>grep -n 'go<em>d’ file　0个或者0个以上</em>之前的字符</p><h2 id="printf-格式化打印">printf 格式化打印</h2><p>printf ‘%s\t %s\t %s\t \n’ $(cat file)<br>printf ‘%10s %5i %5i \n’ $(cat file)</p><h2 id="awk">awk</h2><p>last | awk '{print $1 “\t” S3 “\t” $4 NF NR}'<br>cat /etc/passwd | awk ‘BEGIN {FS=&quot;:&quot;} $3 &lt; 10 {print $1 “\t” $3 }’</p><p>cat /etc/passwd | awk ‘NR==1{printf &quot;%10s %10s %10s %10s “,$1,$2,$3,“total”}<br>NR&gt;=2{total=$2+$3;<br>printf “%10s %10d %10d %## f”,$1,$2,$3,total”}’</p><h2 id="参考文献">参考文献</h2><ol><li>《鸟哥的LINUX私房菜》</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;pipe&quot;&gt;PIPE&lt;/h2&gt;
&lt;p&gt;管道命令仅会处理stdout并不会处理stderrout，管道命令必须要能接受前一个命令传回来的数据成为stdinput&lt;/p&gt;
&lt;h2 id=&quot;head-tail&quot;&gt;head tail&lt;/h2&gt;
&lt;h2 id=&quot;cut&quot;&gt;c
      
    
    </summary>
    
      <category term="linux" scheme="http://mxxhcm.github.io/categories/linux/"/>
    
    
      <category term="linux" scheme="http://mxxhcm.github.io/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>linux cp, scp vs rsync</title>
    <link href="http://mxxhcm.github.io/2019/06/16/linux-cp-scp-vs-rsync/"/>
    <id>http://mxxhcm.github.io/2019/06/16/linux-cp-scp-vs-rsync/</id>
    <published>2019-06-16T08:29:17.000Z</published>
    <updated>2019-08-24T11:26:25.269Z</updated>
    
    <content type="html"><![CDATA[<h2 id="cp">cp</h2><h3 id="参数介绍">参数介绍</h3><h3 id="示例">示例</h3><h2 id="scp">scp</h2><h3 id="参数介绍-v2">参数介绍</h3><h3 id="示例-v2">示例</h3><h2 id="rsync">rsync</h2><p>rsync相对于scp有以下优点：<br>1.支持断点续传<br>2.支持ssh<br>3.可分块传输<br>~$:rsync options source destination</p><h3 id="参数介绍-v3">参数介绍</h3><p>rsync [-zvra] source destination<br>-z  传输前进行压缩<br>-v  显示详细信息<br>-r  递归拷贝<br>-a  保留时间戳，owner,group<br>-e ssh  使用ssh<br>–partial    单个文件的断点续传<br>–progress  显示同步进度<br>-P等于–paritial和–progress一同使用<br>–include   只同步某些目录<br>–exclude   不同步某些目录</p><h3 id="示例-v3">示例</h3><p>~$:rsync -avc --exclude=**<strong>pycache</strong>  --exclude=**data --exclude=**tmp --exclude=**result_pictures experimental/ ~/<br>~$:rsync -rP source_dir target_dir</p><h2 id="参考文献">参考文献</h2><p>1.<a href="https://stackoverflow.com/questions/4585929/how-to-use-cp-command-to-exclude-a-specific-directory" target="_blank" rel="noopener">https://stackoverflow.com/questions/4585929/how-to-use-cp-command-to-exclude-a-specific-directory</a><br>2.<a href="https://www.cnblogs.com/bangerlee/archive/2013/04/07/3003243.html" target="_blank" rel="noopener">https://www.cnblogs.com/bangerlee/archive/2013/04/07/3003243.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;cp&quot;&gt;cp&lt;/h2&gt;
&lt;h3 id=&quot;参数介绍&quot;&gt;参数介绍&lt;/h3&gt;
&lt;h3 id=&quot;示例&quot;&gt;示例&lt;/h3&gt;
&lt;h2 id=&quot;scp&quot;&gt;scp&lt;/h2&gt;
&lt;h3 id=&quot;参数介绍-v2&quot;&gt;参数介绍&lt;/h3&gt;
&lt;h3 id=&quot;示例-v2&quot;&gt;示例&lt;/h3&gt;
&lt;h2 
      
    
    </summary>
    
      <category term="linux" scheme="http://mxxhcm.github.io/categories/linux/"/>
    
    
      <category term="linux" scheme="http://mxxhcm.github.io/tags/linux/"/>
    
      <category term="cp" scheme="http://mxxhcm.github.io/tags/cp/"/>
    
      <category term="scp" scheme="http://mxxhcm.github.io/tags/scp/"/>
    
      <category term="rsync" scheme="http://mxxhcm.github.io/tags/rsync/"/>
    
  </entry>
  
  <entry>
    <title>linux ssh tunnel内网穿透</title>
    <link href="http://mxxhcm.github.io/2019/06/10/linux-ssh-tunnel%E5%86%85%E7%BD%91%E7%A9%BF%E9%80%8F/"/>
    <id>http://mxxhcm.github.io/2019/06/10/linux-ssh-tunnel内网穿透/</id>
    <published>2019-06-10T13:33:16.000Z</published>
    <updated>2019-06-30T13:12:34.008Z</updated>
    
    <content type="html"><![CDATA[<h2 id="ssh-命令介绍">ssh 命令介绍</h2><p>ssh是一种安全传输协议，此外还有tunnel转发功能，可以用来内网渗透。</p><h3 id="参数介绍">参数介绍</h3><p>-L port:host:hostport，访问本机的port端口就相当于访问host的hostport端口。<br>将本机的某个端口转发到远端指定机器的指定端口。本机上分了一个socket监听port端口，一旦该端口有了连接，就通过一个ssh转发出去。<br>-R port:host:hostport，将远程主机的某个端口转发到指定的本地机器的指定端口。远程主机上分了一个socket监听port端口，一旦该端口有了连接，就通过一个ssh转发到指定的本地机器的指定端口。<br>-N 不指定脚本或者命令<br>-f 后台认证，需要和-N连用</p><p>-L和-R的区别，-L是ssh隧道，-R是ssh反向隧道。</p><h2 id="示例">示例</h2><h3 id="翻墙ssh-l">翻墙ssh -L</h3><p>执行以下命令的本机(localhost)通过中间服务器(45.32.22.289)访问被屏蔽的网站(google)。<br>~$:ssh -L 1234:google_ip:80 root@45.32.22.289<br>拿这个举个例子，可能不是很恰当，但是有助于理解。我自己的机器(A)是不能访问google©的，但是我有一台vps(B)，地址为45.32.22.289是可以访问google的，可以通过ssh隧道将A的端口(1234)通过B映射到C的端口(80)。</p><h3 id="本机访问局域网的tensorboard-server">本机访问局域网的tensorboard server</h3><h4 id="本机设置">本机设置</h4><p>~$:ssh -L 12345:10.1.114.50:6006 <a href="mailto:mxxmhh@127.0.0.1" target="_blank" rel="noopener">mxxmhh@127.0.0.1</a><br>将本机的12345端口映射到10.1.114.50的6006端口，中间服务器使用的是本机。<br>或者可以使用10.1.114.50作为中间服务器。<br>~$:ssh -L 12345:10.1.114.50:6006 <a href="mailto:liuchi@10.1.114.50" target="_blank" rel="noopener">liuchi@10.1.114.50</a><br>或者可以使用如下方法：<br>~$:ssh -L 12345:127.0.0.1:6006 <a href="mailto:liuchi@10.1.114.50" target="_blank" rel="noopener">liuchi@10.1.114.50</a><br>从这个方法中，可以看出127.0.0.1这个ip是中间服务器可以访问的ip。<br>以上三种方法中，-L后的端口号12345可以随意设置，只要不冲突即可。</p><h4 id="服务端设置">服务端设置</h4><p>然后在服务端运行以下命令：<br>~$:tensorboard --logdir logdir -port 6006<br>这个端口号也是可以任意设置的，不冲突即可。</p><h4 id="运行">运行</h4><p>然后在本机访问<br><a href="https://127.0.0.1:12345" target="_blank" rel="noopener">https://127.0.0.1:12345</a>即可。</p><h3 id="内网穿透ssh-r">内网穿透ssh -R</h3><p>外网A(123.123.123.123)访问处于内网B的(127.0.0.1)的机器。<br>~$:ssh -N -f -R 2222:127.0.0.1:22 <a href="mailto:root@123.123.123.123" target="_blank" rel="noopener">root@123.123.123.123</a></p><p>可以在外网机器A(123.123.123.123)上通过如下命令访问(-R)指定的内网机器B：<br>~$:ssh -p 2222 userB@localhost</p><h3 id="报错">报错</h3><p>Host key verification failed<br>直接把/home/username/.ssh/known_hosts中相应的给删了。</p><h3 id="内网访问内网-挖洞">内网访问内网（挖洞）</h3><p>A是家里的内网（无公网IP）上机器(196.)，B是VPS（有公网IP）(45.32.)，C是公司内网（无公网IP）机器(10.)。<br>要在家里的内网访问公司的内网，即A访问C。在C上建立ssh反向隧道：<br>~$:ssh -N -f -R 2222:127.0.0.1:22 userB@B.ip<br>在A上访问：<br>~$:ssh -p  2222 userC@B.ip</p><h2 id="参考文献">参考文献</h2><p>1.<a href="https://blog.trackets.com/2014/05/17/ssh-tunnel-local-and-remote-port-forwarding-explained-with-examples.html" target="_blank" rel="noopener">https://blog.trackets.com/2014/05/17/ssh-tunnel-local-and-remote-port-forwarding-explained-with-examples.html</a><br>2.<a href="https://blog.creke.net/722.html" target="_blank" rel="noopener">https://blog.creke.net/722.html</a><br>3.<a href="http://arondight.me/2016/02/17/%E4%BD%BF%E7%94%A8SSH%E5%8F%8D%E5%90%91%E9%9A%A7%E9%81%93%E8%BF%9B%E8%A1%8C%E5%86%85%E7%BD%91%E7%A9%BF%E9%80%8F/" target="_blank" rel="noopener">http://arondight.me/2016/02/17/使用SSH反向隧道进行内网穿透/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;ssh-命令介绍&quot;&gt;ssh 命令介绍&lt;/h2&gt;
&lt;p&gt;ssh是一种安全传输协议，此外还有tunnel转发功能，可以用来内网渗透。&lt;/p&gt;
&lt;h3 id=&quot;参数介绍&quot;&gt;参数介绍&lt;/h3&gt;
&lt;p&gt;-L port:host:hostport，访问本机的port端口就相当
      
    
    </summary>
    
      <category term="linux" scheme="http://mxxhcm.github.io/categories/linux/"/>
    
    
      <category term="linux" scheme="http://mxxhcm.github.io/tags/linux/"/>
    
      <category term="ssh" scheme="http://mxxhcm.github.io/tags/ssh/"/>
    
  </entry>
  
  <entry>
    <title>windows IIS ftp服务器</title>
    <link href="http://mxxhcm.github.io/2019/06/09/windows-IIS-ftp%E6%9C%8D%E5%8A%A1%E5%99%A8/"/>
    <id>http://mxxhcm.github.io/2019/06/09/windows-IIS-ftp服务器/</id>
    <published>2019-06-09T02:45:35.000Z</published>
    <updated>2019-08-29T04:03:27.430Z</updated>
    
    <content type="html"><![CDATA[<h2 id="在服务端搭建ftp服务器">在服务端搭建ftp服务器</h2><h3 id="创建ftp目录">创建ftp目录</h3><p>这里我选择在D盘，创建目录D://Ftp作为我的ftp文件存放目录</p><h2 id="添加ftp用户-可选">添加ftp用户（可选）</h2><p>其实这里的ftp用户就是windows 操作系统的用户，为了安全起见，我们不选我们工作时登录的用户，选择重新创建一个新的用户。这里添加了新用户ftpuser作为ftp登录用户。</p><h3 id="开启iis服务">开启IIS服务</h3><p>打开控制面板下的程序，选择启用或关闭Windows功能，选中Internet Information Services中的FTP服务器和Web管理工具（如下图）。等待其加载完所有组件</p><h3 id="设置ftp站点">设置ftp站点</h3><p>执行完第3步以后，打开计算机管理，在服务和应用程序下我们就可以看到Internet Infromation Service，点击它，右击网站选择新建一个ftp站点，配置见下图，完成以后启动该ftp。</p><h2 id="防火墙允许通过">防火墙允许通过</h2><p>打开 控制面板&gt;系统和安全&gt;Windows防火墙&gt;允许的应用<br>点击允许其他应用，这时添加windows服务主进程的路径&quot;C:\Windows\System32\svchost.exe&quot;，这时候防火墙就允许ftp访问通过了。</p><h2 id="在客户机登录ftp服务器">在客户机登录ftp服务器</h2><p>在资源管理器中输入ftp://ip地址，输入账号密码后即可成功访问ftp服务器。</p><h2 id="参考文献">参考文献</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;在服务端搭建ftp服务器&quot;&gt;在服务端搭建ftp服务器&lt;/h2&gt;
&lt;h3 id=&quot;创建ftp目录&quot;&gt;创建ftp目录&lt;/h3&gt;
&lt;p&gt;这里我选择在D盘，创建目录D://Ftp作为我的ftp文件存放目录&lt;/p&gt;
&lt;h2 id=&quot;添加ftp用户-可选&quot;&gt;添加ftp用户（可
      
    
    </summary>
    
      <category term="windows" scheme="http://mxxhcm.github.io/categories/windows/"/>
    
    
      <category term="ftp" scheme="http://mxxhcm.github.io/tags/ftp/"/>
    
      <category term="windows" scheme="http://mxxhcm.github.io/tags/windows/"/>
    
  </entry>
  
  <entry>
    <title>numpy backpropagation</title>
    <link href="http://mxxhcm.github.io/2019/06/08/numpy-backpropagation/"/>
    <id>http://mxxhcm.github.io/2019/06/08/numpy-backpropagation/</id>
    <published>2019-06-08T15:10:17.000Z</published>
    <updated>2019-06-09T03:11:15.893Z</updated>
    
    <content type="html"><![CDATA[<h2 id="numpy-代码示例">numpy 代码示例</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></span><br><span class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></span><br><span class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create random input and output data</span></span><br><span class="line">x = np.random.randn(N, D_in)</span><br><span class="line">y = np.random.randn(N, D_out)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Randomly initialize weights</span></span><br><span class="line">w1 = np.random.randn(D_in, H)</span><br><span class="line">w2 = np.random.randn(H, D_out)</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">1e-6</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</span><br><span class="line">    <span class="comment"># Forward pass: compute predicted y</span></span><br><span class="line">    h = x.dot(w1)</span><br><span class="line">    h_relu = np.maximum(h, <span class="number">0</span>)</span><br><span class="line">    y_pred = h_relu.dot(w2)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute and print loss</span></span><br><span class="line">    loss = np.square(y_pred - y).sum()</span><br><span class="line">    print(t, loss)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Backprop to compute gradients of w1 and w2 with respect to loss</span></span><br><span class="line">    grad_y_pred = <span class="number">2.0</span> * (y_pred - y)</span><br><span class="line">    grad_w2 = h_relu.T.dot(grad_y_pred)</span><br><span class="line">    grad_h_relu = grad_y_pred.dot(w2.T)</span><br><span class="line">    grad_h = grad_h_relu.copy()</span><br><span class="line">    grad_h[h &lt; <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">    grad_w1 = x.T.dot(grad_h)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update weights</span></span><br><span class="line">    w1 -= learning_rate * grad_w1</span><br><span class="line">    w2 -= learning_rate * grad_w2</span><br></pre></td></tr></table></figure><h2 id="推理">推理</h2><p><img src="/2019/06/08/numpy-backpropagation/bp.jpg" alt="bp1"><br><img src="/2019/06/08/numpy-backpropagation/bp2.jpg" alt="bp2"></p><h2 id="参考文献">参考文献</h2><p>1.<a href="http://pytorch.org/tutorials/beginner/pytorch_with_examples.html" target="_blank" rel="noopener">http://pytorch.org/tutorials/beginner/pytorch_with_examples.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;numpy-代码示例&quot;&gt;numpy 代码示例&lt;/h2&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span
      
    
    </summary>
    
      <category term="机器学习" scheme="http://mxxhcm.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="numpy" scheme="http://mxxhcm.github.io/tags/numpy/"/>
    
      <category term="back propagation" scheme="http://mxxhcm.github.io/tags/back-propagation/"/>
    
  </entry>
  
  <entry>
    <title>pytorch nn.Conv2d vs nn.functional.conv2d</title>
    <link href="http://mxxhcm.github.io/2019/06/07/pytorch-nn-Conv2d-vs-nn-functional-conv2d/"/>
    <id>http://mxxhcm.github.io/2019/06/07/pytorch-nn-Conv2d-vs-nn-functional-conv2d/</id>
    <published>2019-06-07T04:45:21.000Z</published>
    <updated>2019-06-07T05:09:47.408Z</updated>
    
    <content type="html"><![CDATA[<h2 id="nn-conv2d-vs-nn-functional-conv2d">nn.Conv2d vs nn.functional.conv2d</h2><ul><li>nn.functional包中是函数接口，是从输入到输出的一个变换，内部没有Variable，不能够构成一个layer；nn包中是nn.functional函数对应的类封装，nn中的类可能有Variable（如Conv2d)，也可能没有（如Dropout，Maxpooling）</li><li>nn中的类一般是nn.Module的子类，继承了nn.Module的方法和属性。</li><li>nn中的类需要传入参数实例化，然后用函数调用的方法调用实例化对象传入数据。而nn.functional是函数，不需要实例化可以直接调用，需要同时传入filters的weights和biases。</li></ul><h2 id="参考文献">参考文献</h2><p>1.<a href="https://www.zhihu.com/question/66782101/answer/579393790" target="_blank" rel="noopener">https://www.zhihu.com/question/66782101/answer/579393790</a><br>2.<a href="https://www.zhihu.com/question/66782101/answer/246460048" target="_blank" rel="noopener">https://www.zhihu.com/question/66782101/answer/246460048</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;nn-conv2d-vs-nn-functional-conv2d&quot;&gt;nn.Conv2d vs nn.functional.conv2d&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;nn.functional包中是函数接口，是从输入到输出的一个变换，内部没有Variable，不能
      
    
    </summary>
    
      <category term="pytorch" scheme="http://mxxhcm.github.io/categories/pytorch/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="pytorch" scheme="http://mxxhcm.github.io/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>python json</title>
    <link href="http://mxxhcm.github.io/2019/06/06/python-json/"/>
    <id>http://mxxhcm.github.io/2019/06/06/python-json/</id>
    <published>2019-06-06T07:58:02.000Z</published>
    <updated>2019-06-06T08:16:11.539Z</updated>
    
    <content type="html"><![CDATA[<h2 id="什么是json">什么是json</h2><p>是一种文件格式<br>json object和python的字典差不多。<br>json在python中可以以字符串形式读入。</p><h2 id="python读取json">python读取json</h2><h3 id="json-loads">json.loads()</h3><p>json.loads()从内存中读取。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import json</span><br><span class="line"></span><br><span class="line">person = &apos;&#123;&quot;name&quot;: &quot;Bob&quot;, &quot;languages&quot;: [&quot;English&quot;, &quot;Fench&quot;]&#125;&apos;</span><br><span class="line">person_dict = json.loads(person)</span><br><span class="line"></span><br><span class="line"># Output: &#123;&apos;name&apos;: &apos;Bob&apos;, &apos;languages&apos;: [&apos;English&apos;, &apos;Fench&apos;]&#125;</span><br><span class="line">print( person_dict)</span><br><span class="line"></span><br><span class="line"># Output: [&apos;English&apos;, &apos;French&apos;]</span><br><span class="line">print(person_dict[&apos;languages&apos;])</span><br></pre></td></tr></table></figure><h3 id="json-load">json.load()</h3><p>json.load()从文件对象中读取。<br>假设有名为person.json的json文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">&quot;name&quot;: &quot;Bob&quot;, </span><br><span class="line">&quot;languages&quot;: [&quot;English&quot;, &quot;Fench&quot;]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>直接从文件对象中读取</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'person.json'</span>) <span class="keyword">as</span> f:</span><br><span class="line">  data = json.load(f)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output: &#123;'name': 'Bob', 'languages': ['English', 'Fench']&#125;</span></span><br><span class="line">print(data)</span><br></pre></td></tr></table></figure><h2 id="python写json文件">python写json文件</h2><h3 id="json-dumps">json.dumps()</h3><p>json.dumps()将字典转化为JSON字符串</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line">person_dict = &#123;<span class="string">'name'</span>: <span class="string">'Bob'</span>,</span><br><span class="line"><span class="string">'age'</span>: <span class="number">12</span>,</span><br><span class="line"><span class="string">'children'</span>: <span class="literal">None</span></span><br><span class="line">&#125;</span><br><span class="line">person_json = json.dumps(person_dict)</span><br><span class="line"></span><br><span class="line">print(person_json)</span><br></pre></td></tr></table></figure><h3 id="json-dump">json.dump()</h3><p>json.dump()直接将字典写入文件对象</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line">person_dict = &#123;<span class="string">"name"</span>: <span class="string">"Bob"</span>,</span><br><span class="line"><span class="string">"languages"</span>: [<span class="string">"English"</span>, <span class="string">"Fench"</span>],</span><br><span class="line"><span class="string">"married"</span>: <span class="literal">True</span>,</span><br><span class="line"><span class="string">"age"</span>: <span class="number">32</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'person.txt'</span>, <span class="string">'w'</span>) <span class="keyword">as</span> json_file:</span><br><span class="line">  json.dump(person_dict, json_file)</span><br></pre></td></tr></table></figure><h2 id="pretty-json">pretty JSON</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line">person_string = <span class="string">'&#123;"name": "Bob", "languages": "English", "numbers": [2, 1.6, null]&#125;'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Getting dictionary</span></span><br><span class="line">person_dict = json.loads(person_string)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Pretty Printing JSON string back</span></span><br><span class="line">print(json.dumps(person_dict, indent = <span class="number">4</span>, sort_keys=<span class="literal">True</span>))</span><br></pre></td></tr></table></figure><h2 id="参考文献">参考文献</h2><p>1.<a href="https://www.programiz.com/python-programming/json" target="_blank" rel="noopener">https://www.programiz.com/python-programming/json</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;什么是json&quot;&gt;什么是json&lt;/h2&gt;
&lt;p&gt;是一种文件格式&lt;br&gt;
json object和python的字典差不多。&lt;br&gt;
json在python中可以以字符串形式读入。&lt;/p&gt;
&lt;h2 id=&quot;python读取json&quot;&gt;python读取json&lt;/h
      
    
    </summary>
    
      <category term="python" scheme="http://mxxhcm.github.io/categories/python/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="json" scheme="http://mxxhcm.github.io/tags/json/"/>
    
  </entry>
  
  <entry>
    <title>linux process ps kill top pstree nice</title>
    <link href="http://mxxhcm.github.io/2019/06/03/linux-process-ps-kill-top-pstree-nice/"/>
    <id>http://mxxhcm.github.io/2019/06/03/linux-process-ps-kill-top-pstree-nice/</id>
    <published>2019-06-03T13:30:04.000Z</published>
    <updated>2019-07-09T02:08:45.424Z</updated>
    
    <content type="html"><![CDATA[<h2 id="概述">概述</h2><p>这一节介绍和process相关的命令，包含ps,top,kill, pstree, nice, fuser, lsof, pidof, /proc/等</p><h2 id="ps查看进程">ps查看进程</h2><h3 id="参数介绍">参数介绍</h3><p>ps [-Aauf] [xlj]<br>-A 所有的进程全部显示出来<br>a 现行终端机下所有程序，包含其他用户<br>u 有效用户相关的进程，主要以用户为主的格式来区分<br>f 用ASCII字符显示树状结构，表达进程间的关系<br>x　通常与a这个参数一块使用，显示所有程序，不以终端机来区分<br>l　较长，较详细的将该PID的信息列出<br>j　工作的格式</p><h3 id="示例">示例</h3><p>~$:ps aux　查看系统所有的进程数据<br>~$:ps -lA　查看所有系统的数据<br>~$:ps axjf　连同部分进程树状态<br>~$:ps aux | grep ‘sslocal’ #查看sslocal程序是否运行<br>~$:ps ax # 显示当前系统进程的列表<br>~$:ps aux #显示当前系统进程详细列表以及进程用户<br>~$:ps -A  #列出进程号<br>~$:ps aux |grep 2222’|grep -v grep  # 找出所有包含2222的进程，grep -v 过滤掉含有grep字符的行</p><h3 id="aux-查看系统所有进程">aux 查看系统所有进程</h3><p>~$:ps aux     # 使用BSD格式显示进程<br>输出<br>USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND<br>USER<br>PID<br>%CPU    该进程使用掉的CPU资源百分比<br>%MEM    该进程占用物理内存百分比<br>VSZ 该进程占用虚拟内存量<br>RSS 该进程占用固定内存量(KB)<br>TTY pts/0　表示由网络连接进主机的进程<br>STAT    进程状态<br>START   该进程被触发的时间<br>TIME    CPU时间<br>COMMAND     该进程实际命令</p><p>僵尸进程(<defunct>)</defunct></p><h3 id="ps-ef">ps -ef</h3><p>~\$:ps -ef  # 使用标准格式显示进程<br>输出<br>UID        PID  PPID  C STIME TTY          TIME CMD<br>UID 用户名<br>PID 进程ID<br>PPID    父进程ID<br>C   CPU占用百分比<br>STIME   进程启动到现在的时间<br>TTY     在哪个终端上运行，ps/0表示网络连接<br>TIME<br>CMD     命令的名称和参数</p><h3 id="l仅查看自己相关的bash进程">-l仅查看自己相关的bash进程</h3><p>~\$:ps -l #仅查看自己相关的bash进程<br>输出<br>F S UID PID PPID C PR NI ADDZ SZ WCHAN TTY TIME CMD<br>F  说明进程权限<br>S　进程状态STAT<br>R(running)　S(sleep)　D(不可被唤醒的睡眠状态,通常是IO的进程)　T(stop)　Z(zombie僵尸状态)进程已终止，但无法被删除到到内存外,PCB还在，但是其他资源全部被收回，是由父进程负责收回资源。<br>UID/PID/PPID<br>C CPU使用率<br>PR/NI  Priority/Nice的缩写，此进程被CPU执行的优先级<br>ADDR/SZ/WCHAN都与内存有关，ADDR是kernel function ,指出该进程在内存的哪个部分，如果是个running的过程，显示-;SZ代表用掉多少内存;WCHAN表示目前进程是否运行，-表示正在运行<br>TTY 使用的终端接口<br>TIME    使用掉的CPU时间，而不是系统时间<br>CMD command缩写,造成此进程被触发的命令</p><h2 id="top-动态查看进程的变化">top 动态查看进程的变化</h2><h3 id="参数介绍-v2">参数介绍</h3><p>top [-d 数字] | top [-bnp]<br>-d　整个进程界面更新的秒数<br>-b  以批次的方式执行top，<br>-n  与-b搭配，意义是，需要进行几次top的输出结果<br>-p  指定某个PID来查看<br>在top执行中可以使用的命令<br>?查询所有的命令<br>P 按CPU使用率排序<br>M Mem排序<br>N PID排序<br>T CPU累计时间排序<br>k 给某个PID一个信号<br>r 给某个PID重新制定一个nice值<br>q 离开top软件</p><h3 id="示例-v2">示例</h3><p>~\$:top -d 2 #每两秒钟刷新一次top，默认为5s<br>~\$:top -b -n 2 &gt; ~/tmp/top.out</p><h3 id="top输出内容">top输出内容</h3><p>第一行top<br>目前时间　<br>开机到现在时间　up n days , hh:mm<br>登陆的用户<br>系统在1,5,15分钟时的负载，batch工作方式负载小于0.8即为这个负载，代表的是1,5,15分钟，系统平均要负责多少个程序。越小说明系统越闲<br>第二行task<br>目前进程总量，分别有多少个处于什么状态，不能有处于zombie的进程，<br>第三行%cpu<br>wa代表的是I/Owait，系统变慢都是由于I/O产生问题较多<br>第四五行内存和swap使用情况，swap被使用的应该尽量少，否则说明物理内存实在不足。<br>第六行<br>PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND<br>PID　每个进程ID<br>USER　该进程所属用户<br>PR　Priority 进程的优先级顺序，越小越早被执行<br>NI　Nice 与Priority有关，越小越早被执行<br>VIRT<br>RES<br>SHR<br>S   STAT<br>%CPU　CPU使用率<br>%MEM　内存使用率<br>TIME+  CPU使用时间的累加<br>COMMAND</p><h2 id="pstree-查看进程树">pstree 查看进程树</h2><h3 id="参数介绍-v3">参数介绍</h3><p>pstree [-A|-U] [-up]<br>-A  各进程树直接以ASCII字符连接<br>-U  各进程树之间以utf-8字符连接<br>-u  显示进程所属账号名<br>-p  显示pid</p><h3 id="示例-v3">示例</h3><p>~\$:pstree -Aup</p><h2 id="kill管理进程">kill管理进程</h2><h3 id="参数介绍-v4">参数介绍</h3><p>kill -signals %jobnumber 杀掉某个job<br>-l  列出所有signal<br>-1  重新读取一次参数的配置文件<br>-2  与ctrl+c　一样<br>-9  强制删除一个job，非正常状态<br>-15 让一个job正常结束</p><h3 id="查看signal种类">查看signal种类</h3><p>~\$:man 7 signal</p><h3 id="kill示例">kill示例</h3><p>kill -signal %jobnum<br>kill -signal pid<br>这两种情况是不同的，第一种是job，第二种是pid,不能弄混</p><h3 id="killall将系统中所有以某个命令启动的服务全部删除">killall将系统中所有以某个命令启动的服务全部删除</h3><p>killall [-iIe]  用来删除某个服务<br>-i iteractive<br>-e exact<br>-I 忽略大小写</p><h3 id="killall示例">killall示例</h3><p>~\$:killall utserver<br>~\$:killall -1 syslogd<br>~\$:killall -9 httpd<br>~\$:killall -i -9 bash</p><h2 id="nice管理进程优先级">nice管理进程优先级</h2><p>PRI(priority)与NI(nice)<br>PRI值是由内核动态调整的，用户无法直接调整PRI值<br>PRI(new)=PRI(old)+nice</p><p>nice值虽然可以影响PRI，但是并不是说原来PRI为50,nice为5,就会让PRI变为55,<br>这是需要经过系统分析之后决定的</p><p>nice值<br>a.可调整范围为-20~19<br>b.root可随意调整任何人的nice值-20~19间的任意一个值<br>c.一般用户仅可以调整自己nice值，且范围在0~19<br>d.一般用户仅可将nice值调高，而无法降低<br>e.调整nice值的方法<br>新执行的命令手动设置nice值<br>nice -n [number] command</p><h3 id="示例-v4">示例</h3><p>~\$:nice -n -5 vim &amp;</p><h3 id="已存在的进程调整nice值">已存在的进程调整nice值</h3><p>renice [number] command<br>~\$:ps -l | grep ‘\*bash\$’<br>~\$:renice 10 \$(ps -l|grep ‘bash\$’ | awk ‘{print \$4}’)</p><h2 id="fuser找到使用某文件的程序">fuser找到使用某文件的程序</h2><h3 id="参数介绍-v5">参数介绍</h3><p>fuser [-muv] [-k [i] [-signal]] name<br>-m  后面接的文件名会主动提到文件顶层<br>-u  user<br>-v  verbose<br>-k  SIGKILL<br>-i  询问用户，与-k搭配<br>-signal  -1,-15等，默认为-9</p><h3 id="示例-v5">示例</h3><p>~\$:mount -o loop ubuntu.iso /mnt/iso<br>~\$:cd /mnt/iso<br>~\$:umount /mnt/iso<br>error<br>~\$:fuser -muv   /mnt/iso<br>…<br>~\$:cd<br>~\$:umount /mnt/iso</p><p>~\$:fuser -muv /proc<br>~\$:fuser -ki /bin/bash</p><h2 id="lsof找到被进程打开的文件">lsof找到被进程打开的文件</h2><h3 id="参数介绍-v6">参数介绍</h3><p>lsof [-uaU] [+d]<br>-a 相当于and连接符<br>-u 某个用户的相关进程打开的文件<br>-U Unix like 的socket文件类型<br>+d 某个目录下被打开的文件</p><h3 id="示例-v6">示例</h3><p>~\$:lsof +d ~/Desktop<br>~\$:lsof -u mxx | grep ‘bash’<br>~\$:lsof -u mxx -a -U</p><h2 id="pidof找出某个正在进行的进程的pid">pidof找出某个正在进行的进程的pid</h2><h3 id="参数介绍-v7">参数介绍</h3><p>pidof [-sx] program_name<br>-s 仅列出一个pid而不列出所有的pid<br>-x 列出该进程可能的ppid的pid</p><h2 id="获取进程id">获取进程id</h2><p>ps -A |grep “command” | awk '{print $1}'<br>pidof 'command’<br>pgrep ‘command’</p><h2 id="proc-文件">/proc/\* 文件</h2><p>/proc/cmdline   加载kernel执行的参数<br>/proc/cpuinfo   CPU相关信息<br>/proc/devices   主要设备代号　与mknod相关<br>/proc/filesystems   目前已加载的文件系统<br>/proc/interrupts    系统上IRQ分配状态<br>/proc/ioports   目前系统上每个设备配置的I/O地址<br>/proc/loadavg   top/uptime显示的负载<br>/proc/kcore 内存大写<br>/proc/meminfo   free<br>/proc/modules   目前系统已加载的模块列表，可想成驱动程序<br>/proc/mounts　mount<br>/proc/swaps  系统加载的内存使用的分区记录<br>/proc/partitions　fdisk -l<br>/proc/pci　PCI总线上每个设备的详细情况　lspci<br>/proc/uptime　uptime<br>/proc/version　内核版本 uname -a<br>/proc/bus/\*　总线设备，USB设备</p><h2 id="具有suid-sgid的程序">具有SUID，SGID的程序</h2><p>如passwd，当触发passwd之后，会取得一个新的进程与PID,该PID产生时通过SUID给予该PID特殊的权限设置。<br>在一个bash中执行passwd会衍生出一个passwd进程，而且权限为root<br>~\$:passwd &amp;<br>~\$:pstree -up找到该进程</p><h2 id="参考文献">参考文献</h2><p>1.《鸟哥的LINUX私房菜》基础篇</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;概述&quot;&gt;概述&lt;/h2&gt;
&lt;p&gt;这一节介绍和process相关的命令，包含ps,top,kill, pstree, nice, fuser, lsof, pidof, /proc/等&lt;/p&gt;
&lt;h2 id=&quot;ps查看进程&quot;&gt;ps查看进程&lt;/h2&gt;
&lt;h3 id=&quot;参
      
    
    </summary>
    
      <category term="linux" scheme="http://mxxhcm.github.io/categories/linux/"/>
    
    
      <category term="linux" scheme="http://mxxhcm.github.io/tags/linux/"/>
    
      <category term="ps" scheme="http://mxxhcm.github.io/tags/ps/"/>
    
      <category term="top" scheme="http://mxxhcm.github.io/tags/top/"/>
    
      <category term="pstree" scheme="http://mxxhcm.github.io/tags/pstree/"/>
    
      <category term="process" scheme="http://mxxhcm.github.io/tags/process/"/>
    
      <category term="kill" scheme="http://mxxhcm.github.io/tags/kill/"/>
    
      <category term="nice" scheme="http://mxxhcm.github.io/tags/nice/"/>
    
  </entry>
  
  <entry>
    <title>linux 系统资源查看</title>
    <link href="http://mxxhcm.github.io/2019/06/03/linux-%E7%B3%BB%E7%BB%9F%E8%B5%84%E6%BA%90%E6%9F%A5%E7%9C%8B/"/>
    <id>http://mxxhcm.github.io/2019/06/03/linux-系统资源查看/</id>
    <published>2019-06-03T13:20:52.000Z</published>
    <updated>2019-06-24T13:37:59.615Z</updated>
    
    <content type="html"><![CDATA[<h2 id="查看内存">查看内存</h2><p>free [-bkmg] -t<br>-t  输出结果中显示free和swap加在一起的总量<br>free -h</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">~$:sudo dmidecode -t memory |grep "Number Of Devices" |awk '&#123;print $NF&#125;'    # 卡槽数量</span><br><span class="line">~$:sudo dmidecode -t memory |grep -A16 "Memory Device$" |grep 'Size:.*MB' |wc -l    # 内存数量</span><br><span class="line">~$:sudo dmidecode -t memory |grep -A16 "Memory Device$" |grep "Type:"  # 内存支持类型</span><br><span class="line">~$:sudo dmidecode -t memory |grep -A16 "Memory Device$" |grep "Speed:" # 每个内存频率</span><br><span class="line">~$:sudo dmidecode -t memory |grep -A16 "Memory Device$" |grep "Size:"  # 每个内存大小</span><br></pre></td></tr></table></figure><h2 id="查看磁盘">查看磁盘</h2><p>df -h 查看磁盘占用情况<br>du -h --max-depth=2 统计当前目录下深度为2的文件和目录大小</p><h2 id="查看cpu占用">查看cpu占用</h2><p>top<br><a href="https://mxxhcm.github.io//2019/05/07/linux-cpu%E4%BF%A1%E6%81%AF%E6%9F%A5%E7%9C%8B/">点击查看cpu信息命令</a></p><h2 id="查看系统与内核相关信息">查看系统与内核相关信息</h2><p>uname [-asrmpi]<br>-a 所有系统相关的信息，<br>-s 系统内核名称<br>-r 内核版本<br>-m 本系统硬件名称<br>-p CPU类型，与-m类似，显示的是CPU类型<br>-i 硬件平台</p><h2 id="查看系统启动时间与工作负载">查看系统启动时间与工作负载</h2><p>uptime</p><h2 id="查看网络">查看网络</h2><p>netstat [-atunlp]<br>-a 将目前系统上所有连接监听，socket列出来<br>-t 列出tcp网络数据包的数据<br>-u 列出udp网络数据包的数据<br>-n 不列出进程的服务名称，以端口号来显示<br>-l 列出目前正在网络监听(listen)的服务<br>-p 列出该网络服务的进程pid<br>~$:netstat -tnlp　　#找出目前系统上已经在监听的网络连接及其PID</p><h2 id="分析内核产生的信息">分析内核产生的信息</h2><p>dmesg | more<br>dmesg | grep -i 'sd’<br>dmesg | grep -i ‘eth’</p><h2 id="动态分析系统资源使用情况">动态分析系统资源使用情况</h2><p>man vmstat 查看各字段含义</p><p>vmstat      动态查看系统资源<br>vmstat [-a]     display active and inactive memory<br>vmstat [-d] report sisk statistics<br>vmstat [-p 分区]　Detailed statistics about partition<br>vmstat [-S 单位]  B K M G<br>vmstat [-f]  displays the num of forks since boot<br>~$:vmstat<br>~$:vmstat -a<br>~$:vmstat -p /dev/sda1<br>~$:vmstat -S M<br>~$:vmstat -f</p><h2 id="参考文献">参考文献</h2><p>1.《鸟哥的LINUX私房菜》</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;查看内存&quot;&gt;查看内存&lt;/h2&gt;
&lt;p&gt;free [-bkmg] -t&lt;br&gt;
-t  输出结果中显示free和swap加在一起的总量&lt;br&gt;
free -h&lt;/p&gt;
&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td cl
      
    
    </summary>
    
      <category term="linux" scheme="http://mxxhcm.github.io/categories/linux/"/>
    
    
      <category term="linux" scheme="http://mxxhcm.github.io/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>linux python调用shell脚本并将输出重定向到文件</title>
    <link href="http://mxxhcm.github.io/2019/06/03/linux-python%E8%B0%83%E7%94%A8shell%E8%84%9A%E6%9C%AC%E5%B9%B6%E5%B0%86%E8%BE%93%E5%87%BA%E9%87%8D%E5%AE%9A%E5%90%91%E5%88%B0%E6%96%87%E4%BB%B6/"/>
    <id>http://mxxhcm.github.io/2019/06/03/linux-python调用shell脚本并将输出重定向到文件/</id>
    <published>2019-06-03T12:11:38.000Z</published>
    <updated>2019-06-26T03:32:08.128Z</updated>
    
    <content type="html"><![CDATA[<h2 id="python执行shell脚本并且重定向输出到文件">python执行shell脚本并且重定向输出到文件</h2><p>目的：有一些shell脚本的参数需要调整，在shell中处理有些麻烦，就用python控制参数，然后调用shell，问题就是如何将shell脚本的输出进行重定向。最开始我想直接用python调用终端中shell重定向的语法，我用的是os.system(command)，command包含重定向的命令，在实践中证明是不可行的。为什么？？？留待解决。</p><p>找到的解决方案是调用subprocess包，将要执行的命令存入一个list，将这个list当做参数传入，获得返回值，进行文件读写。这里拿ls命令举个例子，注意一点是，包含重定向语句的ls命令使用os.system()也能重定向成功，我使用的一个命令就不行了。</p><h3 id="call-0">call <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">subprocess.call([<span class="string">'ls'</span>, <span class="string">'-l'</span>, <span class="string">'.'</span>]) <span class="comment"># 直接将程序执行结果输出，没有返回值。</span></span><br></pre></td></tr></table></figure></h3><h3 id="popen">Popen</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">results = subprocess.Popen([<span class="string">'ls'</span>, <span class="string">'-l'</span>, <span class="string">'.'</span>], </span><br><span class="line">            stdout=subprocess.PIPE, </span><br><span class="line">            stderr=subprocess.STDOUT)</span><br><span class="line">stdout, stderr = results.communicate() </span><br><span class="line">res = stdout.decode(<span class="string">'utf-8'</span>) <span class="comment"># 利用res将结果输出到文件</span></span><br></pre></td></tr></table></figure><h3 id="run">run</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">result = subprocess.run([<span class="string">'ls'</span>, <span class="string">'-l'</span>], stdout=subprocess.PIPE) </span><br><span class="line">res = result.stdout.decode(<span class="string">'utf-8'</span>)  <span class="comment"># 利用res将结果输出到文件</span></span><br></pre></td></tr></table></figure><h2 id="参考文献">参考文献</h2><p>1.<a href="https://stackoverflow.com/questions/4760215/running-shell-command-and-capturing-the-output" target="_blank" rel="noopener">https://stackoverflow.com/questions/4760215/running-shell-command-and-capturing-the-output</a><br>2.<a href="https://linuxhandbook.com/execute-shell-command-python/" target="_blank" rel="noopener">https://linuxhandbook.com/execute-shell-command-python/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;python执行shell脚本并且重定向输出到文件&quot;&gt;python执行shell脚本并且重定向输出到文件&lt;/h2&gt;
&lt;p&gt;目的：有一些shell脚本的参数需要调整，在shell中处理有些麻烦，就用python控制参数，然后调用shell，问题就是如何将shell脚
      
    
    </summary>
    
      <category term="linux" scheme="http://mxxhcm.github.io/categories/linux/"/>
    
      <category term="python" scheme="http://mxxhcm.github.io/categories/python/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="工具" scheme="http://mxxhcm.github.io/tags/%E5%B7%A5%E5%85%B7/"/>
    
      <category term="linux" scheme="http://mxxhcm.github.io/tags/linux/"/>
    
      <category term="常见问题" scheme="http://mxxhcm.github.io/tags/%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/"/>
    
      <category term="shell" scheme="http://mxxhcm.github.io/tags/shell/"/>
    
  </entry>
  
  <entry>
    <title>linux custom configure file</title>
    <link href="http://mxxhcm.github.io/2019/06/03/linux-custom-configure-file/"/>
    <id>http://mxxhcm.github.io/2019/06/03/linux-custom-configure-file/</id>
    <published>2019-06-03T02:40:12.000Z</published>
    <updated>2019-07-03T06:57:20.520Z</updated>
    
    <content type="html"><![CDATA[<h2 id="bashrc自定义配置">bashrc自定义配置</h2><p>~/.bashrc文件详细内容，<a href="https://github.com/mxxhcm/code/blob/master/shell/bashrc" target="_blank" rel="noopener">代码地址</a></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> 使用alias给常用文件夹起别名</span><br><span class="line">alias post-dir='/home/mxxmhh/mxxhcm/blog/source/_posts'</span><br><span class="line">alias code-dir='/home/mxxmhh/mxxhcm/code/'</span><br><span class="line">alias matplotlib-dir='/home/mxxmhh/mxxhcm/code/tools/matplotlib/'</span><br><span class="line">alias numpy-dir='/home/mxxmhh/mxxhcm/code/tools/numpy/'</span><br><span class="line">alias shell-dir='/home/mxxmhh/mxxhcm/code/shell'</span><br><span class="line">alias github-dir='/home/mxxmhh/github/'</span><br><span class="line">alias torch-dir='/home/mxxmhh/mxxhcm/code/pytorch'</span><br><span class="line">alias tf-dir='/home/mxxmhh/mxxhcm/code/tf'</span><br><span class="line">alias rl-dir='/home/mxxmhh/mxxhcm/code/rl'</span><br><span class="line">alias ops-dir='/home/mxxmhh/mxxhcm/code/tf/ops'</span><br><span class="line">alias paper-dir='/home/mxxmhh/mxxhcm/papers'</span><br><span class="line">alias tf-dir='/home/mxxmhh/mxxhcm/code/tf'</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 使用alias直接cd到常用文件夹</span><br><span class="line">alias post='cd /home/mxxmhh/mxxhcm/blog/source/_posts'</span><br><span class="line">alias code='cd /home/mxxmhh/mxxhcm/code/'</span><br><span class="line">alias matplotlib='cd /home/mxxmhh/mxxhcm/code/tools/matplotlib/'</span><br><span class="line">alias numpy='cd /home/mxxmhh/mxxhcm/code/tools/numpy/'</span><br><span class="line">alias shell='cd /home/mxxmhh/mxxhcm/code/shell'</span><br><span class="line">alias github='cd /home/mxxmhh/github/'</span><br><span class="line">alias torch='cd /home/mxxmhh/mxxhcm/code/pytorch'</span><br><span class="line">alias tf='cd /home/mxxmhh/mxxhcm/code/tf'</span><br><span class="line">alias rl='cd /home/mxxmhh/mxxhcm/code/rl'</span><br><span class="line">alias ops='cd /home/mxxmhh/mxxhcm/code/tf/ops'</span><br><span class="line">alias paper='cd /home/mxxmhh/mxxhcm/papers'</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> hexo博客相关命令</span><br><span class="line">alias update='hexo g -d'</span><br><span class="line">alias n='hexo n '</span><br><span class="line">alias new='hexo n '</span><br><span class="line"><span class="meta">#</span> git相关命令</span><br><span class="line">alias status='git status'</span><br><span class="line">alias add='git add .'</span><br><span class="line">alias remove='git rm'</span><br><span class="line">alias commit='git commit -m '</span><br><span class="line">alias branch='git branch'</span><br><span class="line">alias check='git checkout '</span><br><span class="line">alias push-master='git push origin master'</span><br><span class="line">alias pull-master='git pull origin master'</span><br><span class="line">alias push-hexo='git push origin hexo'</span><br><span class="line">alias pull-hexo='git pull origin hexo'</span><br><span class="line">alias utorrent='utserver -settingspath /opt/utorrent/'</span><br><span class="line"><span class="meta">#</span> shadowsocks local运行</span><br><span class="line">alias ssr='nohup sslocal -c /etc/shadowsocks_v6.json &lt;/dev/null &amp;&gt;&gt;~/.log/ss-local.log &amp; '</span><br><span class="line">alias ssr4='nohup sslocal -c /etc/shadowsocks_v4.json &lt;/dev/null &amp;&gt;&gt;~/.log/ss-local.log &amp; '</span><br><span class="line">alias ssr5='nohup sslocal -c /etc/shadowsocks_v6.json &lt;/dev/null &amp;&gt;&gt;~/.log/ss-local.log &amp; '</span><br><span class="line">alias ssr6='nohup sslocal -c /etc/shadowsocks_v6.json &lt;/dev/null &amp;&gt;&gt;~/.log/ss-local.log &amp; '</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 获得sslocal进程id</span><br><span class="line">function sspid()&#123;</span><br><span class="line">ps aux |grep 'sslocal'</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> kill sslocal进程</span><br><span class="line">function killss()&#123;</span><br><span class="line">pid=`ps -A | grep 'sslocal' |awk '&#123;print $1&#125;'`</span><br><span class="line">echo $pid</span><br><span class="line">kill -9 $pid</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 失效了</span><br><span class="line">function anaconda_on()&#123;</span><br><span class="line">export PATH=/home/mxxmhh/anaconda3/bin:$PATH</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 连接到我的vultr账户</span><br><span class="line">function vultr()&#123;</span><br><span class="line">ssh root@2001:19f0:7001:20f8:5400:01ff:fee6:aff6</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 定义函数实现hexo博客的add, commit, push整个流程</span><br><span class="line">function deploy-upload-hexo()&#123;</span><br><span class="line">git add .</span><br><span class="line">git commit -m "update blog"</span><br><span class="line">git push origin hexo</span><br><span class="line">hexo g -d</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 定义函数实现普通repo的add, commit, push整个流程</span><br><span class="line">function upload-master()&#123;</span><br><span class="line">git add .</span><br><span class="line">git commit -m "update code"</span><br><span class="line">git push origin master</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 获得当前folder的大小</span><br><span class="line">function folder-size()&#123;</span><br><span class="line">dir=`pwd`</span><br><span class="line">du -h --max-depth=1</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 更新环境变量</span><br><span class="line"><span class="meta">#</span> PATH</span><br><span class="line">export PATH=/home/mxxmhh/anaconda3/bin:$PATH</span><br><span class="line">export PATH=/usr/local/cuda/bin$&#123;PATH:+:$&#123;PATH&#125;&#125;</span><br><span class="line"><span class="meta">#</span> CUDA PATH</span><br><span class="line">export LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64/:$&#123;LD_LIBRARY_PATH:+:$&#123;LD_LIBRARY_PATH&#125;&#125;</span><br><span class="line">export CUDA_HOME=/usr/local/cuda</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> terminal走privoxy设置</span><br><span class="line">export https_proxy="http://127.0.0.1:8118"</span><br><span class="line">export http_proxy="http://127.0.0.1:8118"</span><br><span class="line">export HTTP_PROXY="http://127.0.0.1:8118"</span><br><span class="line">export HTTPS_PROXY="http://127.0.0.1:8118"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 关闭terminal走privoxy设置</span><br><span class="line">function proxyv6_off()&#123;</span><br><span class="line">unset https_proxy</span><br><span class="line">unset http_proxy</span><br><span class="line">unset HTTP_PROXY</span><br><span class="line">unset HTTPS_PROXY</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="bash-aliases">.bash_aliases</h2><p>在.bashrc文件中会发现下面几行：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">if [ -f ~/.bash_aliases ]; then</span><br><span class="line">    . ~/.bash_aliases</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><p>即.bashrc包含.bash_aliases文件。<br>.bash_aliases放置常用的命令别名。</p><h2 id="vim自定义配置">vim自定义配置</h2><p>关于vim详细介绍，可以查看<a href="https://mxxhcm.github.io/2019/05/07/linux-vim/">linux-vim</a><br>vimrc文件如下，<a href="https://github.com/mxxhcm/code/blob/master/shell/vimrc" target="_blank" rel="noopener">代码地址</a></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> 使用四个空格代替tab键</span><br><span class="line">set expandtab</span><br><span class="line">set tabstop=4</span><br><span class="line">set shiftwidth=4</span><br><span class="line">set softtabstop=4</span><br><span class="line"></span><br><span class="line">" 打开文件类型检测</span><br><span class="line">filetype on</span><br><span class="line">" 根据不同的文件类型加载插件</span><br><span class="line">filetype plugin on</span><br><span class="line">set ignorecase</span><br><span class="line"></span><br><span class="line">" 定义前缀键</span><br><span class="line">" let mappleader=";"</span><br><span class="line"></span><br><span class="line">" 设置ctrl+a全选，ctrl+c复制，;y粘贴</span><br><span class="line">vnoremap ; "+</span><br><span class="line">nnoremap ; "+</span><br><span class="line">nmap ;p "+p</span><br><span class="line">nnoremap &lt;C-C&gt; "+y</span><br><span class="line">vnoremap &lt;C-C&gt; "+y</span><br><span class="line">nnoremap &lt;C-A&gt; ggVG</span><br><span class="line">vnoremap &lt;C-A&gt; ggVG</span><br><span class="line"></span><br><span class="line">" 删除#号开头</span><br><span class="line">nnoremap ;d3 :g/^#.*$/d&lt;CR&gt;</span><br><span class="line">nnoremap ;d# :g/^#.*$/d&lt;CR&gt;</span><br><span class="line">" 删除空行</span><br><span class="line">nnoremap ;ds :g/^\s*$/d&lt;CR&gt;</span><br><span class="line">" 删除以tab开头的tab</span><br><span class="line">nnoremap ;rt :0,$s/^\t//g&lt;CR&gt;</span><br><span class="line">" 用\^代替^</span><br><span class="line">nnoremap ;r6 :0,$s/\^/\\^/g&lt;CR&gt;</span><br><span class="line">nnoremap ;r^ :0,$s/\^/\\^/g&lt;CR&gt;</span><br><span class="line">" 用\\\\代替\\</span><br><span class="line">nnoremap ;r/ :0,$s/\\\\/\\\\\\\\/g&lt;CR&gt;</span><br><span class="line">nnoremap ;r? :0,$s/\\\\/\\\\\\\\/g&lt;CR&gt;</span><br><span class="line"></span><br><span class="line">" 给选中行加注释</span><br><span class="line">" cnoremap &lt;C-#&gt; s/^/# /g&lt;CR&gt;</span><br><span class="line">nmap ;ic :s/^/# /g&lt;CR&gt;</span><br><span class="line">vmap ;ic :s/^/# /g&lt;CR&gt;</span><br><span class="line">nmap ;dc :s/^# //g&lt;CR&gt;</span><br><span class="line">vmap ;dc :s/^# //g&lt;CR&gt;</span><br><span class="line">"vmap &lt;C-#&gt; :s/^/#/g&lt;CR&gt;</span><br><span class="line">"nmap &lt;C-#&gt; :s/^/#/g&lt;CR&gt;</span><br><span class="line"></span><br><span class="line">""" 状态栏设置</span><br><span class="line">" 总是显示状态栏</span><br><span class="line">set laststatus=2</span><br><span class="line">" 状态信息</span><br><span class="line">set statusline=%f%m%r%h%w\ %=#%n\ [%&#123;&amp;fileformat&#125;:%&#123;(&amp;fenc==\"\"?&amp;enc:&amp;fenc).((exists(\"\+bomb\")\ &amp;&amp;\ &amp;bomb)?\"\+B\":\"\").\"\"&#125;:%&#123;strlen(&amp;ft)?&amp;ft:'**'&#125;]\ [%c,%l/%L]\ %p%%</span><br><span class="line"></span><br><span class="line">"""光标设置</span><br><span class="line">" 设置显示光标当前位置</span><br><span class="line">set ruler</span><br><span class="line"></span><br><span class="line">" 开启行号显示</span><br><span class="line">set number</span><br><span class="line">" 高亮显示当前行/列</span><br><span class="line">set cursorline</span><br><span class="line">" set cursorcolumn</span><br><span class="line">" 高亮显示搜索结果</span><br><span class="line">set hlsearch</span><br><span class="line">" 显示文件名</span><br><span class="line"></span><br><span class="line">" 开启语法高亮</span><br><span class="line">syntax enable</span><br><span class="line">" 允许用指定语法高亮配色方案替换默认方案</span><br><span class="line">syntax on</span><br><span class="line">" 将制表符扩展为空格</span><br><span class="line">" 设置编辑时制表符占用空格数</span><br><span class="line">" 设置格式化时制表符占用空格数</span><br><span class="line">" 让 vim 把连续数量的空格视为一个制表符</span><br><span class="line">set autoindent</span><br></pre></td></tr></table></figure><h2 id="参考文献">参考文献</h2><p>1.《鸟哥的LINUX私房菜》基础篇<br>2.<a href="https://github.com/yangyangwithgnu/use_vim_as_ide" target="_blank" rel="noopener">https://github.com/yangyangwithgnu/use_vim_as_ide</a><br>3.<a href="https://vi.stackexchange.com/questions/9028/what-is-the-command-for-select-all-in-vim-and-vsvim/9029" target="_blank" rel="noopener">https://vi.stackexchange.com/questions/9028/what-is-the-command-for-select-all-in-vim-and-vsvim/9029</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;bashrc自定义配置&quot;&gt;bashrc自定义配置&lt;/h2&gt;
&lt;p&gt;~/.bashrc文件详细内容，&lt;a href=&quot;https://github.com/mxxhcm/code/blob/master/shell/bashrc&quot; target=&quot;_blank&quot; r
      
    
    </summary>
    
      <category term="linux" scheme="http://mxxhcm.github.io/categories/linux/"/>
    
    
      <category term="linux" scheme="http://mxxhcm.github.io/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>linux stdin stdout和stderr</title>
    <link href="http://mxxhcm.github.io/2019/06/03/linux-stdin-stdout%E5%92%8Cstderr/"/>
    <id>http://mxxhcm.github.io/2019/06/03/linux-stdin-stdout和stderr/</id>
    <published>2019-06-02T16:09:27.000Z</published>
    <updated>2019-07-01T15:02:07.886Z</updated>
    
    <content type="html"><![CDATA[<h2 id="stdin-stdout-stderr">stdin, stdout, stderr</h2><p>standard input(file handle 0)：标准输入，process利用这个file handle从用户读取数据<br>standard output(file handle 1)：标准输出，process向这个file handle写normal infor，会将输出输出到屏幕<br>standard error(file handle 2)：标准错误，process向这个file hanle写error infor</p><h2 id="数据流重定向">数据流重定向</h2><blockquote><p>, &gt;&gt;, &lt;, &lt;&lt;</p></blockquote><h3 id="输出重定向">输出重定向</h3><blockquote><p>是将std output进行重定向，&gt;&gt;是将std output进行追加。如果要将std error重定向或者追加，需要使用2表示表示std error。<br>~$:ls -ld /etc/ &gt;~/etc_dir.txt</p></blockquote><h4 id="std-output-重定向">std output 重定向</h4><p>~$:find / -name ~/.bashrc &gt;find_result</p><h4 id="std-errort重定向">std errort重定向</h4><p>~$:find / -name ~/.bashrc &gt;fing_right 2&gt; find_error<br>~$:find / -name ~/.bashrc &gt; find_result 2&gt;/dev/null<br>~$:find / -name ~/.bashrc &gt;find_result 2&gt;&amp;1</p><h3 id="输入重定向">输入重定向</h3><h4 id="从键盘读入">&lt;从键盘读入</h4><p>~$:cat &gt; catfile<br>&gt;<br>&gt;<br>&gt; ctrl + D</p><h4 id="从文件中读入数据">从文件中读入数据</h4><p>cat &gt; catfile &lt; ~/.bashrc</p><h4 id="重定义输入结束符">重定义输入结束符</h4><p>cat &gt; catfile &lt;&lt; “eof”</p><h2 id="示例">示例</h2><h3 id="示例1">示例1</h3><p>~$:my_command &lt;inputfile 2&gt;errorfile | grep XYZ<br>执行my_command，<br>打开inputfile文件作为标准输入，<br>打开errorfile文件作为标准错误，<br>|重定向上述命令的结果到grep 命令</p><h3 id="示例2">示例2</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">echo "hello"</span><br><span class="line"><span class="meta">#</span> 是将"hello"给了echo的stdin的简写</span><br><span class="line">echo &lt; text.txt</span><br><span class="line"><span class="meta">#</span> 将text.txt给echo的stdin</span><br><span class="line">ps | grep 1000</span><br><span class="line"><span class="meta">#</span> |将ps的stdout给了grep的stdin</span><br><span class="line">ls . &gt; current_dir_list.txt</span><br><span class="line"><span class="meta">#</span> &gt;将ls .的stdout输出到相应文件</span><br><span class="line">ls. &gt;&gt; current_dir_list.txt</span><br><span class="line"><span class="meta">#</span> &gt;&gt;是追加，&gt;不是</span><br></pre></td></tr></table></figure><h2 id="2-1和">2&gt;&amp;1和&amp;&gt;</h2><p>2&gt;&amp;1是将stderr重定向到stdout，比如&quot;command &gt;/dev/null 2&gt;&amp;1&quot;是先将comand的stdout重定向到/dev/null中，然后将stderr重定向到stdout，因为stdout已经指向了/dev/null，所以stderr就重定向到了/dev/null。&quot;command &gt;out.txt 2&gt;&amp;1&quot;是先将stdout重定向到out.txt，然后将stderr重定向到stdout，也就是out.txt。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">command &gt; output_error.txt 2&gt;&amp;1，</span><br><span class="line">#可以将2&gt;&amp;1看成将stderr重定向到stdout，如果写成2&gt;1的话看起来像是将stderr重定向到一个名为$1$的文件。在redirece的上下文中，&amp;可以看成file descriptor的意思。为什么不写成&amp;2&gt;&amp;1，这会被解析成&amp; 和2&amp;1，第一个&amp;会被解析成后台运行，然后剩下的就是2&gt;&amp;1了。</span><br><span class="line"># 将command的stdout和stderr都输出到该文件</span><br><span class="line">command &amp;&gt; /dev/null</span><br><span class="line"># 将command的stderr和stdout输出到/dev/null，将会什么也不输出</span><br></pre></td></tr></table></figure><p>&amp;&gt;是bash的扩展，而2&gt;&amp;1是standard Bourne/POSIX shell。</p><h2 id="一个神奇的命令-dev-null">一个神奇的命令&lt;/dev/null &amp;&gt;</h2><p>/dev/null是一个神奇的文件，它代表null device，会抛弃所有写入它的数据，但是会report写操作成功了，并且它不会向读取它的任何process提供任何数据，也就是向读取它的程序发送一个EOF。<br>所以&lt;/dev/null会发送一个EOF到stdin，&amp;就是将程序放在后台运行，&amp;的详细介绍可见<a href>linux process章节</a><br>这时候如果再加一个重定向命令，就是将命令的输出重定向到某个文件中而不在stdout中显示。</p><h2 id="示例-v2">示例</h2><p>~$:nohup sslocal -c /etc/shadowsocks_v6.json &lt;/dev/null &amp;&gt;&gt;~/.log/ss-local.log &amp;  # 后台运行sslocal<br>~$:nohup /home/mxxmhh/anaconda3/bin/python <a href="http://main.py" target="_blank" rel="noopener">main.py</a> &gt;log_name 2&gt;&amp;1 &amp;</p><h2 id="参考文献">参考文献</h2><p>1.<a href="https://unix.stackexchange.com/questions/27955/the-usage-of-dev-null-in-the-command-line" target="_blank" rel="noopener">https://unix.stackexchange.com/questions/27955/the-usage-of-dev-null-in-the-command-line</a><br>2.<a href="https://stackoverflow.com/questions/3385201/confused-about-stdin-stdout-and-stderr" target="_blank" rel="noopener">https://stackoverflow.com/questions/3385201/confused-about-stdin-stdout-and-stderr</a><br>3.<a href="https://stackoverflow.com/questions/818255/in-the-shell-what-does-21-mean" target="_blank" rel="noopener">https://stackoverflow.com/questions/818255/in-the-shell-what-does-21-mean</a><br>4.<a href="https://askubuntu.com/questions/635065/what-is-the-differences-between-and-21" target="_blank" rel="noopener">https://askubuntu.com/questions/635065/what-is-the-differences-between-and-21</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;stdin-stdout-stderr&quot;&gt;stdin, stdout, stderr&lt;/h2&gt;
&lt;p&gt;standard input(file handle 0)：标准输入，process利用这个file handle从用户读取数据&lt;br&gt;
standard out
      
    
    </summary>
    
      <category term="linux" scheme="http://mxxhcm.github.io/categories/linux/"/>
    
    
      <category term="linux" scheme="http://mxxhcm.github.io/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>reinforcement learning an introduction 第6章笔记</title>
    <link href="http://mxxhcm.github.io/2019/06/02/reinforcement-learning-an-introduction-%E7%AC%AC6%E7%AB%A0%E7%AC%94%E8%AE%B0/"/>
    <id>http://mxxhcm.github.io/2019/06/02/reinforcement-learning-an-introduction-第6章笔记/</id>
    <published>2019-06-02T06:31:49.000Z</published>
    <updated>2019-08-30T03:41:47.120Z</updated>
    
    <content type="html"><![CDATA[<h2 id="td-learning">TD Learning</h2><p>TD方法是DP和MC方法的结合，像MC一样，TD可以不需要model直接从experience中学习，像DP一样，TD是bootstrap的方法。<br>本章的结构和之前一样，首先研究policy evaluation或者叫prediction问题，即给定一个policy $\pi$，估计$v_{\pi}$；然后研究control问题。DP,TD,MC方法都是使用GPI方法解control问题，不同点在于prediction问题的解法。<br>为什么叫TD？<br>因为TD更新是基于不同时间上两个estimate的估计进行的。</p><h2 id="td-prediction">TD prediction</h2><p>TD和MC都是利用采样获得的experience求解prediction问题。给定policy $\pi$下的一个experience，TD和MC方法使用该experience中出现的non-terminal state $S_t$估计$v_{\pi}$的$V$。他们的不同之处在于MC需要等到整个experience的return知道以后，把这个return当做$V(S_t)$的target，every visit MC方法的更新规则如下：<br>$$V(S_t) = V(S_t) + \alpha \left[G_t - V(S_t)\right]\tag{1}$$<br>其中$G_t$是从时刻$t$到这个episode结束的return，$\alpha$是一个常数的步长，这个方法叫做$constant-\alpha$ MC。MC方法必须等到一个episode结束，才能进行更新，因为只有这个时候$G_t$才知道。为了更方便的训练，就有了TD方法。TD方法做的改进是使用$t+1$时刻state $V(S_{t+1})$的估计值和reward $R_{t+1}$的和作为target：<br>$$V(S_t) = V(S_t) + \alpha \left[R_{t+1}+\gamma V(S_{t+1}) - V(S_t)\right]\tag{2}$$<br>如果V在变的话，是不是应该是下面的公式？？<br>$$V_{t+1}(S_t) = V_t(S_t) + \alpha \left[R_{t+1}+\gamma V_t(S_{t+1}) - V_t(S_t)\right]$$<br>即只要有了到$S_{t+1}$的transition并且接收到了reward $R_{t+1}$就可以进行上述更新。MC方法的target是$G_t$，而TD方法的target是$\gamma V(S_{t+1} + R_{t+1})$，这种TD方法叫做$TD-0$或者$one\ step\ TD$，它是$TD(\lambda)$和$n-step\ TD$的一种特殊情况。</p><h3 id="算法">算法</h3><p>下面是$TD(0)$的完整算法：<br>算法1 Tabular TD(0) for $V$<br>输入： 待评估的policy $\pi$<br>算法参数：步长$\alpha \in (0,1]$<br>初始化： $V(s), \forall s\in S^{+}，V(terminal) = 0$<br><strong>Loop</strong> for each episode<br>$\qquad$初始化$S$<br>$\qquad A\leftarrow \pi(a|S)$<br>$\qquad$<strong>Loop</strong> for each step of episode<br>$\qquad\qquad$执行action $A$，得到$S’$和$R$<br>$\qquad\qquad V(S) = V(S) + \alpha \left[R + \gamma V(S’) - V(S)\right]$<br>$\qquad\qquad$$S\leftarrow S’$<br>$\qquad$<strong>Until</strong> $S$ 是terminal state</p><p>$TD(0)$是bootstrap方法，因为它基于其他state的估计value进行更新。从第三章中我们知道：<br>\begin{align*}<br>v_{\pi}(s) &amp; = \mathbb{E}_{\pi}\left[G_t\right]\tag{3}\\<br>&amp; = \mathbb{E}_{\pi}\left[R_{t+1}+\gamma G_{t+1}| S_t = s\right]\tag{4}\\<br>&amp; = \mathbb{E}_{\pi}\left[R_{t+1}+\gamma v_{\pi}(S_{t+1})|S_t = s\right]\tag{5}\\<br>\end{align*}<br>MC使用式子$(3)$的estimate作为target，而DP使用式子$(5)$的estimate作为target。MC方法用一个sample的return代替式子$(3)$中真实的未知expected return $G_t$；DP是用$V(S_{t+1})$作为$v_{\pi}(S_{t+1})$的一个估计，因为$v_{\pi}(S_{t+1})$的真实值是不知道的。TD结合了MC的采样以及DP的bootstrap，它对式子$(4)$的tranisition进行sample，同时使用$v_{\pi}$的估计值$V$进行计算。<br><img src="/2019/06/02/reinforcement-learning-an-introduction-第6章笔记/backup_td.png" alt="backup_TD"><br>TD的backup图如图所示。TD和MC updates被称为sample updates，因为这两个算法的更新都牵涉到采样一个sample successor state，使用这个state的value和它后继的这条路上的reward计算一个backed-up value，然后根据这个值更新该state的value。sample updates和DP之类的expected updates的不同在于，sample updates使用一个sample successor进行更新，expected updates使用所有可能的successors distribution进行更新。<br>$R_{t+1} + \gamma V(S_{t+1}) - V(S_t)$可以看成一种error，衡量了$S_t$当前的estimated value $V(S_t)$和一个更好的estimated value之间的差异$R_{t+1} +\gamma V(S_{t+1})$，我们把它叫做$TD-error$，用$\delta_t$表示。$\delta_t$是$t$时刻的$TD-error$，在$t+1$时刻可用，用公式表示是：<br>$$\delta_t = R_{t+1} + \gamma V(S_{t+1}) - V(S_t) \tag{6}$$<br>如果$V$在一个episode中改变的话，那么上述公式是不是应该写成：<br>$$\delta_t = R_{t+1} + \gamma V_t(S_{t+1}) - V_t(S_t)$$<br>应该在$t$时刻，计算的TD error是用来更新$t+1$时刻的value的。如果$V$在一个episdoe中不变的话，就像MC方法一样，那么MC error可以写成TD errors的和。<br>\begin{align*}<br>G_t - V(S_t) &amp; = R_{t+1} + \gamma G_{t+1} - V(S_t) + \gamma V(S_{t+1}) - \gamma V(S_{t+1})\\<br>&amp; = R_{t+1} + \gamma V(S_{t+1}) - V(S_t) + \gamma G_{t+1} - \gamma V(S_{t+1})\\<br>&amp; = \delta_t + \gamma G_{t+1} - \gamma V(S_{t+1})\\<br>&amp; = \delta_t + \gamma(G_{t+1} - V(S_{t+1}))\\<br>&amp; = \delta_t + \gamma\delta_{t+1} + \gamma^2(G_{t+2} - V(S_{t+2}))\\<br>&amp; = \delta_t + \gamma\delta_{t+1} + \gamma^2\delta_{t+2} + \cdots + \gamma^{T-t-1}\delta_{T-1} + \gamma^{T-t}(G_T-V(S_T))\\<br>&amp; = \delta_t + \gamma\delta_{t+1} + \gamma^2\delta_{t+2} + \cdots + \gamma^{T-t-1}\delta_{T-1} + \gamma^{T-t}(0-0)\\<br>&amp; = \sum_{k=t}^{T-1} \gamma^{k-t}\delta_k \tag{7}\\<br>\end{align*}<br>如果$V$在一个episode中改变了的话，像$TD(0)$一样，这个公式就不精确成立了，如果$\alpha$足够小的话，还是近似成立的。<br>$$\delta_t = R_{t+1} + \gamma V_t(S_{t+1}) - V_t(S_t)$$<br>\begin{align*}<br>V_{t+1}(S_t) &amp;= V_t(S_t) + \alpha \left[R_{t+1}+\gamma V_t(S_{t+1}) - V_t(S_t)\right]\\<br>&amp;= V_t(S_t) + \alpha \delta_t<br>\end{align*}</p><p>\begin{align*}<br>G_t - V_t(S_t) &amp; = R_{t+1} + \gamma G_{t+1} - V_t(S_t) + \gamma V_{t+1}(S_{t}) - \gamma V_{t+1}(S_{t})\\<br>&amp; = R_{t+1} + \gamma V_{t+1}(S_{t}) - V_t(S_t) + \gamma G_{t+1}- \gamma V_{t+1}(S_{t})\\<br>&amp; = R_{t+1} + \gamma (V_t(S_t) + \alpha \delta_t) - V_t(S_t) + \gamma G_{t+1}- \gamma V_{t+1}(S_{t})\\<br>&amp; = R_{t+1} + \gamma V_t(S_t) - V_t(S_t) + \gamma \alpha \delta_t + \gamma G_{t+1}- \gamma V_{t+1}(S_{t})\\<br>\end{align*}<br>然而上面是错误的，因为$\delta_t$需要的是$V_t(S_{t+1})$<br>\begin{align*}<br>G_t - V_t(S_t) &amp; = R_{t+1} + \gamma G_{t+1} - V_t(S_t) + \gamma V_{t+1}(S_{t+1}) - \gamma V_{t+1}(S_{t+1})\\<br>&amp; = R_{t+1} + \gamma V_{t+1}(S_{t+1}) - V_t(S_t) + \gamma G_{t+1}- \gamma V_{t+1}(S_{t+1})\\<br>\end{align*}<br>\begin{align*}<br>\delta_t &amp;= R_{t+1} + \gamma V_t(S_{t+1}) - V_t(S_t)\\<br>\delta_{t+1} &amp;= R_{t+2} + \gamma V_{t+1}(S_{t+2}) - V_{t+1}(S_{t+1})\\<br>\delta_{t+2} &amp;= R_{t+3} + \gamma V_{t+2}(S_{t+3}) - V_{t+2}(S_{t+2})\\<br>\delta_{t+3} &amp;= R_{t+4} + \gamma V_{t+3}(S_{t+4}) - V_{t+3}(S_{t+3})\\<br>\end{align*}</p><p>\begin{align*}<br>&amp;\delta_t+\delta_{t+1}+\delta_{t+2}+\delta_{t+3}\\<br>= &amp;R_{t+1} + \gamma V_t(S_{t+1}) - V_t(S_t)\\<br>+&amp;R_{t+2} + \gamma V_{t+1}(S_{t+2}) - V_{t+1}(S_{t+1})\\<br>+&amp;R_{t+3} + \gamma V_{t+2}(S_{t+3}) - V_{t+2}(S_{t+2})\\<br>+&amp;R_{t+4} + \gamma V_{t+3}(S_{t+4}) - V_{t+3}(S_{t+3})\\<br>\end{align*}<br>OK。。。还是没有算出来。。</p><h3 id="td例子">TD例子</h3><p>TD的一个例子。每天下班的时候，你会估计需要多久能到家。你回家的事件和星期，天气等相关。在周五的晚上6点，下班之后，你估计需要30分钟到家。到车旁边是$6:05$，而且天快下雨了。下雨的时候会有些堵车，所以估计从现在开始大概还需要$35$分钟才能到家。十五分钟后，下了高速，这个时候你估计总共的时间是$35$分钟（包括到达车里的$5$分钟）。然后就遇到了堵车，真正到达家里的街道是$6:40$，三分钟后到家了。</p><table><thead><tr><th style="text-align:center">State</th><th style="text-align:center">Elapsed Time</th><th style="text-align:center">Predicted Time to Go</th><th style="text-align:center">Predicted Total Time</th></tr></thead><tbody><tr><td style="text-align:center">leaveing office</td><td style="text-align:center">0</td><td style="text-align:center">30</td><td style="text-align:center">30</td></tr><tr><td style="text-align:center">reach car</td><td style="text-align:center">5</td><td style="text-align:center">35</td><td style="text-align:center">40</td></tr><tr><td style="text-align:center">下高速</td><td style="text-align:center">20</td><td style="text-align:center">15</td><td style="text-align:center">35</td></tr><tr><td style="text-align:center">堵车</td><td style="text-align:center">30</td><td style="text-align:center">10</td><td style="text-align:center">40</td></tr><tr><td style="text-align:center">门口的街道</td><td style="text-align:center">40</td><td style="text-align:center">3</td><td style="text-align:center">43</td></tr><tr><td style="text-align:center">到家</td><td style="text-align:center">43</td><td style="text-align:center">0</td><td style="text-align:center">43</td></tr></tbody></table><p>rewards是每一个journey leg的elapsed times，这里我们研究的是evaluation问题，所以可以直接使用elapsed time，如果是control问题，要在elapsed times前加负号。state value是expected time。上面的第一列数值是reward，第二列是当前state的value估计值。<br>如果使用$\alpha = 1$的TD和MC方法。对于MC方法，对于$S_t$的所有state，都有：<br>\begin{align*}<br>V(S_t) &amp;= V(S_t) + (G_t - V(S_t))\\<br>&amp; = G_t \\<br>&amp; = 43<br>\end{align*}<br>对于TD方法，让$\gamma=1$，有：<br>\begin{align*}<br>V(S_t) &amp;= V_t(S_t) + \alpha (R_{t+1} +  \gamma V_t(S_{t+1}) - V(S_t))\\<br>&amp;= R_{t+1} + V_t(S_{t+1})<br>\end{align*}</p><h3 id="td-prediction的好处">TD Prediction的好处</h3><p>TD是bootstrap方法，相对于MC和DP来说，TD的好处有以下几个：</p><ol><li>相对于DP，不需要environment, reward model以及next-state probability distribution。</li><li>相对于MC，TD是online，incremental的。MC需要等到一个episode结束，而TD只需要等一个时间步（本节介绍的TD0）。</li><li>TD在table-base case可以为证明收敛，而general linear function不一定收敛。</li></ol><p>但是具体TD好还是MC好，目前还没有明确的数学上的理论证明。而实践上表明，TD往往要比constant $\alpha$ MC算法收敛的快。</p><h2 id="td-0-的优势">TD(0)的优势</h2><p>如果我们只有很少的experience的话，比如有$10$个episodes，或者有$100$个timesteps。这种情况下，我们会重复的使用这些这些experience进行训练直到算法收敛。具体方法是，给定一个approximate value function $V$，在每一个只要不是terminal state的时间$t$处，计算MC和TD增量，最后使用所有增量之和只更新value function一次。举个例子好了，假如我们有三个episdoes，<br>A,B,C<br>B,A<br>A,A<br>更新的方法是，$V(A) = V(A) + \alpha(G_1 - V(A) + G_2 - V(A) + G_{31} - V(A) + G_{32} -V(A))$<br>这种方法叫做batch updating，因为只有在一个batch完全处理完之后才进行更新，其实这个和DP挺像的，只不过DP直接利用的是environment dynamic，而我们使用的是样本。<br>在batch updating中，TD(0)一定会收敛到一个与$\alpha$无关的结果，只要$\alpha$足够下即可，同理batch constant $\alpha$ MC算法同样条件下也会收敛到一个确定的结果，只不过和batch TD结果不同而已。Normal updating的方法并没有朝着整个batch increments的方法移动，但是大概方向差不多。其实就是一个把整个batch的所有experience的increment加起来一起更新，一个是每一个experience更新一次，就这么点区别。<br>具体来说，batch TD和batch MC哪个更好一些呢？这就牵扯到他们的原理了。Batch MC的目标是最小化training set上的mse，而batch TD的目标是寻找Markov process的最大似然估计。一般来说，maximum likeliood estimate是进行参数估计的。在这里的话，TD使用mle从已有episodes中生成markov process模型的参数：从$i$到$j$的transition probatility是观测到从$i$到$j$的transition所占的百分比，对应的expected reward是观测到的rewards的均值。给出了这个model之后，如果这个模型是exactly correct的话，那么我们就可以准确的计算出value function的estimate，这个成为certainty-equivalence estimate，因为它相当于假设markov process的model是一致的，而不是approximated，一般来说，batch TD(0)收敛到cetainty-equivalence estimate。<br>从而，我们可以简单的解释以下为什么batch TD比batch MC收敛的快。因为batch TD计算的是真实的cetainty-equivalence estimate。同样的，对于non batch的TD和MC来说，虽然TD没有使用cetainty-equivalence，但是它们大概在向那个方向移动。<br>尽管cetrinty-equivalence是最优解，但是，但是，但是，cost太大了，如果有$n$个states，计算mle需要$n^2$的空间，计算value function时候，需要$n^3$的计算步数。当states太多的话，实际上并不可行，还是老老实实的使用TD把，只会用不超过$n$的空间。。</p><h2 id="td具体算法介绍">TD具体算法介绍</h2><h3 id="sarsa">Sarsa</h3><h4 id="介绍">介绍</h4><p>Sarsa是一个on-policy的 TD control算法。按照GPI的思路来，先进行policy evaluation，在进行policy improvement。首先解prediction问题，按照以下action value的$TD(0)$公式估计当前policy $\pi$下，所有action和state的$q$值$q_{\pi}(s,a)$：<br>$$Q(S_t,A_T) \leftarrow Q(S_t,A_t) + \alpha \left[R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) -Q(S_t,A_t)\right] \tag{8}$$<br>当$S_{t+1} = 0$时，$Q(S_{t+1}, A_{t+1})=0$，相应的backup diagram如下图所示。<br><img src="/2019/06/02/reinforcement-learning-an-introduction-第6章笔记/ff.png" alt="f"><br>第二步解control问题，在on-policy的算法中，不断的估计behaviour policy $\pi$的$q_{\pi}$，同时改变$\pi$朝着$q_{\pi}$更大的方向移动。Sarsa算法中，behaviour policy和target policy是一样的，在不断的改变。完整的算法如下：<br>Sarsa算法(on-policy control) 估计$Q\approx q_*$<br>对于所有$s\in S^{+}, a\in A(s)$，随机初始化$Q(s,a)$，$Q(terminal, \cdot) = 0$<br>Loop for each episode<br>$\qquad$ 获得初始状态$S$<br>$\qquad$ 使用policy（如$\epsilon$-greedy算法）根据state $S$选择当前动作$A$<br>$\qquad$ Loop for each step of episode<br>$\qquad\qquad$ 采取action，得到R和S’<br>$\qquad\qquad$ 使用policy（和上面的policy一样）根据S’选择A’<br>$\qquad\qquad Q(S,A) \leftarrow Q(S,A) + \alpha \left[R+ \gamma Q(S’,A’) - Q(S,A)\right]$<br>$\qquad\qquad S\leftarrow S’, A\leftarrow A’$<br>$\qquad$ until $S$是terminal</p><h4 id="示例">示例</h4><h3 id="q-learning">Q-learning</h3><p>$$Q(S_t,A_T) \leftarrow Q(S_t,A_t) + \alpha \left[R_{t+1} + \gamma max Q(S_{t+1}, A_{t+1}) -Q(S_t,A_t)\right]\tag{9}$$<br>这一节介绍的是off-policy的TD contrl算法，Q-learning。对于off-policy算法来说，behaviour policy用来选择action，target policy是要评估的算法。在Q-learning算法中，直接学习的就是target policy的optimal action value function $q_{*}$，和behaviour policy无关。完整的Q-learning算法如下：<br>Q-learning算法(off-policy control) 估计$\pi \approx \pi_{*}$<br>对于所有$s\in S^{+}, a\in A(s)$，随机初始化$Q(s,a)$，$Q(terminal, \cdot) = 0$<br>Loop for each episode<br>$\qquad$ 获得初始状态$S$<br>$\qquad$ Loop for each step of episode<br>$\qquad\qquad$ 使用policy（如$\epsilon$-greedy算法）根据state $S$选择当前动作$A$<br>$\qquad\qquad$ 执行action $A$，得到$R$和$S’$<br>$\qquad\qquad Q(S,A) \leftarrow Q(S,A) + \alpha \left[R+ \gamma max Q(S’,A’) - Q(S,A)\right]$<br>$\qquad\qquad S\leftarrow S’$<br>$\qquad$ until $S$是terminal</p><h3 id="expected-sarsa">Expected Sarsa</h3><p>Q-learning对所有next state-action pairs取了max操作。如果不是取max，而是取期望呢？<br>\begin{align*}<br>Q(S_t,A_T) &amp; \leftarrow Q(S_t,A_t) + \alpha \left[R_{t+1} + \gamma \mathbb{E}_{\pi}\left[ Q(S_{t+1}, A_{t+1})| S_{t+1} \right] -Q(S_t,A_t)\right]\\<br>&amp;\leftarrow Q(S_t,A_t) + \alpha \left[R_{t+1} + \gamma \sum_a\pi(a|S_{t+1})Q -Q(S_t,A_t)\right]\tag{10}<br>\end{align*}<br>其他的和Q-learning保持一致。给定next state $S_{t+1}$，算法在expectation上和sarsa移动的方向一样，所以被称为expected sarsa。这个算法可以是on-policy，但是通常它是是off-policy的。比如，on-policy的话，policy使用$\epsilon$ greedy算法，off-policy的话，behaviour policy使用stochastic policy，而target policy使用greedy算法，这其实就是Q-learning算法了。所以，Expected Sarsa实际上是对Q-learning的一个归纳，同时又有对Sarsa的改进。<br>Q-learning和Expected Sarsa的backup diagram如下所示：<br><img src="/2019/06/02/reinforcement-learning-an-introduction-第6章笔记/q_learning_and_expected_Sarsa_backup_diagram.png" alt="q_learning_and_expected_Sarsa_backup_diagram"></p><h3 id="sarsa-vs-q-learing-vs-expected-sarsa">Sarsa vs Q-learing vs Expected Sarsa</h3><p>on-policy的sarsa，policy一直在变（$\epsilon$在变），但是behaviour policy和target policy一直都是一样的。<br>而off-policy的Q-learning，target policy和behaviour policy一直都不变（可能$\epsilon$会变，但是这个不是Q-learning的重点），behaviour policy保证exploration，target policy是greedy算法。为什么这个不需要importance sampling？Importance sampling的作用是为了使用policy $b$下观察到的rewards估计policy $\pi$下的expected rewards。<br>Q(0)和Expected Sarsa(0)都没有使用importance sampling，因为在$Q(s,a)$中，action $a$已经被选择了，用哪个policy选择的是无关紧要的，TD error可以使用$Q(s’,*)$上的boostrap进行计算，而不需要behaviour policy。<br>j</p><h2 id="maximization-bias和double-learning">Maximization Bias和Double Learning</h2><p>目前介绍的所有control算法，都涉及到target polices的maximization操作。Q-learning中有greedy target policy，Sarsa的policy通常是$\epsilon$ greedy，也会牵扯到maximization。Max操作会引入一个问题，加入某一个state，它的许多action对应的$q(s,a)=0$，然后它的估计值$Q(s,a)$是不确定的，可能比$0$大，可能比$0$小，还可能就是$0$。如果使用max $Q(s,a)$的话，得到的值一定是大于等于$0$的，显然有一个positive bias，这就叫做maximization bias。</p><h3 id="maximization-bias例子">Maximization Bias例子</h3><p>给出如下的一个例子：<br><img src="/2019/06/02/reinforcement-learning-an-introduction-第6章笔记/example_6_7.png" alt="example_6_7"><br>这个MDP有四个state，A,B,C,D，C和D是terminal state，A总是start state，并且有left和right两个action，right action转换到C，reward是0,left action转换到B，reward是$0$，B有很多个actions，都是转换到$D$，但是rewards是不同，reward服从一个均值为$-0.5$，方差为$1.0$的正态分布。所以reward的期望是负的，$-0.5$。这就意味着在大量实验中，reward的均值往往是小于$0$的。<br>基于这个假设，在A处总是选择left action是很蠢的，但是因为其中有一些reward是positive，如果使用max操作的话，整个policy会倾向于选择left action，这就造成了在一些episodes中，reward是正的，但是如果在long run中，reward的期望就是负的。</p><h3 id="maximizaiton-bias出现的直观解释">Maximizaiton Bias出现的直观解释</h3><p>那么为什么会出现这种问题呢？<br>用$X1$和$X2$表示reward的两组样本数据。如下所示：<br><img src="/2019/06/02/reinforcement-learning-an-introduction-第6章笔记/maximization_bias.png" alt="maximization_bias"><br>在$X1$这组样本中，样本均值是$-0.43$，X2样本均值是$-0.36$。在增量式计算样本均值$\mu$时，得到的最大样本均值的期望是$0.09$，而实际上计算出来的期望的最大值$\mathbb{E}(X)$是$-0.36$。要使用$\mathbb{E} \left[max\ (\mu)\right]$估计$max\ \mathbb{E}(X)$，显然它们的差距有点大，$max(\mu)$是$max E(X)$的有偏估计。也就是说使用$max Q(s’,a’)$更新$Q(s,a)$时，$Q(s,a)$并没有朝着它的期望$-0.5$移动。估计这只是一个直观的解释，严格的证明可以从论文中找。</p><h3 id="如何解决maximization-bias问题">如何解决Maximization Bias问题</h3><p>那么怎么解决这个问题呢，就是同时学习两个$Q$函数$Q_1, Q_2$，这两个$Q$函数的地位是一样的，每次随机选择一个选择action，然后更新另一个。证明的话，Van Hasselt证明了$\mathbb{E}(Q_2(s’,a*)\le max\ Q_1(s’,a*)$，也就是说$Q_1(s,a)$不再使用它自己的max value进行更新了。<br>下面是$Q$-learning和Double $Q$-learning在训练过程中在A处选择left的统计：<br><img src="/2019/06/02/reinforcement-learning-an-introduction-第6章笔记/q_learning_vs_double_q_learning.png" alt="q_learning_vs_double_q_learning"><br>可以看出来，Double $Q$-learning要比$Q$-learning收敛的快和好。<br>当然，Sarsa和Expected Sarsa也有maximization bias问题，然后有对应的double版本，Double Sarsa和Double Expected Sarsa。</p><h2 id="afterstates">Afterstates</h2><p>之前介绍了state value function和action value function。这里介绍一个afterstate value function，afterstate value function就是在某个state采取了某个action之后再进行评估，一开始我想这步就是action value function。事实上不是的，action value function估计的是$Q(s,a)$，重点是state和action这些pair，对于afterstate value来说，可能有很多个state和action都能到同一个next state，这个时候它们的作用是一样的，因为我们估计的是next state的value。<br>象棋就是一个这样的例子。。这里只是介绍一下，还有很多各种各样特殊的形式，它们可以用来解决各种各样的特殊问题。具体可以自己多了解一下。</p><h2 id="总结">总结</h2><p>这一章主要介绍了最简单的一种TD方法，one-step，tabular以及model-free。接下来的两章会介绍一些n-step的TD方法，可以和MC方法联系起来，以及包含一个模型的方法，和DP联系起来。在第二部分的时候，会将tabular的TD扩展到function approximation的形式，和deep learning以及artificial neural networks联系起来。</p><h2 id="参考文献">参考文献</h2><p>1.《reinforcement learning an introduction》第二版<br>2.<a href="https://stats.stackexchange.com/a/297892" target="_blank" rel="noopener">https://stats.stackexchange.com/a/297892</a><br>3.<a href="https://towardsdatascience.com/double-q-learning-the-easy-way-a924c4085ec3" target="_blank" rel="noopener">https://towardsdatascience.com/double-q-learning-the-easy-way-a924c4085ec3</a><br>4.<a href="https://stats.stackexchange.com/a/347090/254953" target="_blank" rel="noopener">https://stats.stackexchange.com/a/347090/254953</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;td-learning&quot;&gt;TD Learning&lt;/h2&gt;
&lt;p&gt;TD方法是DP和MC方法的结合，像MC一样，TD可以不需要model直接从experience中学习，像DP一样，TD是bootstrap的方法。&lt;br&gt;
本章的结构和之前一样，首先研究policy
      
    
    </summary>
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>reinforcment learning的一些术语</title>
    <link href="http://mxxhcm.github.io/2019/05/29/reinforcement-learning-some-terms/"/>
    <id>http://mxxhcm.github.io/2019/05/29/reinforcement-learning-some-terms/</id>
    <published>2019-05-29T09:59:28.000Z</published>
    <updated>2019-08-08T02:04:40.808Z</updated>
    
    <content type="html"><![CDATA[<h2 id="分类方式">分类方式</h2><h3 id="online-vs-offline">online vs offline</h3><p>online方法中训练数据一直在不断增加，基本上强化学习都是online的，而监督学习是offline的。</p><h3 id="on-policy-vs-off-policy">on-policy vs off-policy</h3><p>behaviour policy是采样的policy。<br>target policy是要evaluation的policy。<br>behaviour policy和target policy是不是相同的，相同的就是on-policy，不同的就是off-policy，带有replay buffer的都是off-policy的方法。</p><h2 id="bootstrap">bootstrap</h2><p>当前value的计算是否基于其他value的估计值。<br>常见的bootstrap算法有DP，TD-gamma<br>MC算法不是bootstrap算法。</p><h2 id="value-based-vs-policy-gradient-vs-actor-critic">value-based vs policy gradient vs actor-critic</h2><h3 id="value-based">value-based</h3><p>values-based方法主要有policy iteration和value iteration。policy iteration又分为policy evaluation和policy improvement。<br>给出一个任务，如果可以使用value-based。随机初始化一个policy，然后可以计算这个policy的value function，这就叫做policy evaluation，然后根据这个value function，可以对policy进行改进，这叫做policy improvement，可以证明policy一定会更好。policy evaluation和policy improvement交替迭代，在线性case下，收敛性是可以证明的，在non-linear情况下，就不一定了。<br>policy iteraion中，policy evaluation每一次都要进行收敛后才进行policy improvemetn，如果policy evalution只进行一次，然后就进行一次policy improvemetn的话，也就是policy evalution的粒度变小后，就是value iteration。</p><h3 id="policy-gradient">policy gradient</h3><p>value-based方法只适用于discrete action space，对于contionous action space的话，就无能为力了。这个时候就有了policy gradient，给出一个state，policy gradient给出一个policy直接计算出相应的action，然后给出一个衡量action好坏的指标，直接对policy的参数求导，最后收敛之后就求解出一个使用与contionous的policy</p><h3 id="actor-critic">actor-critic</h3><p>如果policy gradient的metrics选择使用value function，一般是aciton value function的话，我们把这个value function叫做critic，然后把policy叫做actor。通过value funciton Q对policy的参数求导进行优化。<br>critic跟policy没有关系，而critic指导actor的训练，通过链式法则实现。critic对a求偏导，a对actor的参数求偏导。</p><h2 id="参考文献">参考文献</h2><p>1.<a href="https://stats.stackexchange.com/questions/897/online-vs-offline-learning" target="_blank" rel="noopener">https://stats.stackexchange.com/questions/897/online-vs-offline-learning</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;分类方式&quot;&gt;分类方式&lt;/h2&gt;
&lt;h3 id=&quot;online-vs-offline&quot;&gt;online vs offline&lt;/h3&gt;
&lt;p&gt;online方法中训练数据一直在不断增加，基本上强化学习都是online的，而监督学习是offline的。&lt;/p&gt;
&lt;h3 i
      
    
    </summary>
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="policy gradient" scheme="http://mxxhcm.github.io/tags/policy-gradient/"/>
    
      <category term="on-policy" scheme="http://mxxhcm.github.io/tags/on-policy/"/>
    
      <category term="online" scheme="http://mxxhcm.github.io/tags/online/"/>
    
      <category term="offline" scheme="http://mxxhcm.github.io/tags/offline/"/>
    
      <category term="off-policy" scheme="http://mxxhcm.github.io/tags/off-policy/"/>
    
      <category term="bootstrap" scheme="http://mxxhcm.github.io/tags/bootstrap/"/>
    
      <category term="model-free" scheme="http://mxxhcm.github.io/tags/model-free/"/>
    
      <category term="model-based" scheme="http://mxxhcm.github.io/tags/model-based/"/>
    
      <category term="value based" scheme="http://mxxhcm.github.io/tags/value-based/"/>
    
      <category term="actor critic" scheme="http://mxxhcm.github.io/tags/actor-critic/"/>
    
  </entry>
  
  <entry>
    <title>skills</title>
    <link href="http://mxxhcm.github.io/2019/05/29/skills/"/>
    <id>http://mxxhcm.github.io/2019/05/29/skills/</id>
    <published>2019-05-29T04:24:41.000Z</published>
    <updated>2019-06-12T02:51:28.660Z</updated>
    
    <content type="html"><![CDATA[<h2 id="累加">累加</h2><h3 id="简单介绍">简单介绍</h3><p>今天在看Reinforcment Learning: an Introduction第五章的时候，写了figure_5_4的代码，然后跟github上作者写出来的效率差了太多。<br>最后对比了一下代码，发现了原因，是因为做了太多重复运算。</p><h3 id="代码示例">代码示例</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="comment"># 目的，计算不断更新的两个列表的乘积的和</span></span><br><span class="line">numbers = <span class="number">100000</span></span><br><span class="line">a = []</span><br><span class="line">b = []</span><br><span class="line">c1 = []</span><br><span class="line"></span><br><span class="line">begin_time = time.time() </span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(numbers):</span><br><span class="line">    a.append(i<span class="number">-1</span>)</span><br><span class="line">    b.append(i+<span class="number">1</span>)</span><br><span class="line">    c1.append(np.sum(np.multiply(a, b)))</span><br><span class="line"><span class="comment"># print(c1)</span></span><br><span class="line">end_time = time.time()</span><br><span class="line">print(<span class="string">"Total time: "</span>, end_time - begin_time)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 我用的是上面的代码，然后，，，，太多重复运算，效率惨不忍睹</span></span><br><span class="line"></span><br><span class="line">a = []</span><br><span class="line">b = []</span><br><span class="line">c2 = []</span><br><span class="line"></span><br><span class="line">begin_time = time.time() </span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(numbers):</span><br><span class="line">    a.append(i<span class="number">-1</span>)</span><br><span class="line">    b.append(i+<span class="number">1</span>)</span><br><span class="line">    c2.append(a[i]*b[i])</span><br><span class="line">results = np.add.accumulate(c2)</span><br><span class="line"><span class="comment"># print(results)</span></span><br><span class="line">end_time = time.time()</span><br><span class="line">print(<span class="string">"Total time: "</span>, end_time - begin_time)</span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">在100000的量级上，差了几万倍出来。。。</span></span><br><span class="line"><span class="string">Total time:  450.6051049232483</span></span><br><span class="line"><span class="string">Total time:  0.03314399719238281</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure><h2 id="参考文献">参考文献</h2><p>1.<a href="https://github.com/ShangtongZhang/reinforcement-learning-an-introduction/tree/master/chapter05" target="_blank" rel="noopener">https://github.com/ShangtongZhang/reinforcement-learning-an-introduction/tree/master/chapter05</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;累加&quot;&gt;累加&lt;/h2&gt;
&lt;h3 id=&quot;简单介绍&quot;&gt;简单介绍&lt;/h3&gt;
&lt;p&gt;今天在看Reinforcment Learning: an Introduction第五章的时候，写了figure_5_4的代码，然后跟github上作者写出来的效率差了太多。&lt;br&gt;

      
    
    </summary>
    
      <category term="python" scheme="http://mxxhcm.github.io/categories/python/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>linux-ubuntu 18.04 gnome-shell自定义操作</title>
    <link href="http://mxxhcm.github.io/2019/05/22/linux-ubuntu-18-04-gnome-shell-%E8%87%AA%E5%AE%9A%E4%B9%89%E6%93%8D%E4%BD%9C/"/>
    <id>http://mxxhcm.github.io/2019/05/22/linux-ubuntu-18-04-gnome-shell-自定义操作/</id>
    <published>2019-05-22T13:22:29.000Z</published>
    <updated>2019-07-30T09:02:01.223Z</updated>
    
    <content type="html"><![CDATA[<h2 id="dock-配置">dock 配置</h2><h3 id="基本设置">基本设置</h3><p>打开Settings &gt;&gt; Dock，可以设置dock的位置和大小，以及自动隐藏。这些是ubuntu安装的默认配置。</p><h3 id="dconf安装">dconf安装</h3><p>为了更多的设置，需要安装dconf-editor，dconf相当于windows注册表的gnome，存储应用程序设置的gnome技术。<br>~\$:sudo apt install dconf-tools<br>按下Win键，搜索dconfig-editor，打开它。<br>找到org&gt;&gt;gnome&gt;&gt;shell&gt;&gt;extensions&gt;&gt;dash-to-dock，然后就可以修改相应的配置了。也可以在命令行中进行相应的设置，这里就不说了，可以查看参考文献尝试。</p><h2 id="ubuntu-18-04合上笔记本盖子后不挂起">ubuntu 18.04合上笔记本盖子后不挂起</h2><p>~\$:sudo apt install gnome-tweak-tool<br>~\$:gnome-tweaks<br>找到Power选项，设置Suspend when lapto lid is closed为OFF。[6]</p><h2 id="显示cpu和gpu温度">显示cpu和gpu温度</h2><h3 id="安装lm-sensors">安装lm-sensors</h3><p>~\$:sudo apt-get install lm-sensors<br>然后执行以下命令进行配置：<br>~\$:sudo sensors-detect<br>执行sensors命令获得各项硬件的温度<br>~\$:sensors</p><h2 id="安装gnome-shell">安装gnome-shell</h2><h3 id="安装gnome-tweak-tool">安装gnome tweak tool</h3><p>~\$:sudo apt install gnome-tweak-tool<br>~\$:gnome-shell --version<br>gnome tweak用来查看本地的gnome 插件。</p><h3 id="从ubuntu-仓库安装extensions">从ubuntu 仓库安装extensions</h3><p>ubuntu 提供了gnome-shell-extensions包，该包中有部分gnome扩展。然后可以使用gnome tweaks查看已经安装的程序。<br>~\$:sudo apt install gnome-shell-extensions</p><h3 id="在浏览器上安装gnome-shell-integration插件">在浏览器上安装gnome shell integration插件</h3><p>在firfox或者chrome上安装相应的gnome shell integration插件，直接google搜索安装就行了。<br>这个时候是不能添加插件的，因为还缺少一个东西，叫做native host connector<br>这种方法和从ubuntu仓库中装extension的不同之处是，ubuntu包中的扩展是固定的一部分，这中方法可以自定义安装。<br>安装完之后可以直接在浏览器的gnome shell integration插件上查看在浏览器上安装的gnome shell扩展，也可以使用gnome tweaks查看浏览器上安装的shell extensions。</p><h4 id="安装chrome-gnome-shell-native-host-connector">安装chrome-gnome-shell native host connector</h4><p>执行以下命令进行安装，chrome-gnome-shell并不是代表chrome浏览器的意思，用任何浏览器都要执行以下命令<br>~\$:sudo apt install chrome-gnome-shell<br>查看gnome shell版本<br>~\$:gnome-shell --version</p><h4 id="安装浏览器附加组件">安装浏览器附加组件</h4><h5 id="浏览器中安装">浏览器中安装</h5><p>直接打开gnome shell extensions图形化界面进行搜索安装</p><h5 id="命令行安装">命令行安装</h5><p>搜索<br>~\$:sudo apt search gnome-shell-extension<br>安装<br>~\$:sudo apt install gnome-shell-extension-package-name</p><h3 id="插件推荐">插件推荐</h3><ul><li>Coverflow Alt-Tab 按alt tab切换程序效果[7]</li></ul><h2 id="修改主题">修改主题</h2><p>下载系统主题文件，解压缩，放置在/usr/share/themes文件夹下。然后在tweaks中的Apperance选项修改。<br>下载鼠标和图标主题，放置在/usr/share/icons文件夹下。</p><h2 id="参考文献">参考文献</h2><p>1.<a href="https://linuxconfig.org/how-to-customize-dock-panel-on-ubuntu-18-04-bionic-beaver-linux" target="_blank" rel="noopener">https://linuxconfig.org/how-to-customize-dock-panel-on-ubuntu-18-04-bionic-beaver-linux</a><br>2.<a href="https://zhuanlan.zhihu.com/p/37852274" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/37852274</a><br>3.<a href="https://askubuntu.com/questions/15832/how-do-i-get-the-cpu-temperature" target="_blank" rel="noopener">https://askubuntu.com/questions/15832/how-do-i-get-the-cpu-temperature</a><br>4.<a href="https://linuxhint.com/install_gnome3_extensions_ubuntu_1804/" target="_blank" rel="noopener">https://linuxhint.com/install_gnome3_extensions_ubuntu_1804/</a><br>5.<a href="https://linux.cn/article-9447-1.html" target="_blank" rel="noopener">https://linux.cn/article-9447-1.html</a><br>6.<a href="https://askubuntu.com/a/1062401" target="_blank" rel="noopener">https://askubuntu.com/a/1062401</a><br>7.<a href="https://zhuanlan.zhihu.com/p/37852274" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/37852274</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;dock-配置&quot;&gt;dock 配置&lt;/h2&gt;
&lt;h3 id=&quot;基本设置&quot;&gt;基本设置&lt;/h3&gt;
&lt;p&gt;打开Settings &amp;gt;&amp;gt; Dock，可以设置dock的位置和大小，以及自动隐藏。这些是ubuntu安装的默认配置。&lt;/p&gt;
&lt;h3 id=&quot;dconf安
      
    
    </summary>
    
      <category term="linux" scheme="http://mxxhcm.github.io/categories/linux/"/>
    
    
      <category term="linux" scheme="http://mxxhcm.github.io/tags/linux/"/>
    
      <category term="ubuntu" scheme="http://mxxhcm.github.io/tags/ubuntu/"/>
    
  </entry>
  
  <entry>
    <title>DNC</title>
    <link href="http://mxxhcm.github.io/2019/05/21/DNC/"/>
    <id>http://mxxhcm.github.io/2019/05/21/DNC/</id>
    <published>2019-05-21T01:07:41.000Z</published>
    <updated>2019-05-22T06:17:46.241Z</updated>
    
    <content type="html"><![CDATA[<h2 id="摘要">摘要</h2><h2 id="引言">引言</h2><p>现代计算机将memory和computation分开，使用处理器进行计算，处理器使用可访问的memory存取数。这样子的好处是可以使用extensible storage写入新信息，可以将memory中的内容当做variables。Variables对于算法的通用性很有用，对于不同的数据，不需要更改算法操作的地址，只需要更改变量的取值即可。而neural network的computation和memory是通过network的weights和neuron activity耦合在一起的。如果memory需要增加的话，networks不能动态增加新的storage，也不能独立的学习network的参数。</p><p>这篇文章中作者提出了differentiable neural computer(DNC)–带可读写external memory的network，解决network不能表示variable和数据结构的问题。整个system是可导的，可以把DNC的memory看做RAM，把network看做CPU。<br>DNC有一个$N\times W$大小的memory matrix $M$，使用可导的attention mechanism，确定在这个memory上的distributions，也就是我们说的weighting（加权）,代表相应的操作在该位置上的权重。DNC提供了三种操作，查询，读和写，对应了三种不同的attention，使用三个head（头）,read head（读头），write head（写头）,lookup head（查找）实现对memory的相应操作。</p><h2 id="参考文献">参考文献</h2><p>1.<a href="https://www.gwern.net/docs/rl/2016-graves.pdf" target="_blank" rel="noopener">https://www.gwern.net/docs/rl/2016-graves.pdf</a><br>2.<a href="http://people.idsia.ch/~rupesh/rnnsymposium2016/slides/graves.pdf" target="_blank" rel="noopener">http://people.idsia.ch/~rupesh/rnnsymposium2016/slides/graves.pdf</a><br>3.<a href="https://deepmind.com/blog/differentiable-neural-computers/" target="_blank" rel="noopener">https://deepmind.com/blog/differentiable-neural-computers/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;摘要&quot;&gt;摘要&lt;/h2&gt;
&lt;h2 id=&quot;引言&quot;&gt;引言&lt;/h2&gt;
&lt;p&gt;现代计算机将memory和computation分开，使用处理器进行计算，处理器使用可访问的memory存取数。这样子的好处是可以使用extensible storage写入新信息，可以将mem
      
    
    </summary>
    
      <category term="机器学习" scheme="http://mxxhcm.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://mxxhcm.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow contrib vs layers vs nn</title>
    <link href="http://mxxhcm.github.io/2019/05/18/tensorflow-nn-layers-contrib/"/>
    <id>http://mxxhcm.github.io/2019/05/18/tensorflow-nn-layers-contrib/</id>
    <published>2019-05-18T07:58:59.000Z</published>
    <updated>2019-07-25T12:39:48.109Z</updated>
    
    <content type="html"><![CDATA[<h2 id="tf-contrib">tf.contrib</h2><p>根据tensorflow官网的说法，tf.contrib模块中包含了易修改的测试代码，</p><blockquote><p>contrib module containing volatile or experimental code.</p></blockquote><p>当其中的某一个模块完成的时候，就会从contrib模块中移除。为了保持对历史版本的兼容性，可能这几个模块会存在同一个函数的不同实现。</p><h2 id="tf-nn-tf-layers和tf-contrib">tf.nn,tf.layers和tf.contrib</h2><p>tf.nn中是low-level的op<br>tf.layers是high-level的op<br>而tf.contrib中的是非正式版本的实现，在后续版本中可能会被弃用。</p><h2 id="tf-nn-conv2d-vs-tf-layers-conv2d">tf.nn.conv2d vs tf.layers.conv2d</h2><h3 id="api">API</h3><h4 id="tf-layer-conv2d">tf.layer.conv2d</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">tf.layers.conv2d(</span><br><span class="line">    inputs, </span><br><span class="line">    filters, </span><br><span class="line">    kernel_size, </span><br><span class="line">    strides=(<span class="number">1</span>, <span class="number">1</span>), </span><br><span class="line">    padding=<span class="string">'valid'</span>, </span><br><span class="line">    data_format=<span class="string">'channels_last'</span>, </span><br><span class="line">    dilation_rate=(<span class="number">1</span>, <span class="number">1</span>), </span><br><span class="line">    activation=<span class="literal">None</span>, </span><br><span class="line">    use_bias=<span class="literal">True</span>, </span><br><span class="line">    kernel_initializer=<span class="literal">None</span>, </span><br><span class="line">    bias_initializer=tf.zeros_initializer(), </span><br><span class="line">    kernel_regularizer=<span class="literal">None</span>, </span><br><span class="line">    bias_regularizer=<span class="literal">None</span>, </span><br><span class="line">    activity_regularizer=<span class="literal">None</span>, </span><br><span class="line">    trainable=<span class="literal">True</span>, </span><br><span class="line">    name=<span class="literal">None</span>, </span><br><span class="line">    reuse=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h4 id="tf-nn-conv2d">tf.nn.conv2d</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.conv2d(</span><br><span class="line">    input, </span><br><span class="line">    filter, </span><br><span class="line">    strides, </span><br><span class="line">    padding, </span><br><span class="line">    use_cudnn_on_gpu=<span class="literal">None</span>, </span><br><span class="line">    data_format=<span class="literal">None</span>, </span><br><span class="line">    name=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="nn-conv2d-vs-layers-conv2d">nn.conv2d vs layers.conv2d</h3><p>tf.nn.conv2d需要手动创建filter的tensor，传入filter的参数[kernel_height, kernel_width, in_channels, num_filters]。<br>tf.layer.conv2d需要传入filter的维度即可。</p><p>对于tf.nn.conv2d，<br>filter:和input的type一样，是一个4D的tensor，shape为[filter_height, filter_width, in_channels, out_channels]<br>对于tf.layers.conv2d，<br>filters:是整数，是需要多少个filters。</p><p>可以使用tf.nn.conv2d来加载一个pretrained model，使用tf.layers.conv2d从头开始训练一个model。</p><h3 id="用法">用法</h3><h4 id="tf-layers-conv2d">tf.layers.conv2d</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Convolution Layer with 32 filters and a kernel size of 5</span></span><br><span class="line">conv1 = tf.layers.conv2d(x, <span class="number">32</span>, <span class="number">5</span>, activation=tf.nn.relu) </span><br><span class="line"><span class="comment"># Max Pooling (down-sampling) with strides of 2 and kernel size of 2</span></span><br><span class="line">conv1 = tf.layers.max_pooling2d(conv1, <span class="number">2</span>, <span class="number">2</span>)</span><br></pre></td></tr></table></figure><h4 id="tf-nn-conv2d-v2">tf.nn.conv2d</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">strides = <span class="number">1</span></span><br><span class="line"><span class="comment"># Weights matrix looks like: [kernel_size(=5), kernel_size(=5), input_channels (=3), filters (= 32)]</span></span><br><span class="line"><span class="comment"># Similarly bias = looks like [filters (=32)]</span></span><br><span class="line">out = tf.nn.conv2d(input, weights, padding=<span class="string">"SAME"</span>, strides = [<span class="number">1</span>, strides, strides, <span class="number">1</span>])</span><br><span class="line">out = tf.nn.bias_add(out, bias)</span><br><span class="line">out = tf.nn.relu(out)</span><br></pre></td></tr></table></figure><h2 id="参考文献">参考文献</h2><p>1.<a href="https://www.tensorflow.org/api_docs/python/tf/contrib" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/contrib</a><br>2.<a href="https://stackoverflow.com/questions/48001759/what-is-right-batch-normalization-function-in-tensorflow" target="_blank" rel="noopener">https://stackoverflow.com/questions/48001759/what-is-right-batch-normalization-function-in-tensorflow</a><br>3.<a href="https://stackoverflow.com/a/48003210" target="_blank" rel="noopener">https://stackoverflow.com/a/48003210</a><br>4.<a href="https://stackoverflow.com/questions/42785026/tf-nn-conv2d-vs-tf-layers-conv2d" target="_blank" rel="noopener">https://stackoverflow.com/questions/42785026/tf-nn-conv2d-vs-tf-layers-conv2d</a><br>5.<a href="https://stackoverflow.com/a/53683545" target="_blank" rel="noopener">https://stackoverflow.com/a/53683545</a><br>6.<a href="https://stackoverflow.com/a/45308609" target="_blank" rel="noopener">https://stackoverflow.com/a/45308609</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;tf-contrib&quot;&gt;tf.contrib&lt;/h2&gt;
&lt;p&gt;根据tensorflow官网的说法，tf.contrib模块中包含了易修改的测试代码，&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;contrib module containing volatile or
      
    
    </summary>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/categories/tensorflow/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow rnn</title>
    <link href="http://mxxhcm.github.io/2019/05/18/tensorflow-rnn/"/>
    <id>http://mxxhcm.github.io/2019/05/18/tensorflow-rnn/</id>
    <published>2019-05-18T07:55:34.000Z</published>
    <updated>2019-05-19T11:40:49.946Z</updated>
    
    <content type="html"><![CDATA[<h2 id="常见cell和函数">常见Cell和函数</h2><ul><li>tf.nn.rnn_cell.BasicRNNCell: 最基本的RNN cell.</li><li>tf.nn.rnn_cell.LSTMCell: LSTM cell</li><li>tf.nn.rnn_cell.LSTMStateTuple: tupled LSTM cell</li><li>tf.nn.rnn_cell.MultiRNNCell: 多层Cell</li><li>tf.nn.rnn_cell.DropoutCellWrapper: 给Cell加上dropout</li><li>tf.nn.dynamic_rnn: 动态rnn</li><li>tf.nn.static_rnn: 静态rnn</li></ul><h2 id="basicrnncell">BasicRNNCell</h2><h3 id="api">API</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">__init__(</span><br><span class="line">    num_units,</span><br><span class="line">    activation=<span class="literal">None</span>,</span><br><span class="line">    reuse=<span class="literal">None</span>,</span><br><span class="line">    name=<span class="literal">None</span>,</span><br><span class="line">    dtype=<span class="literal">None</span>,</span><br><span class="line">    **kwargs</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="示例">示例</h3><p><a href>完整代码地址</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">   myrnn = rnn.BasicRNNCell(rnn_size,activation=tf.nn.relu)</span><br><span class="line">   zero_state = myrnn.zero_state(batch_size, dtype=tf.float32)</span><br><span class="line">   outputs, states = rnn.static_rnn(myrnn, x, initial_state=zero_state, dtype=tf.float32)</span><br><span class="line"><span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure><h3 id="其他">其他</h3><p>TF 2.0将会弃用，等价于tf.keras.layers.SimpleRNNCell()</p><h2 id="lstmcell">LSTMCell</h2><h3 id="api-v2">API</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">__init__(</span><br><span class="line">    num_units, <span class="comment"># 隐藏层的大小</span></span><br><span class="line">    use_peepholes=<span class="literal">False</span>, <span class="comment"># </span></span><br><span class="line">    cell_clip=<span class="literal">None</span>,</span><br><span class="line">    initializer=<span class="literal">None</span>, <span class="comment"># 权重的初始化构造器</span></span><br><span class="line">    num_proj=<span class="literal">None</span>,</span><br><span class="line">    proj_clip=<span class="literal">None</span>,</span><br><span class="line">    num_unit_shards=<span class="literal">None</span>,</span><br><span class="line">    num_proj_shards=<span class="literal">None</span>,</span><br><span class="line">    forget_bias=<span class="number">1.0</span>,</span><br><span class="line">    state_is_tuple=<span class="literal">True</span>, <span class="comment"># c_state和m_state的元组</span></span><br><span class="line">    activation=<span class="literal">None</span>,</span><br><span class="line">    reuse=<span class="literal">None</span>,</span><br><span class="line">    name=<span class="literal">None</span>,</span><br><span class="line">    dtype=<span class="literal">None</span>,</span><br><span class="line">    **kwargs</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="示例-v2">示例</h3><p><a href>完整代码地址</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">lstm = rnn.BasicLSTMCell(lstm_size, forget_bias=<span class="number">1</span>, state_is_tuple=<span class="literal">True</span>)</span><br><span class="line">   zero_state = lstm.zero_state(batch_size, dtype=tf.float32)</span><br><span class="line">   outputs, states = rnn.static_rnn(lstm, x, initial_state=zero_state, dtype=tf.float32)</span><br><span class="line"><span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure><h3 id="其他-v2">其他</h3><p>TF 2.0将会弃用，等价于tf.keras.layers.LSTMCell</p><h2 id="lstmstatetuple">LSTMStateTuple</h2><p>和LSTMCell一样，只不过state用的是tuple。</p><h3 id="其他-v3">其他</h3><p>TF 2.0将会弃用，等价于tf.keras.layers.LSTMCell</p><h2 id="multirnncell">MultiRNNCell</h2><p>这个类可以实现多层RNN。</p><h3 id="api-v3">API</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">__init__(</span><br><span class="line">    cells,</span><br><span class="line">    state_is_tuple=<span class="literal">True</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="示例-v3">示例</h3><h4 id="代码1">代码1</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">num_units = [<span class="number">128</span>, <span class="number">64</span>]</span><br><span class="line">cells = [BasicLSTMCell(num_units=n) <span class="keyword">for</span> n <span class="keyword">in</span> num_units]</span><br><span class="line">stacked_rnn_cell = MultiRNNCell(cells)</span><br><span class="line">outputs, state = tf.nn.dynamic_rnn(cell=stacked_rnn_cell,</span><br><span class="line">                                   inputs=data,</span><br><span class="line">                                   dtype=tf.float32)</span><br></pre></td></tr></table></figure><h4 id="代码2">代码2</h4><p><a href>完整代码地址</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">   lstm_cell = rnn.BasicLSTMCell(lstm_size, forget_bias=<span class="number">1</span>, state_is_tuple=<span class="literal">True</span>)</span><br><span class="line">   cell = rnn.MultiRNNCell([lstm_cell]*layers, state_is_tuple=<span class="literal">True</span>)</span><br><span class="line">   state = cell.zero_state(batch_size, dtype=tf.float32)</span><br><span class="line">   outputs = []</span><br><span class="line">   <span class="keyword">with</span> tf.variable_scope(<span class="string">"Multi_Layer_RNN"</span>, reuse=reuse):</span><br><span class="line">       <span class="keyword">for</span> time_step <span class="keyword">in</span> range(time_steps):</span><br><span class="line">           <span class="keyword">if</span> time_step &gt; <span class="number">0</span>:</span><br><span class="line">               tf.get_variable_scope().reuse_variables()</span><br><span class="line">           </span><br><span class="line">           cell_outputs, state = cell(x[time_step], state)</span><br><span class="line">           outputs.append(cell_outputs)</span><br><span class="line"><span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure><h3 id="其他-v4">其他</h3><p>TF 2.0将会弃用，等价于tf.keras.layers.StackedRNNCells</p><h2 id="dropoutcellwrapper">DropoutCellWrapper</h2><h3 id="api-v4">API</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">__init__(</span><br><span class="line">    cell, <span class="comment"># </span></span><br><span class="line">    input_keep_prob=<span class="number">1.0</span>,</span><br><span class="line">    output_keep_prob=<span class="number">1.0</span>,</span><br><span class="line">    state_keep_prob=<span class="number">1.0</span>,</span><br><span class="line">    variational_recurrent=<span class="literal">False</span>,</span><br><span class="line">    input_size=<span class="literal">None</span>,</span><br><span class="line">    dtype=<span class="literal">None</span>,</span><br><span class="line">    seed=<span class="literal">None</span>,</span><br><span class="line">    dropout_state_filter_visitor=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="示例-v4">示例</h3><p><a href>完整代码地址</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">   lstm_cell = rnn.BasicLSTMCell(lstm_size, forget_bias=<span class="number">1</span>, state_is_tuple=<span class="literal">True</span>)</span><br><span class="line">   lstm_cell = rnn.DropoutWrapper(lstm_cell, output_keep_prob=<span class="number">0.9</span>)</span><br><span class="line">   cell = rnn.MultiRNNCell([lstm_cell]*layers, state_is_tuple=<span class="literal">True</span>)</span><br><span class="line">   state = cell.zero_state(batch_size, dtype=tf.float32)</span><br><span class="line">   outputs = []</span><br><span class="line">   <span class="keyword">with</span> tf.variable_scope(<span class="string">"Multi_Layer_RNN"</span>):</span><br><span class="line">       <span class="keyword">for</span> time_step <span class="keyword">in</span> range(time_steps):</span><br><span class="line">           <span class="keyword">if</span> time_step &gt; <span class="number">0</span>:</span><br><span class="line">               tf.get_variable_scope().reuse_variables()</span><br><span class="line">           cell_outputs, state = cell(x[time_step], state)</span><br><span class="line">           outputs.append(cell_outputs)</span><br><span class="line"><span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure><h3 id="其他-v5">其他</h3><h2 id="static-rnn">static_rnn</h2><h3 id="api-v5">API</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.static_rnn(</span><br><span class="line">    cell, <span class="comment"># RNNCell的具体对象</span></span><br><span class="line">    inputs, <span class="comment"># 输入，长度为T的输入列表，列表中每一个Tensor的shape都是[batch_size, input_size]</span></span><br><span class="line">    initial_state=<span class="literal">None</span>, <span class="comment"># rnn的初始状态，如果cell.state_size是整数，它的shape需要是[batch_size, cell.state_size]，如果cell.state_size是元组，那么终究会是一个tensors的元组，[batch_size, s] for s in cell.state_size</span></span><br><span class="line">    dtype=<span class="literal">None</span>, <span class="comment"># </span></span><br><span class="line">    sequence_length=<span class="literal">None</span>, <span class="comment"># </span></span><br><span class="line">    scope=<span class="literal">None</span></span><br><span class="line">)</span><br><span class="line"><span class="comment"># 最简单形式的RNN，就是该API的参数都是用默认值，给定cell和inputs，相当于做了以下操作：</span></span><br><span class="line"><span class="comment">#    state = cell.zero_state(...)</span></span><br><span class="line"><span class="comment">#    outputs = []</span></span><br><span class="line"><span class="comment">#    for input_ in inputs:</span></span><br><span class="line"><span class="comment">#      output, state = cell(input_, state)</span></span><br><span class="line"><span class="comment">#      outputs.append(output)</span></span><br><span class="line"><span class="comment">#    return (outputs, state)</span></span><br></pre></td></tr></table></figure><h3 id="示例-v5">示例</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">myrnn = tf.nn.rnn_cell.BasicRNNCell(rnn_size,activation=tf.nn.relu)</span><br><span class="line">   zero_state = myrnn.zero_state(batch_size, dtype=tf.float32)</span><br><span class="line">   outputs, states = tf.nn.static_rnn(myrnn, x, initial_state=zero_state, dtype=tf.float32)</span><br></pre></td></tr></table></figure><h2 id="dynamic-rnn">dynamic rnn</h2><h3 id="api-v6">API</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.dynamic_rnn(</span><br><span class="line">    cell, <span class="comment"># RNNCell的具体对象</span></span><br><span class="line">    inputs, <span class="comment"># RNN的输入,time_major = False, [batch_size, max_time, ...],time_major=True, [max_time, batch_size, ...]</span></span><br><span class="line">    sequence_length=<span class="literal">None</span>, <span class="comment"># </span></span><br><span class="line">    initial_state=<span class="literal">None</span>, <span class="comment"># rnn的初始状态，如果cell.state_size是整数，它的shape需要是[batch_size, cell.state_size]，如果cell.state_size是元组，那么就会是一个tensors的元组，[batch_size, s] for s in cell.state_size</span></span><br><span class="line">    dtype=<span class="literal">None</span>,</span><br><span class="line">    parallel_iterations=<span class="literal">None</span>,</span><br><span class="line">    swap_memory=<span class="literal">False</span>, <span class="comment">#</span></span><br><span class="line">    time_major=<span class="literal">False</span>, <span class="comment"># 如果为True,如果为False，对应不同的inputs </span></span><br><span class="line">    scope=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="示例-v6">示例</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 例子1.创建一个BasicRNNCell</span></span><br><span class="line">rnn_cell = tf.nn.rnn_cell.BasicRNNCell(hidden_size)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义初始化状态</span></span><br><span class="line">initial_state = rnn_cell.zero_state(batch_size, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 'outputs' shape [batch_size, max_time, cell_state_size]</span></span><br><span class="line"><span class="comment"># 'state' shape [batch_size, cell_state_size]</span></span><br><span class="line">outputs, state = tf.nn.dynamic_rnn(rnn_cell, input_data,</span><br><span class="line">                                   initial_state=initial_state,</span><br><span class="line">                                   dtype=tf.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 例子2.创建两个LSTMCells</span></span><br><span class="line">rnn_layers = [tf.nn.rnn_cell.LSTMCell(size) <span class="keyword">for</span> size <span class="keyword">in</span> [<span class="number">128</span>, <span class="number">256</span>]]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个多层RNNCelss。</span></span><br><span class="line">multi_rnn_cell = tf.nn.rnn_cell.MultiRNNCell(rnn_layers)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 'outputs' is a tensor of shape [batch_size, max_time, 256]</span></span><br><span class="line"><span class="comment"># 'state' is a N-tuple where N is the number of LSTMCells containing a</span></span><br><span class="line"><span class="comment"># tf.contrib.rnn.LSTMStateTuple for each cell</span></span><br><span class="line">outputs, state = tf.nn.dynamic_rnn(cell=multi_rnn_cell,</span><br><span class="line">                                   inputs=data,</span><br><span class="line">                                   dtype=tf.float32)</span><br></pre></td></tr></table></figure><h2 id="static-rnn-vs-dynamic-rnn">static_rnn vs dynamic_rnn</h2><h3 id="tf-keras-layers-rnn-cell">tf.keras.layers.RNN(cell)</h3><p>在tensorflow 2.0中，上述两个API都会被弃用，使用新的keras.layers.RNN(cell)</p><h2 id="tf-nn-rnn-cell">tf.nn.rnn_cell</h2><p>该模块提供了许多RNN cell类和rnn函数。</p><h3 id="类">类</h3><ul><li>class BasicRNNCell: 最基本的RNN cell.</li><li>class BasicLSTMCell: 弃用了，使用tf.nn.rnn_cell.LSTMCell代替，就是下面那个</li><li>class LSTMCell: LSTM cell</li><li>class LSTMStateTuple: tupled LSTM cell</li><li>class GRUCell: GRU cell (引用文献 <a href="http://arxiv.org/abs/1406.1078" target="_blank" rel="noopener">http://arxiv.org/abs/1406.1078</a>).</li><li>class RNNCell: 表示一个RNN cell的抽象对象</li><li>class MultiRNNCell: 由很多个简单cells顺序组合成的RNN cell</li><li>class DeviceWrapper: 保证一个RNNCell在一个特定的device运行的op.</li><li>class DropoutWrapper: 添加droput到给定cell的的inputs和outputs的op.</li><li>class ResidualWrapper: 确保cell的输入被添加到输出的RNNCell warpper。</li></ul><h3 id="函数">函数</h3><ul><li>static_rnn(…) # 未来将被弃用，和tf.contrib.rnn.static_rnn是一样的。</li><li>dynamic_rnn(…) # 未来将被弃用</li><li>static_bidirectional_rnn(…) # 未来将被弃用</li><li>bidirectional_dynamic_rnn(…) # 未来将被弃用</li><li>raw_rnn(…)</li></ul><h2 id="tf-contrib-rnn">tf.contrib.rnn</h2><p>该模块提供了RNN和Attention RNN的类和函数op。</p><h3 id="类-v2">类</h3><ul><li>class RNNCell: # 抽象类，所有Cell都要继承该类。所有的Warpper都要直接继承该Cell。</li><li>class LayerRNNCell: # 所有的下列定义的Cell都要使用继承该Cell，该Cell继承RNNCell，所以所有下列Cell都间接继承RNNCell。</li><li>class BasicRNNCell:</li><li>class BasicLSTMCell: # 将被弃用，使用下面的LSTMCell。</li><li>class LSTMCell:</li><li>class LSTMStateTuple:</li><li>class GRUCell:</li><li>class MultiRNNCell:</li><li>class ConvLSTMCell:</li><li>class GLSTMCell:</li><li>class Conv1DLSTMCell:</li><li>class Conv2DLSTMCell:</li><li>class Conv3DLSTMCell:</li><li>class BidirectionalGridLSTMCell:</li><li>class AttentionCellWrapper:</li><li>class CompiledWrapper:</li><li>class CoupledInputForgetGateLSTMCell:</li><li>class DeviceWrapper:</li><li>class DropoutWrapper:</li><li>class EmbeddingWrapper:</li><li>class FusedRNNCell:</li><li>class FusedRNNCellAdaptor:</li><li>class GRUBlockCell:</li><li>class GRUBlockCellV2:</li><li>class GridLSTMCell:</li><li>class HighwayWrapper:</li><li>class IndRNNCell:</li><li>class IndyGRUCell:</li><li>class IndyLSTMCell:</li><li>class InputProjectionWrapper:</li><li>class IntersectionRNNCell:</li><li>class LSTMBlockCell:</li><li>class LSTMBlockFusedCell:</li><li>class LSTMBlockWrapper:</li><li>class LayerNormBasicLSTMCell:</li><li>class NASCell:</li><li>class OutputProjectionWrapper:</li><li>class PhasedLSTMCell:</li><li>class ResidualWrapper:</li><li>class SRUCell:</li><li>class TimeFreqLSTMCell:</li><li>class TimeReversedFusedRNN:</li><li>class UGRNNCell:</li></ul><h3 id="函数-v2">函数</h3><ul><li>static_rnn(…) # 将被弃用，和tf.nn.static_rnn是一样的</li><li>static_bidirectional_rnn(…) # 将被弃用</li><li>best_effort_input_batch_size(…)</li><li>stack_bidirectional_dynamic_rnn(…)</li><li>stack_bidirectional_rnn(…)</li><li>static_state_saving_rnn(…)</li><li>transpose_batch_time(…)</li></ul><h2 id="tf-contrib-rnn-vs-tf-nn-rnn-cell">tf.contrib.rnn vs tf.nn.rnn_cell</h2><p>事实上，这两个模块中都定义了许多RNN cell，contrib定义的是测试性的代码，而nn.rnn_cell是contrib中经过测试后的代码。<br>contrib中的代码会经常修改，而nn中的代码比较稳定。<br>contrib中的cell类型比较多，而nn中的比较少。<br>contrib和nn中有重复的cell，基本上nn中有的contrib中都有。</p><h2 id="参考文献">参考文献</h2><p>1.<a href="https://www.tensorflow.org/api_docs/python/tf/nn/rnn_cell/RNNCell" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/nn/rnn_cell/RNNCell</a><br>2.<a href="https://www.tensorflow.org/api_docs/python/tf/nn/rnn_cell/BasicRNNCell" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/nn/rnn_cell/BasicRNNCell</a><br>3.<a href="https://www.tensorflow.org/api_docs/python/tf/nn/rnn_cell/LSTMCell" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/nn/rnn_cell/LSTMCell</a><br>4.<a href="https://www.tensorflow.org/api_docs/python/tf/nn/rnn_cell/MultiRNNCell" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/nn/rnn_cell/MultiRNNCell</a><br>5.<a href="https://www.tensorflow.org/api_docs/python/tf/nn/rnn_cell/LSTMStateTuple" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/nn/rnn_cell/LSTMStateTuple</a><br>6.<a href="https://www.tensorflow.org/api_docs/python/tf/nn/rnn_cell/DropoutWrapper" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/nn/rnn_cell/DropoutWrapper</a><br>7.<a href="https://www.tensorflow.org/api_docs/python/tf/nn/static_rnn" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/nn/static_rnn</a><br>8.<a href="https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn</a><br>9.<a href="https://www.tensorflow.org/api_docs/python/tf/contrib/rnn" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/contrib/rnn</a><br>10.<a href="https://www.tensorflow.org/api_docs/python/tf/nn/rnn_cell" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/nn/rnn_cell</a><br>11.<a href="https://www.cnblogs.com/wuzhitj/p/6297992.html" target="_blank" rel="noopener">https://www.cnblogs.com/wuzhitj/p/6297992.html</a><br>12.<a href="https://stackoverflow.com/questions/48001759/what-is-right-batch-normalization-function-in-tensorflow" target="_blank" rel="noopener">https://stackoverflow.com/questions/48001759/what-is-right-batch-normalization-function-in-tensorflow</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;常见cell和函数&quot;&gt;常见Cell和函数&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;tf.nn.rnn_cell.BasicRNNCell: 最基本的RNN cell.&lt;/li&gt;
&lt;li&gt;tf.nn.rnn_cell.LSTMCell: LSTM cell&lt;/li&gt;
&lt;li&gt;t
      
    
    </summary>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/categories/tensorflow/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow layers</title>
    <link href="http://mxxhcm.github.io/2019/05/18/tensorflow-layers-module/"/>
    <id>http://mxxhcm.github.io/2019/05/18/tensorflow-layers-module/</id>
    <published>2019-05-18T07:37:50.000Z</published>
    <updated>2019-05-19T08:43:15.232Z</updated>
    
    <content type="html"><![CDATA[<h2 id="tf-layers">tf.layers</h2><p>这个模块定义在tf.contrib.layers中。主要是构建神经网络，正则化和summaries等op。它包括1个模块，19个类，以及一系列函数。</p><h2 id="模块">模块</h2><h3 id="experimental-module">experimental module</h3><p>tf.layers.experimental的公开的API</p><h2 id="类">类</h2><h3 id="class-conv2d">class Conv2D</h3><p>二维卷积类。</p><h4 id="api">API</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">__init__(</span><br><span class="line">    filters, <span class="comment"># 卷积核的数量</span></span><br><span class="line">    kernel_size, <span class="comment"># 卷积核的大小</span></span><br><span class="line">    strides=(<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">    padding=<span class="string">'valid'</span>,</span><br><span class="line">    data_format=<span class="string">'channels_last'</span>, <span class="comment"># string, "channels_last", "channels_first"</span></span><br><span class="line">    dilation_rate=(<span class="number">1</span>, <span class="number">1</span>), <span class="comment">#</span></span><br><span class="line">    activation=<span class="literal">None</span>, <span class="comment"># 激活函数</span></span><br><span class="line">    use_bias=<span class="literal">True</span>,</span><br><span class="line">    kernel_initializer=<span class="literal">None</span>, <span class="comment"># 卷积核的构造器</span></span><br><span class="line">    bias_initializer=tf.zeros_initializer(), <span class="comment"># bias的构造器</span></span><br><span class="line">    kernel_regularizer=<span class="literal">None</span>, <span class="comment">#  卷积核的正则化</span></span><br><span class="line">    bias_regularizer=<span class="literal">None</span>,</span><br><span class="line">    activity_regularizer=<span class="literal">None</span>,</span><br><span class="line">    kernel_constraint=<span class="literal">None</span>,</span><br><span class="line">    bias_constraint=<span class="literal">None</span>,</span><br><span class="line">    trainable=<span class="literal">True</span>, <span class="comment"># 如果为True的话，将变量添加到TRANABLE_VARIABELS collection中</span></span><br><span class="line">    name=<span class="literal">None</span>,</span><br><span class="line">    **kwargs</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h4 id="示例">示例</h4><h4 id="其他">其他</h4><h3 id="所有类">所有类</h3><ul><li>class AveragePooling1D</li><li>class AveragePooling2D</li><li>class AveragePooling3D</li><li>class BatchNormalization</li><li>class Conv1D</li><li>class Conv2D</li><li>class Conv2DTranspose</li><li>class Conv3D</li><li>class Conv3DTranspose</li><li>class Dense</li><li>class Dropout</li><li>class Flatten</li><li>class InputSpec</li><li>class Layer</li><li>class MaxPooling1D</li><li>class MaxPooling2D</li><li>class MaxPooling3D</li><li>class SeparableConv1D</li><li>class SeparableConv2D</li></ul><h2 id="函数">函数</h2><h3 id="conv2d">conv2d</h3><h4 id="api-v2">API</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">tf.layers.conv2d(</span><br><span class="line">    inputs, <span class="comment"># 输入</span></span><br><span class="line">    filters, <span class="comment">#  一个整数,输出的维度，就是有几个卷积核</span></span><br><span class="line">    kernel_size,</span><br><span class="line">    strides=(<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">    padding=<span class="string">'valid'</span>,</span><br><span class="line">    data_format=<span class="string">'channels_last'</span>,</span><br><span class="line">    dilation_rate=(<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">    activation=<span class="literal">None</span>,</span><br><span class="line">    use_bias=<span class="literal">True</span>,</span><br><span class="line">    kernel_initializer=<span class="literal">None</span>,</span><br><span class="line">    bias_initializer=tf.zeros_initializer(),</span><br><span class="line">    kernel_regularizer=<span class="literal">None</span>,</span><br><span class="line">    bias_regularizer=<span class="literal">None</span>,</span><br><span class="line">    activity_regularizer=<span class="literal">None</span>,</span><br><span class="line">    kernel_constraint=<span class="literal">None</span>,</span><br><span class="line">    bias_constraint=<span class="literal">None</span>,</span><br><span class="line">    trainable=<span class="literal">True</span>,</span><br><span class="line">    name=<span class="literal">None</span>,</span><br><span class="line">    reuse=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h4 id="示例-v2">示例</h4><h4 id="其他-v2">其他</h4><h3 id="所有函数">所有函数</h3><p>需要注意的是，下列所有函数在以后版本都将被弃用。</p><ul><li>average_pooling1d(…)</li><li>average_pooling2d(…)</li><li>average_pooling3d(…)</li><li>batch_normalization(…)</li><li>conv1d(…)</li><li>conv2d(…)</li><li>conv2d_transpose(…)</li><li>conv3d(…)</li><li>conv3d_transpose(…)</li><li>dense(…)</li><li>dropout(…)</li><li>flatten(…)</li><li>max_pooling1d(…)</li><li>max_pooling2d(…)</li><li>max_pooling3d(…)</li><li>separable_conv1d(…)</li><li>separable_conv2d(…)</li></ul><h2 id="tf-layers-conv2d-vs-tf-layers-conv2d">tf.layers.conv2d vs tf.layers.Conv2d</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">tf.layers.Conv2d.__init__(</span><br><span class="line">    filters,</span><br><span class="line">    kernel_size,</span><br><span class="line">    strides=(<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">    padding=<span class="string">'valid'</span>,</span><br><span class="line">    data_format=<span class="string">'channels_last'</span>,</span><br><span class="line">    dilation_rate=(<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">    activation=<span class="literal">None</span>,</span><br><span class="line">    use_bias=<span class="literal">True</span>,</span><br><span class="line">    kernel_initializer=<span class="literal">None</span>,</span><br><span class="line">    bias_initializer=tf.zeros_initializer(),</span><br><span class="line">    kernel_regularizer=<span class="literal">None</span>,</span><br><span class="line">    bias_regularizer=<span class="literal">None</span>,</span><br><span class="line">    activity_regularizer=<span class="literal">None</span>,</span><br><span class="line">    kernel_constraint=<span class="literal">None</span>,</span><br><span class="line">    bias_constraint=<span class="literal">None</span>,</span><br><span class="line">    trainable=<span class="literal">True</span>,</span><br><span class="line">    name=<span class="literal">None</span>,</span><br><span class="line">    **kwargs</span><br><span class="line">)</span><br><span class="line">tf.layers.conv2d(</span><br><span class="line">    inputs,</span><br><span class="line">    filters,</span><br><span class="line">    kernel_size,</span><br><span class="line">    strides=(<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">    padding=<span class="string">'valid'</span>,</span><br><span class="line">    data_format=<span class="string">'channels_last'</span>,</span><br><span class="line">    dilation_rate=(<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">    activation=<span class="literal">None</span>,</span><br><span class="line">    use_bias=<span class="literal">True</span>,</span><br><span class="line">    kernel_initializer=<span class="literal">None</span>,</span><br><span class="line">    bias_initializer=tf.zeros_initializer(),</span><br><span class="line">    kernel_regularizer=<span class="literal">None</span>,</span><br><span class="line">    bias_regularizer=<span class="literal">None</span>,</span><br><span class="line">    activity_regularizer=<span class="literal">None</span>,</span><br><span class="line">    kernel_constraint=<span class="literal">None</span>,</span><br><span class="line">    bias_constraint=<span class="literal">None</span>,</span><br><span class="line">    trainable=<span class="literal">True</span>,</span><br><span class="line">    name=<span class="literal">None</span>,</span><br><span class="line">    reuse=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>conv2d是函数；Conv2d是类。<br>conv2d运行的时候需要传入卷积核参数，输入；Conv2d在构造的时候需要实例化卷积核参数，实例化后，可以使用不用的输入得到不同的输出。<br>调用conv2d就相当于调用Conv2d对象的apply(inputs)函数。</p><h2 id="参考文献">参考文献</h2><p>1.<a href="https://www.tensorflow.org/api_docs/python/tf/layers" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/layers</a><br>4.<a href="https://www.tensorflow.org/api_docs/python/tf/layers/Conv2D" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/layers/Conv2D</a><br>5.<a href="https://www.tensorflow.org/api_docs/python/tf/layers/conv2d" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/layers/conv2d</a><br>6.<a href="https://stackoverflow.com/questions/52011509/what-is-difference-between-tf-layers-conv2d-and-tf-layers-conv2d/52035621" target="_blank" rel="noopener">https://stackoverflow.com/questions/52011509/what-is-difference-between-tf-layers-conv2d-and-tf-layers-conv2d/52035621</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;tf-layers&quot;&gt;tf.layers&lt;/h2&gt;
&lt;p&gt;这个模块定义在tf.contrib.layers中。主要是构建神经网络，正则化和summaries等op。它包括1个模块，19个类，以及一系列函数。&lt;/p&gt;
&lt;h2 id=&quot;模块&quot;&gt;模块&lt;/h2&gt;
&lt;h3 
      
    
    </summary>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/categories/tensorflow/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow nn module</title>
    <link href="http://mxxhcm.github.io/2019/05/18/tensorflow-nn-module/"/>
    <id>http://mxxhcm.github.io/2019/05/18/tensorflow-nn-module/</id>
    <published>2019-05-18T07:25:34.000Z</published>
    <updated>2019-05-19T02:02:44.284Z</updated>
    
    <content type="html"><![CDATA[<h2 id="tf-nn">tf.nn</h2><p>提供神经网络op。包含构建RNN cell的rnn_cell模块和一些函数。</p><h2 id="tf-nn-rnn-cell">tf.nn.rnn_cell</h2><p>rnn_cell 用于构建RNN cells<br>包括以下几个类：</p><ul><li>class BasicLSTMCell: 弃用了，使用tf.nn.rnn_cell.LSTMCell代替。</li><li>class BasicRNNCell: 最基本的RNN cell.</li><li>class DeviceWrapper: 保证一个RNNCell在一个特定的device运行的op.</li><li>class DropoutWrapper: 添加droput到给定cell的的inputs和outputs的op.</li><li>class GRUCell: GRU cell (引用文献 <a href="http://arxiv.org/abs/1406.1078" target="_blank" rel="noopener">http://arxiv.org/abs/1406.1078</a>).</li><li>class LSTMCell: LSTM cell</li><li>class LSTMStateTuple: tupled LSTM cell</li><li>class MultiRNNCell: 由很多个简单cells顺序组合成的RNN cell</li><li>class RNNCell: 表示一个RNN cell的抽象对象</li><li>class ResidualWrapper: 确保cell的输入被添加到输出的RNNCell warpper。</li></ul><h2 id="函数">函数</h2><h3 id="conv2d">conv2d(…)</h3><p>给定一个4d输入和filter，计算2d卷积。</p><h4 id="api">API</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.conv2d(</span><br><span class="line">    input, <span class="comment"># 输入，[batch, in_height, in_width, in_channels]</span></span><br><span class="line">    filter, <span class="comment"># 4d tensor, [filter_height, filter_width, in_channels, out_channles]</span></span><br><span class="line">    strides, <span class="comment"># 长度为4的1d tensor。</span></span><br><span class="line">    padding, <span class="comment"># string, 可选"SAME"或者"VALID"</span></span><br><span class="line">    use_cudnn_on_gpu=<span class="literal">True</span>, <span class="comment">#</span></span><br><span class="line">    data_format=<span class="string">'NHWC'</span>, <span class="comment">#</span></span><br><span class="line">    dilations=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], <span class="comment">#</span></span><br><span class="line">    name=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h4 id="示例">示例</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv2d</span><span class="params">(inputs, output_dim, kernel_size, stride, initializer, activation_fn,</span></span></span><br><span class="line"><span class="function"><span class="params">           padding=<span class="string">'VALID'</span>, data_format=<span class="string">'NHWC'</span>, name=<span class="string">"conv2d"</span>, reuse=False)</span>:</span></span><br><span class="line">    kernel_shape = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(name, reuse=reuse):</span><br><span class="line">        <span class="keyword">if</span> data_format == <span class="string">'NCHW'</span>:</span><br><span class="line">            stride = [<span class="number">1</span>, <span class="number">1</span>, stride[<span class="number">0</span>], stride[<span class="number">1</span>]]</span><br><span class="line">            kernel_shape = [kernel_size[<span class="number">0</span>], kernel_size[<span class="number">1</span>], inputs.get_shape()[<span class="number">1</span>], output_dim]</span><br><span class="line">        <span class="keyword">elif</span> data_format == <span class="string">'NHWC'</span>:</span><br><span class="line">            stride = [<span class="number">1</span>, stride[<span class="number">0</span>], stride[<span class="number">1</span>], <span class="number">1</span>]</span><br><span class="line">            kernel_shape = [kernel_size[<span class="number">0</span>], kernel_size[<span class="number">1</span>], inputs.get_shape()[<span class="number">-1</span>], output_dim ]</span><br><span class="line"></span><br><span class="line">        w = tf.get_variable(<span class="string">'w'</span>, kernel_shape, tf.float32, initializer=initializer)</span><br><span class="line">        conv = tf.nn.conv2d(inputs, w, stride, padding, data_format=data_format)</span><br><span class="line"></span><br><span class="line">        b = tf.get_variable(<span class="string">'b'</span>, [output_dim], tf.float32, initializer=tf.constant_initializer(<span class="number">0.0</span>))</span><br><span class="line">        out = tf.nn.bias_add(conv, b, data_format=data_format)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> activation_fn <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        out = activation_fn(out)</span><br><span class="line">    <span class="keyword">return</span> out, w, b</span><br></pre></td></tr></table></figure><h3 id="convolution">convolution</h3><h4 id="api-v2">API</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.convolution(</span><br><span class="line">    input, <span class="comment"># 输入</span></span><br><span class="line">    filter, <span class="comment"># 卷积核</span></span><br><span class="line">    padding, <span class="comment"># string, 可选"SAME"或者"VALID"</span></span><br><span class="line">    strides=<span class="literal">None</span>, <span class="comment"># 步长</span></span><br><span class="line">    dilation_rate=<span class="literal">None</span>,</span><br><span class="line">    name=<span class="literal">None</span>,</span><br><span class="line">    data_format=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h4 id="和tf-nn-conv2d对比">和tf.nn.conv2d对比</h4><p>tf.nn.conv2d是2d卷积<br>tf.nn.convolution是nd卷积</p><h3 id="conv2d-transpose">conv2d_transpose</h3><p>反卷积</p><h4 id="api-v3">API</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.conv2d_transpose(</span><br><span class="line">    value, <span class="comment"># 输入，4d tensor，[batch, in_channels, height, width] for NCHW,或者[batch,height, width, in_channels] for NHWC</span></span><br><span class="line">    filter, <span class="comment"># 4d卷积核，shape是[height, width, output_channels, in_channels]</span></span><br><span class="line">    output_shape, <span class="comment"># 表示反卷积输出的shape一维tensor</span></span><br><span class="line">    strides, <span class="comment"># 步长</span></span><br><span class="line">    padding=<span class="string">'SAME'</span>,</span><br><span class="line">    data_format=<span class="string">'NHWC'</span>,</span><br><span class="line">    name=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h4 id="示例-v2">示例</h4><h3 id="max-pool">max_pool</h3><p>实现max pooling</p><h4 id="api-v4">API</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.max_pool(</span><br><span class="line">    value, <span class="comment"># 输入，4d tensor</span></span><br><span class="line">    ksize, <span class="comment"># 4个整数的list或者tuple，max pooling的kernel size</span></span><br><span class="line">    strides, <span class="comment"># 4个整数的list或者tuple</span></span><br><span class="line">    padding, <span class="comment"># string, 可选"VALID"或者"VALID"</span></span><br><span class="line">    data_format=<span class="string">'NHWC'</span>, <span class="comment"># string,可选"NHWC", "NCHW", NCHW_VECT_C"</span></span><br><span class="line">    name=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="几个常用的函数">几个常用的函数</h3><ul><li>bias_add(…)</li><li>raw_rnn(…)</li><li>static_rnn(…) # 未来将被弃用</li><li>dynamic_rnn(…) # 未来将被弃用</li><li>static_bidirectional_rnn(…) # 未来将被弃用</li><li>bidirectional_dynamic_rnn(…) # 未来将被弃用</li><li>dropout(…)</li><li>leaky_relu(…)</li><li>l2_loss(…)</li><li>log_softmax(…) # 参数弃用</li><li>softmax(…) # 参数弃用</li><li>softmax_cross_entropy_with_logits(…)# 未来将被弃用</li><li>softmax_cross_entropy_with_logits_v2(…) # 参数弃用</li><li>sparse_softmax_cross_entropy_with_logits(…)</li></ul><h4 id="全部函数">全部函数</h4><ul><li>all_candidate_sampler(…)</li><li>atrous_conv2d(…)</li><li>atrous_conv2d_transpose(…)</li><li>avg_pool(…)</li><li>avg_pool3d(…)</li><li>batch_norm_with_global_normalization(…)</li><li>batch_normalization(…)</li><li>bias_add(…)</li><li>bidirectional_dynamic_rnn(…)</li><li>collapse_repeated(…)</li><li>compute_accidental_hits(…)</li><li>conv1d(…)</li><li>conv2d(…)</li><li>conv2d_backprop_filter(…)</li><li>conv2d_backprop_input(…)</li><li>conv2d_transpose(…)</li><li>conv3d(…)</li><li>conv3d_backprop_filter(…)</li><li>conv3d_backprop_filter_v2(…)</li><li>conv3d_transpose(…)</li><li>convolution(…) - crelu(…)</li><li>ctc_beam_search_decoder(…)</li><li>ctc_beam_search_decoder_v2(…)</li><li>ctc_greedy_decoder(…)</li><li>ctc_loss(…)</li><li>ctc_loss_v2(…)</li><li>ctc_unique_labels(…)</li><li>depth_to_space(…)</li><li>depthwise_conv2d(…)</li><li>depthwise_conv2d_backprop_filter(…)</li><li>depthwise_conv2d_backprop_input(…)</li><li>depthwise_conv2d_native(…)</li><li>depthwise_conv2d_native_backprop_filter(…)</li><li>depthwise_conv2d_native_backprop_input(…)</li><li>dilation2d(…)</li><li>dropout(…)</li><li>dynamic_rnn(…)</li><li>elu(…)</li><li>embedding_lookup(…)</li><li>embedding_lookup_sparse(…)</li><li>erosion2d(…)</li><li>fixed_unigram_candidate_sampler(…)</li><li>fractional_avg_pool(…)</li><li>fractional_max_pool(…)</li><li>fused_batch_norm(…)</li><li>in_top_k(…)</li><li>l2_loss(…)</li><li>l2_normalize(…)</li><li>leaky_relu(…)</li><li>learned_unigram_candidate_sampler(…)</li><li>local_response_normalization(…)</li><li>log_poisson_loss(…)</li><li>log_softmax(…)</li><li>log_uniform_candidate_sampler(…)</li><li>lrn(…)</li><li>max_pool(…)</li><li>max_pool3d(…)</li><li>max_pool_with_argmax(…)</li><li>moments(…)</li><li>nce_loss(…)</li><li>normalize_moments(…)</li><li>pool(…)</li><li>quantized_avg_pool(…)</li><li>quantized_conv2d(…)</li><li>quantized_max_pool(…)</li><li>quantized_relu_x(…)</li><li>raw_rnn(…)</li><li>relu(…)</li><li>relu6(…)</li><li>relu_layer(…)</li><li>safe_embedding_lookup_sparse(…)</li><li>sampled_softmax_loss(…)</li><li>selu(…)</li><li>separable_conv2d(…)</li><li>sigmoid(…)</li><li>sigmoid_cross_entropy_with_logits(…)</li><li>softmax(…)</li><li>softmax_cross_entropy_with_logits(…)</li><li>softmax_cross_entropy_with_logits_v2(…)</li><li>softplus(…)</li><li>softsign(…)</li><li>space_to_batch(…)</li><li>space_to_depth(…)</li><li>sparse_softmax_cross_entropy_with_logits(…)</li><li>static_bidirectional_rnn(…)</li><li>static_rnn(…)</li><li>static_state_saving_rnn(…)</li><li>sufficient_statistics(…)</li><li>tanh(…)</li><li>top_k(…)</li><li>uniform_candidate_sampler(…)</li><li>weighted_cross_entropy_with_logits(…)</li><li>weighted_moments(…)</li><li>with_space_to_batch(…)</li><li>xw_plus_b(…)</li><li>zero_fraction(…)</li></ul><h2 id="参考文献">参考文献</h2><p>1.<a href="https://www.tensorflow.org/api_docs/python/tf/nn" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/nn</a><br>2.<a href="https://www.tensorflow.org/api_docs/python/tf/nn/rnn_cell" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/nn/rnn_cell</a><br>3.<a href="https://www.tensorflow.org/api_docs/python/tf/nn/conv2d" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/nn/conv2d</a><br>4.<a href="https://stackoverflow.com/questions/38601452/what-is-tf-nn-max-pools-ksize-parameter-used-for" target="_blank" rel="noopener">https://stackoverflow.com/questions/38601452/what-is-tf-nn-max-pools-ksize-parameter-used-for</a><br>5.<a href="https://www.tensorflow.org/api_docs/python/tf/nn/convolution" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/nn/convolution</a><br>6.<a href="https://stackoverflow.com/questions/47775244/difference-between-tf-nn-convolution-and-tf-nn-conv2d" target="_blank" rel="noopener">https://stackoverflow.com/questions/47775244/difference-between-tf-nn-convolution-and-tf-nn-conv2d</a><br>7.<a href="https://www.tensorflow.org/api_docs/python/tf/nn/conv2d_transpose" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/nn/conv2d_transpose</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;tf-nn&quot;&gt;tf.nn&lt;/h2&gt;
&lt;p&gt;提供神经网络op。包含构建RNN cell的rnn_cell模块和一些函数。&lt;/p&gt;
&lt;h2 id=&quot;tf-nn-rnn-cell&quot;&gt;tf.nn.rnn_cell&lt;/h2&gt;
&lt;p&gt;rnn_cell 用于构建RNN cell
      
    
    </summary>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/categories/tensorflow/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow softmax</title>
    <link href="http://mxxhcm.github.io/2019/05/16/tensorflow-softmax/"/>
    <id>http://mxxhcm.github.io/2019/05/16/tensorflow-softmax/</id>
    <published>2019-05-16T01:04:48.000Z</published>
    <updated>2019-05-16T02:27:50.558Z</updated>
    
    <content type="html"><![CDATA[<h2 id="各种softmax">各种softmax</h2><ul><li>tf.nn.softmax</li><li>tf.nn.log_softmax</li><li>tf.nn.softmax_cross_entropy_with_logits</li><li>tf,sparse_softmax_cross_entropy_with_logits</li></ul><h2 id="logits">logits</h2><p>什么是logits</p><h3 id="数学上">数学上</h3><p>假设一个事件发生的概率为 p，那么该事件的logits为$\text{logit}§ = \log\frac{p}{1-p}$.</p><h3 id="machine-learning中">Machine Learning中</h3><p>logits在机器学习中就是前向传播网络的分类结果，是未归一化的概率，总和不为$0$，输入softmax函数之后可以得到归一化的概率。</p><h2 id="tf-nn-softmax">tf.nn.softmax</h2><h3 id="api">API</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.softmax(</span><br><span class="line">logits,</span><br><span class="line">axis=<span class="literal">None</span>,</span><br><span class="line">name=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="功能">功能</h3><p>上面函数实现了如下的功能：<br>softmax = tf.exp(logits) / tf.reduce_sum(tf.exp(logits), axis)<br>就是将输入的logits经过softmax做归一化。</p><h2 id="tf-nn-log-softmax">tf.nn.log_softmax</h2><h3 id="api-v2">API</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.softmax(</span><br><span class="line">logits,</span><br><span class="line">axis=<span class="literal">None</span>,</span><br><span class="line">name=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="功能-v2">功能</h3><p>该函数实现了如下功能。<br>logsoftmax = logits - log(reduce_sum(exp(logits), axis))</p><h2 id="tf-nn-softmax-cross-entropy-with-logits">tf.nn.softmax_cross_entropy_with_logits</h2><h3 id="api-v3">API</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.softmax_cross_entropy_with_logits(</span><br><span class="line">    _sentinel=<span class="literal">None</span>,  <span class="comment"># pylint: disable=invalid-name</span></span><br><span class="line">    labels=<span class="literal">None</span>,</span><br><span class="line">    logits=<span class="literal">None</span>,</span><br><span class="line">    dim=<span class="number">-1</span>,</span><br><span class="line">    name=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="功能-v3">功能</h3><p>计算logits经过softmax之后和labels之间的交叉熵</p><h2 id="tf-sparse-softmax-cross-entropy-with-logits">tf.sparse_softmax_cross_entropy_with_logits</h2><h3 id="api-v4">API</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.sparse_softmax_cross_entropy_with_logits(</span><br><span class="line">    _sentinel=<span class="literal">None</span>,  <span class="comment"># pylint: disable=invalid-name</span></span><br><span class="line">    labels=<span class="literal">None</span>,</span><br><span class="line">    logits=<span class="literal">None</span>,</span><br><span class="line">    name=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="功能-v4">功能</h3><p>计算logits和labels之间的稀疏softmax交叉熵</p><h2 id="参考文献">参考文献</h2><p>1.<a href="http://landcareweb.com/questions/789/shi-yao-shi-logits-softmaxhe-softmax-cross-entropy-with-logits" target="_blank" rel="noopener">http://landcareweb.com/questions/789/shi-yao-shi-logits-softmaxhe-softmax-cross-entropy-with-logits</a><br>2.<a href="https://stackoverflow.com/questions/41455101/what-is-the-meaning-of-the-word-logits-in-tensorflow" target="_blank" rel="noopener">https://stackoverflow.com/questions/41455101/what-is-the-meaning-of-the-word-logits-in-tensorflow</a><br>3.<a href="https://stackoverflow.com/a/43577384" target="_blank" rel="noopener">https://stackoverflow.com/a/43577384</a><br>4.<a href="https://stackoverflow.com/a/47852892" target="_blank" rel="noopener">https://stackoverflow.com/a/47852892</a><br>5.<a href="https://www.tensorflow.org/tutorials/estimators/cnn" target="_blank" rel="noopener">https://www.tensorflow.org/tutorials/estimators/cnn</a><br>6.<a href="https://www.zhihu.com/question/60751553" target="_blank" rel="noopener">https://www.zhihu.com/question/60751553</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;各种softmax&quot;&gt;各种softmax&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;tf.nn.softmax&lt;/li&gt;
&lt;li&gt;tf.nn.log_softmax&lt;/li&gt;
&lt;li&gt;tf.nn.softmax_cross_entropy_with_logits&lt;/li&gt;
&lt;l
      
    
    </summary>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/categories/tensorflow/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow collection</title>
    <link href="http://mxxhcm.github.io/2019/05/13/tensorflow-collection/"/>
    <id>http://mxxhcm.github.io/2019/05/13/tensorflow-collection/</id>
    <published>2019-05-13T02:28:29.000Z</published>
    <updated>2019-06-19T12:18:15.176Z</updated>
    
    <content type="html"><![CDATA[<h2 id="tf-collection">tf.collection</h2><p>Tensorflow用graph collection来管理不同类型的对象。tf.GraphKeys中定义了默认的collection，tf通过调用各种各样的collection操作graph中的变量。比如tf.Optimizer只优化tf.GraphKeys.TRAINABLE_VARIABLES collection中的变量。常见的collection如下：</p><ul><li>GLOBAL_VARIABLES: 所有的Variable对象在创建的时候自动加入该colllection，且在分布式环境中共享（model variables是它的子集）。一般来说，TRAINABLE_VARIABLES包含在MODEL_VARIABLES中，MODEL_VARIABLES包含在GLOBAL_VARIABLES中。也就是说TRAINABLE_VARIABLES$\le$MODEL_VARIABLES$\le$GLOBAL_VARIABLES。一般tf.train.Saver()对应的是GLOBAL_VARIABLES的变量。</li><li>LOCAL_VARIABLES: 它是GLOBAL_VARIABLES不同的是在本机器上的Variable子集。使用tf.contrib.framework.local_variable将变量添加到这个collection.</li><li>MODEL_VARIABLES: 模型变量，在构建模型中，所有用于前向传播的Variable都将添加到这里。使用 tf.contrib.framework.model_variable向这个collection添加变量。</li><li>TRAINALBEL_VARIABLES: 所有用于反向传播的Variable，可以被optimizer训练，进行参数更新的变量。tf.Variable对象同样会自动加入这个collection。</li><li>SUMMARIES: graph创建的所有summary Tensor都会记录在这里面。</li><li>QUEUE_RUNNERS:</li><li>MOVING_AVERAGE_VARIABLES: 保持Movering average的变量子集。</li><li>REGULARIZATION_LOSSES: 创建graph的regularization loss。</li></ul><p>这里主要介绍三类collection，一种是GLOBAL_VARIABLES，一种是SUMMARIES，一种是自定义的collections。</p><p>下面的一些collection也被定义了，但是并不会自动添加</p><blockquote><p>The following standard keys are defined, but their collections are not automatically populated as many of the others are:</p></blockquote><ul><li>WEIGHTS</li><li>BIASES</li><li>ACTIVATIONS</li></ul><h2 id="global-variable-collection">GLOBAL_Variable collection</h2><p>tf.Variable()对象在生成时会被默认添加到tf.GraphKeys中的GLOBAL_VARIABLES和TRAINABLE_VARIABLES collection中。</p><h3 id="代码示例">代码示例</h3><p><a href="https://github.com/mxxhcm/code/blob/master/tf/ops/tf_global_trainable_variables_collections.py" target="_blank" rel="noopener">代码地址</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">a = tf.Variable([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">b = tf.get_variable(<span class="string">"bbb"</span>, shape=[<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">tf.constant([<span class="number">3</span>])</span><br><span class="line">c = tf.ones([<span class="number">3</span>])</span><br><span class="line">d = tf.random_uniform([<span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">e = tf.log(c)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看GLOBAL_VARIABLES collection中的变量</span></span><br><span class="line">global_variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)</span><br><span class="line"><span class="keyword">for</span> var <span class="keyword">in</span> global_variables:</span><br><span class="line">   print(var)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看TRAINABLE_VARIABLES collection中的变量</span></span><br><span class="line">trainable_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)</span><br><span class="line"><span class="keyword">for</span> var <span class="keyword">in</span> global_variables:</span><br><span class="line">   print(var)</span><br></pre></td></tr></table></figure><h2 id="summary-collection">Summary collection</h2><p>Summary op产生的变量会被添加到tf.GraphKeys.SUMMARIES collection中。<br><a href="https://mxxhcm.github.io/2019/05/08/tensorflow-summary/">点击查看关于tf.summary的详细介绍</a></p><h3 id="代码示例-v2">代码示例</h3><p><a href="https://github.com/mxxhcm/code/blob/master/tf/ops/tf_summary_collection.py" target="_blank" rel="noopener">代码地址</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成一个图</span></span><br><span class="line">graph = tf.Graph()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> graph.as_default():</span><br><span class="line">    <span class="comment"># 指定模型参数</span></span><br><span class="line">    w = tf.Variable([<span class="number">0.3</span>], name=<span class="string">"w"</span>, dtype=tf.float32)</span><br><span class="line">    b = tf.Variable([<span class="number">0.2</span>], name=<span class="string">"b"</span>, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 输入数据placeholder</span></span><br><span class="line">    x = tf.placeholder(tf.float32, name=<span class="string">"inputs"</span>)</span><br><span class="line">    y = tf.placeholder(tf.float32, name=<span class="string">"outputs"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 前向传播</span></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">'linear_model'</span>):</span><br><span class="line">        linear = w * x + b</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算loss</span></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">'cal_loss'</span>):</span><br><span class="line">        loss = tf.reduce_mean(input_tensor=tf.square(y - linear), name=<span class="string">'loss'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义summary saclar op</span></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">'add_summary'</span>):</span><br><span class="line">        summary_loss = tf.summary.scalar(<span class="string">'MSE'</span>, loss)</span><br><span class="line">        summary_b = tf.summary.scalar(<span class="string">'b'</span>, b[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义优化器</span></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">'train_model'</span>):</span><br><span class="line">        optimizer = tf.train.GradientDescentOptimizer(<span class="number">0.01</span>)</span><br><span class="line">        train = optimizer.minimize(loss)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session(graph=graph) <span class="keyword">as</span> sess:</span><br><span class="line">inputs = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]</span><br><span class="line">outputs = [<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]</span><br><span class="line">    <span class="comment"># 定义写入文件类</span></span><br><span class="line">    writer = tf.summary.FileWriter(<span class="string">"./summary/"</span>, graph)</span><br><span class="line">    <span class="comment"># 获取所有的summary op，不用一个一个去单独run</span></span><br><span class="line">    merged = tf.summary.merge_all()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化</span></span><br><span class="line">    init_op = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5000</span>):</span><br><span class="line"><span class="comment"># 运行summary op merged</span></span><br><span class="line">        _, summ = sess.run([train, merged], feed_dict=&#123;x: inputs, y: outputs&#125;)</span><br><span class="line"><span class="comment"># 将summary op返回的变量转化为事件，写入文件</span></span><br><span class="line">        writer.add_summary(summ, global_step=i)</span><br><span class="line"></span><br><span class="line">    w_, b_, l_ = sess.run([w, b, loss], feed_dict=&#123;x: inputs, y: outputs&#125;)</span><br><span class="line">    print(<span class="string">"w: "</span>, w_, <span class="string">"b: "</span>, b_, <span class="string">"loss: "</span>, l_)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 查看SUMMARIES collection</span></span><br><span class="line">    <span class="keyword">for</span> var <span class="keyword">in</span> tf.get_collection(tf.GraphKeys.SUMMARIES):</span><br><span class="line">        print(var)</span><br></pre></td></tr></table></figure><h2 id="自定义collection">自定义collection</h2><p>通过tf.add_collection()和tf.get_collection()可以添加和访问custom collection。</p><h3 id="示例代码">示例代码</h3><p><a href="https://github.com/mxxhcm/code/blob/master/tf/ops/tf_custom_collection.py" target="_blank" rel="noopener">代码地址</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义第1个loss</span></span><br><span class="line">x1 = tf.constant(<span class="number">1.0</span>)</span><br><span class="line">l1 = tf.nn.l2_loss(x1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义第2个loss</span></span><br><span class="line">x2 = tf.constant([<span class="number">2.5</span>, <span class="number">-0.3</span>])</span><br><span class="line">l2 = tf.nn.l2_loss(x2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将loss添加到losses collection中</span></span><br><span class="line">tf.add_to_collection(<span class="string">"losses"</span>, l1)</span><br><span class="line">tf.add_to_collection(<span class="string">"losses"</span>, l2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看losses collection中的内容</span></span><br><span class="line">losses = tf.get_collection(<span class="string">'losses'</span>)</span><br><span class="line"><span class="keyword">for</span> var <span class="keyword">in</span> tf.get_collection(<span class="string">'losses'</span>):</span><br><span class="line">    print(var)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 建立session运行</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init)</span><br><span class="line">    losses_val = sess.run(losses)</span><br><span class="line">    print(losses_val)</span><br></pre></td></tr></table></figure><h2 id="疑问">疑问</h2><p>collection是和graph绑定在一起的，那么如果定义了很多个图，如何获得非默认图的tf.GraphKeys中定义的collection？？</p><h2 id="参考文献">参考文献</h2><p>1.<a href="https://blog.csdn.net/shenxiaolu1984/article/details/52815641" target="_blank" rel="noopener">https://blog.csdn.net/shenxiaolu1984/article/details/52815641</a><br>2.<a href="https://blog.csdn.net/hustqb/article/details/80398934" target="_blank" rel="noopener">https://blog.csdn.net/hustqb/article/details/80398934</a><br>3.<a href="https://www.tensorflow.org/api_docs/python/tf/GraphKeys?hl=zh_cn" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/GraphKeys?hl=zh_cn</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;tf-collection&quot;&gt;tf.collection&lt;/h2&gt;
&lt;p&gt;Tensorflow用graph collection来管理不同类型的对象。tf.GraphKeys中定义了默认的collection，tf通过调用各种各样的collection操作grap
      
    
    </summary>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/categories/tensorflow/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow graph和session</title>
    <link href="http://mxxhcm.github.io/2019/05/12/tensorflow-graph%E5%92%8Csession/"/>
    <id>http://mxxhcm.github.io/2019/05/12/tensorflow-graph和session/</id>
    <published>2019-05-12T13:45:04.000Z</published>
    <updated>2019-07-18T12:21:17.513Z</updated>
    
    <content type="html"><![CDATA[<h2 id="tf-graph和tf-session">tf.Graph和tf.Session</h2><p>Graph和Session之间的区别和联系。</p><ul><li>Graph定义了如何进行计算，但是并没有进行计算，graph不会hold任何值，它仅仅定义code中指定的各种operation</li><li>Session用来执行graph或者graph的一部分。它会分配资源（一个机器或者多个机器），并且会保存中间结果和variables的值。在不同session的执行过程也是分开的。</li></ul><h2 id="tf-graph">tf.Graph</h2><p>tf.Graph包含两类信息：</p><ul><li>Node和Edge，用来表示各个op如何进行组合。</li><li>collections。使用tf.add_to_collection和tf.get_collection对collection进行操作。一个常见的例子是创建tf.Variable的时候，默认会将它加入到&quot;global variables&quot;和&quot;trainable variables&quot; collection中。<br>当调用tf.train.Saver和tf.train.Optimizer的时候，它会使用这些collection中的变量作为默认参数。<br>常见的定义在tf.GraphKeys上的collection:<br>VARIABLES, TRAINABLE_VARIABLES, MOVING_AVERAGE_VARIABLES, LOCAL_VARIABLES, MODEL_VARIABLE,SUMMARIES.<br><a href="https://mxxhcm.github.io/2019/05/13/tensorflow-collection/">关于collections的详细介绍可点击这里</a></li></ul><h2 id="构建tf-graph">构建tf.Graph</h2><p>调用tensorflow API就会构建新的tf.Operation和tf.Tensor，并将他们添加到tf.Graph实例中去。</p><ul><li>调用 tf.constant(42.0) 创建单个 tf.Operation，该操作可以生成值 42.0，将该值添加到默认图中，并返回表示常量值的 tf.Tensor。</li><li>调用 tf.matmul(x, y) 可创建单个 tf.Operation，该操作会将 tf.Tensor 对象 x 和 y 的值相乘，将其添加到默认图中，并返回表示乘法运算结果的 tf.Tensor。</li><li>执行 v = tf.Variable(0) 可向图添加一个 tf.Operation，该操作可以存储一个可写入的张量值，该值在多个 tf.Session.run 调用之间保持恒定。tf.Variable 对象会封装此操作，并可以像张量一样使用，即读取已存储值的当前值。tf.Variable 对象也具有 assign 和 assign_add 等方法，这些方法可创建 tf.Operation 对象，这些对象在执行时将更新已存储的值。（请参阅变量了解关于变量的更多信息。）</li><li>调用 tf.train.Optimizer.minimize 可将操作和张量添加到计算梯度的默认图中，并返回一个 tf.Operation，该操作在运行时会将这些梯度应用到一组变量上。</li></ul><h2 id="获得默认图">获得默认图</h2><p>用 tf.get_default_graph，它会返回一个 tf.Graph 对象：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Print all of the operations in the default graph.</span></span><br><span class="line">g = tf.get_default_graph()</span><br></pre></td></tr></table></figure><h2 id="清空默认图">清空默认图</h2><p>tf.reset_default_graph()</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 清空当前session的默认图</span></span><br><span class="line">tf.reset_default_graph()</span><br></pre></td></tr></table></figure><h2 id="命名空间">命名空间</h2><p>tf.Graph 对象会定义一个命名空间（为其包含的 tf.Operation 对象）。TensorFlow 会自动为图中的每个指令选择一个唯一名称，也可以指定描述性名称，让程序阅读和调试起来更轻松。TensorFlow API 提供两种方法来指定op名称：</p><ul><li>如果API会创建新的op或返回新的 tf.Tensor，就可选 name 参数。例如，tf.constant(42.0, name=“answer”) 会创建一个新的 tf.Operation（名为 “answer”）并返回一个 tf.Tensor（名为 “answer:0”）。如果默认图已包含名为 “answer” 的操作，则 TensorFlow 会在名称上附加 “_1”、&quot;_2&quot; 等字符，以便让名称具有唯一性。</li><li>借助 tf.name_scope 函数，可以向在特定上下文中创建的所有op添加name_scope。当前name_scope是一个用 “/” 分隔的名称列表，其中包含所有活跃的 tf.name_scope 上下文管理器名称。如果某个name_scope已在当前上下文中被占用，TensorFlow 将在该作用域上附加 “_1”、&quot;_2&quot; 等字符。例如：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">c_0 = tf.constant(<span class="number">0</span>, name=<span class="string">"c"</span>)  <span class="comment"># =&gt; operation named "c"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Already-used names will be "uniquified".</span></span><br><span class="line">c_1 = tf.constant(<span class="number">2</span>, name=<span class="string">"c"</span>)  <span class="comment"># =&gt; operation named "c_1"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Name scopes add a prefix to all operations created in the same context.</span></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">"outer"</span>):</span><br><span class="line">  c_2 = tf.constant(<span class="number">2</span>, name=<span class="string">"c"</span>)  <span class="comment"># =&gt; operation named "outer/c"</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Name scopes nest like paths in a hierarchical file system.</span></span><br><span class="line">  <span class="keyword">with</span> tf.name_scope(<span class="string">"inner"</span>):</span><br><span class="line">    c_3 = tf.constant(<span class="number">3</span>, name=<span class="string">"c"</span>)  <span class="comment"># =&gt; operation named "outer/inner/c"</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Exiting a name scope context will return to the previous prefix.</span></span><br><span class="line">  c_4 = tf.constant(<span class="number">4</span>, name=<span class="string">"c"</span>)  <span class="comment"># =&gt; operation named "outer/c_1"</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Already-used name scopes will be "uniquified".</span></span><br><span class="line">  <span class="keyword">with</span> tf.name_scope(<span class="string">"inner"</span>):</span><br><span class="line">    c_5 = tf.constant(<span class="number">5</span>, name=<span class="string">"c"</span>)  <span class="comment"># =&gt; operation named "outer/inner_1/c"</span></span><br></pre></td></tr></table></figure><p>请注意，tf.Tensor 对象以输出张量的op明确命名。张量名称的形式为 “&lt;OP_NAME&gt;:&lt;i&gt;”，其中：</p><ul><li>“&lt;OP_NAME&gt;” 是生成该张量的操作的名称。</li><li>“&lt;i&gt;” 是一个整数，表示该张量在该op的输出中的索引。</li></ul><h2 id="获得图中的op">获得图中的op</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">c_0 = tf.constant(<span class="number">0</span>, name=<span class="string">"c"</span>)  <span class="comment"># =&gt; operation named "c"</span></span><br><span class="line"><span class="comment"># Already-used names will be "uniquified".  c_1 = tf.constant(2, name="c")  # =&gt; operation named "c_1"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Name scopes add a prefix to all operations created in the same context.</span></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">"outer"</span>):</span><br><span class="line">  c_2 = tf.constant(<span class="number">2</span>, name=<span class="string">"c"</span>)  <span class="comment"># =&gt; operation named "outer/c"</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Name scopes nest like paths in a hierarchical file system.</span></span><br><span class="line">  <span class="keyword">with</span> tf.name_scope(<span class="string">"inner"</span>):</span><br><span class="line">    c_3 = tf.constant(<span class="number">3</span>, name=<span class="string">"c"</span>)  <span class="comment"># =&gt; operation named "outer/inner/c"</span></span><br><span class="line"></span><br><span class="line">g = tf.get_default_graph()</span><br><span class="line">print(g.get_operations())</span><br><span class="line"><span class="comment"># [&lt;tf.Operation 'c' type=Const&gt;, &lt;tf.Operation 'c_1' type=Const&gt;, &lt;tf.Operation 'outer/c' type=Const&gt;, &lt;tf.Operation 'outer/inner/c' type=Const&gt;]</span></span><br></pre></td></tr></table></figure><h2 id="类张量对象">类张量对象</h2><p>许多 TensorFlow op都会接受一个或多个 tf.Tensor 对象作为参数。例如，tf.matmul 接受两个 tf.Tensor 对象，tf.add_n 接受一个具有 n 个 tf.Tensor 对象的列表。为了方便起见，这些函数将接受类张量对象来取代 tf.Tensor，并将它明确转换为 tf.Tensor（通过 tf.convert_to_tensor 方法）。类张量对象包括以下类型的元素：</p><ul><li>tf.Tensor</li><li>tf.Variable</li><li>numpy.ndarray</li><li>list（以及类似于张量的对象的列表）</li><li>标量 Python 类型：bool、float、int、str</li></ul><p><strong>注意</strong> 默认情况下，每次使用同一个类张量对象时，TensorFlow 将创建新的 tf.Tensor。如果类张量对象很大（例如包含一组训练样本的 numpy.ndarray），且多次使用该对象，则可能会耗尽内存。要避免出现此问题，请在类张量对象上手动调用 tf.convert_to_tensor 一次，并使用返回的 tf.Tensor。</p><h2 id="tf-session">tf.Session</h2><h3 id="api">API</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tf.Session.init(</span><br><span class="line">target, <span class="comment"># 可选参数，指定设备。</span></span><br><span class="line">graph, <span class="comment">#可选参数，默认情况下，新的session绑定到默认graph</span></span><br><span class="line">confi <span class="comment"># 可选参数，常见的一个选择为gpu_options.allow_growth。将此参数设置为 True 可更改 GPU 内存分配器，使该分配器逐渐增加分配的内存量，而不是在启动时分配掉大多数内存。</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="创建session">创建session</h3><h4 id="默认session">默认session</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create a default in-process session.</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  <span class="comment"># ...</span></span><br></pre></td></tr></table></figure><h4 id="none"></h4><h3 id="执行op">执行op</h3><p>tf.Session.run 方法是运行 tf.Operation 或评估 tf.Tensor 的主要机制。传入一个或多个 tf.Operation 或 tf.Tensor 对象到 tf.Session.run，TensorFlow 将执行计算结果所需的操作。<br>tf.Session.run 需要指定一组 fetch，这些 fetch 可确定返回值，并且可能是 tf.Operation、tf.Tensor 或类张量类型，例如 tf.Variable。这些 fetch 决定了必须执行哪些子图（属于整体 tf.Graph）以生成结果：该子图包含 fetch 列表中指定的所有op，以及其输出用于计算 fetch 值的所有操作。例如，以下代码段说明了 tf.Session.run 的不同参数如何导致执行不同的子图：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">x = tf.constant([[<span class="number">37.0</span>, <span class="number">-23.0</span>], [<span class="number">1.0</span>, <span class="number">4.0</span>]])</span><br><span class="line">w = tf.Variable(tf.random_uniform([<span class="number">2</span>, <span class="number">2</span>]))</span><br><span class="line">y = tf.matmul(x, w)</span><br><span class="line">output = tf.nn.softmax(y)</span><br><span class="line">init_op = w.initializer</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  <span class="comment"># 初始化w</span></span><br><span class="line">  sess.run(init_op)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Evaluate `output`. `sess.run(output)` will return a NumPy array containing</span></span><br><span class="line">  <span class="comment"># the result of the computation.</span></span><br><span class="line">  <span class="comment"># 计算output</span></span><br><span class="line">  print(sess.run(output))</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Evaluate `y` and `output`. Note that `y` will only be computed once, and its</span></span><br><span class="line">  <span class="comment"># result used both to return `y_val` and as an input to the `tf.nn.softmax()`</span></span><br><span class="line">  <span class="comment"># op. Both `y_val` and `output_val` will be NumPy arrays.</span></span><br><span class="line">  <span class="comment"># 计算y和output</span></span><br><span class="line">  y_val, output_val = sess.run([y, output])</span><br></pre></td></tr></table></figure><p>tf.Session.run 也可以接受 feed dict，该字典是从 tf.Tensor 对象（通常是 tf.placeholder 张量），在执行时会替换这些张量的值（通常是 Python 标量、列表或 NumPy 数组）的映射。例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Define a placeholder that expects a vector of three floating-point values,</span></span><br><span class="line"><span class="comment"># and a computation that depends on it.</span></span><br><span class="line">x = tf.placeholder(tf.float32, shape=[<span class="number">3</span>])</span><br><span class="line">y = tf.square(x)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  <span class="comment"># Feeding a value changes the result that is returned when you evaluate `y`.</span></span><br><span class="line">  print(sess.run(y, &#123;x: [<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>]&#125;))  <span class="comment"># =&gt; "[1.0, 4.0, 9.0]"</span></span><br><span class="line">  print(sess.run(y, &#123;x: [<span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">5.0</span>]&#125;))  <span class="comment"># =&gt; "[0.0, 0.0, 25.0]"</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Raises &lt;a href="../api_docs/python/tf/errors/InvalidArgumentError"&gt;&lt;code&gt;tf.errors.InvalidArgumentError&lt;/code&gt;&lt;/a&gt;, because you must feed a value for</span></span><br><span class="line">  <span class="comment"># a `tf.placeholder()` when evaluating a tensor that depends on it.</span></span><br><span class="line">  sess.run(y)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Raises `ValueError`, because the shape of `37.0` does not match the shape</span></span><br><span class="line">  <span class="comment"># of placeholder `x`.</span></span><br><span class="line">  sess.run(y, &#123;x: <span class="number">37.0</span>&#125;)</span><br></pre></td></tr></table></figure><p>tf.Session.run 也接受可选的 options 参数（允许指定与调用有关的选项）和可选的 run_metadata 参数（允许收集与执行有关的元数据）。例如，可以同时使用这些选项来收集与执行有关的跟踪信息：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">y = tf.matmul([[<span class="number">37.0</span>, <span class="number">-23.0</span>], [<span class="number">1.0</span>, <span class="number">4.0</span>]], tf.random_uniform([<span class="number">2</span>, <span class="number">2</span>]))</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  <span class="comment"># Define options for the `sess.run()` call.</span></span><br><span class="line">  options = tf.RunOptions()</span><br><span class="line">  options.output_partition_graphs = <span class="literal">True</span></span><br><span class="line">  options.trace_level = tf.RunOptions.FULL_TRACE</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Define a container for the returned metadata.</span></span><br><span class="line">  metadata = tf.RunMetadata()</span><br><span class="line"></span><br><span class="line">  sess.run(y, options=options, run_metadata=metadata)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Print the subgraphs that executed on each device.</span></span><br><span class="line">  print(metadata.partition_graphs)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Print the timings of each operation that executed.</span></span><br><span class="line">  print(metadata.step_stats)</span><br></pre></td></tr></table></figure><h2 id="不同session的结果">不同session的结果</h2><p><a href>代码地址</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">graph = tf.Graph()</span><br><span class="line"></span><br><span class="line">with graph.as_default():</span><br><span class="line">    variable = tf.Variable(10, name=&quot;foo&quot;)</span><br><span class="line">    initialize = tf.global_variables_initializer()</span><br><span class="line">    assign = variable.assign(12)</span><br><span class="line"></span><br><span class="line">with tf.Session(graph=graph) as sess:</span><br><span class="line">    sess.run(initialize)</span><br><span class="line">    sess.run(assign)</span><br><span class="line">    print(sess.run(variable))</span><br><span class="line"></span><br><span class="line">with tf.Session(graph=graph) as sess:</span><br><span class="line">    print(sess.run(variable))</span><br></pre></td></tr></table></figure><h2 id="访问当前sess的图">访问当前sess的图。</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sess = tf.Session()</span><br><span class="line">sess.graph</span><br></pre></td></tr></table></figure><h2 id="可视化图">可视化图</h2><p>使用图可视化工具。最简单的方法是传递tf.Graph到tf.summary.FileWriter中。如下示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Build your graph.</span></span><br><span class="line">x = tf.constant([[<span class="number">37.0</span>, <span class="number">-23.0</span>], [<span class="number">1.0</span>, <span class="number">4.0</span>]])</span><br><span class="line">w = tf.Variable(tf.random_uniform([<span class="number">2</span>, <span class="number">2</span>]))</span><br><span class="line">y = tf.matmul(x, w)</span><br><span class="line"><span class="comment"># ...</span></span><br><span class="line">loss = ...</span><br><span class="line">train_op = tf.train.AdagradOptimizer(<span class="number">0.01</span>).minimize(loss)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  <span class="comment"># `sess.graph` provides access to the graph used in a &lt;a href="../api_docs/python/tf/Session"&gt;&lt;code&gt;tf.Session&lt;/code&gt;&lt;/a&gt;.</span></span><br><span class="line">  writer = tf.summary.FileWriter(<span class="string">"/tmp/log/..."</span>, sess.graph)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Perform your computation...</span></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">    sess.run(train_op)</span><br><span class="line">    <span class="comment"># ...</span></span><br><span class="line"></span><br><span class="line">  writer.close()</span><br></pre></td></tr></table></figure><p>然后可以在 tensorboard 中打开日志并转到“图”标签，查看图结构的概要可视化图表。</p><h2 id="创建多个图">创建多个图</h2><p>TensorFlow 提供了一个“默认图”，此图明确传递给同一上下文中的所有 API 函数。TensorFlow 提供了操作默认图的方法，在更高级的用例中，这些方法可能有用。</p><ul><li>tf.Graph 会定义 tf.Operation 对象的命名空间：单个图中的每个操作必须具有唯一名称。如果请求的名称已被占用，TensorFlow 将在操作名称上附加 “_1”、&quot;_2&quot; 等字符，以便确保名称的唯一性。通过使用多个明确创建的图，可以更有效地控制为每个op指定什么样的名称。</li><li>默认图会存储与添加的每个 tf.Operation 和 tf.Tensor 有关的信息。如果程序创建了大量未连接的子图，更有效的做法是使用另一个 tf.Graph 构建每个子图，以便回收不相关的状态。</li></ul><h3 id="创建两个图">创建两个图</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">g_1 = tf.Graph()</span><br><span class="line"><span class="keyword">with</span> g_1.as_default():</span><br><span class="line">  <span class="comment"># Operations created in this scope will be added to `g_1`.</span></span><br><span class="line">  c = tf.constant(<span class="string">"Node in g_1"</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Sessions created in this scope will run operations from `g_1`.</span></span><br><span class="line">  sess_1 = tf.Session()</span><br><span class="line"></span><br><span class="line">g_2 = tf.Graph()</span><br><span class="line"><span class="keyword">with</span> g_2.as_default():</span><br><span class="line">  <span class="comment"># Operations created in this scope will be added to `g_2`.</span></span><br><span class="line">  d = tf.constant(<span class="string">"Node in g_2"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Alternatively, you can pass a graph when constructing a &lt;a href="../api_docs/python/tf/Session"&gt;&lt;code&gt;tf.Session&lt;/code&gt;&lt;/a&gt;:</span></span><br><span class="line"><span class="comment"># `sess_2` will run operations from `g_2`.</span></span><br><span class="line">sess_2 = tf.Session(graph=g_2)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> c.graph <span class="keyword">is</span> g_1</span><br><span class="line"><span class="keyword">assert</span> sess_1.graph <span class="keyword">is</span> g_1</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> d.graph <span class="keyword">is</span> g_2</span><br><span class="line"><span class="keyword">assert</span> sess_2.graph <span class="keyword">is</span> g_2</span><br></pre></td></tr></table></figure><h2 id="参考文献">参考文献</h2><p>1.<a href="https://www.tensorflow.org/guide/graphs?hl=zh_cn" target="_blank" rel="noopener">https://www.tensorflow.org/guide/graphs?hl=zh_cn</a><br>2.<a href="https://blog.csdn.net/shenxiaolu1984/article/details/52815641" target="_blank" rel="noopener">https://blog.csdn.net/shenxiaolu1984/article/details/52815641</a><br>3.<a href="https://danijar.com/what-is-a-tensorflow-session/" target="_blank" rel="noopener">https://danijar.com/what-is-a-tensorflow-session/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;tf-graph和tf-session&quot;&gt;tf.Graph和tf.Session&lt;/h2&gt;
&lt;p&gt;Graph和Session之间的区别和联系。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Graph定义了如何进行计算，但是并没有进行计算，graph不会hold任何值，它仅仅定义co
      
    
    </summary>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/categories/tensorflow/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow Varaible</title>
    <link href="http://mxxhcm.github.io/2019/05/12/tensorflow-Varaible/"/>
    <id>http://mxxhcm.github.io/2019/05/12/tensorflow-Varaible/</id>
    <published>2019-05-12T12:41:34.000Z</published>
    <updated>2019-07-09T12:27:12.102Z</updated>
    
    <content type="html"><![CDATA[<h2 id="创建variable">创建Variable</h2><p>Tensorflow有两种方式创建Variable：tf.Variable()和tf.get_variable()，这两种方式获得的都是tensorflow.python.ops.variables.Variable类型的对象，但是他们的输入参数还有些不一样。</p><table><thead><tr><th style="text-align:center"></th><th style="text-align:center">tf.Variable()</th><th style="text-align:center">tf.get_variable()</th></tr></thead><tbody><tr><td style="text-align:center">name</td><td style="text-align:center">不需要，已存在的变量名，会在后面加上递增的数值用来区分</td><td style="text-align:center">必须，已存在的会报错</td></tr><tr><td style="text-align:center">shape</td><td style="text-align:center">不需要，或者说已经包含在初值中了</td><td style="text-align:center">需要</td></tr><tr><td style="text-align:center">初值</td><td style="text-align:center">必须</td><td style="text-align:center">不需要</td></tr><tr><td style="text-align:center">复用</td><td style="text-align:center">不可以</td><td style="text-align:center">可以</td></tr></tbody></table><p>两种方法事实上都可以指定name和初值。而tf.Variable()的初值中已经包含了shape，所以不需要再显示传入shape了。这里的需要和不需要指的是必要不必要，如果没有传入需要的参数，就会报错，不需要的参数则不会影响。</p><h2 id="tf-variable">tf.Variable()</h2><h3 id="一句话介绍">一句话介绍</h3><p>创建一个类操作全局变量。在TensorFlow内部，tf.Variable会存储持久性张量，允许各种op读取和修改它的值。这些修改在多个Session之间是可见的，因此对于一个tf.Variable，多个工作器可以看到相同的值。</p><h3 id="和tf-tensor对比">和tf.Tensor对比</h3><p>tf.Variable 表示可通过对其运行op来改变其值的张量。与 tf.Tensor对象不同，tf.Variable 存在于单个session.run调用的上下文之外。tf.Tensor的值是不可以改变的，tf.Tensor没有assign函数。</p><h3 id="api">API</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">tf.Variable.__init__(</span><br><span class="line">initial_value=<span class="literal">None</span>,  <span class="comment"># 指定变量的初值</span></span><br><span class="line">trainable=<span class="literal">True</span>,  <span class="comment"># 是否在BP时训练该参数</span></span><br><span class="line">collections=<span class="literal">None</span>, <span class="comment"># 指定变量的collection</span></span><br><span class="line">validate_shape=<span class="literal">True</span>, </span><br><span class="line">caching_device=<span class="literal">None</span>, </span><br><span class="line">name=<span class="literal">None</span>,  <span class="comment"># 指定变量的名字</span></span><br><span class="line">...</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="代码示例">代码示例</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor1 = tf.Variable([[<span class="number">1</span>,<span class="number">2</span>], [<span class="number">3</span>,<span class="number">5</span>]])</span><br><span class="line">tensor2 = tf.Variable(tf.constant([[<span class="number">1</span>,<span class="number">2</span>], [<span class="number">3</span>,<span class="number">5</span>]]))</span><br><span class="line">sess.run(tf.global_variables_initializer())</span><br><span class="line">sess.run(tensor1)</span><br><span class="line">sess.run(tensor2)</span><br></pre></td></tr></table></figure><h3 id="初始化">初始化</h3><p>tf.Variable()生成的变量必须初始化，tf.constant()可以不用初始化。</p><ul><li>使用全局初始化<br>sess.run(tf.global_variables_initializer())</li><li>使用checkpoint</li><li>使用tf.assign赋值</li></ul><h2 id="tf-get-variable">tf.get_variable()</h2><h3 id="一句话介绍-v2">一句话介绍</h3><p>获取一个已经存在的变量或者创建一个新的变量。主要目的，变量复用。</p><h3 id="api-v2">API</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">tf.get_variable(</span><br><span class="line">    name, <span class="comment"># 指定变量的名字，必选项</span></span><br><span class="line">    shape=<span class="literal">None</span>, <span class="comment"># 指定变量的shape，可选项</span></span><br><span class="line">    dtype=<span class="literal">None</span>, <span class="comment"># 指定变量类型</span></span><br><span class="line">    initializer=<span class="literal">None</span>, <span class="comment"># 指定变量初始化器</span></span><br><span class="line">    regularizer=<span class="literal">None</span>,</span><br><span class="line">    trainable=<span class="literal">None</span>,</span><br><span class="line">    collections=<span class="literal">None</span>,</span><br><span class="line">    caching_device=<span class="literal">None</span>,</span><br><span class="line">    partitioner=<span class="literal">None</span>,</span><br><span class="line">    validate_shape=<span class="literal">True</span>,</span><br><span class="line">    use_resource=<span class="literal">None</span>,</span><br><span class="line">    custom_getter=<span class="literal">None</span>,</span><br><span class="line">    constraint=<span class="literal">None</span>,</span><br><span class="line">    synchronization=tf.VariableSynchronization.AUTO,</span><br><span class="line">    aggregation=tf.VariableAggregation.NONE</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="代码示例-v2">代码示例</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"model"</span>) <span class="keyword">as</span> scope:</span><br><span class="line">  output1 = my_image_filter(input1)</span><br><span class="line">  scope.reuse_variables()</span><br><span class="line">  output2 = my_image_filter(input2)</span><br></pre></td></tr></table></figure><h2 id="variable和collection">Variable和collection</h2><p><a href="https://mxxhcm.github.io/2019/05/13/tensorflow-collection/">点击查看关于collecion的详细介绍</a><br>默认情况下，每个tf.Variable()都会添加到以下两个collection中：</p><ul><li>tf.GraphKeys.GLOBAL_VARIABLES - 可以在多台设备间共享的变量，</li><li>tf.GraphKeys.TRAINABLE_VARIABLES - TensorFlow 将计算其梯度的变量。</li></ul><p>如果不希望变量是可训练的，可以在创建时指定其collection为 tf.GraphKeys.LOCAL_VARIABLES collection中。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">my_local = tf.get_variable(<span class="string">"my_local"</span>, shape=(), collections=[tf.GraphKeys.LOCAL_VARIABLES])</span><br></pre></td></tr></table></figure><p>或者可以指定 trainable=False：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">my_non_trainable = tf.get_variable(<span class="string">"my_non_trainable"</span>,</span><br><span class="line">                                   shape=(),</span><br><span class="line">                                   trainable=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><h3 id="获取collection">获取collection</h3><p>要检索放在某个collection中的所有变量的列表，可以使用：</p><h4 id="代码示例-v3">代码示例</h4><p><a href="https://github.com/mxxhcm/code/tree/master/tf/ops/tf_Variable_collection.py" target="_blank" rel="noopener">代码地址</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">a = tf.Variable([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">b = tf.get_variable(<span class="string">"bbb"</span>, shape=[<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">tf.constant([<span class="number">3</span>])</span><br><span class="line">c = tf.ones([<span class="number">3</span>])</span><br><span class="line">d = tf.random_uniform([<span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">print(tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES))</span><br><span class="line"><span class="comment"># [&lt;tf.Variable 'Variable:0' shape=(3,) dtype=int32_ref&gt;, &lt;tf.Variable 'bbb:0' shape=(2, 3) dtype=float32_ref&gt;]</span></span><br><span class="line"><span class="comment"># 可以看出来，只有tf.Variable()和tf.get_variable()产生的变量会加入到这个图中</span></span><br></pre></td></tr></table></figure><h3 id="自定义collection">自定义collection</h3><h4 id="添加自定义collection">添加自定义collection</h4><p>可以使用自定义的collection。collection名称可为任何字符串，且无需显式创建。创建对象（包括Variable和其他）后调用 tf.add_to_collection将其添加到相应collection中。以下代码将 my_local 变量添加到名为 my_collection_name 的collection中：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.add_to_collection(<span class="string">"my_collection_name"</span>, my_local)</span><br></pre></td></tr></table></figure><h2 id="初始化变量">初始化变量</h2><h3 id="初始化所有变量">初始化所有变量</h3><p>调用 tf.global_variables_initializer()在训练开始前一次性初始化所有可训练变量。此函数会返回一个op，负责初始化 tf.GraphKeys.GLOBAL_VARIABLES collection中的所有变量。运行此op会初始化所有变量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sess.run(tf.global_variables_initializer())</span><br></pre></td></tr></table></figure><h3 id="初始化单个变量">初始化单个变量</h3><p>运行变量的初始化器op。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sess.run(my_variable.initializer)</span><br></pre></td></tr></table></figure><h3 id="查询未初始化变量">查询未初始化变量</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(sess.run(tf.report_uninitialized_variables()))</span><br></pre></td></tr></table></figure><h2 id="共享变量">共享变量</h2><p>TensorFlow 支持两种共享变量的方式：</p><ul><li>显式传递 tf.Variable 对象。</li><li>将 tf.Variable 对象隐式封装在 tf.variable_scope 对象内。</li></ul><h3 id="variable-scope">variable_scope</h3><h4 id="代码示例1">代码示例1</h4><p>使用variable_scope区分weights和biases。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_relu</span><span class="params">(input, kernel_shape, bias_shape)</span>:</span></span><br><span class="line">    <span class="comment"># Create variable named "weights".</span></span><br><span class="line">    weights = tf.get_variable(<span class="string">"weights"</span>, kernel_shape,</span><br><span class="line">        initializer=tf.random_normal_initializer())</span><br><span class="line">    <span class="comment"># Create variable named "biases".</span></span><br><span class="line">    biases = tf.get_variable(<span class="string">"biases"</span>, bias_shape,</span><br><span class="line">        initializer=tf.constant_initializer(<span class="number">0.0</span>))</span><br><span class="line">    conv = tf.nn.conv2d(input, weights,</span><br><span class="line">        strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line">    <span class="keyword">return</span> tf.nn.relu(conv + biases)</span><br></pre></td></tr></table></figure><h4 id="代码示例2">代码示例2</h4><p>使用variable_scope声明不同作用域</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_image_filter</span><span class="params">(input_images)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"conv1"</span>):</span><br><span class="line">        <span class="comment"># Variables created here will be named "conv1/weights", "conv1/biases".</span></span><br><span class="line">        relu1 = conv_relu(input_images, [<span class="number">5</span>, <span class="number">5</span>, <span class="number">32</span>, <span class="number">32</span>], [<span class="number">32</span>])</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"conv2"</span>):</span><br><span class="line">        <span class="comment"># Variables created here will be named "conv2/weights", "conv2/biases".</span></span><br><span class="line">        <span class="keyword">return</span> conv_relu(relu1, [<span class="number">5</span>, <span class="number">5</span>, <span class="number">32</span>, <span class="number">32</span>], [<span class="number">32</span>])</span><br></pre></td></tr></table></figure><h3 id="共享方式1">共享方式1</h3><p>设置reuse=True</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"model"</span>):</span><br><span class="line">  output1 = my_image_filter(input1)</span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"model"</span>, reuse=<span class="literal">True</span>):</span><br><span class="line">  output2 = my_image_filter(input2)</span><br></pre></td></tr></table></figure><h3 id="共享方式2">共享方式2</h3><p>调用scope.reuse_variables触发重用</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"model"</span>) <span class="keyword">as</span> scope:</span><br><span class="line">  output1 = my_image_filter(input1)</span><br><span class="line">  scope.reuse_variables()</span><br><span class="line">  output2 = my_image_filter(input2)</span><br></pre></td></tr></table></figure><h2 id="参考文献">参考文献</h2><p>1.<a href="https://blog.csdn.net/MrR1ght/article/details/81228087" target="_blank" rel="noopener">https://blog.csdn.net/MrR1ght/article/details/81228087</a><br>2.<a href="https://www.tensorflow.org/guide/variables?hl=zh_cn" target="_blank" rel="noopener">https://www.tensorflow.org/guide/variables?hl=zh_cn</a><br>3.<a href="https://www.tensorflow.org/api_docs/python/tf/get_variable?hl=zh_cn" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/get_variable?hl=zh_cn</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;创建variable&quot;&gt;创建Variable&lt;/h2&gt;
&lt;p&gt;Tensorflow有两种方式创建Variable：tf.Variable()和tf.get_variable()，这两种方式获得的都是tensorflow.python.ops.variables.V
      
    
    </summary>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/categories/tensorflow/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow list of placeholder</title>
    <link href="http://mxxhcm.github.io/2019/05/12/tensorflow-list-of-placeholder/"/>
    <id>http://mxxhcm.github.io/2019/05/12/tensorflow-list-of-placeholder/</id>
    <published>2019-05-12T07:55:49.000Z</published>
    <updated>2019-05-12T12:31:11.231Z</updated>
    
    <content type="html"><![CDATA[<h2 id="list-of-placeholder">list of placeholder</h2><h3 id="目的">目的</h3><p>计算图中定义了一个placeholder list，如何使用feed_dict传入值。</p><h3 id="代码示例">代码示例</h3><p><a href="https://github.com/mxxhcm/code/blob/master/tf/ops/tf_placeholder_list.py" target="_blank" rel="noopener">代码地址</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个长度为n的placeholder list</span></span><br><span class="line">n = <span class="number">4</span></span><br><span class="line">ph_list = [tf.placeholder(tf.float32, [<span class="literal">None</span>, <span class="number">10</span>]) <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">4</span>)]</span><br><span class="line"><span class="comment"># 对这个ph list的操作</span></span><br><span class="line">result = tf.Variable(<span class="number">0.0</span>)</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> ph_list:</span><br><span class="line">    result = tf.add(result, x)</span><br><span class="line">hhhh = tf.log(result)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    sess = tf.Session()</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 生成数据</span></span><br><span class="line">    inputs = []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(n):</span><br><span class="line">        x = np.random.rand(<span class="number">16</span>, <span class="number">10</span>)</span><br><span class="line">        inputs.append(x)</span><br><span class="line">    <span class="comment"># 声明一个字典，存放placeholder和value键值对</span></span><br><span class="line">    feed_dictionary = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> k,v <span class="keyword">in</span> zip(ph_list, inputs):</span><br><span class="line">       feed_dictionary[k] = v</span><br><span class="line">    <span class="comment"># feed 数据</span></span><br><span class="line">    print(sess.run(hhhh, feed_dict=feed_dictionary).shape)</span><br></pre></td></tr></table></figure><h2 id="参考文献">参考文献</h2><p>1.<a href="https://stackoverflow.com/questions/51128427/how-to-feed-list-of-values-to-a-placeholder-list-in-tensorflow" target="_blank" rel="noopener">https://stackoverflow.com/questions/51128427/how-to-feed-list-of-values-to-a-placeholder-list-in-tensorflow</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;list-of-placeholder&quot;&gt;list of placeholder&lt;/h2&gt;
&lt;h3 id=&quot;目的&quot;&gt;目的&lt;/h3&gt;
&lt;p&gt;计算图中定义了一个placeholder list，如何使用feed_dict传入值。&lt;/p&gt;
&lt;h3 id=&quot;代码示例&quot;&gt;代
      
    
    </summary>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/categories/tensorflow/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow gather</title>
    <link href="http://mxxhcm.github.io/2019/05/11/tensorflow-gather/"/>
    <id>http://mxxhcm.github.io/2019/05/11/tensorflow-gather/</id>
    <published>2019-05-11T13:03:00.000Z</published>
    <updated>2019-05-12T12:35:59.200Z</updated>
    
    <content type="html"><![CDATA[<h2 id="tf-gather-nd">tf.gather_nd</h2><h3 id="一句话介绍">一句话介绍</h3><p>按照索引将输入tensor的某些维度拼凑成一个新的tenosr</p><h3 id="api">API</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tf.gather_nd(</span><br><span class="line">    params, <span class="comment"># 输入参数</span></span><br><span class="line">    indices, <span class="comment"># 索引</span></span><br><span class="line">    name=<span class="literal">None</span> <span class="comment">#</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>indices是一个K维的整形tensor。<br>indices的最后一维至多和params的rank一样大，如果indices.shape==params.rank，那么对应的是elements，如果indices.shape $\lt$ params.rank，那么对应的是slices。输出的tensor shape是：<br>indices.shape[:-1] + params.shape[indices.shape[-1]:]<br>原文如下：</p><blockquote><p>The last dimension of indices corresponds to elements (if indices.shape[-1] == params.rank) or slices (if indices.shape[-1] &lt; params.rank) along dimension indices.shape[-1] of params. The output tensor has shape<br>indices.shape[:-1] + params.shape[indices.shape[-1]:]</p></blockquote><p>如果indices是两维的，那么就相当于用第二维的indices去访问params，然后indices的第一维度相当于把第二维的tensor放入一个列表。<br>indices是高维（大于两维）的话，反正就是找最后一维的维度，然后到params中找对应的数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">    indices = [[[<span class="number">1</span>]], [[<span class="number">0</span>]]]</span><br><span class="line">    params = [[[<span class="string">'a0'</span>, <span class="string">'b0'</span>], [<span class="string">'c0'</span>, <span class="string">'d0'</span>]],</span><br><span class="line">              [[<span class="string">'a1'</span>, <span class="string">'b1'</span>], [<span class="string">'c1'</span>, <span class="string">'d1'</span>]]]</span><br><span class="line">    output = [[[[<span class="string">'a1'</span>, <span class="string">'b1'</span>], [<span class="string">'c1'</span>, <span class="string">'d1'</span>]]],</span><br><span class="line">              [[[<span class="string">'a0'</span>, <span class="string">'b0'</span>], [<span class="string">'c0'</span>, <span class="string">'d0'</span>]]]]</span><br><span class="line"><span class="comment"># 直接看indices的最后一维，然后到params中找，比如[1]，找params[1]=[['a1', 'b1'], ['c1', 'd1']]],params[0]=[['a0', 'b0'], ['c0', 'd0']]。然后在组成output，shape怎么确定？我的理解是，直接用params[1]的结果去替换indices中的[1]，也就是[[params[1]]]</span></span><br><span class="line"></span><br><span class="line">    indices = [[[<span class="number">0</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">0</span>]], [[<span class="number">0</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">1</span>]]]</span><br><span class="line">    params = [[[<span class="string">'a0'</span>, <span class="string">'b0'</span>], [<span class="string">'c0'</span>, <span class="string">'d0'</span>]],</span><br><span class="line">              [[<span class="string">'a1'</span>, <span class="string">'b1'</span>], [<span class="string">'c1'</span>, <span class="string">'d1'</span>]]]</span><br><span class="line">    output = [[[<span class="string">'c0'</span>, <span class="string">'d0'</span>], [<span class="string">'a1'</span>, <span class="string">'b1'</span>]],</span><br><span class="line">              [[<span class="string">'a0'</span>, <span class="string">'b0'</span>], [<span class="string">'c1'</span>, <span class="string">'d1'</span>]]]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    indices = [[[<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>]], [[<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>]]]</span><br><span class="line">    params = [[[<span class="string">'a0'</span>, <span class="string">'b0'</span>], [<span class="string">'c0'</span>, <span class="string">'d0'</span>]],</span><br><span class="line">              [[<span class="string">'a1'</span>, <span class="string">'b1'</span>], [<span class="string">'c1'</span>, <span class="string">'d1'</span>]]]</span><br><span class="line">    output = [[<span class="string">'b0'</span>, <span class="string">'b1'</span>], [<span class="string">'d0'</span>, <span class="string">'c1'</span>]]</span><br></pre></td></tr></table></figure><h3 id="代码示例1">代码示例1</h3><p><a href>代码地址</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line"></span><br><span class="line">data = np.array([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>],</span><br><span class="line">          [<span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>],</span><br><span class="line">          [<span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>, <span class="number">16</span>, <span class="number">17</span>],</span><br><span class="line">          [<span class="number">18</span>, <span class="number">19</span>, <span class="number">20</span>, <span class="number">21</span>, <span class="number">22</span>, <span class="number">23</span>],</span><br><span class="line">          [<span class="number">24</span>, <span class="number">25</span>, <span class="number">26</span>, <span class="number">27</span>, <span class="number">28</span>, <span class="number">29</span>]])</span><br><span class="line">data = np.reshape(np.arange(<span class="number">30</span>), [<span class="number">5</span>, <span class="number">6</span>])</span><br><span class="line">x = tf.constant(data)</span><br><span class="line">print(sess.run(x))</span><br><span class="line"><span class="comment"># [[ 0  1  2  3  4  5]</span></span><br><span class="line"><span class="comment">#  [ 6  7  8  9 10 11]</span></span><br><span class="line"><span class="comment">#  [12 13 14 15 16 17]</span></span><br><span class="line"><span class="comment">#  [18 19 20 21 22 23]</span></span><br><span class="line"><span class="comment">#  [24 25 26 27 28 29]]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Collecting elements from a tensor of rank 2</span></span><br><span class="line">result = tf.gather_nd(x, [<span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line">print(sess.run(result))</span><br><span class="line"><span class="comment"># indices.shape=(2,), indices.shape[:-1]=(), indices.shape[-1]=2, params.shape=(5,6), params.shape[indices.shape[-1]:]=(), outputs.shape=()+() = () </span></span><br><span class="line"><span class="comment"># 8 </span></span><br><span class="line">result = tf.gather_nd(x, [[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">2</span>,<span class="number">3</span>]])</span><br><span class="line">print(sess.run(result))</span><br><span class="line"><span class="comment"># indices.shape=(2,2), indices.shape[:-1]=(2,), indices.shape[-1]=2, params.shape=(5,6), params.shape[indices.shape[-1]:]=(), outputs.shape=(2,)+() = (2,) </span></span><br><span class="line"><span class="comment"># [8, 15]</span></span><br><span class="line"><span class="comment"># Collecting rows from a tensor of rank 2</span></span><br><span class="line">result = tf.gather_nd(x, [[<span class="number">1</span>],[<span class="number">2</span>]])</span><br><span class="line">print(sess.run(result))</span><br><span class="line"><span class="comment"># indices.shape=(2, 1), indices.shape[:-1]=(2,), indices.shape[-1]=1, params.shape=(5,6), params.shape[indices.shape[-1]:]=(6,), outputs.shape=(2,)+(6,) = (2,6,) </span></span><br><span class="line"><span class="comment"># [[ 6  7  8  9 10 11]</span></span><br><span class="line"><span class="comment">#  [12 13 14 15 16 17]]</span></span><br></pre></td></tr></table></figure><h3 id="代码示例2">代码示例2</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line"></span><br><span class="line">data = np.array([[[<span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">          [<span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">          [<span class="number">4</span>, <span class="number">5</span>]],</span><br><span class="line">         [[<span class="number">6</span>, <span class="number">7</span>],</span><br><span class="line">          [<span class="number">8</span>, <span class="number">9</span>],</span><br><span class="line">          [<span class="number">10</span>,<span class="number">11</span>]]])</span><br><span class="line">data = np.reshape(np.arange(<span class="number">12</span>), [<span class="number">2</span>, <span class="number">3</span>, <span class="number">2</span>])</span><br><span class="line">x = tf.constant(data)</span><br><span class="line">print(sess.run(x))</span><br><span class="line"><span class="comment">#[[[ 0  1]</span></span><br><span class="line"><span class="comment">#  [ 2  3]</span></span><br><span class="line"><span class="comment">#  [ 4  5]]</span></span><br><span class="line"><span class="comment"># [[ 6  7]</span></span><br><span class="line"><span class="comment">#  [ 8  9]</span></span><br><span class="line"><span class="comment">#  [10 11]]]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Collecting elements from a tensor of rank 3</span></span><br><span class="line">result = tf.gather_nd(x, [[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>]])</span><br><span class="line">print(sess.run(result))</span><br><span class="line"><span class="comment"># indices.shape=(2, 3), indices.shape[:-1]=(2,), indices.shape[-1]=3, params.shape=(2, 3, 2), params.shape[indices.shape[-1]:]=(), outputs.shape=(2,)+() = (2,) </span></span><br><span class="line"><span class="comment"># [0 11]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Collecting batched rows from a tensor of rank 3</span></span><br><span class="line">result = tf.gather_nd(x, [[[<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">1</span>]], [[<span class="number">1</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">1</span>]]])</span><br><span class="line">print(sess.run(result))</span><br><span class="line"><span class="comment"># indices.shape=(2, 2, 2), indices.shape[:-1]=(2, 2, ), indices.shape[-1]=2, params.shape=(2, 3, 2), params.shape[indices.shape[-1]:]=(2,), outputs.shape=(2, 2)+(2, ) = (2, 2, 2) </span></span><br><span class="line"><span class="comment"># [[[0 1]</span></span><br><span class="line"><span class="comment">#  [2 3]]</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># [[6 7]</span></span><br><span class="line"><span class="comment">#  [8 9]]]</span></span><br><span class="line"></span><br><span class="line">result = tf.gather_nd(x, [[<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">1</span>]])</span><br><span class="line">print(sess.run(result))</span><br><span class="line"><span class="comment"># indices.shape=(4, 2), indices.shape[:-1]=(4,), indices.shape[-1]=2, params.shape=(2, 3, 2), params.shape[indices.shape[-1]:]=(2,), outputs.shape=(4,)+(2,) = (4, 2) </span></span><br><span class="line"><span class="comment"># [[0 1]</span></span><br><span class="line"><span class="comment">#  [2 3]</span></span><br><span class="line"><span class="comment">#  [6 7]</span></span><br><span class="line"><span class="comment">#  [8 9]]</span></span><br></pre></td></tr></table></figure><h2 id="参考文献">参考文献</h2><p>1.<a href="https://www.tensorflow.org/api_docs/python/tf/gather_nd" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/gather_nd</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;tf-gather-nd&quot;&gt;tf.gather_nd&lt;/h2&gt;
&lt;h3 id=&quot;一句话介绍&quot;&gt;一句话介绍&lt;/h3&gt;
&lt;p&gt;按照索引将输入tensor的某些维度拼凑成一个新的tenosr&lt;/p&gt;
&lt;h3 id=&quot;api&quot;&gt;API&lt;/h3&gt;
&lt;figure class
      
    
    </summary>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/categories/tensorflow/"/>
    
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow cond</title>
    <link href="http://mxxhcm.github.io/2019/05/10/tensorflow-cond/"/>
    <id>http://mxxhcm.github.io/2019/05/10/tensorflow-cond/</id>
    <published>2019-05-10T09:01:14.000Z</published>
    <updated>2019-05-12T12:35:04.910Z</updated>
    
    <content type="html"><![CDATA[<h2 id="tf-cond">tf.cond</h2><h3 id="一句话介绍">一句话介绍</h3><p>和if语句的功能和很像，如果条件为真，返回一个函数，如果条件为假，返回另一个函数。</p><h3 id="api">API</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">tf.cond(</span><br><span class="line">    pred, <span class="comment"># 条件</span></span><br><span class="line">    true_fn=<span class="literal">None</span>, <span class="comment"># 如果条件为真，执行该函数</span></span><br><span class="line">    false_fn=<span class="literal">None</span>, <span class="comment"># 如果条件为假，执行该函数</span></span><br><span class="line">    strict=<span class="literal">False</span>,</span><br><span class="line">    name=<span class="literal">None</span>,</span><br><span class="line">    fn1=<span class="literal">None</span>,</span><br><span class="line">    fn2=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>最后返回的是true_fn或者false_fn返回的还是tf.Tensor类型的变量。</p><h3 id="代码示例1">代码示例1</h3><p><a href>代码地址</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x = tf.placeholder(tf.int32, [<span class="number">10</span>])</span><br><span class="line">y = tf.constant([<span class="number">10</span>, <span class="number">3.2</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># for i in range(10):</span></span><br><span class="line"><span class="comment">#     if tf.equal(x[i], 0):</span></span><br><span class="line"><span class="comment">#         y = tf.add(y, 1)</span></span><br><span class="line"><span class="comment">#     else:</span></span><br><span class="line"><span class="comment">#         y = tf.add(y, 10)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 上面的代码起到了和下面代码相同的作用，但是上面的代码在tensorflow中会报错，不能运行，因为x[i]==0返回的不是python的bool类型，而是bool类型的tf.Tensor。</span></span><br><span class="line"><span class="comment"># TypeError: Using a tf.Tensor as a Python bool is not allowed.</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    y = tf.cond(tf.equal(x[i], <span class="number">0</span>), <span class="keyword">lambda</span>: tf.add(y, <span class="number">1</span>), <span class="keyword">lambda</span>: tf.add(y, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">result = tf.log(y)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">   inputs = np.array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>])</span><br><span class="line">   print(sess.run(result, feed_dict=&#123;x: inputs&#125;))</span><br></pre></td></tr></table></figure><h3 id="代码示例2">代码示例2</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">myfunc</span><span class="params">(x)</span>:</span></span><br><span class="line">   <span class="keyword">if</span> (x &gt; <span class="number">0</span>):</span><br><span class="line">      <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">   <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    x = tf.constant(<span class="number">4</span>)</span><br><span class="line">    <span class="comment"># print(myfunc(x))</span></span><br><span class="line">    <span class="comment"># raise TypeError("Using a `tf.Tensor` as a Python `bool` is not allowed. "</span></span><br><span class="line">    <span class="comment"># TypeError: Using a `tf.Tensor` as a Python `bool` is not allowed. Use `if t is not None:` instead of `if t:` to test if a tensor is defined, and use TensorFlow ops such as tf.cond to execute subgraphs conditioned on the value of a tensor.</span></span><br><span class="line">    result = tf.cond(tf.greater(x, <span class="number">0</span>), <span class="keyword">lambda</span>: <span class="number">1</span>, <span class="keyword">lambda</span>: <span class="number">0</span>)</span><br><span class="line">    print(type(result))</span><br><span class="line">    print(result.eval())</span><br></pre></td></tr></table></figure><p>上述代码中定义了一个函数，实现判断某个值是否大于0。但是这个函数是错误的，因为$x\gt 0$返回一个bool类型的tf.Tensor不能用作if的判断条件，所以需要使用tf.cond语句。</p><h3 id="代码示例3">代码示例3</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Example 3</span></span><br><span class="line">x = tf.constant(<span class="number">4</span>)</span><br><span class="line">y = tf.constant(<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(x) </span><br><span class="line">    print(y) </span><br><span class="line">    <span class="keyword">if</span> x == y:</span><br><span class="line">      print(<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      print(<span class="literal">False</span>)</span><br><span class="line">    result = tf.equal(x, y)</span><br><span class="line">    print(result.eval())</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">f1</span><span class="params">()</span>:</span> </span><br><span class="line">      print(<span class="string">"f1 declare"</span>)</span><br><span class="line">      <span class="keyword">return</span> [<span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">f2</span><span class="params">()</span>:</span></span><br><span class="line">      print(<span class="string">"f2 declare"</span>)</span><br><span class="line">      <span class="keyword">return</span> [<span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">    res = tf.cond(tf.equal(x, y), f1, f2)</span><br><span class="line">    print(res)</span><br></pre></td></tr></table></figure><h2 id="参考文献">参考文献</h2><p>1.<a href="https://www.tensorflow.org/api_docs/python/tf/cond" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/cond</a><br>2.<a href="https://stackoverflow.com/questions/48571521/tensorflow-error-using-a-tf-tensor-as-a-python-bool-is-not-allowed" target="_blank" rel="noopener">https://stackoverflow.com/questions/48571521/tensorflow-error-using-a-tf-tensor-as-a-python-bool-is-not-allowed</a><br>3.<a href="https://blog.csdn.net/Cerisier/article/details/79819248" target="_blank" rel="noopener">https://blog.csdn.net/Cerisier/article/details/79819248</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;tf-cond&quot;&gt;tf.cond&lt;/h2&gt;
&lt;h3 id=&quot;一句话介绍&quot;&gt;一句话介绍&lt;/h3&gt;
&lt;p&gt;和if语句的功能和很像，如果条件为真，返回一个函数，如果条件为假，返回另一个函数。&lt;/p&gt;
&lt;h3 id=&quot;api&quot;&gt;API&lt;/h3&gt;
&lt;figure class
      
    
    </summary>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/categories/tensorflow/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>python cv2.imresize图像缩放</title>
    <link href="http://mxxhcm.github.io/2019/05/09/python-cv2-imresize/"/>
    <id>http://mxxhcm.github.io/2019/05/09/python-cv2-imresize/</id>
    <published>2019-05-09T13:37:30.000Z</published>
    <updated>2019-05-10T11:37:24.676Z</updated>
    
    <content type="html"><![CDATA[<h2 id="cv2-resize">cv2.resize</h2><p>cv2是python的opencv包，实现的功能是对一个图片进行缩放。<br>python3下安装命令：<br>~$:pip install opencv-python</p><h3 id="示例代码">示例代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">img = np.random.rand(<span class="number">210</span>, <span class="number">160</span> ,<span class="number">3</span>)</span><br><span class="line">print(img.shape)</span><br><span class="line">img_scale = cv2.resize(img, (<span class="number">84</span>, <span class="number">84</span>))</span><br><span class="line">print(img_scale.shape)</span><br></pre></td></tr></table></figure><h2 id="参考文献">参考文献</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;cv2-resize&quot;&gt;cv2.resize&lt;/h2&gt;
&lt;p&gt;cv2是python的opencv包，实现的功能是对一个图片进行缩放。&lt;br&gt;
python3下安装命令：&lt;br&gt;
~$:pip install opencv-python&lt;/p&gt;
&lt;h3 id=&quot;示例
      
    
    </summary>
    
      <category term="python" scheme="http://mxxhcm.github.io/categories/python/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="opencv" scheme="http://mxxhcm.github.io/tags/opencv/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow reduction</title>
    <link href="http://mxxhcm.github.io/2019/05/09/tensorflow-reduction/"/>
    <id>http://mxxhcm.github.io/2019/05/09/tensorflow-reduction/</id>
    <published>2019-05-09T03:19:00.000Z</published>
    <updated>2019-05-12T12:36:46.605Z</updated>
    
    <content type="html"><![CDATA[<h2 id="tf-reduction">tf.Reduction</h2><ul><li>tf.reduce_sum(input_tensor, reduction_indices=None, keep_dims=False, name=None)  # 计算input_tensor的和，可指定dim。</li><li>tf.reduce_mean(input_tensor, reduction_indices=None, keep_dims=False, name=None) # 计算input_tensor的均值，可指定dim。</li><li>tf.reduce_min(input_tensor, reduction_indices=None, keep_dims=False, name=None) # 计算input_tensor的最小值，可指定dim。</li><li>tf.reduce_max(input_tensor, reduction_indices=None, keep_dims=False, name=None) # 计算input_tensor的最大值，可指定dim。</li><li>tf.recude_proc(input_tensor, reduction_indices=None, keep_dims=False, name=None) # 计算input_tensor的乘积，可指定dim。</li><li>tf.reduce_all(input_tensor, reduction_indices=None, keep_dims=False, name=None) # 计算input_tensor中所有元素的逻辑与，可指定dim。</li><li>tf.reduce_any(input_tensor, reduction_indices=None, keep_dims=False, name=None) # 计算input_tensor的所有元素的逻辑或，可指定dim。</li><li>tf.accumulate_n(inputs, shape=None, tensor_dtype=None, name=None) # 计算inputs的和。</li><li>tf.cumsum(x, axis=0, exclusive=False, reverse=False, name=None) # 计算input_tensor的累积和。</li></ul><h2 id="代码示例">代码示例</h2><h3 id="tf-reduce-sum">tf.reduce_sum()</h3><p><a href="https://github.com/mxxhcm/code/blob/master/tf/some_ops/tf_reduce_sum.py" target="_blank" rel="noopener">代码地址</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x = tf.placeholder(dtype=tf.float32, shape=[<span class="literal">None</span>, <span class="number">2</span>])</span><br><span class="line">y = tf.log(x)</span><br><span class="line"><span class="comment"># 对所有y求和</span></span><br><span class="line">loss = tf.reduce_sum(y)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess :</span><br><span class="line">    <span class="comment"># inputs = tf.constant([1.0, 2.0])</span></span><br><span class="line">    inputs = np.array([[<span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line">    l = sess.run(loss, feed_dict=&#123;x: inputs&#125;)</span><br><span class="line">    print(l)</span><br></pre></td></tr></table></figure><h2 id="参考文献">参考文献</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;tf-reduction&quot;&gt;tf.Reduction&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;tf.reduce_sum(input_tensor, reduction_indices=None, keep_dims=False, name=None)  # 计算input_
      
    
    </summary>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/categories/tensorflow/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>pytorch</title>
    <link href="http://mxxhcm.github.io/2019/05/08/pytorch-basic/"/>
    <id>http://mxxhcm.github.io/2019/05/08/pytorch-basic/</id>
    <published>2019-05-08T14:07:42.000Z</published>
    <updated>2019-05-17T08:14:01.433Z</updated>
    
    <content type="html"><![CDATA[<h2 id="torch">torch</h2><p>torch提供了很多基础操作，包括数学操作等等。</p><h2 id="torch-cat">torch.cat</h2><h3 id="函数原型">函数原型</h3><p>将多个tensor在某一个维度上（默认是第0维）拼接到一起（除了拼接的维度上，其他维度的shape必须一定），最后返回一个tensor。<br>torch.cat(tensors, dim=0, out=None) → Tensor</p><blockquote><p>Concatenates the given sequence of seq tensors in the given dimension. All tensors must either have the same shape (except in the concatenating dimension) or be empty.</p></blockquote><h3 id="参数">参数</h3><p>tensors (sequence of Tensors) – 任意类型相同python序列或者tensor<br>dim (int, optional) - 在第几个维度上进行拼接(只有在拼接的维度上可以不同，其余维度必须相同。<br>out (Tensor, optional) – 输出的tensor</p><h3 id="例子">例子</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x1 = torch.randn(<span class="number">3</span>, <span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line">x2 = torch.randn(<span class="number">3</span>, <span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line">x = torch.cat([x1, x2], <span class="number">1</span>)</span><br><span class="line">print(x.size())</span><br></pre></td></tr></table></figure><p>输出如下：</p><blockquote><p>torch.Size([3, 5, 4])</p></blockquote><h2 id="torch中图像-img-格式">torch中图像(img)格式</h2><p>torch中图像的shape是(‘RGB’,width, height)，而numpy和matplotlib中都是(width, height, ‘RGB’)<br>matplotlib.pyplot.imshow()需要的参数是图像矩阵，如果矩阵中是整数，那么它的值需要在区间[0,255]之内，如果是浮点数，需要在[0,1]之间。</p><blockquote><p>Clipping input data to the valid range for imshow with RGB data ([0…1] for floats or [0…255] for integers).</p></blockquote><h2 id="参考文献">参考文献</h2><p>1.<a href="https://pytorch.org/docs/stable/torch.html" target="_blank" rel="noopener">https://pytorch.org/docs/stable/torch.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;torch&quot;&gt;torch&lt;/h2&gt;
&lt;p&gt;torch提供了很多基础操作，包括数学操作等等。&lt;/p&gt;
&lt;h2 id=&quot;torch-cat&quot;&gt;torch.cat&lt;/h2&gt;
&lt;h3 id=&quot;函数原型&quot;&gt;函数原型&lt;/h3&gt;
&lt;p&gt;将多个tensor在某一个维度上（默认是第
      
    
    </summary>
    
      <category term="pytorch" scheme="http://mxxhcm.github.io/categories/pytorch/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="pytorch" scheme="http://mxxhcm.github.io/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>pytorch initialize parameters</title>
    <link href="http://mxxhcm.github.io/2019/05/08/pytorch-initialize-parameters/"/>
    <id>http://mxxhcm.github.io/2019/05/08/pytorch-initialize-parameters/</id>
    <published>2019-05-08T14:01:24.000Z</published>
    <updated>2019-05-17T08:21:56.489Z</updated>
    
    <content type="html"><![CDATA[<h2 id="神经网络参数初始化">神经网络参数初始化</h2><h3 id="方法-1-model-apply-fn">方法$1$.Model.apply(fn)</h3><p><a href="https://github.com/mxxhcm/myown_code/blob/master/pytorch/tutorials/initialize/apply.py" target="_blank" rel="noopener">示例</a>如下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_weights</span><span class="params">(m)</span>:</span></span><br><span class="line">  print(m)</span><br><span class="line">  <span class="keyword">if</span> type(m) == nn.Linear:</span><br><span class="line">    m.weight.data.fill_(<span class="number">1.0</span>)</span><br><span class="line">    print(m.weight)</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(nn.Linear(<span class="number">2</span>, <span class="number">2</span>), nn.Linear(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">net.apply(init_weights)</span><br></pre></td></tr></table></figure><p>输出结果如下：</p><blockquote><p>Linear(in_features=2, out_features=2, bias=True)<br>Parameter containing:<br>tensor([[1., 1.],<br>[1., 1.]], requires_grad=True)<br>Linear(in_features=2, out_features=2, bias=True)<br>Parameter containing:<br>tensor([[1., 1.],<br>[1., 1.]], requires_grad=True)<br>Sequential(<br>(0): Linear(in_features=2, out_features=2, bias=True)<br>(1): Linear(in_features=2, out_features=2, bias=True)<br>)<br>Linear(in_features=2, out_features=2, bias=True)<br>Linear(in_features=2, out_features=2, bias=True)</p></blockquote><p>其中最后两行为net对象调用self.children()函数返回的模块，就是模型中所有网络的参数。事实上，调用net.apply(fn)函数，会对self.children()中的所有模块应用fn函数，</p><h2 id="参考文献">参考文献</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;神经网络参数初始化&quot;&gt;神经网络参数初始化&lt;/h2&gt;
&lt;h3 id=&quot;方法-1-model-apply-fn&quot;&gt;方法$1$.Model.apply(fn)&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/mxxhcm/myown_code/b
      
    
    </summary>
    
      <category term="pytorch" scheme="http://mxxhcm.github.io/categories/pytorch/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="pytorch" scheme="http://mxxhcm.github.io/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>pytorch optim</title>
    <link href="http://mxxhcm.github.io/2019/05/08/pytorch-optim/"/>
    <id>http://mxxhcm.github.io/2019/05/08/pytorch-optim/</id>
    <published>2019-05-08T13:58:19.000Z</published>
    <updated>2019-05-17T08:20:33.900Z</updated>
    
    <content type="html"><![CDATA[<h2 id="torch-optim">torch.optim</h2><h3 id="基类class-optimizer-object">基类class Optimizer(object)</h3><p>Optimizer是所有optimizer的基类。<br>调用任何优化器都要先初始化Optimizer类，这里拿Adam优化器举例子。Adam optimizer的init函数如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Adam</span><span class="params">(Optimizer)</span>:</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">        params (iterable): iterable of parameters to optimize or dicts defining</span></span><br><span class="line"><span class="string">            parameter groups</span></span><br><span class="line"><span class="string">        lr (float, optional): learning rate (default: 1e-3)</span></span><br><span class="line"><span class="string">        betas (Tuple[float, float], optional): coefficients used for computing</span></span><br><span class="line"><span class="string">            running averages of gradient and its square (default: (0.9, 0.999))</span></span><br><span class="line"><span class="string">        eps (float, optional): term added to the denominator to improve</span></span><br><span class="line"><span class="string">            numerical stability (default: 1e-8)</span></span><br><span class="line"><span class="string">        weight_decay (float, optional): weight decay (L2 penalty)</span></span><br><span class="line"><span class="string">        amsgrad (boolean, optional): whether to use the AMSGrad variant of this</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, params, lr=<span class="number">1e-3</span>, betas=<span class="params">(<span class="number">0.9</span>, <span class="number">0.999</span>)</span>, eps=<span class="number">1e-8</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 weight_decay=<span class="number">0</span>, amsgrad=False)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="number">0.0</span> &lt;= lr:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">"Invalid learning rate: &#123;&#125;"</span>.format(lr))</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="number">0.0</span> &lt;= eps:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">"Invalid epsilon value: &#123;&#125;"</span>.format(eps))</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="number">0.0</span> &lt;= betas[<span class="number">0</span>] &lt; <span class="number">1.0</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">"Invalid beta parameter at index 0: &#123;&#125;"</span>.format(betas[<span class="number">0</span>]))</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="number">0.0</span> &lt;= betas[<span class="number">1</span>] &lt; <span class="number">1.0</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">"Invalid beta parameter at index 1: &#123;&#125;"</span>.format(betas[<span class="number">1</span>]))</span><br><span class="line">        defaults = dict(lr=lr, betas=betas, eps=eps,</span><br><span class="line">                        weight_decay=weight_decay, amsgrad=amsgrad)</span><br><span class="line">        super(Adam, self).__init__(params, defaults)</span><br></pre></td></tr></table></figure><p>上述代码将学习率lr,beta,epsilon,weight_decay,amsgrad等封装在一个dict中，然后将其传给Optimizer的init函数，其代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Optimizer</span><span class="params">(object)</span>:</span></span><br><span class="line">    .. warning::</span><br><span class="line">        Parameters need to be specified <span class="keyword">as</span> collections that have a deterministic</span><br><span class="line">        ordering that <span class="keyword">is</span> consistent between runs. Examples of objects that don<span class="string">'t</span></span><br><span class="line"><span class="string">        satisfy those properties are sets and iterators over values of dictionaries.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        params (iterable): an iterable of :class:`torch.Tensor` s or</span></span><br><span class="line"><span class="string">            :class:`dict` s. Specifies what Tensors should be optimized.</span></span><br><span class="line"><span class="string">        defaults: (dict): a dict containing default values of optimization</span></span><br><span class="line"><span class="string">            options (used when a parameter group doesn'</span>t specify them).</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    def __init__(self, params, defaults):</span></span><br><span class="line"><span class="string">        self.defaults = defaults</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        if isinstance(params, torch.Tensor):</span></span><br><span class="line"><span class="string">            raise TypeError("params argument given to the optimizer should be "</span></span><br><span class="line"><span class="string">                            "an iterable of Tensors or dicts, but got " +</span></span><br><span class="line"><span class="string">                            torch.typename(params))</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        self.state = defaultdict(dict)</span></span><br><span class="line"><span class="string">        self.param_groups = []</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        param_groups = list(params)</span></span><br><span class="line"><span class="string">        if len(param_groups) == 0:</span></span><br><span class="line"><span class="string">            raise ValueError("optimizer got an empty parameter list")</span></span><br><span class="line"><span class="string">        if not isinstance(param_groups[0], dict):</span></span><br><span class="line"><span class="string">            param_groups = [&#123;'params': param_groups&#125;]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        for param_group in param_groups:</span></span><br><span class="line"><span class="string">            self.add_param_group(param_group)</span></span><br></pre></td></tr></table></figure><p>从这里可以看出来，每个pytorch给出的optimizer至少有以下三个属性和四个函数：<br>属性：</p><ul><li>self.defaults # 字典类型，主要包含学习率等值</li><li>self.state # defaultdict(&lt;class ‘dict’&gt;, {}) state存放的是</li><li>self.param_gropus # &lt;class ‘list’&gt;:[]，prama_groups是一个字典类型的列表，用来存放parameters。</li></ul><p>函数：</p><ul><li>self.zero_grad()  # 将optimizer中参数的梯度置零</li><li>self.step()  # 将梯度应用在参数上</li><li>self.state_dict() # 返回optimizer的state,包括state和param_groups。</li><li>self.load_state_dict()  # 加载optimizer的state。</li><li>self.add_param_group()  # 将一个param group添加到param_groups。可以用在fine-tune上，只添加我们需要训练的层数，然后其他层不动。</li></ul><p>如果param已经是一个字典列表的话，就无需操作，否则就需要把param转化成一个字典param_groups。然后对param_groups中的每一个param_group调用add_param_group(param_group)函数将param_group字典和defaults字典拼接成一个新的param_group字典添加到self.param_groups中。</p><h2 id="参考文献">参考文献</h2><p>1.<a href="https://pytorch.org/docs/stable/optim.html" target="_blank" rel="noopener">https://pytorch.org/docs/stable/optim.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;torch-optim&quot;&gt;torch.optim&lt;/h2&gt;
&lt;h3 id=&quot;基类class-optimizer-object&quot;&gt;基类class Optimizer(object)&lt;/h3&gt;
&lt;p&gt;Optimizer是所有optimizer的基类。&lt;br&gt;
调用任何
      
    
    </summary>
    
      <category term="pytorch" scheme="http://mxxhcm.github.io/categories/pytorch/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="pytorch" scheme="http://mxxhcm.github.io/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>pytorch utils data</title>
    <link href="http://mxxhcm.github.io/2019/05/08/pytorch-utils-data/"/>
    <id>http://mxxhcm.github.io/2019/05/08/pytorch-utils-data/</id>
    <published>2019-05-08T13:56:15.000Z</published>
    <updated>2019-05-17T07:45:03.279Z</updated>
    
    <content type="html"><![CDATA[<h2 id="torch-utils-data">torch.utils.data</h2><h3 id="dataloader">Dataloader</h3><h4 id="原型">原型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">torch</span>.<span class="title">utils</span>.<span class="title">data</span>.<span class="title">DataLoader</span><span class="params">(</span></span></span><br><span class="line"><span class="class"><span class="params">dataset, # 从哪加载数据</span></span></span><br><span class="line"><span class="class"><span class="params">batch_size=<span class="number">1</span>, # batch大小 <span class="params">(default: <span class="number">1</span>)</span>.</span></span></span><br><span class="line"><span class="class"><span class="params">shuffle=False,# 每个epoch的数据是否打乱 <span class="params">(default: False)</span>.</span></span></span><br><span class="line"><span class="class"><span class="params">sampler=None, # 定义采样策略。如果指定这个参数, shuffle必须是False.</span></span></span><br><span class="line"><span class="class"><span class="params">batch_sampler=None, </span></span></span><br><span class="line"><span class="class"><span class="params">num_workers=<span class="number">0</span>,# 多少个子进程用来进行数据加载。<span class="number">0</span>代表使用主进程加载数据 <span class="params">(default: <span class="number">0</span>)</span></span></span></span><br><span class="line"><span class="class"><span class="params">collate_fn=&lt;function default_collate&gt;, </span></span></span><br><span class="line"><span class="class"><span class="params">pin_memory=False, </span></span></span><br><span class="line"><span class="class"><span class="params">drop_last=False, </span></span></span><br><span class="line"><span class="class"><span class="params">timeout=<span class="number">0</span>, </span></span></span><br><span class="line"><span class="class"><span class="params">worker_init_fn=None</span></span></span><br><span class="line"><span class="class"><span class="params">)</span></span></span><br></pre></td></tr></table></figure><h4 id="例子">例子</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line">dataset = torchvision.datasets.CIFAR100(root=<span class="string">'./data'</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=<span class="literal">None</span>)</span><br><span class="line">train_loader = torch.utils.data.DataLoader(dataset, bath_size=<span class="number">16</span>, shuffle=<span class="literal">False</span>, num_worker=<span class="number">2</span>)</span><br></pre></td></tr></table></figure><h4 id="如何访问dataloader返回值">如何访问DataLoader返回值</h4><p>train_loader不是整数，所以不能用range，这里用enumerate()，i是</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i, data <span class="keyword">in</span> enumerate(train_loader):</span><br><span class="line">    images, labels = data</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;torch-utils-data&quot;&gt;torch.utils.data&lt;/h2&gt;
&lt;h3 id=&quot;dataloader&quot;&gt;Dataloader&lt;/h3&gt;
&lt;h4 id=&quot;原型&quot;&gt;原型&lt;/h4&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;ta
      
    
    </summary>
    
      <category term="pytorch" scheme="http://mxxhcm.github.io/categories/pytorch/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="pytorch" scheme="http://mxxhcm.github.io/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>pytorch torchvision</title>
    <link href="http://mxxhcm.github.io/2019/05/08/pytorch-torchvision/"/>
    <id>http://mxxhcm.github.io/2019/05/08/pytorch-torchvision/</id>
    <published>2019-05-08T13:55:57.000Z</published>
    <updated>2019-05-17T08:12:15.631Z</updated>
    
    <content type="html"><![CDATA[<h2 id="torchvision">torchvision</h2><p>torchvision是pytorch提供的一些工具包，主要包含下列几个模块</p><ul><li>torchvision.datasets</li><li>torchvision.utils</li><li>torchvision.transforms</li><li>torchvision.models</li></ul><h2 id="torchvision-datasets">torchvision.datasets</h2><p>torchvision提供了很多数据集</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line">print(torchvision.datasets.__all__)</span><br></pre></td></tr></table></figure><blockquote><p>(‘LSUN’, ‘LSUNClass’, ‘ImageFolder’, ‘DatasetFolder’, ‘FakeData’, ‘CocoCaptions’,     ‘CocoDetection’, ‘CIFAR10’, ‘CIFAR100’, ‘EMNIST’, ‘FashionMNIST’, ‘MNIST’, ‘STL10’,     ‘SVHN’, ‘PhotoTour’, ‘SEMEION’, ‘Omniglot’)</p></blockquote><h3 id="cifar10">CIFAR10</h3><h4 id="原型">原型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">calss torchvision.datasets.CIFAR10(root, train=<span class="literal">True</span>, transform=<span class="literal">None</span>, target_transform=<span class="literal">None</span>, download=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><h4 id="参数">参数</h4><p>root (string) – cifar-10-batches-py的存放目录或者download设置为True时将会存放的目录。<br>train (bool, optional) – 设置为True的时候, 从training set创建dataset, 否则从test set创建dataset.<br>transform (callable, optional) – 输入是一个 PIL image，返回一个transformed的版本。如，transforms.RandomCrop<br>target_transform (callable, optional) – A function/transform that takes in the target and transforms it.<br>download (bool, optional) – If true, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again.</p><h4 id="例子">例子</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line">trainset = torchvision.datasets.CIFAR100(root=<span class="string">"./datasets"</span>, train=<span class="literal">True</span>, transform=    <span class="literal">None</span>, download=<span class="literal">True</span>)</span><br><span class="line">testset = torchvision.datasets.CIFAR100(root=<span class="string">"./datasets"</span>, train=<span class="literal">False</span>, transform=    <span class="literal">None</span>, download=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h2 id="torchvision-models">torchvision.models</h2><p>模型</p><h2 id="torchvision-transforms">torchvision.transforms</h2><p>transform</p><h2 id="torchvision-utils">torchvision.utils</h2><p>一些工具包</p><h2 id="参考文献">参考文献</h2><p>1.<a href="https://pytorch.org/docs/stable/torchvision/index.html" target="_blank" rel="noopener">https://pytorch.org/docs/stable/torchvision/index.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;torchvision&quot;&gt;torchvision&lt;/h2&gt;
&lt;p&gt;torchvision是pytorch提供的一些工具包，主要包含下列几个模块&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;torchvision.datasets&lt;/li&gt;
&lt;li&gt;torchvision.utils
      
    
    </summary>
    
      <category term="pytorch" scheme="http://mxxhcm.github.io/categories/pytorch/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="pytorch" scheme="http://mxxhcm.github.io/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>pytorch nn functional</title>
    <link href="http://mxxhcm.github.io/2019/05/08/pytorch-nn-functional/"/>
    <id>http://mxxhcm.github.io/2019/05/08/pytorch-nn-functional/</id>
    <published>2019-05-08T13:55:37.000Z</published>
    <updated>2019-05-17T08:17:14.369Z</updated>
    
    <content type="html"><![CDATA[<h2 id="torch-nn-functional">torch.nn.functional</h2><p>该包提供了很多网络函数</p><h2 id="convoludion-functions">convoludion functions</h2><h3 id="conv2d">conv2d</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torch.nn as nn</span><br><span class="line">import torch.nn.functional as F</span><br><span class="line">from torch.autograd import Variable</span><br><span class="line"></span><br><span class="line">inputs = Variable(torch.randn(64,3,32,32))</span><br><span class="line"></span><br><span class="line">filters1 = Variable(torch.randn(16,3,3,3))</span><br><span class="line">output1 = F.conv2d(inputs,filters1)</span><br><span class="line">print(output1.size())</span><br><span class="line"></span><br><span class="line">filters2 = Variable(torch.randn(16,3,3,3))</span><br><span class="line">output2 = F.conv2d(inputs,filters2,padding=1)</span><br><span class="line">print(output2.size())</span><br></pre></td></tr></table></figure><p>输出</p><blockquote><p>torch.Size([64, 16, 30, 30])<br>torch.Size([64, 16, 32, 32])</p></blockquote><h2 id="relu-functions">relu functions</h2><h2 id="pooling-functions">pooling functions</h2><h2 id="dropout-functions">dropout functions</h2><h3 id="例子">例子</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"> </span><br><span class="line">x = torch.randn(<span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line">y = F.dropout(x, <span class="number">0.5</span>, <span class="literal">True</span>)</span><br><span class="line">y = F.dropout2d(x, <span class="number">0.5</span>)</span><br><span class="line"> </span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure><p>注意$2$中说的问题，不过可能已经被改正了，注意一些就是了。</p><h2 id="linear-functions">linear functions</h2><h2 id="loss-functions">loss functions</h2><h2 id="参考文献">参考文献</h2><p>1.<a href="https://pytorch.org/docs/stable/nn.html#torch-nn-functional" target="_blank" rel="noopener">https://pytorch.org/docs/stable/nn.html#torch-nn-functional</a><br>2.<a href="https://pytorch.org/docs/stable/nn.html#torch-nn-functional" target="_blank" rel="noopener">https://pytorch.org/docs/stable/nn.html#torch-nn-functional</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;torch-nn-functional&quot;&gt;torch.nn.functional&lt;/h2&gt;
&lt;p&gt;该包提供了很多网络函数&lt;/p&gt;
&lt;h2 id=&quot;convoludion-functions&quot;&gt;convoludion functions&lt;/h2&gt;
&lt;h3 id=&quot;c
      
    
    </summary>
    
      <category term="pytorch" scheme="http://mxxhcm.github.io/categories/pytorch/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="pytorch" scheme="http://mxxhcm.github.io/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>pytorch nn</title>
    <link href="http://mxxhcm.github.io/2019/05/08/pytorch-nn/"/>
    <id>http://mxxhcm.github.io/2019/05/08/pytorch-nn/</id>
    <published>2019-05-08T13:55:18.000Z</published>
    <updated>2019-06-07T10:14:29.318Z</updated>
    
    <content type="html"><![CDATA[<h2 id="parameter">Parameter</h2><h3 id="一句话介绍">一句话介绍</h3><p>torch.Tensor的子类，nn.Paramter()声明的变量被赋值给module的属性时，这个变量会自动添加到moudule的parameters list中，parameters()等函数返回的迭代器中可以访问。</p><h3 id="api">API</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Parameter</span><span class="params">(torch.Tensor)</span></span></span><br><span class="line">    # data是weights,requires_grad是是否需要梯度</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__new__</span><span class="params">(cls, data=None, requires_grad=True)</span></span></span><br></pre></td></tr></table></figure><h3 id="代码示例">代码示例</h3><p>下面的代码实现了和nn.Conv2d同样的功能，使用nn.Parameter()将手动创建的变量设置为module的paramters。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CNN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(CNN, self).__init__()</span><br><span class="line">        </span><br><span class="line">        self.cnn1_weight = nn.Parameter(torch.rand(<span class="number">16</span>, <span class="number">1</span>, <span class="number">5</span>, <span class="number">5</span>))</span><br><span class="line">        self.bias1_weight = nn.Parameter(torch.rand(<span class="number">16</span>))</span><br><span class="line">        </span><br><span class="line">        self.cnn2_weight = nn.Parameter(torch.rand(<span class="number">32</span>, <span class="number">16</span>, <span class="number">5</span>, <span class="number">5</span>))</span><br><span class="line">        self.bias2_weight = nn.Parameter(torch.rand(<span class="number">32</span>))</span><br><span class="line">        </span><br><span class="line">        self.linear1_weight = nn.Parameter(torch.rand(<span class="number">4</span> * <span class="number">4</span> * <span class="number">32</span>, <span class="number">10</span>))</span><br><span class="line">        self.bias3_weight = nn.Parameter(torch.rand(<span class="number">10</span>))</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = x.view(x.size(<span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line">        out = F.conv2d(x, self.cnn1_weight, self.bias1_weight)</span><br><span class="line">        out = F.relu(out)</span><br><span class="line">        out = F.max_pool2d(out)</span><br><span class="line">        </span><br><span class="line">        out = F.conv2d(x, self.cnn2_weight, self.bias2_weight)</span><br><span class="line">        out = F.relu(out)</span><br><span class="line">        out = F.max_pool2d(out)</span><br><span class="line">        </span><br><span class="line">        out = F.linear(x, self.linear1_weight, self.bias3_weight)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure><h2 id="container">Container</h2><h3 id="module">Module</h3><p>Module是所有模型的基类。<br>它有以下几个常用的函数和常见的属性。</p><h4 id="常见的函数">常见的函数</h4><ul><li>add_module(name, module)  # 添加一个子module到当前module</li><li>apply(fn) # 对模型中的每一个submodule（调用.children()得到的）都调用fn函数</li><li>_apply() # in-place</li><li>children() # 返回module中所有的子module，不包含整个module，<a href="https://github.com/mxxhcm/code/blob/master/pytorch/pytorch_test/torch_module_child_parameter.py" target="_blank" rel="noopener">详情可见</a></li><li>buffers(recurse=True)     # 返回module buffers的迭代器</li><li>cuda(device=None)    # 将model para和buffer转换到gpu</li><li>cpu()     # 将model para和buffer转换到cpu</li><li>double()  #将float的paramters和buffer转换成double</li><li>float()</li><li>forward(*input) # 前向传播</li><li>eval()    # 将module置为evaluation mode，只影响特定的traing和evaluation modules表现不同的module，比如Dropout和BatchNorm，一般Dropout在训练时使用，在测试时关闭。</li><li>train(mode=True)  # 设置模型为train mode</li><li>load_state_dict(state_dict, strict=True)  # 加载模型参数</li><li>modules()  # 返回network中所有的module的迭代器，包含整个module，<a href="https://github.com/mxxhcm/code/blob/master/pytorch/pytorch_test/torch_module_child_parameter.py" target="_blank" rel="noopener">详情可见</a></li><li>named_modules() # 同时返回包含module和module名字的迭代器</li><li>named_children() # 同时返回包含子module和子module名字的迭代器</li><li>named_parameters() # 同时返回包含parameter和paramter名字的迭代器</li><li>parameters() # 返回模型参数的迭代器</li><li>state_dict(destination=None, prefix=’’, keep_vars=False)  # 返回整个module的state，包含parameters和buffers。</li><li>zero_grad()   # 设置model parameters的gradients为$0$</li><li>to(*args, **kwargs) # 移动或者改变parameters和buffer的类型或位置</li></ul><h4 id="常见的属性">常见的属性</h4><ul><li>self._backend = thnn_backend</li><li>self._parameters = OrderedDict()</li><li>self._buffers = OrderedDict()</li><li>self._backward_hooks = OrderedDict()</li><li>self._forward_hooks = OrderedDict()</li><li>self._forward_pre_hooks = OrderedDict()</li><li>self._state_dict_hooks = OrderedDict()</li><li>self._load_state_dict_pre_hooks = OrderedDict()</li><li>self._modules = OrderedDict()</li><li>self.training = True</li></ul><h4 id="代码示例-v2">代码示例</h4><h5 id="apply">apply</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_weights</span><span class="params">(m)</span>:</span></span><br><span class="line">    print(m)</span><br><span class="line">    <span class="keyword">if</span> type(m) == nn.Linear:</span><br><span class="line">        m.weight.data.fill_(<span class="number">1.0</span>)</span><br><span class="line">        print(m.weight)</span><br><span class="line">net = nn.Sequential(nn.Linear(<span class="number">2</span>, <span class="number">2</span>), nn.Linear(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">net.apply(init_weights)</span><br></pre></td></tr></table></figure><h5 id="module-v2">module</h5><p>关于module,children_modules,parameters的<a href="https://github.com/mxxhcm/code/blob/master/pytorch/pytorch_test/torch_module_child_parameter.py" target="_blank" rel="noopener">代码</a></p><h3 id="sequential">Sequential</h3><h2 id="convolution-layers">Convolution Layers</h2><h3 id="torch-nn-conv2d">torch.nn.Conv2d</h3><h4 id="api-v2">API</h4><p>2维卷积。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.Conv2d(</span><br><span class="line">    in_channels,    <span class="comment"># int，输入图像的通道</span></span><br><span class="line">    out_channels,   <span class="comment"># int，卷积产生的输出通道数（也就是有几个kernel） </span></span><br><span class="line">    kernel_size,    <span class="comment"># int or tuple – kernel的大小 </span></span><br><span class="line">    stride=<span class="number">1</span>,       <span class="comment"># int or tuple, 可选，步长，默认为$1$</span></span><br><span class="line">    padding=<span class="number">0</span>,      <span class="comment"># int or tuple, 可选，向各边添加Zero-padding的数量，默认为$0$</span></span><br><span class="line">    dilation=<span class="number">1</span>,     <span class="comment"># int or tuple, 可选，Spacing between kernel elements. Default: 1</span></span><br><span class="line">    groups=<span class="number">1</span>,       <span class="comment"># int, 可选， Number of blocked connections from input channels to output channels. Default: 1</span></span><br><span class="line">    bias=<span class="literal">True</span>       <span class="comment"># bool，可选，如果为True,给output添加一个可以学习的bias</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h4 id="示例">示例</h4><h5 id="例子1">例子1</h5><p>用$6$个$5\times 5$的filter处理维度为$32\times 32\times 1$的图像。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">model = torch.nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">input = torch.randn(<span class="number">16</span>, <span class="number">1</span>, <span class="number">32</span>, <span class="number">32</span>)</span><br><span class="line">output = model(input)</span><br><span class="line">print(output.size())</span><br><span class="line"><span class="comment"># output: torch.Size([16, 6, 28, 28])</span></span><br></pre></td></tr></table></figure><h5 id="例子2-stride和padding">例子2，stride和padding</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"></span><br><span class="line">inputs = Variable(torch.randn(<span class="number">64</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>))</span><br><span class="line"></span><br><span class="line">m1 = nn.Conv2d(<span class="number">3</span>, <span class="number">16</span>, <span class="number">3</span>)</span><br><span class="line">print(m1)</span><br><span class="line"><span class="comment"># output: Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))</span></span><br><span class="line">output1 = m1(inputs)</span><br><span class="line">print(output1.size())</span><br><span class="line"><span class="comment"># output: torch.Size([64, 16, 30, 30])</span></span><br><span class="line"></span><br><span class="line">m2 = nn.Conv2d(<span class="number">3</span>, <span class="number">16</span>, <span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">print(m2)</span><br><span class="line"><span class="comment"># output: Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span></span><br><span class="line">output2 = m2(inputs)</span><br><span class="line">print(output2.size())</span><br><span class="line"><span class="comment"># output: torch.Size([64, 16, 32, 32])</span></span><br></pre></td></tr></table></figure><h2 id="pooling-layers">Pooling Layers</h2><h3 id="maxpool2dd">MaxPool2dd</h3><h4 id="api-v3">API</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">torch</span>.<span class="title">nn</span>.<span class="title">MaxPool2d</span><span class="params">(</span></span></span><br><span class="line"><span class="class"><span class="params">    kernel_size,</span></span></span><br><span class="line"><span class="class"><span class="params">    stride=None,</span></span></span><br><span class="line"><span class="class"><span class="params">    padding=<span class="number">0</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    dilation=<span class="number">1</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    return_indices=False,</span></span></span><br><span class="line"><span class="class"><span class="params">    ceil_mode=False</span></span></span><br><span class="line"><span class="class"><span class="params">)</span></span></span><br></pre></td></tr></table></figure><p>MaxPool2d默认layer stride默认是和kernel_size相同的</p><h4 id="代码示例-v3">代码示例</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch import nn</span><br><span class="line">from torch.autograd import Variable</span><br><span class="line"># maxpool2d</span><br><span class="line"></span><br><span class="line">input = Variable(torch.randn(30,20,32,32))</span><br><span class="line">print(input.size())</span><br><span class="line"># outputtorch.Size([30, 20, 32, 32])</span><br><span class="line"></span><br><span class="line">m1 = nn.MaxPool2d(2)</span><br><span class="line">output = m1(input)</span><br><span class="line">print(output.size())</span><br><span class="line"># output: torch.Size([30, 20, 16, 16])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">m2 = nn.MaxPool2d(5)</span><br><span class="line">print(m2)</span><br><span class="line"># output: MaxPool2d (size=(5, 5), stride=(5, 5), dilation=(1, 1))</span><br><span class="line"></span><br><span class="line">for param in m2.parameters():</span><br><span class="line">  print(param)</span><br><span class="line"></span><br><span class="line">print(m2.state_dict().keys())</span><br><span class="line"># output: []</span><br><span class="line"></span><br><span class="line">output = m2(input)</span><br><span class="line">print(output.size())</span><br><span class="line"># output: torch.Size([30, 20, 6, 6])</span><br></pre></td></tr></table></figure><h2 id="padding-layers">Padding Layers</h2><h2 id="linear-layers">Linear layers</h2><h3 id="linear">Linear</h3><h4 id="api-v4">API</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.Linear(</span><br><span class="line">    in_features, </span><br><span class="line">    out_features, </span><br><span class="line">    bias=<span class="literal">True</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h4 id="代码示例-v4">代码示例</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">m = nn.Linear(<span class="number">20</span>, <span class="number">30</span>)</span><br><span class="line">input = torch.randn(<span class="number">128</span>, <span class="number">20</span>)</span><br><span class="line">output = m(input)</span><br><span class="line">print(output.size())</span><br><span class="line">torch.Size([<span class="number">128</span>, <span class="number">30</span>])</span><br></pre></td></tr></table></figure><h2 id="dropout-layers">Dropout layers</h2><h3 id="drop2d">Drop2D</h3><h4 id="api-v5">API</h4><h4 id="代码示例-v5">代码示例</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line">m = nn.Dropout2d(<span class="number">0.3</span>)</span><br><span class="line">print(m)</span><br><span class="line">inputs = torch.randn(<span class="number">1</span>,<span class="number">28</span>,<span class="number">28</span>)</span><br><span class="line">outputs = m(inputs)</span><br><span class="line">print(outputs)</span><br></pre></td></tr></table></figure><p>输出：</p><blockquote><p>Dropout2d(p=0.3)<br>([[[ 0.8535,  1.0314,  2.7904,  1.2136,  2.7561, -2.0429,  0.0772,<br>-1.9372, -0.0864, -1.4132, -0.1648,  0.2403,  0.5727,  0.8102,<br>0.4544,  0.1414,  0.1547, -0.9266, -0.6033,  0.5813, -1.3541,<br>-0.0536,  0.9574,  0.0554,  0.8368,  0.7633, -0.3377, -1.4293],<br>[ 0.0000,  0.0000, -0.0000, -0.0000,  0.0000, -0.0000,  0.0000,<br>0.0000,  0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000,<br>0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000,<br>-0.0000,  0.0000,  0.0000, -0.0000, -0.0000, -0.0000, -0.0000],<br>…<br>[ 0.6452, -0.6455,  0.2370,  0.1088, -0.5421, -0.5120, -2.2915,<br>0.2061,  1.6384,  2.2276,  2.4022,  0.2033,  0.6984,  0.1254,<br>1.1627,  1.0699, -2.1868,  1.1293, -0.7030,  0.0454, -1.5428,<br>-2.4052, -0.3204, -1.5984,  0.1282,  0.2127, -2.3506, -2.2395]]])</p></blockquote><p>会发现输出的数组中有很多被置为$0$了。</p><h2 id="loss-function">Loss function</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;parameter&quot;&gt;Parameter&lt;/h2&gt;
&lt;h3 id=&quot;一句话介绍&quot;&gt;一句话介绍&lt;/h3&gt;
&lt;p&gt;torch.Tensor的子类，nn.Paramter()声明的变量被赋值给module的属性时，这个变量会自动添加到moudule的parameters
      
    
    </summary>
    
      <category term="pytorch" scheme="http://mxxhcm.github.io/categories/pytorch/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="pytorch" scheme="http://mxxhcm.github.io/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>pytorch Variable(torch.autograd.Variable)</title>
    <link href="http://mxxhcm.github.io/2019/05/08/pytorch-Variable/"/>
    <id>http://mxxhcm.github.io/2019/05/08/pytorch-Variable/</id>
    <published>2019-05-08T13:54:53.000Z</published>
    <updated>2019-05-17T08:14:27.886Z</updated>
    
    <content type="html"><![CDATA[<h3 id="variable-class-torch-autograd-variable">Variable(class torch.autograd.Variable)</h3><h4 id="声明一个tensor">声明一个tensor</h4><ul><li>torch.zeros</li><li>torch.ones</li><li>torch.rand</li><li>torch.full()</li><li>torch.empyt()</li><li>torch.rand()</li><li>torch.randn()</li><li>torch.ones_like()</li><li>torch.zeros_like()</li><li>torch.randn_like()</li><li>torch.Tensor</li></ul><h4 id="代码示例">代码示例</h4><p><a href="https://github.com/mxxhcm/code/blob/master/pytorch/pytorch_test/torch_tensor.py" target="_blank" rel="noopener">代码地址</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">torch.manual_seed(<span class="number">5</span>)</span><br><span class="line">x = torch.empty(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">print(torch.empty(<span class="number">5</span>, <span class="number">3</span>)) <span class="comment"># construct a 5x3 matrix, uninitialized</span></span><br><span class="line"><span class="comment"># tensor([[4.6179e-38, 4.5845e-41, 4.6179e-38],</span></span><br><span class="line"><span class="comment">#         [4.5845e-41, 6.3010e-36, 6.3010e-36],</span></span><br><span class="line"><span class="comment">#         [2.5204e-35, 6.3010e-36, 1.0082e-34],</span></span><br><span class="line"><span class="comment">#         [6.3010e-36, 6.3010e-36, 6.6073e-30],</span></span><br><span class="line"><span class="comment">#         [6.3010e-36, 6.3010e-36, 6.3010e-36]])</span></span><br><span class="line"></span><br><span class="line">print(torch.rand(<span class="number">3</span>, <span class="number">4</span>))  <span class="comment"># construct a 4x3 matrix, uniform [0,1] </span></span><br><span class="line"><span class="comment"># tensor([[0.8303, 0.1261, 0.9075, 0.8199],</span></span><br><span class="line"><span class="comment">#         [0.9201, 0.1166, 0.1644, 0.7379],</span></span><br><span class="line"><span class="comment">#         [0.0333, 0.9942, 0.6064, 0.5646]])</span></span><br><span class="line"></span><br><span class="line">print(torch.randn(<span class="number">5</span>, <span class="number">3</span>)) <span class="comment"># construct a 5x3 matrix, normal distribution</span></span><br><span class="line"><span class="comment"># tensor([[-1.4017, -0.7626,  0.6312],</span></span><br><span class="line"><span class="comment">#         [-0.8991, -0.5578,  0.6907],</span></span><br><span class="line"><span class="comment">#         [ 0.2225, -0.6662,  0.6846],</span></span><br><span class="line"><span class="comment">#         [ 0.5740, -0.5829,  0.7679],</span></span><br><span class="line"><span class="comment">#         [ 0.5740, -0.5829,  0.7679],</span></span><br><span class="line"></span><br><span class="line">print(torch.randn(<span class="number">2</span>, <span class="number">3</span>).type())</span><br><span class="line"><span class="comment"># torch.FloatTensor</span></span><br><span class="line"></span><br><span class="line">print(torch.zeros(<span class="number">5</span>, <span class="number">3</span>)) <span class="comment"># construct a 5x3 matrix filled zeros</span></span><br><span class="line"><span class="comment"># tensor([[0., 0., 0.],</span></span><br><span class="line"><span class="comment">#         [0., 0., 0.],</span></span><br><span class="line"><span class="comment">#         [0., 0., 0.],</span></span><br><span class="line"><span class="comment">#         [0., 0., 0.],</span></span><br><span class="line"><span class="comment">#         [0., 0., 0.]])</span></span><br><span class="line"></span><br><span class="line">print(torch.ones(<span class="number">5</span>, <span class="number">3</span>)) <span class="comment"># construct a 5x3 matrix filled ones</span></span><br><span class="line"><span class="comment"># tensor([[1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 1., 1.]])</span></span><br><span class="line"></span><br><span class="line">print(torch.ones(<span class="number">5</span>, <span class="number">3</span>, dtype=torch.long)) <span class="comment"># construct a tensor with dtype=torch.long</span></span><br><span class="line"><span class="comment"># tensor([[1, 1, 1],</span></span><br><span class="line"><span class="comment">#         [1, 1, 1],</span></span><br><span class="line"><span class="comment">#         [1, 1, 1],</span></span><br><span class="line"><span class="comment">#         [1, 1, 1],</span></span><br><span class="line"><span class="comment">#         [1, 1, 1]])</span></span><br><span class="line"></span><br><span class="line">print(torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])) <span class="comment"># construct a tensor direct from data</span></span><br><span class="line"><span class="comment"># tensor([1, 2, 3])</span></span><br><span class="line"></span><br><span class="line">print(x.new_ones(<span class="number">5</span>,<span class="number">4</span>)) <span class="comment"># constuct a tensor has the same property as x</span></span><br><span class="line"><span class="comment"># tensor([[1., 1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 1., 1., 1.]])</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(torch.full([<span class="number">4</span>,<span class="number">3</span>],<span class="number">9</span>))  <span class="comment"># construct a tensor with a value </span></span><br><span class="line"><span class="comment"># tensor([[9., 9., 9.],</span></span><br><span class="line"><span class="comment">#         [9., 9., 9.],</span></span><br><span class="line"><span class="comment">#         [9., 9., 9.],</span></span><br><span class="line"><span class="comment">#         [9., 9., 9.]])</span></span><br><span class="line"></span><br><span class="line">print(x.new_ones(<span class="number">5</span>,<span class="number">4</span>,dtype=torch.int)) <span class="comment"># construct a tensor with the same property as x, and also can have the specified type.</span></span><br><span class="line"><span class="comment"># tensor([[1, 1, 1, 1],</span></span><br><span class="line"><span class="comment">#         [1, 1, 1, 1],</span></span><br><span class="line"><span class="comment">#         [1, 1, 1, 1],</span></span><br><span class="line"><span class="comment">#         [1, 1, 1, 1],</span></span><br><span class="line"><span class="comment">#         [1, 1, 1, 1]], dtype=torch.int32)</span></span><br><span class="line"></span><br><span class="line">print(torch.randn_like(x,dtype=torch.float)) <span class="comment"># construct a tensor with the same shape with x, </span></span><br><span class="line"><span class="comment"># tensor([[ 0.4699, -1.9540, -0.5587],</span></span><br><span class="line"><span class="comment">#         [ 0.4295, -2.2643, -0.2017],</span></span><br><span class="line"><span class="comment">#         [ 1.0677,  0.3246, -0.0684],</span></span><br><span class="line"><span class="comment">#         [-0.9959,  1.1563, -0.3992],</span></span><br><span class="line"><span class="comment">#         [ 1.2153, -0.8115, -0.8848]])</span></span><br><span class="line"></span><br><span class="line">print(torch.ones_like(x))</span><br><span class="line"><span class="comment"># tensor([[1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 1., 1.]])</span></span><br><span class="line"></span><br><span class="line">print(torch.zeros_like(x))</span><br><span class="line"><span class="comment"># tensor([[0., 0., 0.],</span></span><br><span class="line"><span class="comment">#         [0., 0., 0.],</span></span><br><span class="line"><span class="comment">#         [0., 0., 0.],</span></span><br><span class="line"><span class="comment">#         [0., 0., 0.],</span></span><br><span class="line"><span class="comment">#         [0., 0., 0.]])</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(torch.Tensor(<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line"><span class="comment"># tensor([[-3.8809e-21,  3.0948e-41,  2.3822e-44,  0.0000e+00],</span></span><br><span class="line"><span class="comment">#         [        nan,  7.2251e+28,  1.3733e-14,  1.8888e+31],</span></span><br><span class="line"><span class="comment">#         [ 4.9656e+28,  4.5439e+30,  7.1426e+22,  1.8759e+28]])</span></span><br><span class="line"></span><br><span class="line">print(torch.Tensor(<span class="number">3</span>,<span class="number">4</span>).uniform_(<span class="number">0</span>,<span class="number">1</span>))</span><br><span class="line"><span class="comment"># tensor([[0.8437, 0.1399, 0.2239, 0.3462],</span></span><br><span class="line"><span class="comment">#         [0.5668, 0.3059, 0.1890, 0.4087],</span></span><br><span class="line"><span class="comment">#         [0.2560, 0.5138, 0.1299, 0.3750]])</span></span><br><span class="line"></span><br><span class="line">print(torch.Tensor(<span class="number">3</span>,<span class="number">4</span>).normal_(<span class="number">0</span>,<span class="number">1</span>))</span><br><span class="line"><span class="comment"># tensor([[-0.5490, -0.0838, -0.1387, -0.5289],</span></span><br><span class="line"><span class="comment">#         [-0.4919, -0.4646, -0.0588,  1.2624],</span></span><br><span class="line"><span class="comment">#         [ 1.1935,  1.5696, -0.8977, -0.1139]])</span></span><br><span class="line"></span><br><span class="line">print(torch.Tensor(<span class="number">3</span>,<span class="number">4</span>).fill_(<span class="number">5</span>))</span><br><span class="line"><span class="comment"># tensor([[5., 5., 5., 5.],</span></span><br><span class="line"><span class="comment">#         [5., 5., 5., 5.],</span></span><br><span class="line"><span class="comment">#         [5., 5., 5., 5.]])</span></span><br><span class="line"></span><br><span class="line">print(torch.arange(<span class="number">1</span>, <span class="number">3</span>, <span class="number">0.4</span>))</span><br><span class="line"><span class="comment"># tensor([1.0000, 1.4000, 1.8000, 2.2000, 2.6000])</span></span><br></pre></td></tr></table></figure><h2 id="tensor的各种操作">tensor的各种操作</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">a = torch.ones(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">b = torch.ones(<span class="number">2</span>,<span class="number">3</span>)</span><br></pre></td></tr></table></figure><h3 id="加操作">加操作</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">print(a+b)                <span class="comment">#方法1</span></span><br><span class="line">c = torch.add(a,b)    <span class="comment">#方法2</span></span><br><span class="line">torch.add(a,b,result)    <span class="comment">#方法3</span></span><br><span class="line">a.add(b)                    <span class="comment">#方法4,将a加上b，且a不变</span></span><br><span class="line">a.add_(b)                <span class="comment">#方法5,将a加上b并将其赋值给a</span></span><br></pre></td></tr></table></figure><h3 id="转置操作">转置操作</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(a.t())               <span class="comment"># 打印出tensor a的转置</span></span><br><span class="line">print(a.t_())                 <span class="comment">#将tensor a 转置，并将其赋值给a</span></span><br></pre></td></tr></table></figure><h3 id="求最大行和列">求最大行和列</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.max(tensor,dim)</span><br><span class="line">np.max(array,dim)</span><br></pre></td></tr></table></figure><h3 id="和relu功能比较类似">和relu功能比较类似。</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.clamp(tensor, min, max,out=<span class="literal">None</span>)</span><br><span class="line">np.maximun(x1, x2)  <span class="comment"># x1 and x2 must hava the same shape</span></span><br></pre></td></tr></table></figure><h2 id="tensor和numpy转化">tensor和numpy转化</h2><h3 id="convert-tensor-to-numpy">convert tensor to numpy</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = torch.ones(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">b = a.numpy()</span><br></pre></td></tr></table></figure><h3 id="convert-numpy-to-tensor">convert numpy to tensor</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a =  numpy.ones(<span class="number">4</span>,<span class="number">3</span>)</span><br><span class="line">b = torch.from_numpy(a)</span><br></pre></td></tr></table></figure><h2 id="variable和tensor">Variable和Tensor</h2><p>Variable<br>图1.Variable</p><h3 id="属性">属性</h3><p>如图1,Variable wrap a Tensor,and it has six attributes,data,grad,requies_grad,volatile,is_leaf and grad_fn.We can acess the raw tensor through .data operation, we can accumualte gradients w.r.t this Variable into .grad,.Finally , creator attribute will tell us how the Variable were created,we can acess the creator attibute by .grad_fn,if the Variable was created by the user,then the grad_fn is None,else it will show us which Function created the Variable.<br>if the grad_fn is None,we call them graph leaves</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Variable.shape  <span class="comment">#查看Variable的size</span></span><br><span class="line">Variable.size()</span><br></pre></td></tr></table></figure><h3 id="parameters">parameters</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.autograd.Variable(data,requires_grad=<span class="literal">False</span>,volatile=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><p>requires_grad : indicate whether the backward() will ever need to be called</p><h3 id="backward">backward</h3><p>backward(gradient=None,retain_graph=None,create_graph=None,retain_variables=None)<br>如果Variable是一个scalar output，我们不需要指定gradient，但是如果Variable不是一个scalar，而是有多个element，我们就需要根据output指定一下gradient，gradient的type可以是tensor也可以是Variable，里面的值为梯度的求值比例，例如</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = Variable(torch.Tensor([<span class="number">3</span>,<span class="number">6</span>,<span class="number">4</span>]),requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = Variable(torch.Tensor([<span class="number">5</span>,<span class="number">3</span>,<span class="number">6</span>]),requires_grad=<span class="literal">True</span>)</span><br><span class="line">z = x+y</span><br><span class="line">z.backward(gradient=torch.Tensor([<span class="number">0.1</span>,<span class="number">1</span>,<span class="number">10</span>]))</span><br></pre></td></tr></table></figure><p>这里[0.1,1,10]分别表示的是对正常梯度分别乘上$0.1,1,10$，然后将他们累积在leaves Variable上</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">detach()    <span class="comment">#</span></span><br><span class="line">detach_()</span><br><span class="line">register_hook()</span><br><span class="line">register_grad()</span><br></pre></td></tr></table></figure><h2 id="参考文献">参考文献</h2><p>1.<a href="https://pytorch.org/docs/stable/tensors.html" target="_blank" rel="noopener">https://pytorch.org/docs/stable/tensors.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;variable-class-torch-autograd-variable&quot;&gt;Variable(class torch.autograd.Variable)&lt;/h3&gt;
&lt;h4 id=&quot;声明一个tensor&quot;&gt;声明一个tensor&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;to
      
    
    </summary>
    
      <category term="pytorch" scheme="http://mxxhcm.github.io/categories/pytorch/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="pytorch" scheme="http://mxxhcm.github.io/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>pytorch Function(torch.autograd.Function)</title>
    <link href="http://mxxhcm.github.io/2019/05/08/pytorch-Function/"/>
    <id>http://mxxhcm.github.io/2019/05/08/pytorch-Function/</id>
    <published>2019-05-08T13:54:22.000Z</published>
    <updated>2019-05-09T05:37:26.595Z</updated>
    
    <content type="html"><![CDATA[<h3 id="function-class-torch-autograd-funtion">Function(class torch.autograd.Funtion)</h3><h4 id="用法">用法</h4><p>Function一般只定义一个操作，并且它无法保存参数，一般适用于激活函数，pooling等，它需要定义三个方法，<strong>init</strong>(),forward(),backward()（这个需要自己定义怎么求导）<br>Model保存了参数，适合定义一层，如线性层(Linear layer),卷积层(conv layer),也适合定义一个网络。<br>和Model的区别，model只需要定义__init()__,foward()方法，backward()不需要我们定义，它可以由自动求导机制计算。</p><p>Function定义只是一个函数，forward和backward都只与这个Function的输入和输出有关</p><h4 id="functions">functions</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyReLU</span><span class="params">(torch.autograd.Function)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    We can implement our own custom autograd Functions by subclassing</span></span><br><span class="line"><span class="string">    torch.autograd.Function and implementing the forward and backward passes</span></span><br><span class="line"><span class="string">    which operate on Tensors.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        In the forward pass we receive a Tensor containing the input and return a</span></span><br><span class="line"><span class="string">        Tensor containing the output. You can cache arbitrary Tensors for use in the</span></span><br><span class="line"><span class="string">        backward pass using the save_for_backward method.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.save_for_backward(input)</span><br><span class="line">        <span class="keyword">return</span> input.clamp(min=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(self, grad_output)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        In the backward pass we receive a Tensor containing the gradient of the loss</span></span><br><span class="line"><span class="string">        with respect to the output, and we need to compute the gradient of the loss</span></span><br><span class="line"><span class="string">        with respect to the input.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        input, = self.saved_tensors</span><br><span class="line">        grad_input = grad_output.clone()</span><br><span class="line">        grad_input[input &lt; <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">        <span class="keyword">return</span> grad_input</span><br><span class="line"></span><br><span class="line">dtype = torch.FloatTensor</span><br><span class="line"><span class="comment"># dtype = torch.cuda.FloatTensor # Uncomment this to run on GPU</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></span><br><span class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></span><br><span class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create random Tensors to hold input and outputs, and wrap them in Variables.</span></span><br><span class="line">x = Variable(torch.randn(N, D_in).type(dtype), requires_grad=<span class="literal">False</span>)</span><br><span class="line">y = Variable(torch.randn(N, D_out).type(dtype), requires_grad=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create random Tensors for weights, and wrap them in Variables.</span></span><br><span class="line">w1 = Variable(torch.randn(D_in, H).type(dtype), requires_grad=<span class="literal">True</span>)</span><br><span class="line">w2 = Variable(torch.randn(H, D_out).type(dtype), requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">1e-6</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</span><br><span class="line">    <span class="comment"># Construct an instance of our MyReLU class to use in our network</span></span><br><span class="line">    relu = MyReLU()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Forward pass: compute predicted y using operations on Variables; we compute</span></span><br><span class="line">    <span class="comment"># ReLU using our custom autograd operation.</span></span><br><span class="line">    y_pred = relu(x.mm(w1)).mm(w2)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute and print loss</span></span><br><span class="line">    loss = (y_pred - y).pow(<span class="number">2</span>).sum()</span><br><span class="line">    print(t, loss.data[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Use autograd to compute the backward pass.</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update weights using gradient descent</span></span><br><span class="line">    w1.data -= learning_rate * w1.grad.data</span><br><span class="line">    w2.data -= learning_rate * w2.grad.data</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Manually zero the gradients after updating weights</span></span><br><span class="line">    w1.grad.data.zero_()</span><br><span class="line">    w2.grad.data.zero_()</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;function-class-torch-autograd-funtion&quot;&gt;Function(class torch.autograd.Funtion)&lt;/h3&gt;
&lt;h4 id=&quot;用法&quot;&gt;用法&lt;/h4&gt;
&lt;p&gt;Function一般只定义一个操作，并且它无法保存参
      
    
    </summary>
    
      <category term="pytorch" scheme="http://mxxhcm.github.io/categories/pytorch/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="pytorch" scheme="http://mxxhcm.github.io/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>pytorch distributions</title>
    <link href="http://mxxhcm.github.io/2019/05/08/pytorch-distributions/"/>
    <id>http://mxxhcm.github.io/2019/05/08/pytorch-distributions/</id>
    <published>2019-05-08T13:53:46.000Z</published>
    <updated>2019-05-17T08:15:22.519Z</updated>
    
    <content type="html"><![CDATA[<h2 id="torch-distributions">torch.distributions</h2><p>这个库和gym.space库很相似，都是提供一些分布，然后从中采样。<br>常见的有ExponentialFamily,Bernoulli,Binomial,Categorical,Exponential,Gamma,Independent,Laplace,Multinomial,MultivariateNormal。这里不做过程陈述，可以看<a href="http://localhost:4000/2019/04/12/gym%E4%BB%8B%E7%BB%8D/" target="_blank" rel="noopener">gym</a>中。</p><h3 id="categorical">Categorical</h3><p>对应tensorflow中的<a href="https://github.com/mxxhcm/myown_code/blob/master/tf/some_ops/tf_multinominal.py" target="_blank" rel="noopener">tf.multinomial</a>。<br>类原型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CLASS torch.distributions.categorical.Categorical(probs=<span class="literal">None</span>, logits=<span class="literal">None</span>, validate_args=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure><p>参数probs只能是$1$维或者$2$维，而且必须是非负，有限非零和的，然后将其归一化到和为$1$。<br>这个类和torch.multinormal是一样的，从${0,\cdots, K-1}$中按照probs的概率进行采样，$K$是probs.size(-1)，即是size()矩阵的最后一列，$2$维时把第$1$维当成了batch。</p><p>举一个简单的例子，<a href="https://github.com/mxxhcm/code/blob/master/pytorch/pytorch_test/torch_distribution.py" target="_blank" rel="noopener">代码</a>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.distributions <span class="keyword">as</span> diss</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">torch.manual_seed(<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">m = diss.Categorical(torch.tensor([<span class="number">0.25</span>, <span class="number">0.25</span>, <span class="number">0.25</span>, <span class="number">0.25</span> ]))</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">    print(m.sample())</span><br><span class="line"></span><br><span class="line">m = diss.Categorical(torch.tensor([[<span class="number">0.5</span>, <span class="number">0.25</span>, <span class="number">0.25</span>], [<span class="number">0.25</span>, <span class="number">0.25</span>, <span class="number">0.5</span>]]))</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">    print(m.sample())</span><br></pre></td></tr></table></figure><p>输出结果如下：</p><blockquote><p>tensor(2)<br>tensor(1)<br>tensor(1)<br>tensor(1)<br>tensor(1)<br>tensor([2, 2])<br>tensor([1, 2])<br>tensor([0, 1])<br>tensor([0, 2])<br>tensor([0, 0])</p></blockquote><p>作为对比，gym.spaces.Discrete示例如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> gym <span class="keyword">import</span> spaces</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.Discrete</span></span><br><span class="line"><span class="comment"># 取值是&#123;0, 1, ..., n - 1&#125;</span></span><br><span class="line">dis = spaces.Discrete(<span class="number">5</span>)</span><br><span class="line">dis.seed(<span class="number">5</span>)</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">    print(dis.sample())</span><br></pre></td></tr></table></figure><p>输出结果是：</p><blockquote><p>3<br>0<br>1<br>0<br>4</p></blockquote><h2 id="参考文献">参考文献</h2><p>1.<a href="https://pytorch.org/docs/stable/distributions.html" target="_blank" rel="noopener">https://pytorch.org/docs/stable/distributions.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;torch-distributions&quot;&gt;torch.distributions&lt;/h2&gt;
&lt;p&gt;这个库和gym.space库很相似，都是提供一些分布，然后从中采样。&lt;br&gt;
常见的有ExponentialFamily,Bernoulli,Binomial,Cat
      
    
    </summary>
    
      <category term="pytorch" scheme="http://mxxhcm.github.io/categories/pytorch/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="pytorch" scheme="http://mxxhcm.github.io/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>pytorch multiprocessing</title>
    <link href="http://mxxhcm.github.io/2019/05/08/pytorch-multiprocessing/"/>
    <id>http://mxxhcm.github.io/2019/05/08/pytorch-multiprocessing/</id>
    <published>2019-05-08T13:52:42.000Z</published>
    <updated>2019-05-17T08:21:22.708Z</updated>
    
    <content type="html"><![CDATA[<h2 id="torch-multiprocessing">torch.multiprocessing</h2><h3 id="join">join</h3><p>等待调用join()方法的线程执行完毕，然后继续执行。<br>可参见github<a href="https://github.com/mxxhcm/myown_code/tree/master/pytorch/tutorials/multiprocess_torch/mnist_hogwild" target="_blank" rel="noopener">官方demo</a>。</p><h3 id="share-memory">share_memory_()</h3><p>在多个线程之间共享参数，如下<a href="https://github.com/mxxhcm/myown_code/blob/master/pytorch/tutorials/multiprocess_torch/share_memory.py" target="_blank" rel="noopener">代码</a>所示。可以用来实现A3C。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.multiprocessing <span class="keyword">as</span> mp</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">proc</span><span class="params">(sec, x)</span>:</span></span><br><span class="line">   print(os.getpid(),<span class="string">"  "</span>, x)</span><br><span class="line">   time.sleep(sec)</span><br><span class="line">   print(os.getpid(), <span class="string">"  "</span>, x)</span><br><span class="line">   x += sec</span><br><span class="line">   print(str(os.getpid()) + <span class="string">"  over.  "</span>, x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">   num_processes = <span class="number">3</span></span><br><span class="line">   processes = []</span><br><span class="line">   x = torch.ones([<span class="number">3</span>,])</span><br><span class="line">   x.share_memory_()</span><br><span class="line">   <span class="keyword">for</span> rank <span class="keyword">in</span> range(num_processes):</span><br><span class="line">     p = mp.Process(target=proc, args=(rank + <span class="number">1</span>, x))</span><br><span class="line">     p.start() </span><br><span class="line">     processes.append(p)</span><br><span class="line">   <span class="keyword">for</span> p <span class="keyword">in</span> processes:</span><br><span class="line">     p.join()</span><br><span class="line">   print(x)</span><br></pre></td></tr></table></figure><p>输出结果如下所示：</p><blockquote><p>python share_memory.py<br>7739    tensor([1., 1., 1.])<br>7738    tensor([1., 1., 1.])<br>7737    tensor([1., 1., 1.])<br>7737    tensor([1., 1., 1.])<br>7737  over.   tensor([2., 2., 2.])<br>7738    tensor([2., 2., 2.])<br>7738  over.   tensor([4., 4., 4.])<br>7739    tensor([4., 4., 4.])<br>7739  over.   tensor([7., 7., 7.])<br>tensor([7., 7., 7.])</p></blockquote><p>我们可以发现$7739$这个线程中，传入的$x$还是和最开始的一样，但是在$7738$线程更新完$x$之后，$7739$使用的$x$就已经变成了更新后的$x$。所以，我猜测这里面应该是有一个对$x$的锁，保证$x$在同一时刻只能被一个线程访问。</p><h2 id="参考文献">参考文献</h2><p>1.<a href="https://pytorch.org/docs/stable/multiprocessing.html" target="_blank" rel="noopener">https://pytorch.org/docs/stable/multiprocessing.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;torch-multiprocessing&quot;&gt;torch.multiprocessing&lt;/h2&gt;
&lt;h3 id=&quot;join&quot;&gt;join&lt;/h3&gt;
&lt;p&gt;等待调用join()方法的线程执行完毕，然后继续执行。&lt;br&gt;
可参见github&lt;a href=&quot;https
      
    
    </summary>
    
      <category term="pytorch" scheme="http://mxxhcm.github.io/categories/pytorch/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="pytorch" scheme="http://mxxhcm.github.io/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>pytorch 常见问题（不定期更新）</title>
    <link href="http://mxxhcm.github.io/2019/05/08/pytorch-problems/"/>
    <id>http://mxxhcm.github.io/2019/05/08/pytorch-problems/</id>
    <published>2019-05-08T13:52:18.000Z</published>
    <updated>2019-05-25T16:07:43.924Z</updated>
    
    <content type="html"><![CDATA[<h2 id="问题1-cudnn-status-arch-mismatch">问题1-CUDNN_STATUS_ARCH_MISMATCH</h2><h3 id="报错">报错</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">RuntimeError: CUDNN_STATUS_ARCH_MISMATCH</span><br></pre></td></tr></table></figure><h3 id="原因">原因</h3><p>CUDNN doesn’t support CUDA arch 2.1 cards.<br>CUDNN requires Compute Capability 3.0, at least.<br>意思是GPU的加速能力不够，CUDNN只支持CUDA Capability 3.0以上的GPU加速，实验室主机是GT620的显卡，2.1的加速能力。<br>GPU对应的capability: <a href="https://developer.nvidia.com/cuda-gpus" target="_blank" rel="noopener">https://developer.nvidia.com/cuda-gpus</a><br>所以，对于不能使用cudnn对cuda加速的显卡，我们可以设置cudnn加速为False，这个默认是为True的<br>torch.backends.cudnn.enabled=False<br>但是，由于显卡版本为2.1，太老了，没有二进制版本。所以，还是会报其他错误，因此，就别使用cpu进行加速啦。</p><h3 id="查看cuda版本">查看cuda版本</h3><p>~#:nvcc --version</p><h2 id="参考文献">参考文献</h2><p>1.<a href="https://pytorch.org/docs/stable/torch.html" target="_blank" rel="noopener">https://pytorch.org/docs/stable/torch.html</a><br>2.<a href="https://pytorch.org/docs/stable/nn.html" target="_blank" rel="noopener">https://pytorch.org/docs/stable/nn.html</a><br>3.<a href="http://pytorch.org/tutorials/beginner/pytorch_with_examples.html" target="_blank" rel="noopener">http://pytorch.org/tutorials/beginner/pytorch_with_examples.html</a><br>4.<a href="https://discuss.pytorch.org/t/distributed-model-parallelism/10377" target="_blank" rel="noopener">https://discuss.pytorch.org/t/distributed-model-parallelism/10377</a><br>5.<a href="https://ptorch.com/news/40.html" target="_blank" rel="noopener">https://ptorch.com/news/40.html</a><br>6.<a href="https://discuss.pytorch.org/t/distributed-data-parallel-freezes-without-error-message/8009" target="_blank" rel="noopener">https://discuss.pytorch.org/t/distributed-data-parallel-freezes-without-error-message/8009</a><br>7.<a href="https://discuss.pytorch.org/t/runtimeerror-cudnn-status-arch-mismatch/3580" target="_blank" rel="noopener">https://discuss.pytorch.org/t/runtimeerror-cudnn-status-arch-mismatch/3580</a><br>8.<a href="https://discuss.pytorch.org/t/error-when-using-cudnn/577/7" target="_blank" rel="noopener">https://discuss.pytorch.org/t/error-when-using-cudnn/577/7</a><br>10.<a href="https://pytorch.org/docs/stable/distributions.html#categorical" target="_blank" rel="noopener">https://pytorch.org/docs/stable/distributions.html#categorical</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;问题1-cudnn-status-arch-mismatch&quot;&gt;问题1-CUDNN_STATUS_ARCH_MISMATCH&lt;/h2&gt;
&lt;h3 id=&quot;报错&quot;&gt;报错&lt;/h3&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;
      
    
    </summary>
    
      <category term="pytorch" scheme="http://mxxhcm.github.io/categories/pytorch/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="常见问题" scheme="http://mxxhcm.github.io/tags/%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/"/>
    
      <category term="pytorch" scheme="http://mxxhcm.github.io/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow assign</title>
    <link href="http://mxxhcm.github.io/2019/05/08/tensorflow-assign/"/>
    <id>http://mxxhcm.github.io/2019/05/08/tensorflow-assign/</id>
    <published>2019-05-08T12:48:06.000Z</published>
    <updated>2019-05-08T13:51:32.261Z</updated>
    
    <content type="html"><![CDATA[<h2 id="tf-assign">tf.assign</h2><h3 id="简单解释">简单解释</h3><p>op = x.assign(y)<br>将y的值赋值给x，执行sess.run(op)后，x的值就变成和y一样了。</p><h3 id="代码示例">代码示例</h3><p><a href="https://github.com/mxxhcm/code/blob/master/tf/some_ops/tf_assign.py" target="_blank" rel="noopener">代码地址</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 声明两个Variable</span></span><br><span class="line">x1 = tf.Variable([<span class="number">3</span>,<span class="number">4</span>])</span><br><span class="line">x2 = tf.Variable([<span class="number">9</span>,<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># y是将x2 assign 给x1的op</span></span><br><span class="line">y = x1.assign(x2)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  sess.run(tf.global_variables_initializer())</span><br><span class="line">  xx1 = sess.run(x1)</span><br><span class="line">  <span class="comment"># 输出x1</span></span><br><span class="line">  print(xx1)</span><br><span class="line">  <span class="comment"># [3 4]</span></span><br><span class="line"></span><br><span class="line">  xx2 = sess.run(x2)</span><br><span class="line">  <span class="comment"># 输出x2</span></span><br><span class="line">  print(xx2)</span><br><span class="line">  <span class="comment"># [9 1]</span></span><br><span class="line"></span><br><span class="line">  print(sess.run(x1))</span><br><span class="line">  <span class="comment"># [3 4]</span></span><br><span class="line">  </span><br><span class="line">  <span class="comment"># 执行y操作</span></span><br><span class="line">  yy = sess.run(y)</span><br><span class="line">  print(yy)</span><br><span class="line">  <span class="comment"># [9 1]</span></span><br><span class="line">  </span><br><span class="line">  <span class="comment"># 发现x1已经用x2赋值了</span></span><br><span class="line">  print(sess.run(x1))</span><br><span class="line">  <span class="comment"># [9 1]</span></span><br><span class="line">  print(sess.run(x2))</span><br><span class="line">  <span class="comment"># [9 1]</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;tf-assign&quot;&gt;tf.assign&lt;/h2&gt;
&lt;h3 id=&quot;简单解释&quot;&gt;简单解释&lt;/h3&gt;
&lt;p&gt;op = x.assign(y)&lt;br&gt;
将y的值赋值给x，执行sess.run(op)后，x的值就变成和y一样了。&lt;/p&gt;
&lt;h3 id=&quot;代码示例&quot;&gt;代码
      
    
    </summary>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/categories/tensorflow/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow Tensor</title>
    <link href="http://mxxhcm.github.io/2019/05/08/tensorflow-Tensor/"/>
    <id>http://mxxhcm.github.io/2019/05/08/tensorflow-Tensor/</id>
    <published>2019-05-08T12:47:50.000Z</published>
    <updated>2019-05-23T08:01:57.086Z</updated>
    
    <content type="html"><![CDATA[<h2 id="tf-tensor">tf.Tensor</h2><h3 id="目的">目的</h3><ul><li>当做另一个op的输入，各个op通过Tensor连接起来，形成数据流。</li><li>可以使用t.eval()得到Tensor的值。。。</li></ul><h3 id="属性">属性</h3><ul><li>数据类型，float32, int32, string等</li><li>形状</li></ul><p>tf.Tensor一般是各种op操作后产生的变量，如tf.add,tf.log等运算，它的值是不可以改变的，没有assign()方法。</p><h2 id="维度">维度</h2><ul><li>0 标量</li><li>1 向量</li><li>2 矩阵</li><li>3 3阶张量</li><li>n n阶张量</li></ul><h3 id="创建0维">创建0维</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">string_scalar = tf.Variable(<span class="string">"Elephat"</span>, tf.string)</span><br><span class="line">int_scalar = tf.Variable(<span class="number">414</span>, tf.int16)</span><br><span class="line">float_scalar = tf.Variable(<span class="number">3.2345</span>, tf.float64)</span><br><span class="line"><span class="comment"># complex_scalar = tf.Variable(12.3 - 5j, tf.complex64)</span></span><br></pre></td></tr></table></figure><h3 id="创建1维">创建1维</h3><p>需要列表作为初值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">string_vec = tf.Variable([<span class="string">"Elephat"</span>], tf.string)</span><br><span class="line">int_vec = tf.Variable([<span class="number">414</span>, <span class="number">32</span>], tf.int16)</span><br><span class="line">float_vec = tf.Variable([<span class="number">3.2345</span>, <span class="number">32</span>], tf.float64)</span><br><span class="line"><span class="comment"># complex_vec = tf.Variable([12.3 - 5j, 1 + j], tf.complex64)</span></span><br></pre></td></tr></table></figure><h3 id="创建2维">创建2维</h3><p>至少需要包含一行和一列</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">bool_mat = tf.Variable([[<span class="literal">True</span>], [<span class="literal">False</span>]], tf.bool)</span><br><span class="line">string_mat = tf.Variable([<span class="string">"Elephat"</span>], tf.string)</span><br><span class="line">int_mat = tf.Variable([[<span class="number">414</span>], [<span class="number">32</span>]], tf.int16)</span><br><span class="line">float_mat = tf.Variable([[<span class="number">3.2345</span>, <span class="number">32</span>]], tf.float64)</span><br><span class="line"><span class="comment"># complex_mat = tf.Variable([[12.3 - 5j], [1 + j]], tf.complex64)</span></span><br></pre></td></tr></table></figure><h3 id="获取维度">获取维度</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.rank(tensor)</span><br></pre></td></tr></table></figure><h2 id="切片">切片</h2><p>0阶标量不需要索引，本身就是一个数字<br>1阶向量，可以传递一个索引访问某个数字<br>2阶矩阵，可以传递两个数字，返回一个标量，传递1个数字返回一个向量。<br>可以使用:访问，表示不操作该维度。</p><h2 id="获得tensor的shape">获得Tensor的shape</h2><ul><li>tf.Tensor.shape</li><li>tf.shape(tensor) # 返回tensor的shape</li><li>tf.Tensor.get_shape()</li></ul><h2 id="改变tensor的shape">改变tensor的shape</h2><h3 id="api">api</h3><p>tf.reshape(tensor, shape, name=None)</p><ul><li>tensor 输入待操作tensor</li><li>shape reshape后的shape</li></ul><h3 id="代码示例">代码示例</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># t = [1, 2, 3, 4, 5, 6, 7, 8, 9]</span></span><br><span class="line">tf.reshape(t, [<span class="number">3</span>, <span class="number">3</span>])  <span class="comment"># [[1, 2, 3,], [4, 5, 6], [7, 8, 9]]</span></span><br></pre></td></tr></table></figure><h2 id="增加数据维度">增加数据维度</h2><h3 id="api-v2">API</h3><p>tf.expand_dims(input, axis=None, name=None, dim=None)</p><h3 id="代码示例-v2">代码示例</h3><p><a href="https://github.com/mxxhcm/code/blob/master/tf/some_ops/tf_expand_dims.py" target="_blank" rel="noopener">代码地址</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x = tf.placeholder(tf.int32, [<span class="literal">None</span>, <span class="number">10</span>])</span><br><span class="line">y1 = tf.expand_dims(x, <span class="number">0</span>)</span><br><span class="line">y2 = tf.expand_dims(x, <span class="number">1</span>)</span><br><span class="line">y3 = tf.expand_dims(x, <span class="number">2</span>)</span><br><span class="line">y4 = tf.expand_dims(x, <span class="number">-1</span>) <span class="comment"># -1表示最后一维</span></span><br><span class="line"><span class="comment"># y5 = tf.expand_dims(x, 3) error</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">   inputs = np.random.rand(<span class="number">12</span>, <span class="number">10</span>)</span><br><span class="line">   r1, r2, r3, r4 = sess.run([y1, y2, y3, y4], feed_dict=&#123;x: inputs&#125;)</span><br><span class="line">   print(r1.shape)</span><br><span class="line">   print(r2.shape)</span><br><span class="line">   print(r3.shape)</span><br><span class="line">   print(r4.shape)</span><br></pre></td></tr></table></figure><h2 id="改变数据类型">改变数据类型</h2><h3 id="api-v3">API</h3><p>tf.cast(x, dtype, name=None)</p><ul><li>x  # 待转换数据</li><li>dtype # 待转换数据类型</li></ul><h3 id="代码示例-v3">代码示例</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = tf.constant([<span class="number">1.8</span>, <span class="number">2.2</span>], dtype=tf.float32)</span><br><span class="line">tf.cast(x, tf.int32)</span><br></pre></td></tr></table></figure><h2 id="评估张量">评估张量</h2><p>tf.Tensor.eval() 返回一个与Tensor内容相同的numpy数组</p><h3 id="代码示例-v4">代码示例</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">constant = tf.constant([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">tensor = constant * constant</span><br><span class="line">print(tensor.eval()) <span class="comment"># 注意，只有eval()处于活跃的Session中才会起作用。</span></span><br></pre></td></tr></table></figure><h2 id="特殊类型">特殊类型</h2><ul><li>tf.Variable 和tf.Tensor还不一样，<a href>点击查看tf.Variable详细介绍</a></li><li>tf.constant</li><li>tf.placeholder</li><li>tf.SparseTensor</li></ul><h3 id="tf-placeholder">tf.placeholder</h3><h4 id="api-v4">API</h4><p>返回一个Tensor<br>tf.placeholder(dtype, shape=None, name = None)</p><ul><li>dtype  # 类型</li><li>shape  # 形状</li></ul><h4 id="代码示例-v5">代码示例</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x = tf.placeholder(tf.float32, shape=(<span class="literal">None</span>, <span class="number">1024</span>))</span><br><span class="line">y = tf.matmul(x, x)</span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line"><span class="comment"># print(sess.run(y))  this will fail</span></span><br><span class="line">rand_array = np.random.rand(<span class="number">1024</span>, <span class="number">1024</span>)</span><br><span class="line">print(sess.run(y, feed_dict=&#123;x: rand_array&#125;))</span><br></pre></td></tr></table></figure><h3 id="tf-constant">tf.constant</h3><h4 id="api-v5">api</h4><p>tf.constant(values, dtype=None, shape=None, name=‘Const’, verify_shape=False)<br>返回一个constant的Tensor。</p><ul><li>values # 初始值</li><li>dtype # 类型</li><li>shape # 形状</li><li>name  # 可选</li><li>verify_shape</li></ul><h4 id="代码示例-v6">代码示例</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor = tf.constant([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>])</span><br><span class="line">tensor = tf.constant(<span class="number">-1.0</span>, shape=[<span class="number">3</span>, <span class="number">4</span>])</span><br></pre></td></tr></table></figure><h3 id="tf-variable">tf.Variable</h3><h4 id="api-v6">api</h4><p>tf.Variable.__init__(initial_value=None, trainable=True, collections=None, validate_shape=True, caching_device=None, name=None, …)</p><h4 id="代码示例-v7">代码示例</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor1 = tf.Variable([[<span class="number">1</span>,<span class="number">2</span>], [<span class="number">3</span>,<span class="number">5</span>]])</span><br><span class="line">tensor2 = tf.Variable(tf.constant([[<span class="number">1</span>,<span class="number">2</span>], [<span class="number">3</span>,<span class="number">5</span>]]))</span><br><span class="line">sess.run(tf.global_variables_initializer())</span><br><span class="line">sess.run(tensor1)</span><br><span class="line">sess.run(tensor2)</span><br></pre></td></tr></table></figure><h3 id="创建常量tensor">创建常量Tensor</h3><ul><li><p>tf.ones(shape, dtype=tf.float32, name=None)</p></li><li><p>tf.zeros(shape, dtype=tf.float32, name=None)</p></li><li><p>tf.fill(shape, value, name=None)</p></li><li><p>tf.constant(value, dtype=None, shape=None, name=‘Const’)</p></li><li><p>tf.ones_like(tensor, dtype=None, name=None)</p></li><li><p>tf.zeros_like(tensor, dtype=None, name=None)</p></li><li><p>tf.linspace()</p></li></ul><h3 id="创建随机tensor">创建随机Tensor</h3><ul><li>tf.random_uniform(shape, minval=0, maxval=None, dtype=tf.float32, seed=None, name=None)<br><a href="https://www.tensorflow.org/versions/r1.8/api_docs/python/tf/random_uniform" target="_blank" rel="noopener">https://www.tensorflow.org/versions/r1.8/api_docs/python/tf/random_uniform</a></li><li>tf.random_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name=None)<br>均值为mean，方差为stddev的正态分布<br><a href="https://www.tensorflow.org/versions/r1.8/api_docs/python/tf/random_normal" target="_blank" rel="noopener">https://www.tensorflow.org/versions/r1.8/api_docs/python/tf/random_normal</a></li><li>tf.truncated_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name=None)<br>均值为mean，方差为stddev的正态分布，保留[mean-2*stddev, mean+2*stddev]之内的随机数。</li><li>tf.random_shuffle(value, seed=None, name=None)<br>对value的第一维重新排列</li></ul><h2 id="代码示例-v8">代码示例</h2><p><a href="https://github.com/mxxhcm/code/blob/master/tf/some_ops/tf_tensor.py" target="_blank" rel="noopener">代码地址</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line"></span><br><span class="line">x = tf.constant([[<span class="number">3</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">8</span>]])</span><br><span class="line">print(sess.run(tf.constant([<span class="number">3</span>,<span class="number">4</span>])))</span><br><span class="line"><span class="comment"># [3 4]</span></span><br><span class="line"></span><br><span class="line">print(sess.run(tf.ones_like(x)))</span><br><span class="line">[[<span class="number">1</span> <span class="number">1</span>]</span><br><span class="line"> [<span class="number">1</span> <span class="number">1</span>]]</span><br><span class="line"></span><br><span class="line">print(sess.run(tf.zeros_like(x)))</span><br><span class="line">[[<span class="number">0</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">0</span> <span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出正态分布的随机采样值</span></span><br><span class="line">print(sess.run(tf.random_normal([<span class="number">2</span>,<span class="number">2</span>])))</span><br><span class="line"><span class="comment"># [[-0.5188188   0.77538687]</span></span><br><span class="line"> [ <span class="number">1.2343276</span>  <span class="number">-0.58534193</span>]]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出均匀[0,1]分布的随机采样值。</span></span><br><span class="line">print(sess.run(tf.random_uniform([<span class="number">2</span>,<span class="number">2</span>])))</span><br><span class="line">[[<span class="number">0.8851745</span>  <span class="number">0.12824357</span>]</span><br><span class="line"> [<span class="number">0.28489232</span> <span class="number">0.76961493</span>]]</span><br><span class="line"></span><br><span class="line">print(sess.run(tf.random_uniform([<span class="number">2</span>,<span class="number">2</span>], dtype=tf.int32, maxval=<span class="number">4</span>)))</span><br><span class="line">[[<span class="number">0</span> <span class="number">2</span>]</span><br><span class="line"> [<span class="number">2</span> <span class="number">1</span>]]</span><br><span class="line"></span><br><span class="line">print(sess.run(tf.ones([<span class="number">3</span>, <span class="number">4</span>])))</span><br><span class="line">[[<span class="number">1.</span> <span class="number">1.</span> <span class="number">1.</span> <span class="number">1.</span>]</span><br><span class="line"> [<span class="number">1.</span> <span class="number">1.</span> <span class="number">1.</span> <span class="number">1.</span>]</span><br><span class="line"> [<span class="number">1.</span> <span class="number">1.</span> <span class="number">1.</span> <span class="number">1.</span>]]</span><br><span class="line"></span><br><span class="line">print(sess.run(tf.zeros([<span class="number">2</span>,<span class="number">2</span>])))</span><br><span class="line">[[<span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span>]]</span><br></pre></td></tr></table></figure><h2 id="参考文献">参考文献</h2><p>1.<a href="https://www.tensorflow.org/guide/tensors?hl=zh_cn" target="_blank" rel="noopener">https://www.tensorflow.org/guide/tensors?hl=zh_cn</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;tf-tensor&quot;&gt;tf.Tensor&lt;/h2&gt;
&lt;h3 id=&quot;目的&quot;&gt;目的&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;当做另一个op的输入，各个op通过Tensor连接起来，形成数据流。&lt;/li&gt;
&lt;li&gt;可以使用t.eval()得到Tensor的值。。。&lt;/li&gt;
&lt;/
      
    
    </summary>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/categories/tensorflow/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow cnn demo</title>
    <link href="http://mxxhcm.github.io/2019/05/08/tensorflow-cnn-demo/"/>
    <id>http://mxxhcm.github.io/2019/05/08/tensorflow-cnn-demo/</id>
    <published>2019-05-08T11:35:01.000Z</published>
    <updated>2019-05-08T12:43:31.108Z</updated>
    
    <content type="html"><![CDATA[<h2 id="tf-nn-conv2d">tf.nn.conv2d</h2><h3 id="代码示例">代码示例</h3><p><a href="https://github.com/mxxhcm/code/blob/master/tf/some_ops/tf_conv2d.py" target="_blank" rel="noopener">代码地址</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv</span><span class="params">(img)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> len(img.shape) == <span class="number">3</span>:</span><br><span class="line">        img = tf.reshape(img, [<span class="number">1</span>]+img.get_shape().as_list())</span><br><span class="line">    fiter = tf.random_normal([<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">1</span>])</span><br><span class="line">    img = tf.nn.conv2d(img, fiter, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line">    print(img.get_shape())</span><br><span class="line">    <span class="keyword">return</span> img</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> skimage <span class="keyword">import</span> data</span><br><span class="line"><span class="comment"># img = data.text()</span></span><br><span class="line">img = data.astronaut()</span><br><span class="line">print(img.shape)</span><br><span class="line">plt.imshow(img)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">x = tf.placeholder(tf.float32, shape=(img.shape))</span><br><span class="line">result = tf.squeeze(conv(x)).eval(feed_dict=&#123;x:img&#125;)</span><br><span class="line">plt.imshow(result)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;tf-nn-conv2d&quot;&gt;tf.nn.conv2d&lt;/h2&gt;
&lt;h3 id=&quot;代码示例&quot;&gt;代码示例&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/mxxhcm/code/blob/master/tf/some_ops/tf_conv2d
      
    
    </summary>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/categories/tensorflow/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow basic operation</title>
    <link href="http://mxxhcm.github.io/2019/05/08/tensorflow-basic-operation/"/>
    <id>http://mxxhcm.github.io/2019/05/08/tensorflow-basic-operation/</id>
    <published>2019-05-08T10:57:41.000Z</published>
    <updated>2019-05-16T01:08:47.714Z</updated>
    
    <content type="html"><![CDATA[<h2 id="创建session">创建Session</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">n = <span class="number">32</span></span><br><span class="line">x = tf.linspace(<span class="number">-3.0</span>, <span class="number">3.0</span>, n)</span><br></pre></td></tr></table></figure><h3 id="普通session">普通Session</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sess = tf.Session()</span><br></pre></td></tr></table></figure><h3 id="交互式session">交互式Session</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">sess = tf.InteractiveSession()</span><br></pre></td></tr></table></figure><h3 id="在sess内执行op">在sess内执行op</h3><h4 id="方法1">方法1</h4><p>sess.run(tf.global_variables_initializer())<br>sess.run(op)<br>代码示例</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sess.run(tf.global_variables_initializer())</span><br><span class="line">result  = sess.run(x)</span><br></pre></td></tr></table></figure><h4 id="方法2">方法2</h4><p>tf.global_variables_initializer().run()<br>sess.run(op)<br>op.eval()<br>代码示例</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tf.global_variables_initializer().run()</span><br><span class="line">x.eval(session=sess)</span><br><span class="line">sess.close()</span><br></pre></td></tr></table></figure><h2 id="新op添加到默认图上">新op添加到默认图上</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">sigma = <span class="number">1.0</span></span><br><span class="line">mean = <span class="number">0.0</span></span><br><span class="line"><span class="comment"># 和x的shape是一样的</span></span><br><span class="line">z = (tf.exp(tf.negative(tf.pow(x - mean, <span class="number">2.0</span>) /</span><br><span class="line">                        (<span class="number">2.0</span> * tf.pow(sigma, <span class="number">2.0</span>)))) *</span><br><span class="line">     (<span class="number">1.0</span> / (sigma * tf.sqrt(<span class="number">2.0</span> * <span class="number">3.1415</span>))))</span><br><span class="line">print(type(z))</span><br><span class="line">print(z.graph <span class="keyword">is</span> tf.get_default_graph())</span><br><span class="line"></span><br><span class="line">plt.plot(z.eval())</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h2 id="查看shape">查看shape</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">print(z.shape)</span><br><span class="line">print(z.get_shape())</span><br><span class="line">print(z.get_shape().as_list())</span><br><span class="line">print(tf.shape(z).eval())</span><br></pre></td></tr></table></figure><h2 id="常用function">常用function</h2><h3 id="tf-stack">tf.stack</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">print(tf.stack([tf.shape(z),tf.shape(z),[<span class="number">3</span>]]).eval())</span><br><span class="line"><span class="comment"># tf.reshape, tf.matmul</span></span><br><span class="line">z_ = tf.matmul(tf.reshape(z, (n, <span class="number">1</span>)), tf.reshape(z, (<span class="number">1</span>, n)))</span><br><span class="line">plt.imshow(z_.eval()) plt.show()</span><br></pre></td></tr></table></figure><h3 id="tf-ones-like-tf-multiply">tf.ones_like, tf.multiply</h3><p>tf.ones_like返回与输入tensor具有相同shape的tensor</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">x = tf.reshape(tf.sin(tf.linspace(- <span class="number">3.0</span>, <span class="number">3.0</span>, n)), (n, <span class="number">1</span>))</span><br><span class="line">print(x.shape)</span><br><span class="line">y = tf.reshape(tf.ones_like(x), (<span class="number">1</span>, n))</span><br><span class="line">print(y.shape)</span><br><span class="line">print(y.eval())</span><br><span class="line">z = tf.multiply(tf.matmul(x,y), z_)</span><br><span class="line">print(z.shape)</span><br><span class="line">plt.imshow(z.eval())</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h2 id="列出graph中所有操作">列出graph中所有操作</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">ops = tf.get_default_graph().get_operations()</span><br><span class="line">print([op <span class="keyword">for</span> op <span class="keyword">in</span> ops])</span><br></pre></td></tr></table></figure><h2 id="代码">代码</h2><p><a href="https://github.com/mxxhcm/code/blob/master/tf/some_ops/tf_basic.py" target="_blank" rel="noopener">完整地址</a></p><h2 id="参考文献">参考文献</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;创建session&quot;&gt;创建Session&lt;/h2&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span c
      
    
    </summary>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/categories/tensorflow/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow boolean_mask</title>
    <link href="http://mxxhcm.github.io/2019/05/08/tensorflow-boolean-mask/"/>
    <id>http://mxxhcm.github.io/2019/05/08/tensorflow-boolean-mask/</id>
    <published>2019-05-08T09:46:26.000Z</published>
    <updated>2019-05-12T09:04:21.196Z</updated>
    
    <content type="html"><![CDATA[<h2 id="tf-boolean-mask">tf.boolean_mask</h2><h3 id="简单解释">简单解释</h3><p>用一个mask数组和输入的tensor做与操作，忽略为0的值。</p><h3 id="api">api</h3><p>定义在tensorflow/python/ops/array_ops.py</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tf.boolean_mask(</span><br><span class="line">    tensor, <span class="comment"># 要处理的tensor</span></span><br><span class="line">    mask, <span class="comment"># 掩码，也需要是一个tensor</span></span><br><span class="line">    name=<span class="string">'boolean_mask'</span>, <span class="comment"># 这个op的名字</span></span><br><span class="line">    axis=<span class="literal">None</span> <span class="comment">#</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="代码示例">代码示例</h3><p><a href="https://github.com/mxxhcm/code/blob/master/tf/some_ops/tf_boolean_mask.py" target="_blank" rel="noopener">代码地址</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line">a = tf.Variable([1, 2, 3])</span><br><span class="line">b = tf.Variable([2, 1.0, 4.0])</span><br><span class="line">c = tf.Variable([2, 1.0, 0.0])</span><br><span class="line">d = tf.Variable([2, 0.0, 4.0])</span><br><span class="line">e = tf.Variable([0, 1.0, 4.0])</span><br><span class="line">f = tf.Variable([0, 1.0, 0.0])</span><br><span class="line">g = tf.Variable([0, 0.0, 0.0])</span><br><span class="line"></span><br><span class="line">sess.run(tf.global_variables_initializer())</span><br><span class="line">print(&quot;a: &quot;, sess.run(a))</span><br><span class="line">print(&quot;b: &quot;, sess.run(b))</span><br><span class="line">print(&quot;c: &quot;, sess.run(c))</span><br><span class="line">print(&quot;d: &quot;, sess.run(d))</span><br><span class="line">print(&quot;e: &quot;, sess.run(e))</span><br><span class="line">print(&quot;f: &quot;, sess.run(f))</span><br><span class="line">print(&quot;g: &quot;, sess.run(g))</span><br><span class="line"># c = tf.maximum(a, b)</span><br><span class="line">a1 = tf.boolean_mask(a, b)</span><br><span class="line">a2 = tf.boolean_mask(a, c)</span><br><span class="line">a3 = tf.boolean_mask(a, d)</span><br><span class="line">a4 = tf.boolean_mask(a, e)</span><br><span class="line">a5 = tf.boolean_mask(a, f)</span><br><span class="line">a6 = tf.boolean_mask(a, g)</span><br><span class="line"></span><br><span class="line">print(&quot;tf.boolean(a, b):\n  &quot;, sess.run(a1))</span><br><span class="line">print(&quot;tf.boolean(a, c):\n  &quot;, sess.run(a2))</span><br><span class="line">print(&quot;tf.boolean(a, d):\n  &quot;, sess.run(a3))</span><br><span class="line">print(&quot;tf.boolean(a, e):\n  &quot;, sess.run(a4))</span><br><span class="line">print(&quot;tf.boolean(a, f):\n  &quot;, sess.run(a5))</span><br><span class="line">print(&quot;tf.boolean(a, g):\n  &quot;, sess.run(a6))</span><br></pre></td></tr></table></figure><p>输出如下：</p><blockquote><p>a:  [1 2 3]<br>b:  [2. 1. 4.]<br>c:  [2. 1. 0.]<br>d:  [2. 0. 4.]<br>e:  [0. 1. 4.]<br>f:  [0. 1. 0.]<br>g:  [0. 0. 0.]<br>tf.boolean(a, b):<br>[1 2 3]<br>tf.boolean(a, c):<br>[1 2]<br>tf.boolean(a, d):<br>[1 3]<br>tf.boolean(a, e):<br>[2 3]<br>tf.boolean(a, f):<br>[2]<br>tf.boolean(a, g):<br>[]</p></blockquote><h2 id="参考文献">参考文献</h2><p>1.<a href="http://landcareweb.com/questions/27920/zai-tensorflowzhong-ru-he-cong-pythonde-zhang-liang-zhong-huo-qu-fei-ling-zhi-ji-qi-suo-yin" target="_blank" rel="noopener">http://landcareweb.com/questions/27920/zai-tensorflowzhong-ru-he-cong-pythonde-zhang-liang-zhong-huo-qu-fei-ling-zhi-ji-qi-suo-yin</a><br>2.<a href="https://www.tensorflow.org/api_docs/python/tf/boolean_mask" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/boolean_mask</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;tf-boolean-mask&quot;&gt;tf.boolean_mask&lt;/h2&gt;
&lt;h3 id=&quot;简单解释&quot;&gt;简单解释&lt;/h3&gt;
&lt;p&gt;用一个mask数组和输入的tensor做与操作，忽略为0的值。&lt;/p&gt;
&lt;h3 id=&quot;api&quot;&gt;api&lt;/h3&gt;
&lt;p&gt;定义在ten
      
    
    </summary>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/categories/tensorflow/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow saver</title>
    <link href="http://mxxhcm.github.io/2019/05/08/tensorflow-saver/"/>
    <id>http://mxxhcm.github.io/2019/05/08/tensorflow-saver/</id>
    <published>2019-05-08T09:39:54.000Z</published>
    <updated>2019-05-17T07:27:10.711Z</updated>
    
    <content type="html"><![CDATA[<h2 id="tf-train-saver">tf.train.Saver()</h2><p>Saver是类，不是函数。可以用来保存和恢复variable，保存和恢复模型。Saver对象提供save和restore等各类op。</p><h2 id="api">API</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">__init__(</span><br><span class="line">    var_list=<span class="literal">None</span>, <span class="comment"># 指定要保存的variablelist</span></span><br><span class="line">    reshape=<span class="literal">False</span>,</span><br><span class="line">    sharded=<span class="literal">False</span>,</span><br><span class="line">    max_to_keep=<span class="number">5</span>, <span class="comment"># 最多保留最近的几个checkpoints</span></span><br><span class="line">    keep_checkpoint_every_n_hours=<span class="number">10000.0</span>,</span><br><span class="line">    name=<span class="literal">None</span>,</span><br><span class="line">    restore_sequentially=<span class="literal">False</span>,</span><br><span class="line">    saver_def=<span class="literal">None</span>,</span><br><span class="line">    builder=<span class="literal">None</span>,</span><br><span class="line">    defer_build=<span class="literal">False</span>,</span><br><span class="line">    allow_empty=<span class="literal">False</span>,</span><br><span class="line">    write_version=tf.train.SaverDef.V2,</span><br><span class="line">    pad_step_number=<span class="literal">False</span>,</span><br><span class="line">    save_relative_paths=<span class="literal">False</span>,</span><br><span class="line">    filename=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h2 id="保存和全部variables">保存和全部variables</h2><ul><li>恢复variable时，无需初始化。</li><li>恢复variable时，使用的是variable的name，不是op的name。只要知道variable的name即可。save和restore的op name不需要相同，只要variable name相同即可。</li><li>对于使用tf.Variable()创建的variable，如果没有指定variable名字的话，系统会为其生成默认名字，在恢复的时候，需要使用tf.get_variable()恢复variable，同时传variable name和shape。</li></ul><h3 id="保存全部variable">保存全部variable</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">saver = tf.train.Saver()</span><br><span class="line">saver.save(sess, save_path) <span class="comment"># 需要指定的是checkpoint的名字而不是目录</span></span><br></pre></td></tr></table></figure><h3 id="恢复全部variable">恢复全部variable</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">saver = tf.train.Saver()</span><br><span class="line">saver.restore(sess, save_path)</span><br></pre></td></tr></table></figure><h2 id="保存和部分variables">保存和部分variables</h2><h3 id="保存全部variable-v2">保存全部variable</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">saver = tf.train.Saver(&#123;<span class="string">"variable_name1"</span>: op_name1,..., <span class="string">"variable_namen"</span>: op_namen&#125;)</span><br><span class="line">saver.save(sess, save_path) <span class="comment"># 需要指定的是checkpoint的名字而不是目录</span></span><br></pre></td></tr></table></figure><h3 id="恢复全部variable-v2">恢复全部variable</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">saver = tf.train.Saver(&#123;<span class="string">"variable_name1"</span>: op_name1,..., <span class="string">"variable_namen"</span>: op_namen&#125;)</span><br><span class="line">saver.restore(sess, save_path)</span><br></pre></td></tr></table></figure><h2 id="查看checkpoint文件中的variable">查看checkpoint文件中的variable</h2><p>使用inspect_ckeckpoint库</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># import the inspect_checkpoint library</span></span><br><span class="line"><span class="keyword">from</span> tensorflow.python.tools <span class="keyword">import</span> inspect_checkpoint <span class="keyword">as</span> chkp</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印checkpoint文件中所有variable</span></span><br><span class="line">chkp.print_tensors_in_checkpoint_file(<span class="string">"saver/variables/all_variables.ckpt"</span>, tensor_name=<span class="string">''</span>, all_tensors=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">chkp.print_tensors_in_checkpoint_file(<span class="string">"saver/variables/all_variables.ckpt"</span>, tensor_name=<span class="string">'v1'</span>, all_tensors=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">chkp.print_tensors_in_checkpoint_file(<span class="string">"saver/variables/all_variables.ckpt"</span>, tensor_name=<span class="string">'v2'</span>, all_tensors=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><h2 id="保存和恢复模型">保存和恢复模型</h2><p>其实和保存，恢复模型没有什么区别。只是把整个模型的variables都save和restore了。</p><h2 id="代码示例">代码示例</h2><p><a href="https://github.com/mxxhcm/code/tree/master/tf/some_ops/saver_restore" target="_blank" rel="noopener">代码地址</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">graph = tf.Graph()</span><br><span class="line"><span class="keyword">with</span> graph.as_default():</span><br><span class="line">    W = tf.Variable([<span class="number">0.3</span>], dtype=tf.float32)</span><br><span class="line">    b = tf.Variable([<span class="number">-0.3</span>], dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># input and output</span></span><br><span class="line">    x = tf.placeholder(tf.float32)</span><br><span class="line">    y = tf.placeholder(tf.float32)</span><br><span class="line">    predicted_y = W*x+b</span><br><span class="line"></span><br><span class="line">    <span class="comment"># MSE loss</span></span><br><span class="line">    loss = tf.reduce_mean(tf.square(y - predicted_y))</span><br><span class="line">    <span class="comment"># optimizer</span></span><br><span class="line">    optimizer = tf.train.GradientDescentOptimizer(<span class="number">0.01</span>)</span><br><span class="line">    train_op = optimizer.minimize(loss)</span><br><span class="line"></span><br><span class="line">inputs = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]</span><br><span class="line">outputs = [<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session(graph=graph) <span class="keyword">as</span> sess:</span><br><span class="line">    saver = tf.train.Saver()</span><br><span class="line">    init_op = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5000</span>):</span><br><span class="line">        sess.run(train_op, feed_dict=&#123;x: inputs, y: outputs&#125;)</span><br><span class="line">    l_, W_, b_ = sess.run([loss, W, b], feed_dict=&#123;x: inputs, y: outputs&#125;)</span><br><span class="line">    print(<span class="string">"loss: "</span>, l_, <span class="string">"w: "</span>, W_, <span class="string">"b:"</span>, b_)</span><br><span class="line">    checkpoint = <span class="string">"./checkpoint/saver1.ckpt"</span></span><br><span class="line">    save_path = saver.save(sess, checkpoint)</span><br><span class="line">    print(<span class="string">"Model has been saved in %s."</span> % save_path)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session(graph=graph) <span class="keyword">as</span> sess:</span><br><span class="line">    saver = tf.train.Saver()</span><br><span class="line">    saver.restore(sess, checkpoint)</span><br><span class="line">    l_, W_, b_ = sess.run([loss, W, b], feed_dict=&#123;x: inputs, y: outputs&#125;)</span><br><span class="line">    print(<span class="string">"loss: "</span>, l_, <span class="string">"w: "</span>, W_, <span class="string">"b:"</span>, b_)</span><br><span class="line">    print(<span class="string">"Model has been restored."</span>)</span><br></pre></td></tr></table></figure><h2 id="参考文献">参考文献</h2><p>1.<a href="https://www.tensorflow.org/guide/saved_model" target="_blank" rel="noopener">https://www.tensorflow.org/guide/saved_model</a><br>2.<a href="https://www.tensorflow.org/api_docs/python/tf/train/Saver" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/train/Saver</a><br>3.<a href="https://www.bilibili.com/read/cv681031/" target="_blank" rel="noopener">https://www.bilibili.com/read/cv681031/</a><br>4.<a href="https://cv-tricks.com/tensorflow-tutorial/save-restore-tensorflow-models-quick-complete-tutorial/" target="_blank" rel="noopener">https://cv-tricks.com/tensorflow-tutorial/save-restore-tensorflow-models-quick-complete-tutorial/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;tf-train-saver&quot;&gt;tf.train.Saver()&lt;/h2&gt;
&lt;p&gt;Saver是类，不是函数。可以用来保存和恢复variable，保存和恢复模型。Saver对象提供save和restore等各类op。&lt;/p&gt;
&lt;h2 id=&quot;api&quot;&gt;API&lt;/h2
      
    
    </summary>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/categories/tensorflow/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow summary</title>
    <link href="http://mxxhcm.github.io/2019/05/08/tensorflow-summary/"/>
    <id>http://mxxhcm.github.io/2019/05/08/tensorflow-summary/</id>
    <published>2019-05-08T09:39:43.000Z</published>
    <updated>2019-06-30T15:19:34.526Z</updated>
    
    <content type="html"><![CDATA[<h2 id="tf-summary">tf.summary</h2><h3 id="目的">目的</h3><p>该模块定义在tensorflow/_api/v1/summary/__init__.py文件中，主要用于可视化。<br>每次运行完一个op之后，调用writer.add_summary()将其写入事件file。因为summary操作实在数据流的外面进行操作的，并不会操作数据，所以需要每次运行完之后，都调用一次写入函数。</p><h2 id="常用api">常用API</h2><h3 id="函数">函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1.定义一个summary scalar op，同时会将这个op加入到tf.GraphKeys.SUMMARIES collection中。</span></span><br><span class="line">tf.summary.scalar(</span><br><span class="line">name, </span><br><span class="line">tensor, <span class="comment"># 一个实数型的Tensor，包含单个的值。</span></span><br><span class="line">collections=<span class="literal">None</span>, <span class="comment"># 可选项，是graph collections keys的list，新的summary op会被添加到这个list of collection。默认的list是[GraphKeys.SUMMARIES]。</span></span><br><span class="line">family=<span class="literal">None</span></span><br><span class="line">)</span><br><span class="line"><span class="comment"># 2.定义一个summary histogram op，同时会将这个op加入到tf.GraphKeys.SUMMARIES collection中。</span></span><br><span class="line">tf.summary.histogram(</span><br><span class="line">    name,</span><br><span class="line">    values, <span class="comment"># 一个实数型的Tensor，任意shape，用来生成直方图。</span></span><br><span class="line">    collections=<span class="literal">None</span>, <span class="comment"># 可选项，是graph collections keys的list，新的summary op会被添加到这个list of collection。默认的list是[GraphKeys.SUMMARIES].</span></span><br><span class="line">    family=<span class="literal">None</span></span><br><span class="line">)</span><br><span class="line"><span class="comment"># 3.将所有定义的summary op集中到一块，如scalar，text，histogram等。</span></span><br><span class="line">tf.summary.merge_all(</span><br><span class="line">    key=tf.GraphKeys.SUMMARIES, <span class="comment">#指定用哪个GraphKey来collect summaries。默认设置为GraphKeys.SUMMARIES.并不是说将他们加入到哪个GraphKey的意思，tf.summary.scalar()等会将op加入到相应的colleection。</span></span><br><span class="line">    scope=<span class="literal">None</span>, <span class="comment">#</span></span><br><span class="line">    name=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h4 id="scalar和histogram的区别">scalar和histogram的区别</h4><p>scalar记录的是一个标量。<br>而histogram记录的是一个分布，可以是任何shape。</p><h4 id="函数示例">函数示例</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">summary_loss = tf.summary.scalar(<span class="string">'loss'</span>, loss)</span><br><span class="line">summary_weights = tf.summary.scalar(<span class="string">'weights'</span>, weights)</span><br><span class="line"><span class="comment"># merged可以代替sumary_loss和summary_weights op。</span></span><br><span class="line">merged = tf.summary.merge_all()</span><br></pre></td></tr></table></figure><p>关于tf.summary.histogram()的示例，<a href="https://github.com/mxxhcm/code/blob/master/tf/some_ops/tf_summary_histogram.py" target="_blank" rel="noopener">可以点击查看。</a></p><h3 id="类">类</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义将Summary数据写入event文件的类</span></span><br><span class="line">tf.summary.FileWriter(</span><br><span class="line">self, </span><br><span class="line">logdir,　</span><br><span class="line">graph=<span class="literal">None</span>, </span><br><span class="line">max_queue=<span class="number">10</span>,</span><br><span class="line">flush_secs=<span class="number">120</span>, </span><br><span class="line">graph_def=<span class="literal">None</span>, </span><br><span class="line">filename_suffix=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h4 id="类内函数">类内函数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将summary op的输出存到event文件(Adds a Summary protocol buffer to the event file.)</span></span><br><span class="line">tf.summary.FileWriter.add_summary(</span><br><span class="line">self,</span><br><span class="line">summary,  <span class="comment"># 一个Summary protocol buffer，一般是sess.run(summary_op)的结果</span></span><br><span class="line">global_step=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h4 id="类示例">类示例</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">writer = tf.summary.FileWriter(<span class="string">"./summary/"</span>)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    summ = sess.run([merged], feed_dict=&#123;x: inputs, y: outputs&#125;)</span><br><span class="line">    writer.add_summary(summ, global_step=i)</span><br></pre></td></tr></table></figure><h2 id="使用流程">使用流程</h2><ol><li>summary_op = tf.summary_scalar() # 声明summary op，会将该op变量加入tf.GraphKeys.SUMMARIES collection</li><li>merged = tf.summary.merge_all() # 将所有summary op合并</li><li>writer = tf.summary.FileWriter() # 声明一个FileWrite文件，用于将Summary数据写入event文件</li><li>output = sess.run([merged]) # 运行merge后的summary op</li><li>writer.add_summary(output) # 将op运行后的结果写入事件文件</li></ol><h2 id="代码示例">代码示例</h2><p><a href="https://github.com/mxxhcm/code/blob/master/tf/some_ops/tf_summary.py" target="_blank" rel="noopener">代码地址</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">graph = tf.Graph()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> graph.as_default():</span><br><span class="line">    <span class="comment"># model parameters</span></span><br><span class="line">    w = tf.Variable([<span class="number">0.3</span>], name=<span class="string">"w"</span>, dtype=tf.float32)</span><br><span class="line">    b = tf.Variable([<span class="number">0.2</span>], name=<span class="string">"b"</span>, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">    x = tf.placeholder(tf.float32, name=<span class="string">"inputs"</span>)</span><br><span class="line">    y = tf.placeholder(tf.float32, name=<span class="string">"outputs"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">'linear_model'</span>):</span><br><span class="line">        linear = w * x + b</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">'cal_loss'</span>):</span><br><span class="line">        loss = tf.reduce_mean(input_tensor=tf.square(y - linear), name=<span class="string">'loss'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">'add_summary'</span>):</span><br><span class="line">        summary_loss = tf.summary.scalar(<span class="string">'MSE'</span>, loss)</span><br><span class="line">        summary_b = tf.summary.scalar(<span class="string">'b'</span>, b[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">'train_model'</span>):</span><br><span class="line">        optimizer = tf.train.GradientDescentOptimizer(<span class="number">0.01</span>)</span><br><span class="line">        train = optimizer.minimize(loss)</span><br><span class="line"></span><br><span class="line">inputs = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]</span><br><span class="line">outputs = [<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session(graph=graph) <span class="keyword">as</span> sess:</span><br><span class="line">    writer = tf.summary.FileWriter(<span class="string">"./summary/"</span>, graph)</span><br><span class="line">    merged = tf.summary.merge_all()</span><br><span class="line"></span><br><span class="line">    init_op = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5000</span>):</span><br><span class="line">        _, summ = sess.run([train, merged], feed_dict=&#123;x: inputs, y: outputs&#125;)</span><br><span class="line">        writer.add_summary(summ, global_step=i)</span><br><span class="line"></span><br><span class="line">    w_, b_, l_ = sess.run([w, b, loss], feed_dict=&#123;x: inputs, y: outputs&#125;)</span><br><span class="line">    print(<span class="string">"w: "</span>, w_, <span class="string">"b: "</span>, b_, <span class="string">"loss: "</span>, l_)</span><br><span class="line">    <span class="keyword">for</span> var <span class="keyword">in</span> tf.get_collection(tf.GraphKeys.SUMMARIES):</span><br><span class="line">    <span class="comment">#for var in tf.get_collection(tf.GraphKeys.MODEL_VARIABLES):</span></span><br><span class="line">        print(var)</span><br></pre></td></tr></table></figure><p>使用tensorboard --logdir ./summary/打开tensorboard<br>打开之后在每个图中会看到两个曲线，一个深色，一个浅色，浅色的是真实的值，深色的是在真实值的基础上进行了平滑。在左侧可以调整平滑系数，默认是0.6，如果是0表示不进行平滑，如果是1就成了一条直线。<br>如果多次运行的话，多次的结果都会在图中显示出来，鼠标移动到图中只能看到最新的那次结果。浅色的线是最新运行的结果的真实值，深色的线是平滑后的，设置为0可以看到深色和浅色重合了。横轴STEP表示按步长，RELATIVE表示按相对时间，WALL表示将它们分开显示。<br>对于histogram来说的话，这个它是把每一步中list的值做成了一个直方图，统计在每个范围内出现的值的个数，然后按照时间步展现出来每一步的直方图。但是这个直方图是做了一定优化的，如果拿几个值来测试，最后的结果跟你想的并不一定一样。<br>所以histogram就是展现出了每一步list的值主要集中在哪个地方。有两个mode，overlay和offset，overlay是重叠的。<br>overlay中横轴是bin的取值，纵轴是每个bin的频率，所有的时间步都在一起，每一条线都代表一个时间步的直方图，鼠标悬停上去会显示每一条线的时间步。<br>offset中横轴是bin的取值，纵轴是时间步，所有的直方图按照时间步进行展开，每一时间步都是一条单独的线，鼠标悬停上去会显示每一条线的频率。<br>。</p><h3 id="官网示例">官网示例</h3><p>加了一定注释，<a href="https://github.com/mxxhcm/code/blob/master/tf/ops/tf_summary_example.py" target="_blank" rel="noopener">可以点击查看</a></p><h2 id="所有api">所有API</h2><h3 id="类-v2">类</h3><ul><li>class Event: A ProtocolMessage</li><li>class FileWriter: Writes Summary protocol buffers to event files.</li><li>class FileWriterCache: Cache for file writers.</li><li>class SessionLog: A ProtocolMessage</li><li>class Summary: A ProtocolMessage</li><li>class SummaryDescription: A ProtocolMessage</li><li>class TaggedRunMetadata: A ProtocolMessage</li></ul><h3 id="函数-v2">函数</h3><ul><li>scalar(…): Outputs a Summary protocol buffer containing a single scalar value.</li><li>histogram(…): Outputs a Summary protocol buffer with a histogram.</li><li>image(…): Outputs a Summary protocol buffer with images.</li><li>tensor_summary(…): Outputs a Summary protocol buffer with a serialized tensor.proto.</li><li>audio(…): Outputs a Summary protocol buffer with audio.</li><li>text(…): Summarizes textual data.</li><li>merge(…): Merges summaries.</li><li>merge_all(…): Merges all summaries collected in the default graph.</li><li>get_summary_description(…): Given a TensorSummary node_def, retrieve its SummaryDescription.</li></ul><h2 id="参考文献">参考文献</h2><p>1.<a href="https://www.tensorflow.org/api_docs/python/tf/summary" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/summary</a><br>2.<a href="https://www.tensorflow.org/api_docs/python/tf/summary/scalar" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/summary/scalar</a><br>3.<a href="https://www.tensorflow.org/api_docs/python/tf/summary/histogram" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/summary/histogram</a><br>4.<a href="https://www.tensorflow.org/api_docs/python/tf/summary/merge_all" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/summary/merge_all</a><br>5.<a href="https://www.tensorflow.org/guide/graphs#visualizing_your_graph" target="_blank" rel="noopener">https://www.tensorflow.org/guide/graphs#visualizing_your_graph</a><br>6.<a href="https://www.tensorflow.org/guide/summaries_and_tensorboard" target="_blank" rel="noopener">https://www.tensorflow.org/guide/summaries_and_tensorboard</a><br>7.<a href="https://www.tensorflow.org/tensorboard/r1/histograms" target="_blank" rel="noopener">https://www.tensorflow.org/tensorboard/r1/histograms</a><br>8.<a href="https://ask.csdn.net/questions/760881" target="_blank" rel="noopener">https://ask.csdn.net/questions/760881</a><br>9.<a href="https://gaussic.github.io/2017/08/16/tensorflow-tensorboard/" target="_blank" rel="noopener">https://gaussic.github.io/2017/08/16/tensorflow-tensorboard/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;tf-summary&quot;&gt;tf.summary&lt;/h2&gt;
&lt;h3 id=&quot;目的&quot;&gt;目的&lt;/h3&gt;
&lt;p&gt;该模块定义在tensorflow/_api/v1/summary/__init__.py文件中，主要用于可视化。&lt;br&gt;
每次运行完一个op之后，调用writer
      
    
    </summary>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/categories/tensorflow/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow math</title>
    <link href="http://mxxhcm.github.io/2019/05/08/tensorflow-math/"/>
    <id>http://mxxhcm.github.io/2019/05/08/tensorflow-math/</id>
    <published>2019-05-08T09:38:46.000Z</published>
    <updated>2019-05-10T11:37:24.676Z</updated>
    
    <content type="html"><![CDATA[<h2 id="tf-math">tf.math</h2><ul><li>tf.add(x, y, name=None) # 求和</li><li>tf.sub(x, y, name=None) # 减法</li><li>tf.mul(x, y, name=None) # 乘法</li><li>tf.div(x, y, name=None) # 除法</li><li>tf.mod(x, y, name=None) # 取模</li><li>tf.maximumd(x, y, name=None) # x &gt; y?x:y</li><li>tf.minimum(x, y, name=None) # x &lt; y?x:y</li><li>tf.abs(x, name=None) # 求绝对值</li><li>tf.neg(x, name=None) # 取负</li><li>tf.sign(x, name=None) # 返回符号</li><li>tf.inv(x, name=None) # 取反</li><li>tf.square(x, name=None) # 平方</li><li>tf.round(x, name=None) # 四舍五入</li><li>tf.sqrt(x, name=None) # 开根号</li><li>tf.pow(x, name=None) #</li><li>tf.exp(x, name=None) #</li><li>tf.log(x, name=None) #</li><li>tf.sin(x, name=None) #</li><li>tf.cos(x, name=None) #</li><li>tf.tan(x, name=None) #</li><li>tf.atan(x, name=None) #</li></ul><h2 id="代码示例">代码示例</h2><h3 id="tf-maximum">tf.maximum</h3><p>比较两个tensor，返回element-wise两个tensor的最大值。<br><a href="https://github.com/mxxhcm/myown_code/blob/master/tf/some_ops/tf_maximum.py" target="_blank" rel="noopener">代码地址示例</a>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line">a = tf.Variable([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">b = tf.Variable([<span class="number">2</span>, <span class="number">1</span>, <span class="number">4</span>])</span><br><span class="line"></span><br><span class="line">sess.run(tf.global_variables_initializer())</span><br><span class="line">print(<span class="string">"a: "</span>, sess.run(a))</span><br><span class="line">print(<span class="string">"b: "</span>, sess.run(b))</span><br><span class="line">c = tf.maximum(a, b)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"tf.maximum(a, b):\n  "</span>, sess.run(c))</span><br></pre></td></tr></table></figure><p>输出如下：</p><blockquote><p>a:  [1 2 3]<br>b:  [2 1 4]<br>tf.maximum(a, b):<br>[2 2 4]</p></blockquote><h2 id="参考文献">参考文献</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;tf-math&quot;&gt;tf.math&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;tf.add(x, y, name=None) # 求和&lt;/li&gt;
&lt;li&gt;tf.sub(x, y, name=None) # 减法&lt;/li&gt;
&lt;li&gt;tf.mul(x, y, name=None) #
      
    
    </summary>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/categories/tensorflow/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow multinomial</title>
    <link href="http://mxxhcm.github.io/2019/05/08/tensorflow-multinomial/"/>
    <id>http://mxxhcm.github.io/2019/05/08/tensorflow-multinomial/</id>
    <published>2019-05-08T09:37:45.000Z</published>
    <updated>2019-05-12T09:12:06.018Z</updated>
    
    <content type="html"><![CDATA[<h2 id="tf-multinomial-1-tf-random-categorical-2">tf.multinomial[1] (tf.random.categorical[2])</h2><p>多项分布，采样。</p><h3 id="更新">更新</h3><p>在tensorflow 13.1版本中，提示这个API在未来会被弃用，需要使用tf.random.categorical替代。</p><h3 id="api">API</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tf.multinomial(</span><br><span class="line">    logits, <span class="comment"># 指定样本概率的tf.Tensor</span></span><br><span class="line">    num_samples, <span class="comment"># 样本个数</span></span><br><span class="line">    seed=<span class="literal">None</span>, <span class="comment">#, 0-D</span></span><br><span class="line">    name=<span class="literal">None</span>,</span><br><span class="line">    output_dtype=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="代码示例">代码示例</h3><p><a href="https://github.com/mxxhcm/myown_code/blob/master/tf/some_ops/tf_multinominal.py" target="_blank" rel="noopener">代码地址</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># tf.multinomial(logits, num_samples, seed=None, name=None)</span></span><br><span class="line"><span class="comment"># logits 是一个二维张量，指定概率，num_samples是采样个数</span></span><br><span class="line">sess = tf.Session()</span><br><span class="line">sample = tf.multinomial([[<span class="number">5.0</span>, <span class="number">5.0</span>, <span class="number">5.0</span>], [<span class="number">5.0</span>, <span class="number">4</span>, <span class="number">3</span>]], <span class="number">10</span>) <span class="comment"># 注意logits必须是float</span></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">  print(sess.run(sample))</span><br></pre></td></tr></table></figure><p>输出结果如下:</p><blockquote><p>[[2 1 2 1 0 2 1 1 1 0]<br>[1 0 0 1 0 1 0 1 0 0]]<br>[[2 2 0 2 2 0 2 0 1 2]<br>[1 0 0 2 0 1 0 1 1 0]]<br>[[0 0 0 2 0 0 1 2 0 1]<br>[0 0 0 1 0 1 0 0 0 0]]<br>[[2 1 0 1 1 1 0 0 2 0]<br>[1 0 0 2 0 0 0 0 0 1]]<br>[[1 0 1 0 0 1 2 2 0 0]<br>[1 0 0 0 0 1 1 1 2 0]]</p></blockquote><h2 id="参考文献">参考文献</h2><p>1.<a href="https://www.tensorflow.org/api_docs/python/tf/random/multinomial" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/random/multinomial</a><br>2.<a href="https://www.tensorflow.org/api_docs/python/tf/random/categorical" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/random/categorical</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;tf-multinomial-1-tf-random-categorical-2&quot;&gt;tf.multinomial[1] (tf.random.categorical[2])&lt;/h2&gt;
&lt;p&gt;多项分布，采样。&lt;/p&gt;
&lt;h3 id=&quot;更新&quot;&gt;更新&lt;/h3&gt;
&lt;p&gt;在
      
    
    </summary>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/categories/tensorflow/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow app</title>
    <link href="http://mxxhcm.github.io/2019/05/08/tensorflow-app/"/>
    <id>http://mxxhcm.github.io/2019/05/08/tensorflow-app/</id>
    <published>2019-05-08T09:35:39.000Z</published>
    <updated>2019-05-08T14:09:23.063Z</updated>
    
    <content type="html"><![CDATA[<h2 id="tf-app-flags">tf.app.flags</h2><h3 id="代码示例">代码示例</h3><p><a href="https://github.com/mxxhcm/code/blob/master/tf/some_ops/tf_app.py" target="_blank" rel="noopener">代码地址</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">flags.py</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">flags = tf.app.flags</span><br><span class="line">flags.DEFINE_string(<span class="string">'model'</span>, <span class="string">'mxx'</span>, <span class="string">'Type of model'</span>)</span><br><span class="line">flags.DEFINE_boolean(<span class="string">'gpu'</span>,<span class="string">'True'</span>, <span class="string">'use gpu?'</span>)</span><br><span class="line">FLAGS = flags.FLAGS</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(_)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> k,v <span class="keyword">in</span> FLAGS.flag_values_dict().items():</span><br><span class="line">        print(k, v)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    tf.app.run(main)</span><br></pre></td></tr></table></figure><p>传递参数的方法有两种，一种是命令行~$:python <a href="http://flags.py" target="_blank" rel="noopener">flags.py</a> --model hhhh ，一种是pycharm中传递参数。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;tf-app-flags&quot;&gt;tf.app.flags&lt;/h2&gt;
&lt;h3 id=&quot;代码示例&quot;&gt;代码示例&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/mxxhcm/code/blob/master/tf/some_ops/tf_app.py
      
    
    </summary>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/categories/tensorflow/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow where</title>
    <link href="http://mxxhcm.github.io/2019/05/08/tensorflow-where/"/>
    <id>http://mxxhcm.github.io/2019/05/08/tensorflow-where/</id>
    <published>2019-05-08T09:34:47.000Z</published>
    <updated>2019-05-08T14:12:24.529Z</updated>
    
    <content type="html"><![CDATA[<h2 id="tf-where">tf.where</h2><h3 id="简单解释">简单解释</h3><p>tf.where(conditon) 返回条件为True的下标。<br>tf.where(condition, x=X, y=Y) 条件为True的对应位置值替换为1,为False替换成0。</p><h3 id="api">API</h3><p>定义在tensorflow/python/ops/array_ops.py中。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tf.where(</span><br><span class="line">    condition, <span class="comment"># 条件</span></span><br><span class="line">    x=<span class="literal">None</span>,  <span class="comment"># 操作数1</span></span><br><span class="line">    y=<span class="literal">None</span>,  <span class="comment"># 操作数2</span></span><br><span class="line">    name=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="tf-where-condition-代码示例">tf.where(condition)代码示例</h3><p><a href="https://github.com/mxxhcm/code/blob/master/tf/some_ops/tf_where.py" target="_blank" rel="noopener">代码地址</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">X = tf.placeholder(tf.int32, [<span class="literal">None</span>, <span class="number">7</span>])</span><br><span class="line"></span><br><span class="line">zeros = tf.zeros_like(X)</span><br><span class="line">index = tf.not_equal(X, zeros)</span><br><span class="line">loc = tf.where(index)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    inputs = np.array([[<span class="number">1</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">0</span>, <span class="number">8</span>, <span class="number">6</span>], [<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>]])</span><br><span class="line">    out = sess.run(loc, feed_dict=&#123;X: inputs&#125;)</span><br><span class="line">    print(np.array(out))</span><br><span class="line">    <span class="comment"># 输出12个坐标，表示这个数组中不为0元素的索引。</span></span><br></pre></td></tr></table></figure><h3 id="tf-where-condition-x-x-y-y-代码示例">tf.where(condition, x=X, y=Y)代码示例</h3><p><a href="https://github.com/mxxhcm/code/blob/master/tf/some_ops/tf_where.py" target="_blank" rel="noopener">代码地址</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">inputs = np.array([[<span class="number">1</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">0</span>, <span class="number">8</span>, <span class="number">6</span>], [<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>]])</span><br><span class="line">X = tf.placeholder(tf.int32, [<span class="literal">None</span>, <span class="number">7</span>])</span><br><span class="line">zeros = tf.zeros_like(X)</span><br><span class="line">ones = tf.ones_like(X)</span><br><span class="line">loc = tf.where(inputs, x=ones, y=zeros)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    out = sess.run(loc, feed_dict=&#123;X: inputs&#125;)</span><br><span class="line">    print(np.array(out))</span><br></pre></td></tr></table></figure><h2 id="参考文献">参考文献</h2><p>1.<a href="https://www.tensorflow.org/api_docs/python/tf/where" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/where</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;tf-where&quot;&gt;tf.where&lt;/h2&gt;
&lt;h3 id=&quot;简单解释&quot;&gt;简单解释&lt;/h3&gt;
&lt;p&gt;tf.where(conditon) 返回条件为True的下标。&lt;br&gt;
tf.where(condition, x=X, y=Y) 条件为True的对应位置值替
      
    
    </summary>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/categories/tensorflow/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>ubuntu 18.04 alt tab快捷键</title>
    <link href="http://mxxhcm.github.io/2019/05/07/linux-ubuntu-18-04-alt-tab%E5%BF%AB%E6%8D%B7%E9%94%AE/"/>
    <id>http://mxxhcm.github.io/2019/05/07/linux-ubuntu-18-04-alt-tab快捷键/</id>
    <published>2019-05-07T13:21:07.000Z</published>
    <updated>2019-06-12T03:11:14.516Z</updated>
    
    <content type="html"><![CDATA[<h2 id="问题描述">问题描述</h2><p>ubuntu 16.04中，可以使用alt+tab快捷键在相同的应用中进行切换，在18.04中alt+tab是在不同的应用中切换。事实上，可以使用alt+`在该应用内切换，但是我还是想用alt+tab。</p><h2 id="设置方法">设置方法</h2><p>打开setting &gt;&gt; Devices &gt;&gt; Keyboard<br>找到Switch windows，设置快捷键为alt+tab即可。</p><h2 id="参考文献">参考文献</h2><p>1.<a href="https://superuser.com/questions/394376/how-to-prevent-gnome-shells-alttab-from-grouping-windows-from-similar-apps" target="_blank" rel="noopener">https://superuser.com/questions/394376/how-to-prevent-gnome-shells-alttab-from-grouping-windows-from-similar-apps</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;问题描述&quot;&gt;问题描述&lt;/h2&gt;
&lt;p&gt;ubuntu 16.04中，可以使用alt+tab快捷键在相同的应用中进行切换，在18.04中alt+tab是在不同的应用中切换。事实上，可以使用alt+`在该应用内切换，但是我还是想用alt+tab。&lt;/p&gt;
&lt;h2 id=
      
    
    </summary>
    
      <category term="linux" scheme="http://mxxhcm.github.io/categories/linux/"/>
    
    
      <category term="linux" scheme="http://mxxhcm.github.io/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>linux file perission</title>
    <link href="http://mxxhcm.github.io/2019/05/07/linux-file-perission/"/>
    <id>http://mxxhcm.github.io/2019/05/07/linux-file-perission/</id>
    <published>2019-05-07T09:08:26.000Z</published>
    <updated>2019-05-08T08:26:22.877Z</updated>
    
    <content type="html"><![CDATA[<h2 id="创建文件和目录">创建文件和目录</h2><h3 id="创建文件">创建文件</h3><p>~$:touch file</p><h3 id="创建目录mkdir">创建目录mkdir</h3><p>~$:mkdir -p /dir/my_dir<br>~$:mkdir -p /dir/{test,test1,test2}</p><h3 id="递归创建">递归创建</h3><p>~$:sudo mkdir -p /test/test<br>选项-R 递归的将某目录下所有的文件以及目录全部修改<br>~$:sudo chown -R root:root test</p><h2 id="输出文件">输出文件</h2><h3 id="一次性输出">一次性输出</h3><ul><li>cat<br>-n --打印行号(包含空白行)<br>-b --打印行号(不含空白行)<br>-a --将一些看不见的字符用特殊符号打出来　空格用&quot;<sup>I</sup>&quot; 回车用&quot;$&quot;</li><li>tac(反向列出)  cat --&gt; tac</li><li>nl(添加行号打印)</li></ul><h3 id="分屏输出">分屏输出</h3><ul><li>more</li><li>less</li><li>head [-n number]</li><li>tail</li><li>od<br>非纯文本文件(二进制文件)<br>-t   c ASCII<br>a   默认字符<br>o   octal 八进制<br>d   decimal 十进制<br>f   浮点数<br>x   十六进制</li></ul><h3 id="输出文件类型">输出文件类型</h3><p>file 命令<br>查看文件类型,data或者ASCII或者binary<br>~$:file /usr/bin/passwd</p><h2 id="rwx权限">rwx权限</h2><h3 id="r-读权限查询文件名数据">r-读权限查询文件名数据</h3><h3 id="w-写权限">w-写权限</h3><ul><li>新建文件与目录</li><li>删除文件或者目录</li><li>重命名以及转移文件或者目录</li></ul><h3 id="x-可执行权限">x-可执行权限</h3><ul><li>进入某目录</li><li>切换到该目录（cd命令）<br>!!!能不能进入某一目录只与该目录的x权限有关，如果不拥有某目录的x权限，即使拥有r权限，那么也无法执行该目录下的任何命令<br>但是即使拥有了x权限，但是没有r权限，能进入该目录但是不能打开该目录，因为没有读取的权限。<br>cd - 回到上一次工作的目录</li></ul><h3 id="改变文件或者目录的权限">改变文件或者目录的权限</h3><p>~$:sudo chmod 777 test<br>~$:sudo chmod +x test<br>~$:sudo chmod u=rwx,g=r,o=r test<br>r. u–user  g–group  o–others  a–all</p><h3 id="改变文件或者目录的属主">改变文件或者目录的属主</h3><p>~$:sudo chown root:root test<br>~$:sudo chown root test</p><h3 id="改变属组">改变属组</h3><p>~$:sudo chgrp root test</p><h2 id="umask">umask</h2><p>用户创建文件时一般不应有执行的权限，所以创建文件的默认权限为666也即-rw-rw-rw-，但是目录需要有执行的权限，应为777,即-rwxrwxrwx，使用如下命令查看当前的umask：<br>~$:umask</p><blockquote><p>0002</p></blockquote><p>~$:umask -S</p><blockquote><p>u=rwx,g=rwx,o=rx</p></blockquote><p>第一个与特殊权限有关，后三个与一般权限有关，在创建文件或者目录时，会将um-ask所对应的权限拿掉，即新建文件时:<br>(-rw-rw-rw-)-(--------w-) = (-rw-rw-r–)所以创建文件的一般权限为-rw-rw-r–<br>同理可得创建目录时的权限应该为drwxrwxr-x即775。要修改umask的值，可直接在输入umask后接所要减去的权限<br>即<br>~$:umask 002<br>一般情况下root用户的umask为022，这是为了安全考虑，一般用户的umask是022,即保留了同用户组的写入权利。如果同一个用户组的不同用户无法修改另一个用户的文件，那么就可能是同组成员的创建文件时的默认权限不同，可以用umask修改。</p><h2 id="修改文件时间">修改文件时间</h2><h3 id="stat-filename">stat filename</h3><p>列出该文件的各种时间</p><h3 id="touch">touch</h3><p>-a 仅修改访问时间<br>-t 后面接欲修改的时间而不用当前时间，格式为[YYMMNNhhmm]<br>-d 接欲修改的日趋而不用当前日期，也可以用–date=“时间或者日期”<br>-c 仅修改文件的时间(文件状态改变的时间)<br>-m 仅修改mtime(文件内容被更改的时间)<br>-d和-t修改的是mtime和atime 但是不能修改　ctime<br>~$:touch -d “2 days ago” testtouch<br>~$:touch -t 150929 testtouch</p><p>ls -l 默认显示的是mtime,是内容修改的时间(modify)<br>touch --time=ctime　　ctime 显示的是状态被改变的时间,指的是文件属性和权限发生改变。<br>touch --time=atime    atime 访问时间显示的是最近文件被访问的时间(acess),cat and more可以,但是像ls和stat不会改变</p><p>ls -lc  # chagne state<br>ls -lu  # acess time访问时间<br>ls -l# modify time</p><h2 id="chattr与lsattr-设置文件的隐藏属性">chattr与lsattr 设置文件的隐藏属性。</h2><p>change attributes<br>chattr -i 设置文件不可以被删除(包括root用户)<br>-a 设置文件只能增加数据，而不能删除或者修改文件(如登陆文件)</p><p>chattr +i +a 可以增加文件的隐藏属性，其他属性不变<br>-i -a 可以除去文件的隐藏属性，其他属性不变<br>=i a  仅有=后面的属性</p><p>lsattr 查看文件的隐藏属性</p><h2 id="文件特殊权限">文件特殊权限</h2><p>在文件或者目录中除了rwx外，还会出现s,t,S,T权限</p><h3 id="suid">SUID</h3><p>当s出现在文件所有者的x权限上时，</p><p>如<br>~$:ls -l /usr/bin/passwd<br>-rwsr-xr-x 1 root root …<br>~$:ls -l /etc/shadow<br>-rw-r----- 1 root shadow</p><p>用户密码存在/etc/shadow内，当用户想要修改密码时，可以使用passwd进行修改<br>用户mxx对于/etc/shadow没有任何权限，但是对于/usr/bin/passwd拥有r-x权限，所以可以执行passwd命令，由于在passwd命令中有SUID权限，所以mxx在执行pass-wd命令时，会暂时获得passwd拥有者即root的权限，所以接下来可以用passwd修改/etc/shadow。</p><p>！！！此权限仅可用于二进制程序中，且仅在执行该程序的过程中有效，此外只对于文件有效，对于目录也是无效的</p><h3 id="sgid">SGID</h3><p>当s权限出现在用户组的x权限时，如<br>~$:ls -l /usr/bin/mlocate /var/lib/mlocate/mlocate.db</p><blockquote><p>-rwx–s--x 1 root mlocate</p></blockquote><p>同SUID类似，程序执行者会获得该程序用户组的支持，还可以用在目录上，若该用户在此目录下具有w权限，用户创建的新文件的用户组与此目录组的用户组相同<br>~$:su root<br>~$:mkdir test<br>~$:ls -l test</p><blockquote><p>drwxrwxr-x 2 root root …</p></blockquote><p>~$:chmod 2777 test<br>~$:ls -l test</p><blockquote><p>drwxrwsrwx 2 root root</p></blockquote><p>~$:su mxx<br>~$:cd test<br>~$:touch test</p><blockquote><p>-rw-rw-r-- 1 mxx root …</p></blockquote><p>！！！此权限对于目录以及文件都有效</p><h3 id="sbit">SBIT</h3><p>当t出现在others的x权限上时<br>~$:ls -l /tmp</p><blockquote><p>drwxrwxrwt 13 root root …</p></blockquote><p>用户对于某个目录具有wx的权限，即可以写入的权限，相当于说目录的属主给了用户属组或者其他人的身份，并拥有w的权限，那么也就是说这个用户具有删除属主<br>创建的文件或者目录的删除等权限。但是如果该目录拥有了SBIT的权限，那么该用户就只能删除自己所创建的文件，而不能删除属主所创建的文件。<br>！！！此权限只针对目录有效</p><h3 id="如何设置文件以及目录的特殊权限-suid-4-sgid-2-sbit-1">如何设置文件以及目录的特殊权限(SUID 4,SGID 2 ,SBIT 1)</h3><p>最前面的一位为文件的特殊权限<br>直接用chmod 4755 filename就可以了<br>还可以通过加法来实现，如SUID为u+s,SGID为g+s,SBIT为o+t<br>此外还有大写的S和T，代表空，如<br>~$:chmod 7666 test<br>~$:ls -l test</p><blockquote><p>-rwSrwSrwT 1 mxx mxx</p></blockquote><p>因为s和t都是替代x的，而当文件所有者以及其他用户用户组都没有x的时候，所以就不用说其他的操作了，所以也就为空了</p><h2 id="常见配置文件">常见配置文件</h2><ul><li>/bin:可以被单用户执行的命令。其下的命令可以被root用户和普通用户执行，如cat,cd,cp,date,chown,chmod,等等</li><li>/sbin/:开机过程所需要的，只能被root用户所执行，普通用户只能进行查询，包括与开机，还原系统所需要的命令</li><li>/usr/bin:绝大部分的用户可使用命令都在这里</li><li>/usr/sbin/:服务器所需要的某些软件程序</li><li>/usr/local/sbin:本机自行安装的软件产生的系统执行文件</li><li>/根目录</li><li>/etc  系统的配置文件</li><li>/lib  执行文件所需要的函数库与内核所需要的模块</li><li>/bin  重要执行文件</li><li>/sbin 重要的系统执行文件</li><li>/dev  所需要的设备文件</li></ul><p>这五个目录必须和根目录放在一块。</p><p>根目录最好小一些，将一些经常用到的文件目录(/home:/usr:/var:/tmp与根目录分到不同的分区。因为越大的分区，放入的数据也就越多，出错的几率也就越大，而如果根目录出现问题，系统就可能会出现问题。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;创建文件和目录&quot;&gt;创建文件和目录&lt;/h2&gt;
&lt;h3 id=&quot;创建文件&quot;&gt;创建文件&lt;/h3&gt;
&lt;p&gt;~$:touch file&lt;/p&gt;
&lt;h3 id=&quot;创建目录mkdir&quot;&gt;创建目录mkdir&lt;/h3&gt;
&lt;p&gt;~$:mkdir -p /dir/my_dir&lt;br&gt;

      
    
    </summary>
    
      <category term="linux" scheme="http://mxxhcm.github.io/categories/linux/"/>
    
    
      <category term="linux" scheme="http://mxxhcm.github.io/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>linux search file command</title>
    <link href="http://mxxhcm.github.io/2019/05/07/linux-search-command/"/>
    <id>http://mxxhcm.github.io/2019/05/07/linux-search-command/</id>
    <published>2019-05-07T09:03:36.000Z</published>
    <updated>2019-06-16T09:06:12.689Z</updated>
    
    <content type="html"><![CDATA[<h2 id="命令与文件的查询">命令与文件的查询</h2><h3 id="文件查询">文件查询</h3><ul><li>find</li><li>whereis</li><li>locate</li></ul><p>whereis和locate利用数据库查找，find查找硬盘。</p><h3 id="命令查询">命令查询</h3><ul><li>which</li></ul><h2 id="find">find</h2><p>find从硬盘中查找文件，还可以查找具有特殊要求的文件，如查找文件所有者，文件大小,SUID等等</p><h3 id="与时间有关的参数">与时间有关的参数</h3><p>~$:find /tmp mtime n/+n/-n</p><h3 id="与用户或者用户组有关的文件">与用户或者用户组有关的文件</h3><p>find / -uid n<br>-gid n<br>-user name<br>-group name<br>-nouser<br>-nogroup</p><h3 id="与文件权限或者名称有关的参数">与文件权限或者名称有关的参数</h3><p>find / -name filename<br>-size [±]SIZE<br>-type TYPE[-fbcdls]<br>-perm mode刚好等于mode<br>-perm -mode全部包含<br>-perm /mode任意一个</p><h3 id="find示例">find示例</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> 查找/home/maddpg目录下所有__pycache__目录和文件</span><br><span class="line">find /home/maddpg -name **__pycache__ </span><br><span class="line"><span class="meta">#</span> 查看根目录下所有权限为7000的文件</span><br><span class="line">find / -perm +7000 -exec ls -l &#123;&#125; \;</span><br><span class="line"><span class="meta">#</span> 查找当前目录下size在1k到5k之间的文件，+表示大于，-表示小于</span><br><span class="line">find . -size -5k -a -size +1k # 是会把当前目录也列出来的</span><br></pre></td></tr></table></figure><h2 id="whereis">whereis</h2><h3 id="参数介绍">参数介绍</h3><p>whereis [-bmsu]<br>-b 二进制文件<br>-m manualz路径下的文件(说明文件)<br>-s source源文件<br>-u 不在上述范围的其他特殊文件</p><h2 id="locate">locate</h2><p>locate  查找/var/lic/mlocate数据库内的数据，该数据库每天更新一次可手动更新，updatedb,因为他是每天更新一次，所以可能会找到已删除的文件或者是找不到新建立的文件。</p><h2 id="which">which</h2><h3 id="参数介绍-v2">参数介绍</h3><p>which -a command 列出所有的位置。<br>which command 列出第一个找到的位置</p><h3 id="示例">示例</h3><p>~$:which -a python</p><blockquote><p>/home/mxxmhh/anaconda3/bin/python<br>/usr/bin/python</p></blockquote><p>~$:which pip</p><blockquote><p>/home/mxxmhh/anaconda3/bin/pip</p></blockquote><h2 id="参考文献">参考文献</h2><p>1.《鸟哥的LINUX私房菜》<br>2.<a href="http://www.cnblogs.com/wanqieddy/archive/2011/06/09/2076785.html" target="_blank" rel="noopener">http://www.cnblogs.com/wanqieddy/archive/2011/06/09/2076785.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;命令与文件的查询&quot;&gt;命令与文件的查询&lt;/h2&gt;
&lt;h3 id=&quot;文件查询&quot;&gt;文件查询&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;find&lt;/li&gt;
&lt;li&gt;whereis&lt;/li&gt;
&lt;li&gt;locate&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;whereis和locate利用数据库查找，f
      
    
    </summary>
    
      <category term="linux" scheme="http://mxxhcm.github.io/categories/linux/"/>
    
    
      <category term="linux" scheme="http://mxxhcm.github.io/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>linux file system</title>
    <link href="http://mxxhcm.github.io/2019/05/07/linux-file-system/"/>
    <id>http://mxxhcm.github.io/2019/05/07/linux-file-system/</id>
    <published>2019-05-07T09:02:37.000Z</published>
    <updated>2019-06-16T08:16:52.986Z</updated>
    
    <content type="html"><![CDATA[<h2 id="碎片整理">碎片整理</h2><p>文件写入的block太碎了，文件的读写性能太差，所以可以通过碎片整理将一个文件的block回合在一起FAT文件系统经常需要碎片整理,但是Ext2文件类型是索引式文件系统，所以不太需要经常碎片整理的。</p><h2 id="dumpe2fs-bh">dumpe2fs [-bh]</h2><p>查询每个区段以及superblock的信息</p><h3 id="参数介绍">参数介绍</h3><p>dumpe2fs [-bh]<br>-b<br>-h 仅列出superblock的数据</p><h2 id="df">df</h2><p>查询挂载的设备</p><h3 id="参数介绍-v2">参数介绍</h3><p>df [-haiT]　[dir/file]显示文件系统的整体磁盘用量<br>-a 列出所有的文件<br>-h 显示文件系统的大写，自行显示格式<br>-i 可用的inode<br>-T 联通分区文件系统的名称</p><h2 id="du">du</h2><p>目录或者文件所占的容量</p><h3 id="参数介绍-v3">参数介绍</h3><p>du [-ashkm] [dir/filename] 默认显示的是目录的容量，不包含文件<br>-s 该目录下所有文件的容量，不细列出来<br>-a 显示所有的目录与文件的容量<br>-h 以人们熟悉的大小方式显示出来<br>-k 以kb列出容量<br>-m 以mb列出容量:</p><h2 id="ln-s-链接文件">ln [-s] 链接文件</h2><p>hard-link硬链接，将某个目录下的block多写入一个数据,磁盘的inode与block数量一般不会改变，磁盘容量也不会改变，而且删除一个文件并不会响另一个文件的读写，但是其对于目录是没有作用的，对于不同的文件系统也是没有用的。<br>sybomlic</p><h2 id="磁盘分区-格式化-检验与挂载">磁盘分区，格式化，检验与挂载</h2><p>df + 目录  查看某个目录挂载的磁盘位置<br>eg: df /</p><p>sudo fdisk [-l]　+ 设备　输出后面设备所有的分区内容　如不加设备名称，列　出整个系统。</p><h2 id="新增或者删除分区">新增或者删除分区</h2><p>sudo fdisk + 设备   对设备进行操作<br>partprobe</p><p>sudo mkfs [-t ext2/ext2/vfat] + 设备名　 将某个设备格式化为某种文件系统</p><p>sudo mke2fs [-b block_size] [-i inode_size]  [-L 卷标] [-cj -c 检查磁盘错误　-j 加入日志文件] + 设备名</p><p>sudo fsck [-CAay] [-t filesystem] + 设备<br>-C  使用直方图显示进度<br>-A  依据/etc/fstab内容，扫描设备<br>-a  自动修复检查到的有问题的扇区<br>-y  与-a 类似<br>ext2 ext3 支持额外参数　　[-fD] -f 强制进入设备进行检查<br>-D 对文件系统下的目录进行优化配置<br>sudo badblocks [-sv] + 设备  -s 在屏幕上列出进度 -v 在屏幕上看到进度</p><h2 id="mount">mount</h2><p>挂载文件系统与磁盘 P227</p><h3 id="参数介绍-v4">参数介绍</h3><p>mount [-aoltnL]<br>-a　按照/etc/fstab的配置信息将所有未挂载的磁盘挂载上来<br>-l　可增加label名称<br>-t　加上文件类型<br>-n　默认情况下会将挂载情况写入/etc/mtab，加入-n可以不写入<br>-L　可以利用卷标名来挂载<br>-o 加一些挂载时的额外参数　<br>ro(只读)　rw(可写)<br>async sync 此文件系统是否使用同步写入或者异步的内存机制<br>auto noauto 允许此分区以mount -a自动挂载(auto)<br>dev nodev 是否运行在此分区创建设备文件<br>suid nosuid<br>exec noexec 是否可拥有可执行binary文件<br>user,nouer 设置user参数可以让一般user能对此分区挂载<br>defaults　默认为rw,suid,dev,exec,auto,nouser,async<br>remount 重新挂载，在系统出错，或者重新更新参数时</p><h3 id="示例">示例</h3><p>mount 设备文件名　挂载点</p><p>用卷标名挂载设备<br>~$:mount -L mxx_logical /medic/mxx</p><p>用磁盘设备名挂载<br>~$:mount /dev/sdb1 /mnt/usb<br>~$:mount -t iso9660 /dev/cdrom /media/cdrom<br>~$:mount -o remount,rw,auto /dir<br>~$:mount -o loop ~/my.iso/myfile.iso /mnt/iso</p><h2 id="磁盘参数修改">磁盘参数修改</h2><p>设备用文件来代表通过文件的major与minor数值来替代<br>Major 主设备代码，Minor　次设备代码<br>/dev/hd*  major = 3<br>/dev/sd*  minor = 8</p><h3 id="mknod">mknod</h3><p>mknod [bcp]<br>b   设置设备名称成为一个外部存储文件，如硬盘<br>c   设置设备名称成为一个外部输入文件，如鼠标/键盘<br>p   设置设备名称成为一个FIFO文件</p><h3 id="e2label">e2label</h3><p>e2label /dev/sdb5 + 新的label名称</p><h3 id="tune2fs">tune2fs</h3><p>tune2fs [-jlL]<br>-l  类似 dupme2fs -h<br>-j  将ext2转换为ext3<br>-L  类似于　e2label</p><h3 id="hdparm">hdparm</h3><p>hdparm -Tt /dev/sd*  测试SATA硬盘的读取性能</p><h2 id="挂载-iso文件">挂载.iso文件</h2><p>mount -o loop /home/mxx/ubuntu16.04 /mnt/ubuntu16.04</p><p>dd命令　创建一个大文件<br>dd if=/dev/zero of=/home/mxx/filename bs=1M count=512<br>if–input file/dev/zero 一直输出0<br>of–output file将if中的内容加入到of接的文件名中<br>bs–block size<br>count共有多少个bs</p><h2 id="构建swap空间">构建swap空间</h2><p>例如将第二快硬盘的第五个分区改为swap分区<br>~$:sudo fdisk -l /dev/sdb<br>p<br>t 5<br>82<br>w<br>partprobe<br>将/dev/sdb5更改为swap类型的文件系统<br>~$:mkswap /dev/sdb5<br>~$:free 查看memory以及swap分区的使用情况<br>~$:swapon /dev/sdb5 使用/deb/sdb5的swap分区<br>~$:swapon -s 查看目前使用的swap设备有哪些<br>~$:swapoff /dev/sdb5</p><h2 id="boost-sector与superblock-的关系">boost sector与superblock 的关系</h2><ol><li>superblock的大小为1024b<br>boost sector与superblock 各占一个block ,可以查看/boot的挂载目录<br>0号block给boost ，1号block给superblock</li><li>superblock的大小大于1024b,如为4096b<br>superblock在0号blok ,但是superblock 只有1024b,所以为了防止空间浪费，于是在0号block内，superblock(1024-2047),boost sector(0-1023),2048后面的空间保留。<br>实际情况中，由于在比较大的block中，我们能将引导装载程序安装到superblock所在的0号block,但事实上还是安装到启动扇区的保留区域。<br>比较正确的说法是，安装到文件系统最前面的1024b内的区域，就是启动扇区</li></ol><h2 id="查看文件的inode编号">查看文件的inode编号</h2><p>~$:ls -i<br>目录并不一定只占一个block，当目录内的文件数太多时，会增加该目录的block</p><h2 id="参考文献">参考文献</h2><p>1.《鸟哥的LINUX私房菜》</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;碎片整理&quot;&gt;碎片整理&lt;/h2&gt;
&lt;p&gt;文件写入的block太碎了，文件的读写性能太差，所以可以通过碎片整理将一个文件的block回合在一起FAT文件系统经常需要碎片整理,但是Ext2文件类型是索引式文件系统，所以不太需要经常碎片整理的。&lt;/p&gt;
&lt;h2 id=&quot;d
      
    
    </summary>
    
      <category term="linux" scheme="http://mxxhcm.github.io/categories/linux/"/>
    
    
      <category term="linux" scheme="http://mxxhcm.github.io/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>linux 压缩和备份</title>
    <link href="http://mxxhcm.github.io/2019/05/07/linux-compress-uncompress/"/>
    <id>http://mxxhcm.github.io/2019/05/07/linux-compress-uncompress/</id>
    <published>2019-05-07T09:01:24.000Z</published>
    <updated>2019-06-16T07:46:34.565Z</updated>
    
    <content type="html"><![CDATA[<h2 id="压缩">压缩</h2><ul><li>gzip</li><li>zcat</li><li>bzip2</li><li>bzcat</li><li>gunzip</li><li>bunzip2</li><li>7z</li><li>zip</li><li>rar</li></ul><h3 id="gzip和bzip2">gzip和bzip2</h3><p>gzip和bzip2公用参数</p><h4 id="参数介绍">参数介绍</h4><p>gzip(bzip2)<br>-d 解压缩参数<br>-$ $取1-9　压缩等级 -1最快<br>-v 显示压缩比<br>-k 保留原文件<br>-z 压缩参数<br>-c 将压缩过程中产生的数据输出到屏幕上(压缩后的数据)可以将其输出重定向<br>-t 检验一个压缩文件的一致性</p><h3 id="zip">zip</h3><h4 id="参数介绍-v2">参数介绍</h4><p>zip [-dmbrfFg]<br>-d　从zip文件中移除一个文件<br>-m  将特定文件移入zip文件，且删除特定文件<br>-g　将文件压缩附加到zip文件中<br>-r  包括子目录<br>-f  以新文件取代旧文件<br>-F　修复已经损毁的压缩文件<br>-b　暂存文件的路径<br>-v　显示详细信息<br>-u　值更新改变过的文件<br>-T　测试zip文件是否正常<br>-x　不需要压缩的文件</p><h4 id="示例">示例</h4><p>~$:zip -r myfile.zip ./*<br>~$:zip -d myfile.zip myfile　 //删除压缩文件内的某个文件<br>~$:zip -g myfile.zip myfile   //向一个压缩文件内添加新文件<br>~$:zip -u myzip</p><h3 id="unzip">unzip</h3><h4 id="参数介绍-v3">参数介绍</h4><p>unzip [-dnovj]<br>-v  查看压缩文件目录，但是不解压<br>-d  指定解压到的目录<br>-n　不覆盖已有文件<br>-o　覆盖已有文件<br>-j  不重建文档的目录结构，把所有文件解压到同一目录下</p><h3 id="7z">7z</h3><h4 id="安装">安装</h4><p>~$:apt-get install p7zip</p><h4 id="参数介绍-v4">参数介绍</h4><p>7z [x|a] [-rotr]<br>a　代表添加文件到压缩包<br>x　代表解压缩文件<br>-r 表示递归所有文件<br>-t 制定压缩类型<br>-o 指定解压到的目录</p><h4 id="示例-v2">示例</h4><p>~$:7z a -t 7z -r myfile.7z  ~/myfile<br>~$:7z x myfile.7z -r -o ~/</p><h3 id="rar">rar</h3><h4 id="安装-v2">安装</h4><p>~$:apt-get install rar</p><h4 id="示例-v3">示例</h4><p>~$:rar x myfile.rar<br>~$:rar a myfile.rar myfile</p><h3 id="tar">tar</h3><p>打包</p><h4 id="参数介绍-v5">参数介绍</h4><p>tar [-cxtvfjzCpP]<br>-c  --create<br>-x  --extract<br>-t  --list<br>-v  --verbose<br>-f  --file<br>-j  --bzip2<br>-z  --gzip --gunzip<br>-C  --directory DIR<br>change to directory DIR<br>-p  --preserve-permissions, --same-permissions<br>-P  --absolute-names</p><pre><code>--exclude=file</code></pre><h4 id="示例-v4">示例</h4><p>~$:tar -cvj -f ~/my_bak/etc.newer.passwd.tar.bz2 --newer-mtime=“2016/09/23”/etc/*<br>~$:tar -cvj -f ~/my_bak/etc.tar.gz   /etc<br>~$:tar -xvj -f ~/my_bak/etc.tar.gz -C /tmp<br>~$:tar -xvj -f ~/my_bak/etc.tar.gz |etc/shadow<br>~$:tar -tfj -f ~/my_bak/etc.tar.gz | grep ‘shadow’<br>~$:tar -cv -f /dev/st0 /home /root /etc     # 磁带机/dev/st0</p><h2 id="备份">备份</h2><ul><li>dump</li><li>restore</li><li>mkisofs</li><li>dd</li></ul><h3 id="dump">dump</h3><h4 id="参数介绍-v6">参数介绍</h4><p>dump    [-SujvWf]<br>-Ssize<br>-uupdate    recode the dump time to /var/lib/dumpdates<br>-u只能对level 0 操作<br>-jadd compress bz2(默认压缩等级为 2)<br>-vverbose   详细的<br>-W列出/etc/fstab中的具有dump设置的分区是否被备份过<br>-f<br>-level备份的等级(0-9) 对于文件系统有九个等级<br>对于单个目录只有0级</p><h4 id="示例-v5">示例</h4><p>~$:dump -0u -f /root/etc.dump /etc</p><h3 id="restore">restore</h3><h4 id="参数介绍-v7">参数介绍</h4><p>restore [-tCir]<br>-tlist<br>-Ccompare<br>-iitera<br>-rr</p><h4 id="示例-v6">示例</h4><p>~$:restore -t -f /root/boot.dump<br>~$:restore -C -f /root/boot.dump<br>~$:mkdir test_restore<br>~$:cd test_restore<br>~$:restore -r -f /root/boot.dump<br>~$:restore -i -f /root/etc.dump.bz2</p><h3 id="mkisofs">mkisofs</h3><p>生成iso文件</p><h4 id="参数介绍-v8">参数介绍</h4><p>mkisofs [-orvVm]<br>-o +生成的镜像名<br>-r 记录更完整的文件信息，包括UID,GID与权限等<br>-v 显示构建iso的过程<br>-V 新建Volume<br>-m exclude 排除某文件<br>-graft-point</p><h4 id="示例-v7">示例</h4><p>~$:mkisofs -r -v -o ~/my.iso/system.iso -m /home/lost+found -graft-point/home=/home /root=/root /etc=/etc</p><h3 id="dd">dd</h3><p>可以备份整块硬盘或者整块磁盘包括superblocks以及boot sector等等</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;压缩&quot;&gt;压缩&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;gzip&lt;/li&gt;
&lt;li&gt;zcat&lt;/li&gt;
&lt;li&gt;bzip2&lt;/li&gt;
&lt;li&gt;bzcat&lt;/li&gt;
&lt;li&gt;gunzip&lt;/li&gt;
&lt;li&gt;bunzip2&lt;/li&gt;
&lt;li&gt;7z&lt;/li&gt;
&lt;li&gt;zip&lt;/li&gt;
      
    
    </summary>
    
      <category term="linux" scheme="http://mxxhcm.github.io/categories/linux/"/>
    
    
      <category term="linux" scheme="http://mxxhcm.github.io/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>linux vim</title>
    <link href="http://mxxhcm.github.io/2019/05/07/linux-vim/"/>
    <id>http://mxxhcm.github.io/2019/05/07/linux-vim/</id>
    <published>2019-05-07T08:58:22.000Z</published>
    <updated>2019-06-11T07:30:17.572Z</updated>
    
    <content type="html"><![CDATA[<h2 id="vim-配置文件">vim 配置文件</h2><p>个人vim配置文件一般在~/.vimrc下，可以自定义各种配置。<br><a href>我的vimrc文件</a></p><h3 id="前缀符号">前缀符号</h3><p>为了缓解快捷键冲突问题，就引入了前缀键，跟参考文献[0]一样，设置;号为前缀键。<br>let mapleader=&quot;;&quot;</p><h3 id="设置显示行号">设置显示行号</h3><p>set number</p><h3 id="底部显示文件路径">底部显示文件路径</h3><p>set laststatus=2 “设置底部状态栏可见<br>set statusline=%F%m%r%h%w\ %=#%n\ [%{&amp;fileformat}:%{(&amp;fenc==”&quot;?&amp;enc:&amp;fenc)    .((exists(&quot;+bomb&quot;)\ &amp;&amp;\ &amp;bomb)?&quot;+B&quot;:&quot;&quot;).&quot;&quot;}:%{strlen(&amp;ft)?&amp;ft:’**’    }]\ [%L\%l,%c]\ %p%%    &quot;statusline显示的信息，来自参考文献[8]。<br>&quot;其中%L是当前文件缓冲区的行数，%P是当前行占总行数的百分比。<br>set ruler &quot;显示光标当前位置</p><h2 id="vim复制到系统寄存器">vim复制到系统寄存器</h2><h3 id="vim寄存器">vim寄存器</h3><p>vim有9种寄存器:</p><ol><li>&quot;是未命名寄存器，vim的默认寄存器，存放删除和复制的文本。</li><li>small delete寄存器 -，存放不超过一行的delete操作（不包括x操作）产生的文本。</li><li>编号为$0,1,2,\cdot, 9$的寄存器，</li><li>$a-za-z$的$26$个字母寄存器,</li><li>只读寄存器 : .,%,$</li><li>表达式寄存器 =</li><li>搜索寄存器 /</li><li>GUI选择寄存器$*,+$。</li><li>黑洞寄存器，向这个寄存器写入的话，什么都不会发生。</li></ol><p>详细介绍可见参考文献[4]。</p><h3 id="使用系统剪切板">使用系统剪切板</h3><p>*和+寄存器适合系统相关的，*和系统缓冲区关联，+和系统剪切板关联。<br>使用+y复制当前行到系统剪切板。<br>使用+ny复制n行到系统剪切板。<br>使用+p粘贴系统剪切板到当前位置。<br>但是有些vim发行版不支持系统剪切板，可以使用如下命令查看自己的系统是否支持系统剪切板。<br>~\$:vim --version|grep clipboard<br>在我的系统上，输出如下：</p><blockquote><p>-clipboard         +jumplist          +persistent_undo   +virtualedit<br>-ebcdic            -mouseshape        +statusline        -xterm_clipboard</p></blockquote><p>如果输出+clipboard说明当前vim支持剪切板，-clipboard说明当前vim不支持系统剪切板，所以就卸载安装支持的版本呗。<br>~\$:sudo apt remove vim<br>~\$:sudo apt install vim-gtk3<br>然后再次查看<br>~\$:vim --version|grep clipboard<br>输出如下：</p><blockquote><p>+clipboard         +jumplist          +persistent_undo   +virtualedit<br>-ebcdic            +mouseshape        +statusline        +xterm_clipboard</p></blockquote><p>说明已经支持系统剪切板，可以使用了。注意记得把之前打开的vim关闭后再试。<br>使用以下命令进行操作：<br>+nyy # 复制从当前行开始的n行到+寄存器<br>+yy # 复制当前行行到+寄存器<br>+p # 粘贴+寄存器中的内容到文本中。<br>这个时候还有一个问题，就是一般的笔记本键盘的+和=号是在一起的，如果要打出=行，需要按一下shit +=，这个时候会向下移动一行，但是无伤大雅，为什么会这样，我还不知道。详细流程可参见参考文献[5]。<br>但是后来我发现这个还不能用。然后就只能继续查找了。在知乎上找到一个回答，发现还要在这些命令前加上一个&quot;号，表示将默认&quot;寄存器中的内容复制到+寄存器中。也就是使用如下命令：<br>&quot;+nyy # 复制从当前行开始的n行到+寄存器<br>&quot;+yy # 复制当前行行到+寄存器<br>&quot;+p # 粘贴+寄存器中的内容到文本中。</p><h3 id="将未命名寄存器和系统寄存器设为同一个">将未命名寄存器和系统寄存器设为同一个。</h3><p>修改vim配置文件<br>~$:vim ~/.vimrc<br>添加下面一句话，重新打开vim即可<br>set clipboard=unnamed</p><h2 id="vim模式和常用命令">vim模式和常用命令</h2><h3 id="vim模式">vim模式</h3><ul><li>正常模式，用vim打开一个文件之后就处于正常模式</li><li>插入模式，在正常模式下输入i,a,o或者I,A,O之后，就进入了命令模式，可以修改文件，按Esc退出。</li><li>Visual模式，可以移动光标选中某些行，进行复制或者删除，在正常模式按v或者V进入Visual模式。</li><li>命令模式，在正常模式按:进入命令模式，可以在窗口底部输入命令。</li><li>替换模式，使用r替换当前字符，使用R从当前字符开始连续替换。</li></ul><h3 id="正常模式">正常模式</h3><h4 id="移动光标">移动光标</h4><p>0 移动到行首<br>$ 移动到行尾<br>h 向左移动一个character<br>j 向下移动一行<br>k 向上移动一行<br>l 向右移动一个character<br>nj nk nh nl 移动n次<br>oO o在当前行的下一行插入，O在当前行的上一行插入<br>iI i在当前光标处插入，I在行首插入<br>aA a在当前光标后插入，A在行尾插入<br>1G 跳到第一行<br>gg 跳到首行<br>G 跳到尾行<br>nG 跳到尾行<br>n-space 跳到光标后第n个character<br>n-ENTER nG 跳到第n行<br>wW w移动到下一个word的开头，W移动到隔了一个空白符的下一个word的开头<br>bB b移动到前一个word的开头，B移动到隔了一个空白符的前一个word开头<br>eE 移动到当前word的结尾，W移动到隔了一个空白符的word结尾。<br>ctrl+f 跳到下一页<br>ctrl+b 跳到上一页</p><h4 id="删除和复制">删除和复制</h4><p>x 删除一个character<br>nx 删除n个characters<br>dd 删除当前行<br>ndd 删除n行<br>yy 复制一行<br>nyy 复制n行<br>p 粘贴</p><h3 id="命令模式">命令模式</h3><h4 id="切屏">切屏</h4><p>:sp [filename]<br>输入:进入命令模式，然后输入sp，空格，要打开的文件名。使用ctrl w在分开的屏幕之间进行切换。</p><h4 id="查找">查找</h4><p>/word ?word<br>n N</p><h4 id="替换和删除">替换和删除</h4><p>:1,10s/word/word.rp/g©<br>:1,$s/word/word.rp/g©<br>利用正则表达式可以实现下面的一些常用命令<br><a href="https://github.com/mxxhcm/code/tree/master/shell/vim_regex" target="_blank" rel="noopener">代码地址</a></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> 替换所有的^为\^</span><br><span class="line">:0,$s/\^/\\^/g</span><br><span class="line"><span class="meta">#</span> 替换所有的\*\*为##</span><br><span class="line">:0,$s/[0-9][0-9]\./## /g</span><br><span class="line"><span class="meta">#</span> 删除所有以tab开头的tab</span><br><span class="line">:0,$s/^\t//g</span><br><span class="line"><span class="meta">#</span> 删除所有以#开头的行</span><br><span class="line">:g/^#\.\*$/d</span><br><span class="line"><span class="meta">#</span> 删除所有空行</span><br><span class="line">:g/^\s\*$/d</span><br><span class="line"><span class="meta">#</span> 用newlines替换,</span><br><span class="line">:0,$s/,/\r/g</span><br><span class="line"><span class="meta">#</span> 在re.*后面加上括号</span><br><span class="line"><span class="meta">#</span> re.Ire.IGNORECASE)</span><br><span class="line"><span class="meta">#</span> re.Lre.LOCALE)</span><br><span class="line"><span class="meta">#</span> re.Mre.MULTILINE)</span><br><span class="line"><span class="meta">#</span> re.sre.DOTALL)</span><br><span class="line"><span class="meta">#</span> re.Ure.UNICODE)</span><br><span class="line"><span class="meta">#</span> re.Xre.VERBOSE)</span><br><span class="line">:m,ns/\(^re\.[A-Z]\)/\1(/g</span><br></pre></td></tr></table></figure><h4 id="其他">其他</h4><p>:w [filename]<br>:r [filename]<br>:n1,n2 w [filename]<br>:set nu</p><h3 id="visual模式">Visual模式</h3><p>见参考文献[9]。</p><h2 id="快捷键映射">快捷键映射</h2><ul><li>namp 正常模式下的递归映射</li><li>vmap Visual模式</li><li>imap 插入模式</li><li>cmap 命令模式</li><li>nnoremap 正常模式下的非递归映射</li><li>vnoremap Visual模式下的非递归映射</li><li>inoremap 插入模式下的非递归映射</li><li>cnoremap 命令模式下的非递归映射</li></ul><h2 id="其他vim使用事项">其他vim使用事项</h2><h3 id="编码">编码</h3><p>tty1-tty6默认不支持中文编码　　<br>修改终端接口语系　<br>LANG=zh_CN.big5</p><h3 id="dos和unix转换">dos和UNIX转换</h3><p>ubuntu  don’t have dos2UNIX or UNIX2dos  but is has tofrodos</p><p>frodos filename<br>todos filename<br>-b .bak<br>-v ver</p><h3 id="语系编码转换">语系编码转换</h3><p>iconv -o保留原文件，-o加新文件名<br>iconv -f big5 -t utf8 filename -o filename</p><h2 id="问题-vim中设置了setexpand不起作用">问题-vim中设置了setexpand不起作用</h2><p>~/.vimrc中进行了如下设置：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">set expandtab</span><br><span class="line">set tabstop=4</span><br><span class="line">set shiftwidth=4</span><br><span class="line">set softtabstop=4</span><br></pre></td></tr></table></figure><p>但是发现在markdown甚至~/.vimrc中expandtab都没有设置成功，但是py文件是正常的，后来发现是多加了一个set paste的原因，把它删了就好了。</p><h3 id="原因">原因</h3><p>因为set paste覆盖了set expandtab。</p><h3 id="解决方案">解决方案</h3><p>删除set paste行。</p><h2 id="我的vimrc文件">我的vimrc文件</h2><p>vimrc文件如下，<a href="https://github.com/mxxhcm/code/blob/master/shell/vimrc" target="_blank" rel="noopener">代码地址</a></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> 使用四个空格代替tab键</span><br><span class="line">set expandtab</span><br><span class="line">set tabstop=4</span><br><span class="line">set shiftwidth=4</span><br><span class="line">set softtabstop=4</span><br><span class="line"></span><br><span class="line">" 打开文件类型检测</span><br><span class="line">filetype on</span><br><span class="line">" 根据不同的文件类型加载插件</span><br><span class="line">filetype plugin on</span><br><span class="line">set ignorecase</span><br><span class="line"></span><br><span class="line">" 定义前缀键</span><br><span class="line">" let mappleader=";"</span><br><span class="line"></span><br><span class="line">" 设置ctrl+a全选，ctrl+c复制，;y粘贴</span><br><span class="line">vnoremap ; "+</span><br><span class="line">nnoremap ; "+</span><br><span class="line">nmap ;p "+p</span><br><span class="line">nnoremap &lt;C-C&gt; "+y</span><br><span class="line">vnoremap &lt;C-C&gt; "+y</span><br><span class="line">nnoremap &lt;C-A&gt; ggVG</span><br><span class="line">vnoremap &lt;C-A&gt; ggVG</span><br><span class="line"></span><br><span class="line">" 删除#号开头</span><br><span class="line">nnoremap ;d3 :g/^#.*$/d&lt;CR&gt;</span><br><span class="line">nnoremap ;d# :g/^#.*$/d&lt;CR&gt;</span><br><span class="line">" 删除空行</span><br><span class="line">nnoremap ;ds :g/^\s*$/d&lt;CR&gt;</span><br><span class="line">" 删除以tab开头的tab</span><br><span class="line">nnoremap ;rt :0,$s/^\t//g&lt;CR&gt;</span><br><span class="line">" 用\^代替^</span><br><span class="line">nnoremap ;r6 :0,$s/\^/\\^/g&lt;CR&gt;</span><br><span class="line">nnoremap ;r^ :0,$s/\^/\\^/g&lt;CR&gt;</span><br><span class="line">" 用\\\\代替\\</span><br><span class="line">nnoremap ;r/ :0,$s/\\\\/\\\\\\\\/g&lt;CR&gt;</span><br><span class="line">nnoremap ;r? :0,$s/\\\\/\\\\\\\\/g&lt;CR&gt;</span><br><span class="line"></span><br><span class="line">" 给选中行加注释</span><br><span class="line">" cnoremap &lt;C-#&gt; s/^/# /g&lt;CR&gt;</span><br><span class="line">nmap ;ic :s/^/# /g&lt;CR&gt;</span><br><span class="line">vmap ;ic :s/^/# /g&lt;CR&gt;</span><br><span class="line">nmap ;dc :s/^# //g&lt;CR&gt;</span><br><span class="line">vmap ;dc :s/^# //g&lt;CR&gt;</span><br><span class="line">"vmap &lt;C-#&gt; :s/^/#/g&lt;CR&gt;</span><br><span class="line">"nmap &lt;C-#&gt; :s/^/#/g&lt;CR&gt;</span><br><span class="line"></span><br><span class="line">""" 状态栏设置</span><br><span class="line">" 总是显示状态栏</span><br><span class="line">set laststatus=2</span><br><span class="line">" 状态信息</span><br><span class="line">set statusline=%f%m%r%h%w\ %=#%n\ [%&#123;&amp;fileformat&#125;:%&#123;(&amp;fenc==\"\"?&amp;enc:&amp;fenc).((exists(\"\+bomb\")\ &amp;&amp;\ &amp;bomb)?\"\+B\":\"\").\"\"&#125;:%&#123;strlen(&amp;ft)?&amp;ft:'**'&#125;]\ [%c,%l/%L]\ %p%%</span><br><span class="line"></span><br><span class="line">"""光标设置</span><br><span class="line">" 设置显示光标当前位置</span><br><span class="line">set ruler</span><br><span class="line"></span><br><span class="line">" 开启行号显示</span><br><span class="line">set number</span><br><span class="line">" 高亮显示当前行/列</span><br><span class="line">set cursorline</span><br><span class="line">" set cursorcolumn</span><br><span class="line">" 高亮显示搜索结果</span><br><span class="line">set hlsearch</span><br><span class="line">" 显示文件名</span><br><span class="line"></span><br><span class="line">" 开启语法高亮</span><br><span class="line">syntax enable</span><br><span class="line">" 允许用指定语法高亮配色方案替换默认方案</span><br><span class="line">syntax on</span><br><span class="line">" 将制表符扩展为空格</span><br><span class="line">" 设置编辑时制表符占用空格数</span><br><span class="line">" 设置格式化时制表符占用空格数</span><br><span class="line">" 让 vim 把连续数量的空格视为一个制表符</span><br><span class="line">set autoindent</span><br></pre></td></tr></table></figure><h2 id="参考文献">参考文献</h2><p>0.<a href="https://github.com/yangyangwithgnu/use_vim_as_ide" target="_blank" rel="noopener">https://github.com/yangyangwithgnu/use_vim_as_ide</a><br>1.<a href="https://askubuntu.com/a/1027647" target="_blank" rel="noopener">https://askubuntu.com/a/1027647</a><br>2.<a href="https://www.zhihu.com/question/19863631/answer/89354508" target="_blank" rel="noopener">https://www.zhihu.com/question/19863631/answer/89354508</a><br>3.鸟哥的LINUX私房菜<br>4.<a href="http://vimdoc.sourceforge.net/htmldoc/change.html#registers" target="_blank" rel="noopener">http://vimdoc.sourceforge.net/htmldoc/change.html#registers</a><br>5.<a href="https://stackoverflow.com/a/11489440" target="_blank" rel="noopener">https://stackoverflow.com/a/11489440</a><br>6.<a href="https://www.brianstorti.com/vim-registers/" target="_blank" rel="noopener">https://www.brianstorti.com/vim-registers/</a><br>7.<a href="http://landcareweb.com/questions/3593/ru-he-zai-vimzhong-yong-jiu-xian-shi-dang-qian-wen-jian-de-lu-jing" target="_blank" rel="noopener">http://landcareweb.com/questions/3593/ru-he-zai-vimzhong-yong-jiu-xian-shi-dang-qian-wen-jian-de-lu-jing</a><br>8.<a href="https://forum.ubuntu.org.cn/viewtopic.php?t=319408" target="_blank" rel="noopener">https://forum.ubuntu.org.cn/viewtopic.php?t=319408</a><br>9.<a href="https://stackoverflow.com/a/1676659/8939281" target="_blank" rel="noopener">https://stackoverflow.com/a/1676659/8939281</a><op selected lines><br>10.<a href="https://vi.stackexchange.com/questions/9028/what-is-the-command-for-select-all-in-vim-and-vsvim/9029" target="_blank" rel="noopener">https://vi.stackexchange.com/questions/9028/what-is-the-command-for-select-all-in-vim-and-vsvim/9029</a><ctrl a><br>11.<a href="https://stackoverflow.com/a/37962622/8939281" target="_blank" rel="noopener">https://stackoverflow.com/a/37962622/8939281</a><set paste><br>12.<a href="https://vim.fandom.com/wiki/Search_and_replace_in_a_visual_selection" target="_blank" rel="noopener">https://vim.fandom.com/wiki/Search_and_replace_in_a_visual_selection</a><br>13.<a href="https://stackoverflow.com/questions/71323/how-to-replace-a-character-by-a-newline-in-vim/71334" target="_blank" rel="noopener">https://stackoverflow.com/questions/71323/how-to-replace-a-character-by-a-newline-in-vim/71334</a>&lt;用newline替换逗号&gt;</set></ctrl></op></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;vim-配置文件&quot;&gt;vim 配置文件&lt;/h2&gt;
&lt;p&gt;个人vim配置文件一般在~/.vimrc下，可以自定义各种配置。&lt;br&gt;
&lt;a href&gt;我的vimrc文件&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;前缀符号&quot;&gt;前缀符号&lt;/h3&gt;
&lt;p&gt;为了缓解快捷键冲突问题，就引入
      
    
    </summary>
    
      <category term="linux" scheme="http://mxxhcm.github.io/categories/linux/"/>
    
    
      <category term="linux" scheme="http://mxxhcm.github.io/tags/linux/"/>
    
      <category term="vim" scheme="http://mxxhcm.github.io/tags/vim/"/>
    
  </entry>
  
  <entry>
    <title>linux git</title>
    <link href="http://mxxhcm.github.io/2019/05/07/git/"/>
    <id>http://mxxhcm.github.io/2019/05/07/git/</id>
    <published>2019-05-07T08:56:44.000Z</published>
    <updated>2019-07-18T12:27:04.557Z</updated>
    
    <content type="html"><![CDATA[<h2 id="linux安装github">linux安装github</h2><h3 id="安装">安装</h3><p>~$:sudo apt-get install git git-core git-gui</p><h2 id="添加ssh公钥到github">添加ssh公钥到github</h2><p>~$:cd ~/.ssh<br>查看是否有ssh keys，没有的话执行下一步<br>~$:ssh-keygen -t rsa -C &quot;github 邮箱&quot;<br>在github上登陆自己的账号，找到Settings-&gt;SSH Keys -&gt; ADD SSH Key 将id_rsa.pub文件中的字符串复制进去，不含空格和回车。</p><h2 id="配置本地文件">配置本地文件</h2><p>~$:git config  --gloabl <a href="http://user.name" target="_blank" rel="noopener">user.name</a> “github用户名”<br>~$:git config  --gloabl user.email “github邮箱”</p><h2 id="测试">测试</h2><p>在github上新建repository 名为 testgit<br>~$:cd mkdir /home/mxx/github/testgit<br>~$:cd github/testgit<br>~$:git init<br>~$:touch Readme<br>~$:git add Readme<br>~$:git commit -m ‘add readme file’</p><p>还可以直接克隆一个已有的仓库，<br>~$:git clone <a href="https://github.com/github%E7%94%A8%E6%88%B7%E5%90%8D/github%E4%BB%93%E5%BA%93.git" target="_blank" rel="noopener">https://github.com/github用户名/github仓库.git</a></p><h2 id="push使用ssh-不用输入密码">push使用ssh，不用输入密码</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">git remote -v  # 查看远程连接的方式</span><br><span class="line">git remote rm origin # 删除https的连接方式，如果是ssh的方式，就不需要了</span><br><span class="line"><span class="meta">#</span> 从github复制ssh地址</span><br><span class="line">git remote add origin git@github.com:mxxhcm/**.git # 添加ssh连接方式</span><br><span class="line">git push --set-upstream origin master</span><br><span class="line">git remote -v  # 再次查看远程连接的方式</span><br></pre></td></tr></table></figure><h2 id="分支管理">分支管理</h2><p>~$:git branch dev 创建分支<br>~$:git checkout dev 切换分支<br>~$:git branch 查看当前分支<br>~$:git merge dev 将分支合并<br>~$:git brancd -d dev 删除分支</p><h2 id="撤销">撤销</h2><h3 id="git-add加入缓存区后撤销">git add加入缓存区后撤销</h3><p>~$:git status # 查看add的文件<br>~$:git reset HEAD  # 撤销上一次add的内容<br>~$:git reset HEAD  xx.file # 撤销上一次add的某个文件</p><h3 id="git-commit后撤销">git commit后撤销</h3><p>~$:git log # 查看提交记录<br>找到相应commit的id，这个commit id不是刚才提交记录的id，而是他之前的那一个<br>commit xxxxxx<br>~$:git reset commit_id # 回退到git add之前，所有代码保留<br>~$:git reset --hard commit_id # 回退到上一次commit后，所有之后的代码都删掉</p><h3 id="git-reset-hard错误回退后撤销">git reset --hard错误回退后撤销</h3><p>使用git reflog<br>~$:git reflog # 找到所有的commit id，然后<br>~$:git reset commit_id<br>~$:git reset --hard commit_id</p><h3 id="push之后撤销">push之后撤销</h3><p>使用git revert<br>~$:git revert HEAD # 撤销前一次的commit<br>~$:git revert HEAD^ # 撤销前前一次的commit<br>~$:git rever commit-id # 撤销commit-id对应的版本</p><p>git revert会提交一个新的版本，将回退当做新的一个push，之前的内容都会保留。</p><h2 id="参考文献">参考文献</h2><p>1.<a href="https://blog.csdn.net/kongbaidepao/article/details/52253774" target="_blank" rel="noopener">https://blog.csdn.net/kongbaidepao/article/details/52253774</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;linux安装github&quot;&gt;linux安装github&lt;/h2&gt;
&lt;h3 id=&quot;安装&quot;&gt;安装&lt;/h3&gt;
&lt;p&gt;~$:sudo apt-get install git git-core git-gui&lt;/p&gt;
&lt;h2 id=&quot;添加ssh公钥到github&quot;&gt;添加
      
    
    </summary>
    
      <category term="git" scheme="http://mxxhcm.github.io/categories/git/"/>
    
    
      <category term="github" scheme="http://mxxhcm.github.io/tags/github/"/>
    
      <category term="linux" scheme="http://mxxhcm.github.io/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>linux bash</title>
    <link href="http://mxxhcm.github.io/2019/05/07/linux-bash/"/>
    <id>http://mxxhcm.github.io/2019/05/07/linux-bash/</id>
    <published>2019-05-07T08:55:17.000Z</published>
    <updated>2019-06-19T03:31:15.534Z</updated>
    
    <content type="html"><![CDATA[<p>jq 解析json字符串</p><p>&amp;&amp; || 命令从左到右依次执行　根据回传码$0的值，继续向右执行命令</p><h2 id="diff-文本比较-通常比较一个文件的不同版本">diff 文本比较，通常比较一个文件的不同版本</h2><p>diff [-bBi] file1 file2<br>-b　忽略一行中仅有多个空白的区别<br>-B  忽略空白行的区别<br>-i  忽略大小写<br>diff test.old test.new<br>diff -Naur test.olc test.new &gt; test.patch<br>patch 补丁<br>cat test.patch<br>patch -pN test.patch 更新旧版<br>patch -R -pN test.patch 恢复为旧版</p><h2 id="nl-打印出文件并加上行号">nl 打印出文件并加上行号</h2><h2 id="echo-与-unset">echo 与 unset</h2><p>~$:echo $PATH<br>~$:echo ${PATH}</p><p>&quot;&quot;内的特殊字符可以保持原有特性　var=“lang is $LANG” 那么 echo $var 输出var=en_US.UTF-8<br>’'内的特殊字符仅保存为一般文本</p><p>反单引号<code>可以获得其他命令的信息 version=</code>uname -r`  =$(uname -r)</p><h2 id="env以及export查看常见变量">env以及export查看常见变量</h2><p>/etc/profile<br>/etc/bash.bashrc</p><p>RANDOM产生0~32767的随机数<br>产生0-9的用declare -i number=$RANDOM*10/32767    echo $number<br>HOME<br>SHELL<br>HISTSIZE<br>MAIL<br>PATH/etc/environment<br>LANG<br>RANDOM</p><p>set查所有变量</p><p>HISTFILE=~/.bash_history<br>MAILCHECK<br>PS1提示符的设置<br>$关于本shell的PID<br>?上个变量的回传码，正确返回0，错误返回其他值，可以利用代码差错<br>OSTYPE HOSTTYPE MACHTYPE主机硬件与内核的等级</p><pre><code>export将自定义变量转换为环境变量locale -a 文件的语系read 赌球来自键盘输入的变量</code></pre><p>-p用户可以输入提示语<br>-t光标等待用户输入时间</p><p>~$:read -p “hello” -t 10 variable</p><pre><code>declare 声明变量的类型　　默认为字符串</code></pre><p>-x声明环境变量<br>-i将变量定义为整形<br>-a将变量定义为数组<br>-r将变量设置为readonly  若要删除该变量，必须退出该bash重进<br>-p单独列出变量的类型</p><pre><code>ulimit 与文件系统以及程序的限制关系</code></pre><p>-a 后面不接任何参数,可以列出所有的限制额度<br>-c 某些进程发生错误，系统可能会将该进程在内存中的信息写成文件，这种文件就称为内核文件(core file)。此为限制每个内核文件的最大容量<br>-f 此文件可以创建的最大文件容量,一般为2G:<br>-d 进程可使用的最大断裂内存(segment)容量<br>-l 可用于锁定(lock)的内存量<br>-t 可使用的最大CPU时间<br>-u 用户可使用的最大进程(process)数量</p><p>-H hard limit 严格的限制　　必须不能超过<br>-S soft limit 警告的限制　　可以超过，但要有警告信息</p><h2 id="变量的使用">变量的使用</h2><p>变量内容的测试与内容替换<br>echo ${variable#<em>}<br>echo ${variable##</em>}</p><pre><code>echo ${variable%*}echo ${variable%%*}echo ${variable/bin/BIN}echo ${variable//bin/BIN}变量的测试与替换new_var=${old_var-content}用新的变量的值区替代旧的变量的值，新旧变量可为同一个，若old_var不存在，则将</code></pre><p>content的值给new_var,而若old_var的值存在则将其赋给new_var;<br>　　加上:的话，即使old_var为空的话，也会用content的值去赋给new_var</p><pre><code>username=&quot;&quot;username=${username:-root}echo $username将-换成=是将原变量一同更改</code></pre><p>将-换成?是当变量不存在时，可以发出错误信息</p><h2 id="bash-shell的操作环境">Bash Shell的操作环境</h2><p>路径与命令查找顺序<br>先由相对路径或者绝对路径寻找<br>a.alias<br>b.builtin<br>c.$PATH这个变量的顺序找到的第一个命令<br>bash的登陆界面以及欢迎信息<br>/etc/issue  #<br>/etc/issue.net  #提供telnet远程登陆，当使用telnet连接到主机时显示该内容<br>/etc/motd(?)-&gt;/etc/update-motd.d/　<br>/etc/issue\d \l \m \n \o \r \t \s \v<br>\d 本地端时间的日期<br>\l 显示第几个终端机<br>\m 显示硬件等级<br>\n 显示主机的网络名称<br>\o 显示domain name<br>\r 操作系统的版本<br>\t 显示本地端时间的时间<br>\s 操作系统的名称<br>\v 操作系统的版本</p><pre><code>  bash的环境配置文件</code></pre><p>login shell 以及non-login shell<br>/etc/profile系统整体的设置<br>~/.profile用户个人设置</p><p>login shell<br>/etc/profile<br>/etc/inputrc/etc/profile.d/*sh<br>~/.profile<br>~/.bashrc/etc/bashrc<br>开始操作bash</p><p>non-login shell<br>取得non-login shell 时，该bash配置文件仅会读取~/.bashrc</p><p>source 配置文件名<br>如<br>source ~/.bashrc<br>. ~/.bashrc</p><p>/etc/manpath.config使用man时man page的路径到哪里去找<br>用tarball的方式安装的时候,那么man page可能放置在/usr/local/softpackage/\man里，需要以手动的方式将该路径加入到/etc/man.config里面</p><pre><code> 终端机的环境设置</code></pre><p>stty　　setting tty(终端机的意思)<br>-a 将所有的stty参数列出来</p><pre><code>如何设置呢  比如将erase设置为ctrl+h来控制stty erase ^hctrl + c 终止目前的命令</code></pre><p>ctrl + d 输入结束，例如邮件结束<br>　　ctrl + m ENTER<br>ctrl + u 在提示符下，将整行命令删除<br>　　ctrl + z 暂停目前的命令</p><p>set<br>　set $-　那个$-变量内容是set的所有设置<br>uvxhHmBC</p><p>/etc/inputrc其他的按键设置功能</p><pre><code> 通配符与特殊符号</code></pre><p>通配符* ? [] - ^<br>特殊字符　# \ | ; ~ $ &amp; ! / &gt;,&gt;&gt; &lt;,&lt;&lt; ‘’ “” ``或者$() () {}</p><h2 id="seq-产生一系列数">seq 产生一系列数</h2><p>seq [-s]</p><p>~$:seq -s &quot; &quot; 3 10<br>3 4 5 6 7 8 9 10</p><h2 id="sh-vxn-my-sh">sh [-vxn] <a href="http://my.sh" target="_blank" rel="noopener">my.sh</a></h2><p>sh -x执行过程<br>sh -n查询语法问题</p><h2 id="id和finger">id和finger</h2><p>id 用来显示某个用户的id信息<br>finger 用来分析某个用户信息</p><h2 id="none"></h2><p>type 查看命令的来自于哪里　　<br>是bash还是外部命令还是别名<br>file外部命令<br>alias别名<br>builtin内置在bash内<br>-t -p -a</p><p>type -t ls<br>~$:alias 以file builtin alias 列出该命令的类型<br>type -a ls 列出所有的名为ls的命令</p><p>学习shell script<br>看一下自己写的/home/mxx/scripts/delete_dir</p><h2 id="echo-num1-operand-num2">echo $(($num1 operand $num2))</h2><p>进行运算</p><h2 id="source-file-sh-sh-file-sh-file-sh">source <a href="http://file.sh" target="_blank" rel="noopener">file.sh</a>   sh <a href="http://file.sh" target="_blank" rel="noopener">file.sh</a>   ./file.sh</h2><p>source 是将该shell拿到父进程中来执行，所以各项操作都会在该bash内执行<br>sh和./是开启一个新的shell来执行</p><h2 id="test">test</h2><p>test [-rwxfd]<br>[-nt -ot -ef ]<br>[-eq -nq -gt -lt -ge -le]<br>[-z ]<br>[-a -o]</p><p>test -r filename<br>test “$filename” == “content”</p><p>[ “$filename” == “$varible” ]</p><h2 id="none-v2">$# $@ $*</h2><p>$#:变量个数<br>$@:变量内容<br>$*:</p><h2 id="别名">别名</h2><p>alias<br>alias lm=‘ls -al’</p><p>unalias</p><h2 id="history命令与文件">history命令与文件</h2><p>history (n)列出最近的第(n)条命令</p><p>!number执行history的第number条命令<br>!command　由最近的命令开始搜寻开头为command的命令<br>!!执行上一个命令</p><p>last最近登录的用户</p><h2 id="参考文献">参考文献</h2><ol><li>《鸟哥的LINUX私房菜》<br>2.<a href="https://www.tomczhen.com/2017/10/15/parsing-json-with-shell-script/" target="_blank" rel="noopener">https://www.tomczhen.com/2017/10/15/parsing-json-with-shell-script/</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;jq 解析json字符串&lt;/p&gt;
&lt;p&gt;&amp;amp;&amp;amp; || 命令从左到右依次执行　根据回传码$0的值，继续向右执行命令&lt;/p&gt;
&lt;h2 id=&quot;diff-文本比较-通常比较一个文件的不同版本&quot;&gt;diff 文本比较，通常比较一个文件的不同版本&lt;/h2&gt;
&lt;p&gt;diff
      
    
    </summary>
    
      <category term="linux" scheme="http://mxxhcm.github.io/categories/linux/"/>
    
    
      <category term="linux" scheme="http://mxxhcm.github.io/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>linux account</title>
    <link href="http://mxxhcm.github.io/2019/05/07/linux-account/"/>
    <id>http://mxxhcm.github.io/2019/05/07/linux-account/</id>
    <published>2019-05-07T08:54:11.000Z</published>
    <updated>2019-06-16T08:20:24.095Z</updated>
    
    <content type="html"><![CDATA[<h2 id="账户配置文件">账户配置文件</h2><ul><li>/etc/passwd</li><li>/etc/shadow</li><li>/etc/group</li><li>/etc/gshadow</li></ul><h3 id="etc-passwd">/etc/passwd</h3><p>mxx❌1000:1000:mxx,:/home/mxx:/bin/bash<br>账号名称,密码，UID,GID,用户信息说明列，主文件夹，shell</p><h3 id="etc-shadow">/etc/shadow</h3><p>mxx:…:17034:0:99999:7:::<br>账号名称，密码，最近密码更改日期，密码更改多久后才能重新更改，密码多长时间需要重新修改，密码需要修改前多少天发送警告，密码过期后宽限时间，账号失效日期，(形式和密码更改日期一样)，保留字段</p><h3 id="etc-group">/etc/group</h3><p>用户组名称，用户组密码,GID,此用户组支持的账号名称</p><h3 id="etc-gshawod">/etc/gshawod</h3><p>用户组名，<br>密码列，开头为!表示无合法密码<br>用户组管理员的账号<br>该用户组的所属账号</p><table><thead><tr><th>UID</th><th>用户</th></tr></thead><tbody><tr><td>0</td><td>系统管理员</td></tr><tr><td>1-99</td><td>系统账户</td></tr><tr><td>100-499</td><td>用户创建的系统账号</td></tr><tr><td>500-65535</td><td>一般用户</td></tr></tbody></table><h2 id="修改密码">修改密码</h2><p>一般账户:passwd<br>root账户:重启后进入单用户维护模式<br>忘记密码后，以各种方式清空/etc/shadow中root的密码字段。登陆后再用passwd修改密码</p><h2 id="uesr管理">uesr管理</h2><p>usermod -G group user将一个用户加入其他用户组<br>初始用户组用户的/etc/passwd的第四个字段即为该用户的初始用户组的GID<br>groups查看当前登陆用户的用户组。第一个为有效用户组<br>newgrp更改用户的有效用户组，但是用户组必须当前用户支持的用户组<br>UID/GID密码参数的设置在 /etc/login.defs</p><h3 id="useradd添加用户">useradd添加用户</h3><h4 id="参数介绍">参数介绍</h4><p>useradd [-ugGmMcdrsef]　　调用/etc/default/useradd的数据<br>-u UID          /etc/skel用户主文件加参考基准目录<br>-g initial group<br>-G 这个账户可以加入的其他用户组<br>-m 创建用户主文件　<br>-M 不创建用户主文件<br>-s 接一个默认shell<br>-r 创建一个系统账户<br>-c /etc/passwd的第五列说明<br>-d 制定某个目录成为主文件夹<br>-e 后面跟一个日期YYYYMMDD写入shadow的第八字段，账号的失效日期从1970年来总日数，若账号失效，无论密码是否正确，都无法登陆<br>-f 后面接shadow的第七字段,判定密码是否会失效,0为立即失效,-1为永不失效(密码只会过期强制登陆时重新设置),大于0的表示如n，如果在n天后，没有登陆修改密码，那么在n天后密码会失效，再也无法登陆，但是在如果在n天内登陆并修改密码，就可以继续使用。<br>-D useradd的默认值</p><h2 id="例子">例子</h2><p>~$:useradd -d /home/mxxhcm -k /etc/skel/ -m mxxhcm -s /bin/bash</p><h3 id="passwd修改密码">passwd修改密码</h3><h4 id="参数介绍-v2">参数介绍</h4><p>passwd [账号]　[–stdin] -[luSnxwi]<br>-l lock<br>-u unlock<br>-S 密码相关参数<br>-n next　多长时间不能修改第四个字段<br>-x 多少天必须修改　第五个字段<br>-w warn第六个字段<br>-i 失效日期　第七个字段</p><h4 id="示例">示例</h4><p>~#:passwd　后面没有接密码，就是修改当前用户的密码<br>~#:echo “passwd” | passwd --stdin user</p><h3 id="change修改user信息">change修改user信息</h3><h4 id="参数介绍-v3">参数介绍</h4><p>chage [-ldEImMW] 账号名<br>-l　列出详细参数<br>-d　第三字段<br>-E　第八字段　账号失效　<br>-I　第七字段　密码失效<br>-m　第四字段<br>-M　第五字段<br>-W　第六字段</p><h4 id="示例-v2">示例</h4><p>chage -d 0 user</p><h3 id="user信息修改">user信息修改</h3><h4 id="参数介绍-v4">参数介绍</h4><p>usermod [-;cdefgGasuLU]<br>-l 修改账户名称<br>-L lock<br>-U unlock<br>修改/etc/shadow<br>-f 第七字段<br>-e 第六字段<br>-c /etc/passwd 第五字段<br>-d /etc/passwd 主文件夹第六字段<br>-g /etc/passwd 第四个字段GID<br>-G 后面接次要用户组，修改这个用户能支持的用户组，修改/etc/group<br>-a 与-G连用，增加次要用户组的支持而非设置<br>-u UID /etc/passwd的第三个字段,UID<br>-s 接shell的实际文件</p><h4 id="示例-v3">示例</h4><p>~#:usermod -l ‘my_usename’ username</p><h3 id="user删除">user删除</h3><h4 id="参数介绍-v5">参数介绍</h4><p>userdel</p><h3 id="示例-v4">示例</h3><p>~#:userdel -r username # 删除主文件夹<br>~#:find / -user username<br>~#:userdel username</p><h2 id="finger查看用户的数据">finger查看用户的数据</h2><p>finger 查看当前用户的数据<br>finger username 查看某用户的信息</p><h2 id="chfn">chfn</h2><p>chfn 就是相当于-c参数,修改当前用户/etc/passwd的第五个字段值<br>chsh -s　修改当前用户的shell<br>chsh -s /bin/bash</p><h2 id="id">id</h2><p>id [username]<br>列出当前用户或者username的所有id</p><h2 id="group操作">group操作</h2><p>groupadd [-gr]<br>-g　指定GID<br>-r　新建系统用户组<br>groupmod [-gn]<br>-n　修改组名<br>groupdel [groupname]</p><p>尽量少修改GID否则会造成系统资源的混乱<br>当用户组为某个用户的初始用户组时，就无法删除该用户组</p><h3 id="gpasswd修改group信息">gpasswd修改group信息</h3><p>gpasswd　[-AMrR] groupname<br>-A 将groupname的控制权交给后面用户<br>gpasswd -A mxx groupname<br>-M 将某些账号加入到这个用户组中<br>-r 将groupname的密码删除<br>-R 将groupname的密码失效</p><p>gpasswd groupname 设置groupname管理密码<br>gpasswd groupname<br>-A 增加groupname的管理员<br>-r让密码删除</p><p>gpasswd -ad username groupname<br>-a增加<br>-d删除</p><h2 id="acl-acess-control-list">ACL  Acess Control List</h2><p>针对单一用户或者目录来进行rwx的权限设置<br>setfacl　[-m|-x]    -m设置acl参数   -x删除后续acl参数<br>[-bkRd]<br>-b删除所有的acl参数;-k删除默认的acl参数;-R递归设置acl;-d设置默认的acl，只对目录有效<br>setfacl [-m|-x] [bkRd]<br>-b　删除所有ACL参数<br>-k　删除默认ACL参数<br>-R　递归设置ACL参数，包括子目录<br>-d  设置默认ACL参数，只对目录有效</p><pre><code>-x　删除后续的ACL参数-m  设置后续的ACL参数-m u:mxx:rw my_file</code></pre><p>getfacl my_file</p><p>针对有效权限mask的设置<br>setfacl -m m:rwx my_file<br>mask在此可以来规定最大允许的权限。取得是mask和用户以及用户组的权限交集。        若用户mxx的权限为rwx 但是mask为r–，那么mxx的权限只能为r–.</p><p>~#:setfacl -m d:u:mxx:rwx file　递归设置目录的acl<br>~#:setfacl -m m:rw acl_test<br>~#:setfacl -m g:mxx:rwx acl_test<br>~#:getfacl acl_test<br>~#:setfacl -b file 删除acl</p><h2 id="切换用户-切换账号">切换用户，切换账号</h2><p>su[- -l -m -c]<br>su - 切换到root用户以login shell变量的读取方式<br>su 切换到root用户，以nologin shell变量的读取方式登陆系统<br>｀｀    su -l 加想要切换的账号login shell<br>su -c 只提升一次到root权限<br>su -m 使用目前用户的环境变量，不读取新用户的配置文件</p><p>su - -c cat /etc/shadow</p><p>sudo -u mxx …提升到mxx权限</p><h2 id="visudo的设置">visudo的设置</h2><p>1.visudo 修改/etc/sudoers<br>其他用户使用root身份<br>root ALL=(ALL) ALL<br>用户账号<br>登陆者的来源主机名，<br>可切换的身份<br>可执行的命令<br>2.最左边加一个%表示用户组<br>利用用户组以及免密码<br>%wheel ALL=(ALL) ALL<br>usermod -a -G wheel user</p><p>免密<br>%wheel ALL=(ALL) NOPASSWD: ALL<br>3.mxx ALL=(root) /usr/bin/passwd<br>mxx可以切换到root的身份使用passwd命令</p><p>mxx ALL=(root) !/usr/bin/passwd, /usr/bin/passwd [A-Za-z]*,<br>!/usr/bin/passwd root<br>4.别名设置<br>User_Alias MYUSER=mxx,mahuihui<br>Cmnd_Alias MYCOMMAND=!/usr/bin/passwd, /usr/bin/passwd [A-Za-z]*,<br>!/usr/bin/passwd root<br>MYUSER all=(root) MYCOMMAND</p><p>5.用自己的密码切换成root</p><p>sudo su -</p><h2 id="用户信息传递">用户信息传递</h2><p>查询用户</p><ul><li>w</li><li>who</li><li>last</li><li>lastlog</li></ul><p>用户对谈<br>mesg y<br>mesg n</p><p>write<br>write mxx tty1</p><p>wall “hello”　每个人都会收到</p><p>mail mahuihui -s “Hi,mahuihui,nihaoa”<br>…<br>…<br>ctrl+d</p><p>mail 收信<br>?查看命令</p><p>q离开,离开后，会将该信件移动到~/home/mbox，收信箱<br>读取<br>mail -f /home/mxx/mbox</p><h2 id="手工添加账号">手工添加账号</h2><ul><li><p>pwck<br>pwconv将/etc/passwd相关信息移动到/etc/shadow,把在/etc/passwd中存在的账号            但是在/etc/shadow没有对应密码的列新增密码</p></li><li><p>grpconv</p></li><li><p>pwunconv</p></li><li><p>chpasswd    修改密码<br>echo “mxx:mypaswd” | chpasswd -m</p></li></ul><h3 id="新建账号">新建账号</h3><p>vim /etc/group<br>mygroup❌1020:</p><p>grpconv</p><p>vim /etc/passwd<br>myuser❌1200:1020::/home/myuser:/bin/bash<br>pwconv</p><p>passwd myuser</p><p>cp -a /etc/skel /home/myuser<br>chmod -R myuser:mygroup /home/myuser<br>chmod 700 /home/myuser</p><h2 id="参考文献">参考文献</h2><p>1.《鸟哥的LINUX私房菜》</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;账户配置文件&quot;&gt;账户配置文件&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;/etc/passwd&lt;/li&gt;
&lt;li&gt;/etc/shadow&lt;/li&gt;
&lt;li&gt;/etc/group&lt;/li&gt;
&lt;li&gt;/etc/gshadow&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;etc-passw
      
    
    </summary>
    
      <category term="linux" scheme="http://mxxhcm.github.io/categories/linux/"/>
    
    
      <category term="linux" scheme="http://mxxhcm.github.io/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>linux quota</title>
    <link href="http://mxxhcm.github.io/2019/05/07/linux-quota/"/>
    <id>http://mxxhcm.github.io/2019/05/07/linux-quota/</id>
    <published>2019-05-07T08:53:04.000Z</published>
    <updated>2019-06-16T08:22:05.854Z</updated>
    
    <content type="html"><![CDATA[<h2 id="quota">quota</h2><p>显示磁盘使用情况和限额<br>~$:sudo apt-get install quota</p><h2 id="quota示例">quota示例</h2><p>文件系统开启quota<br>~#:df -h /home<br>~#:mount -o remount,usrquota,grpquota /home<br>~#:mount | grep ‘home’</p><p>查看/etc/mtab文件<br>cat /etc/mtab</p><p>或者直接写入/etc/fstab<br>~#:vim /etc/fstab<br>添加：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">LABEL=/home /home ext4 defaults,usrquota,groquota</span><br></pre></td></tr></table></figure><p>~#:umount /home<br>~#:mount -a<br>~#:mount | grep ‘home’</p><h2 id="新建quota配置文件">新建quota配置文件</h2><p>quotacheck [-avugmf]<br>-c 创建磁盘配额数据文件<br>-a 创建在/etc/mtab所有磁盘的配额数据库文件，使用此参数，后无需加挂载点<br>-u 创建用户的磁盘配额数据库文件<br>-g 用户组的<br>-m 把一起的磁盘配额信息清除，对/分区创建时，必须用此参数<br>-v 显示创建的过程</p><h2 id="启动quota">启动quota</h2><p>quotaon [-avug]　[挂载点]<br>-a 根据/etc/fstab的设置来启动有关的quota<br>-v</p><h2 id="关闭quota">关闭quota</h2><p>quotaoff [-aug] [mount-point]</p><h2 id="编辑quota">编辑quota</h2><p>edquota [-ugtp]<br>-t 修改宽限时间<br>-p 复制范本，，模板账号为已存在并设置好quota的账号<br>edquota -p 范本账号 -u 新账号<br>edquota -u myquota<br>edquota -g myquotagrp<br>edquota -t<br>edquota -p quotagrp -u myquota</p><h2 id="quota报表">quota报表</h2><p>单一用户<br>quota [-ugvs]<br>-s 以1024的整数倍显示</p><p>repquota [-a] [-uvgs]<br>/dev/sda[012]<br>warnquotaroot给用户以及root发邮件　P461<br>在 /etc/warnquota.conf 中设置邮件内容<br>setquota [-u|-g] name block(soft) block(hard) inode(soft) inode(hard) 文件系统<br>setquota -u myquota5 2000 3000 0 0 /home<br>quota-uv myquota5</p><h2 id="参考文献">参考文献</h2><p>1.《鸟哥的LINUX私房菜》</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;quota&quot;&gt;quota&lt;/h2&gt;
&lt;p&gt;显示磁盘使用情况和限额&lt;br&gt;
~$:sudo apt-get install quota&lt;/p&gt;
&lt;h2 id=&quot;quota示例&quot;&gt;quota示例&lt;/h2&gt;
&lt;p&gt;文件系统开启quota&lt;br&gt;
~#:df -h /ho
      
    
    </summary>
    
      <category term="linux" scheme="http://mxxhcm.github.io/categories/linux/"/>
    
    
      <category term="linux" scheme="http://mxxhcm.github.io/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>linux raid lvm</title>
    <link href="http://mxxhcm.github.io/2019/05/07/linux-raid-lvm/"/>
    <id>http://mxxhcm.github.io/2019/05/07/linux-raid-lvm/</id>
    <published>2019-05-07T08:52:12.000Z</published>
    <updated>2019-06-16T08:26:01.500Z</updated>
    
    <content type="html"><![CDATA[<h2 id="磁盘阵列">磁盘阵列</h2><h3 id="mdadm新建raid">mdadm新建raid</h3><h4 id="参数介绍">参数介绍</h4><p>mdadm --detail  后面接的那个磁盘阵列设备的具体信息<br>madam --create 为新建raid的参数<br>–auto=yes /dev/md[01…]<br>–raid-device=N 使用几个raid作为磁盘阵列的设备<br>–spare-device=N　使用几个磁盘作为备用<br>–level=[0125]  这组磁盘阵列的等级是0,1还是5之类的</p><h4 id="示例">示例</h4><p>~#:mdadm --create --auto=yes /dev/md0 --level=5 --raid-device=3 --spare-device=1 /dev/sdb{5,6,7,8}<br>创建raid需要时间，创建好之后<br>~#:mdadm --detail /dev/md0<br>查看建好的RAID<br>~#:cat /proc/mdstat<br>~#:mkfs -t ext4 /dev/md0<br>~#:mkdir /mnt/raid<br>~#:mount /dev/md0 /mnt/raid<br>~#:df</p><h3 id="mdadm管理raid">mdadm管理raid</h3><h4 id="参数介绍-v2">参数介绍</h4><p>mdadm --manage /dev/md[0-9] [–add 设备] [–remove 设备] [–fail 设备]<br>~#:mdadm --manage /dev/md0 --fail /dev/sdb6<br>~#:mdadm --detail /dev/md0<br>~#:cat /proc/mdstat<br>过一段时间在执行会发现以及将坏的设备更新了<br>~#:mdadm --detail /dev/md0<br>~#:mdadm --manage /dev/md0 --add /dev/sdb9 --remove /dev/sdb10</p><h3 id="开机自动加载raid">开机自动加载raid</h3><p>~#:mdadm --detail /dev/md0 | grep -i ‘uuid’<br>~#:vim /etc/mdadm/mdadm.conf<br>ARRAY /dev/md0 UUID=…<br>~#:vim /etc/fstab<br>/dev/md0 /mnt/raid ext4 defaults 1 2<br>~#:umount /dev/md0<br>~#:mount -a<br>~#:df</p><h3 id="关闭raid">关闭raid</h3><p>~#:vim /etc/fstab<br># /dev/md0 …<br>~#:mdadm --stop /dev/md0<br>~#:cat /proc/mdstat<br>~#:vim /etc/mdadm/mdadm.conf</p><h2 id="lvm的制作">LVM的制作</h2><p>LVM Logical Volume Manager<br>PV physical volume<br>VG volume group<br>PE physical extend<br>LV logical volume</p><h3 id="lv的写入机制">LV的写入机制</h3><ul><li>线性机制<br>若有两个设备/dev/sda1,/dev/sdb1,他们都在一个VG中，并且只有一个LV，线性机制就是在一个设备完全写满之后，再向另一个设备写入</li><li>交错模式</li></ul><h3 id="新建分区">新建分区</h3><p>~#:sudo fdisk /dev/sdb<br>new /dev/sdb{5,6,7,8,9,10}<br>t 8e(Linux LVM)<br>w<br>~#:partprobe</p><h3 id="安装应用">安装应用</h3><p>sudo apt-get install lvm2</p><h3 id="pv物理卷的新建">PV物理卷的新建</h3><p>pvcreate 将物理分区新建为PV分区<br>pvscan 查询目前系统里具有PV的磁盘<br>pvdisplay 显示目前系统上面的PV状态<br>pvremove 将PV属性删除</p><p>pvmove 将某个设备内的pe给移动到另一个设备<br>pvmove /dev/sdb5 /dev/sdb9</p><p>~#:pvscan<br>~#:pvcreate /dev/sdb{5,6,7,8}<br>~#:pvscan<br>~#:pvdisplay</p><h3 id="vg卷用户组的新建">VG卷用户组的新建</h3><p>vgcreate 新建VG<br>vgscan 查找目前系统上的VG<br>vgdisplay 显示目前系统上的VG状态<br>vgextend 在VG内新增额外的VG<br>vgreduce 在VG内删除PV<br>vgchange 设置VG是否启动<br>vgremove 删除一个VG<br>VG名称是自己定义的。而PV名称实际上是分区的设备文件名</p><p>vgcreate [-s] VG名称 PV名称<br>-s　后面接PE的大写,单位可以是m,g,t (支持大小写)<br>~#:vgcreate -s 16M mxxvg /dev/sdb{5,6,7}<br>~#:vgscan<br>~#:pvscan<br>~#:vgdisplay<br>vgextend VG名称　PV名称<br>~#:vgextend mxxvg /dev/sdb8<br>~#:vgdisplay</p><h3 id="lv逻辑卷的新建">LV逻辑卷的新建</h3><p>lvcreate 新建lv<br>lvscan 查询系统上的lv<br>lvdisplay 展示系统上的lv<br>lvextend 在lv里增加容量<br>lvreduce 在lv里减少容量<br>lvremove 删除一个lv<br>lvresise 对lv的大小进行重新调整</p><p>lvcreate [-lLs] [-n lv名称]　vg名称<br>-l 后接的是PE的个数<br>-L 后接的是vg的容量<br>-n 后接lv的名称<br>-s snapshot 快照<br>~#:lvcreate -l 252 -n mxxlv mxxvg<br>~#:ls -l /dev/mxxvg/mxxlv<br>~#:lvscan<br>~#:lvdisplay</p><h3 id="文件系统新建">文件系统新建</h3><p>~#:mkfs -t ext4 /dev/mxxvg/mxxlv<br>~#:mkdir /mnt/lvm<br>~#:mount /dev/mxxvg/mxxlv /mnt/lvm<br>~#:df -h .</p><h3 id="增加lv容量">增加lv容量</h3><p>~#:sudo fdisk /dev/sdb<br>new /dev/sdb9<br>~#:pvcreate /dev/sdb9<br>~#:pvscan<br>~#:vgextend mxxvg /dev/sdb9<br>~#:vgdisplay<br>增加lv的容量<br>~#:lvresize -l +63 /dev/mxxvg/mxxlv<br>~#:lvdisplay<br>~#:df -h /mnt/lvm</p><p>此时虽然lv显示的容量增大，但是对应的/dev/mxxvg/mxxlv文件系统还没有改变<br>~#:dumpe2fs /dev/mxxvg/mxxlv<br>重新计算文件系统<br>resizefs [-f] [device] [size]<br>-f 强制进行resize<br>device 后接的文件系统或者是设备名<br>size 如果没有size默认为整个文件系统，如果有size的话，必须给一个             单位<br>~#:resize2fs /dev/mxxvg/mxxlv   //可在线进行resize</p><h3 id="缩小lv容量">缩小lv容量</h3><p>先计算需要缩小多少<br>~#:pvscan<br>~#:pvdisplay<br>　　　缩小文件系统容量<br>放大可以直接进行，但是缩小需要先卸载<br>~#:umount /dev/mxxvg/mxxlv<br>~#:resize2fs /dev/mxxvg/mxxlv 3900M<br>报错需要用e2fsck<br>~#:e2fsckk -f /dev/mxxvg/mxxlv<br>~#:resize2fs /dev/mxxvg/mxxlv 3900M<br>~#:mount /dev/mxxvg/mxxlv /mnt/lvm<br>~#:df -h /mnt/lvm<br>　　　降低lv容量<br>~#:lvresize -l -63 /dev/mxxvg/mxxlv<br>~#:lvdisplay<br>转移pv<br>~#:pvdisplay<br>~#:pvmove /dev/sdb5 /dev/sdb9<br>　　　删除vg<br>~#:vgreduce mxxvg /dev/sdb5<br>删除pv<br>~#:pvscan<br>~#:pvremove /dev/sdb5</p><h2 id="lvm的快照">LVM的快照</h2><p>需要有未使用的PE块<br>所以需要新加入一个PV块</p><p>~#:vgdisplay<br>~#:pvcreate /dev/sdb5<br>~#:vgextend mxxvg /dev/sdb5<br>~#:vgdisplay<br>~#:lvcreate -l 40 -s -n mxxlv_ss /dev/mxxvg/mxxlv<br>-s snapshot<br>~#:lvdisplay<br>复原的数据是不能比快照区的大小大的，此处不能大于40个PE</p><p>接下来改变LVM中的数据，会发现lvm与快照区是不同的<br>~#:cd /mnt/lvm<br>~#:cp -a /home/mxx/my.iso /mnt/lvm<br>~#:lvdisplay 会发现lv的快照区已经被使用了<br>~#:df 会发现原始文件与快照区文件系统也是不同的</p><h3 id="利用快照区进行备份">利用快照区进行备份</h3><p>~#:tar -cvj -f /home/mxx/my.bak/lvm.bak.tar.bz2 *<br>~#:umount /mnt/snapshot</p><p>将快照区进行删除，因为已经被备份<br>~#:lvremove /dev/mxxvg/mxxlv_ss</p><p>~#:umount /mnt/lvm<br>~#:mkfs -t ext4 /mnt/lvm<br>~#:mount /dev/mxxvg/mxxlv /mnt/lvm</p><p>将备份的数据还原，那么这个文件系统就会和原来一样了<br>~#:tar -xvj -f /home/mxx/my.bak/lvm.bak.tar.bz2 /mnt/lvm<br>~#:ls -l /mnt/lvm</p><h3 id="lvm的关闭">LVM的关闭</h3><p>先卸载lvm系统，包括快照与原系统<br>再使用lvremove删除LV<br>使用vgchange -a n VG 名称 让其不再为active<br>使用vgremove删除VG<br>使用pvremove删除PV<br>最后使用sudo fdisk 修改System ID</p><p>~#:umount /mnt/lvm<br>~#:lvremove /dev/mxxvg/mxxlv<br>~#:vgchage -a n mxxvg<br>~#:vgremove mxxvg<br>~#:pvremover /dev/sdb{5,6,7,8}<br>~#:sudo fdisk -l</p><h2 id="参考文献">参考文献</h2><p>1.《鸟哥的LINUX私房菜》</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;磁盘阵列&quot;&gt;磁盘阵列&lt;/h2&gt;
&lt;h3 id=&quot;mdadm新建raid&quot;&gt;mdadm新建raid&lt;/h3&gt;
&lt;h4 id=&quot;参数介绍&quot;&gt;参数介绍&lt;/h4&gt;
&lt;p&gt;mdadm --detail  后面接的那个磁盘阵列设备的具体信息&lt;br&gt;
madam --creat
      
    
    </summary>
    
      <category term="linux" scheme="http://mxxhcm.github.io/categories/linux/"/>
    
    
      <category term="linux" scheme="http://mxxhcm.github.io/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>linux at cron anacron mail</title>
    <link href="http://mxxhcm.github.io/2019/05/07/linux-at-cron-anacron-mail/"/>
    <id>http://mxxhcm.github.io/2019/05/07/linux-at-cron-anacron-mail/</id>
    <published>2019-05-07T08:50:33.000Z</published>
    <updated>2019-06-23T01:12:20.608Z</updated>
    
    <content type="html"><![CDATA[<h2 id="例行性工作">例行性工作</h2><ul><li>at    仅执行一次</li><li>cron  周期性执行</li><li>anacron   适合不常开机的设置</li></ul><h2 id="at仅执行一次的工作调度">at仅执行一次的工作调度</h2><h3 id="参数说明">参数说明</h3><p>at [-lmdvc] TIME<br>-m 当at完成时，即使没有输出信息，以mail通知用户<br>-v 可以使用较明显的时间格式列出at调度中的工作<br>-c 可以列出后面接的该项工作的实际命令内容<br>-d 相当于atrm，可以取消一个at工作<br>-l 相当于atq，列出目前系统上所有该用户的at调度<br>-b 相当于batch<br>TIME<br>HH:MM04:00<br>HH:MM YYYY-MM-DD    05:00 2016-10-05<br>HH:MM[pm|am] [Month] [Date] 04 January 10<br>HH:MM [am|pm] + number [minutes|hours|days|weeks]<br>now + 5 minutes<br>05pm + 3days<br>04pm + 10 days</p><h3 id="示例">示例</h3><h4 id="创建一个job">创建一个job</h4><p>~$:at now+1minutes<br>at&gt;echo &quot;create a job&quot;<br>按ctrl+D结束<br>OK，但是这样子我找不到任何程序的输出在哪里。<br>所以可以改成这样子<br>at&gt;echo “create a job” &gt; at_job.output<br>或者<br>~$:echo “create a job” &gt; at_job.output | at now</p><h4 id="列出所有at-jobs">列出所有at jobs</h4><p>~$:at -l   # 列出at的所有任务<br>~$:atq</p><h4 id="列出某个job">列出某个job</h4><p>~$:at -c [number](1, 2…) # 如果当前没有相应的job，会输出cannot find jobid x</p><h4 id="删除某个job">删除某个job</h4><p>~$:at -r 8<br>~$:atrm 1</p><h3 id="配置文件">配置文件</h3><p>/etc/at.allow   # 哪些人能使用<br>/etc/at.deny    # 哪些人不能使用<br>使用at命令的话，先查找at.allow，如果存在并且有内容，那么只有这些人能使用。如果不存在的话，就去找at.deny。</p><h2 id="batch">batch</h2><p>当空闲时执行，空闲指的是CPU占用率在$0.8$以下</p><h2 id="crond例行性工作调度">crond例行性工作调度</h2><h3 id="参数介绍">参数介绍</h3><p>crontab [-u user] [-ler]<br>-u　只有root能设置这个参数<br>-l　列出当前用户的所有crontab工作内容<br>-e  编辑crontab的内容<br>-r　删除所有crontab的内容</p><h3 id="示例-v2">示例</h3><h4 id="新建crontab">新建crontab</h4><p><strong>注意：周与月日不可共存</strong><br>~$:crontab -e<br>* * * * * cmd<br>分钟　小时　日期　月份　星期<br>*表示任何取值，<br>-表示时间范围 0-59,<br>&quot;,“表示分隔 3,6,9<br>”/n&quot;，如*/5每过五个单位(分钟，小时，天)<br>比如添加每一小时给荟荟发一封邮件，需要添加以下内容</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">* */1 * * * echo &quot;I love you.&quot; | mail -s &quot;huhui&quot; 18811367922@163.com</span><br></pre></td></tr></table></figure><h4 id="删除一个crontab">删除一个crontab</h4><p>~$:crontab -e<br>然后手动编辑要删除的crontab</p><h4 id="删除所有crontab">删除所有crontab</h4><p>~$:crontab -r # 删除所有的crontab</p><h3 id="开启-var-log-cron-log">开启/var/log/cron.log</h3><p>~$:vim /etc/rsyslog.d/50-default.conf<br>将rsylog文件中的#cron.*前的#去掉<br>~$:service rsyslog restart<br>~$:service cron restart<br>~$:vim /var/log/cron.log</p><h3 id="系统任务">系统任务</h3><p>/etc/crontab 为系统的例行性任务，它会执行以下run-parts</p><ul><li>/etc/cron.daily/</li><li>/etc/cron.hourly/</li><li>/etc/cron.monthly/</li><li>/etc/cron.weekly/</li></ul><h3 id="自定义run-parts">自定义run-parts</h3><p>直接编辑/etc/crontab文件，在其中添加</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># m h dom mon dow usercommand</span><br><span class="line">\*/2 \* \* \* \* root   run-parts /etc/cron.minutely</span><br><span class="line">\*/5 \* \* \* \* root   run-parts /root/runcron</span><br><span class="line"># 上述两条命令中，需要对应的目录存在或者直接执行一个shell脚本</span><br><span class="line">\* \* \* \* \* mxxmhh /bin/bash /home/mxxmhh/outputtime_minutes.sh</span><br></pre></td></tr></table></figure><p>outputtime_minutes.sh脚本如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#! /bin/bash</span><br><span class="line">time=`date`</span><br><span class="line">echo $time &gt;&gt; /home/mxxmhh/test.log</span><br></pre></td></tr></table></figure><h3 id="crontab-e-vs-vim-etc-crontab">crontab -e vs vim /etc/crontab</h3><p>他们的格式不同，一个需要指定用户，一个不需要<br>只有root能够修改/etc/crontab，而crontab -e所有不在cron.deny中的用户都可以<br>/etc/crontab是系统的任务，crontab -e是用户的任务</p><h3 id="配置文件-v2">配置文件</h3><p>ubuntu中没有下面两个配置项<br>/etc/cron.allow<br>/etc/cron.deny<br>即默认为所有用户都可以使用crontab</p><h3 id="cron-spool">cron spool</h3><p>/var/spool/cron/crontabs/<br>该目录下为不同账号的crontab内容</p><h2 id="anacron-处理非24小时开机的系统">anacron 处理非24小时开机的系统</h2><h3 id="参数介绍-v2">参数介绍</h3><p>anacron [-usfn] [job]<br>-u 更新记录文件的时间戳<br>-s 开始连续执行各项job，依据记录文件的时间戳判断是否进行<br>-f 强制执行，不管时间戳<br>-n 立即进行未进行的任务，而不延迟</p><h3 id="示例-v3">示例</h3><p>系统的anacron文件都在目录/etc/cron*/*ana*存放<br>/etc/cron.daily/0anacron<br>0表示最先被执行，让时间戳先被更新，避免anacron误判<br>/etc/anacronanacron的设置</p><p>/var/spool/anacron/*<br>记录最近一次执行anacron的时间戳</p><h2 id="mail命令介绍">mail命令介绍</h2><p>mail -s “title” target_email_address<br>echo &quot;content |mail -s “title” target_email_address<br>mail -s &quot;title target_email_address &lt; file #将file的内容当做邮件正文</p><h2 id="mail发送邮件">mail发送邮件</h2><h3 id="安装相应软件">安装相应软件</h3><p>~$:sudo apt-get install postfix mailutils libsasl2-2 ca-certificates libsasl2-modules<br>编辑/etc/postfix/main.cf文件，在文件末尾添加下列内容</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># 指定默认的邮件发送服务器</span><br><span class="line">relayhost = [smtp.gmail.com]:587</span><br><span class="line"># 激活sasl认证</span><br><span class="line">smtp_sasl_auth_enable = yes</span><br><span class="line"># 指定sasl密码配置文件</span><br><span class="line">smtp_sasl_password_maps = hash:/etc/postfix/sasl_passwd</span><br><span class="line"># 非匿名登录</span><br><span class="line">smtp_sasl_security_options = noanonymous</span><br><span class="line"># linux用户与发件人的对应关系配置文件</span><br><span class="line">sender_canonical_maps = hash:/etc/postfix/sender_canonical </span><br><span class="line">smtp_tls_CApath = /etc/ssl/certs</span><br><span class="line">smtpd_tls_CApath = /etc/ssl/certs</span><br><span class="line">smtp_use_tls = yes</span><br></pre></td></tr></table></figure><h3 id="创建密码配置文件">创建密码配置文件</h3><p>~$:vim /etc/postfix/sasl_passwd<br>添加如下内容</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 163邮箱格式</span><br><span class="line">[smtp.163.com]:25 your163mail:your163mailpassword   #注意这里如果直接用passwd是会报错的，需要使用授权码</span><br><span class="line"># gamil邮箱格式</span><br><span class="line">[smtp.gmail.com]:587 yourgmail:yourgmailpassword</span><br></pre></td></tr></table></figure><p>~$:sudo postmap /etc/postfix/sasl_passwd</p><h3 id="创建用户与发件人对应文件">创建用户与发件人对应文件</h3><p>~$:vim /etc/postfix/sender_canonical<br>添加如下内容</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">root your163mail</span><br><span class="line">user1 yourgmail</span><br></pre></td></tr></table></figure><p>~$:sudo postmap /etc/postfix/sender_canonical</p><h3 id="重启postfix服务">重启postfix服务</h3><p>~$:sudo /etc/init.d/postfix reload<br>或者<br>~$:sudo systemctl relaod postfix.service<br>或者<br>~$:sudo service postfix restart</p><h3 id="测试">测试</h3><p>~$:echo “Hello.” |mail -s “I love you.” <a href="mailto:18811376816@163.com" target="_blank" rel="noopener">18811376816@163.com</a><br>这种方式应该是不支持中文的。。</p><h2 id="附录">附录</h2><p>更多at命令的TIME格式</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">noon</span><br><span class="line">midnight</span><br><span class="line">teatime</span><br><span class="line">tomorrow</span><br><span class="line">noon tomorrow</span><br><span class="line">next week</span><br><span class="line">next monday</span><br><span class="line">fri</span><br><span class="line">NOV</span><br><span class="line">9:00 AM</span><br><span class="line">2:30 PM</span><br><span class="line">1430</span><br><span class="line">2:30 PM tomorrow</span><br><span class="line">2:30 PM next month</span><br><span class="line">2:30 PM Fri</span><br><span class="line">2:30 PM 10/21</span><br><span class="line">2:30 PM Oct 21</span><br><span class="line">2:30 PM 10/21/2014</span><br><span class="line">2:30 PM 21.10.14</span><br><span class="line">now + 30 minutes</span><br><span class="line">now + 1 hour</span><br><span class="line">now + 2 days</span><br><span class="line">4 PM + 2 days</span><br><span class="line">now + 3 weeks</span><br><span class="line">now + 4 months</span><br><span class="line">now + 5 years</span><br></pre></td></tr></table></figure><h2 id="参考文献">参考文献</h2><p>1.《鸟哥的LINUX私房菜》<br>2.<a href="https://zhidao.baidu.com/question/249718018.html" target="_blank" rel="noopener">https://zhidao.baidu.com/question/249718018.html</a><br>3.<a href="https://askubuntu.com/questions/1112772/send-system-mail-ubuntu-18-04" target="_blank" rel="noopener">https://askubuntu.com/questions/1112772/send-system-mail-ubuntu-18-04</a><br>4.<a href="https://www.cnblogs.com/tugeler/p/6620150.html" target="_blank" rel="noopener">https://www.cnblogs.com/tugeler/p/6620150.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;例行性工作&quot;&gt;例行性工作&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;at    仅执行一次&lt;/li&gt;
&lt;li&gt;cron  周期性执行&lt;/li&gt;
&lt;li&gt;anacron   适合不常开机的设置&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;at仅执行一次的工作调度&quot;&gt;at仅执行一次的工作
      
    
    </summary>
    
      <category term="linux" scheme="http://mxxhcm.github.io/categories/linux/"/>
    
    
      <category term="linux" scheme="http://mxxhcm.github.io/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>linux service and daemon</title>
    <link href="http://mxxhcm.github.io/2019/05/07/linux-service/"/>
    <id>http://mxxhcm.github.io/2019/05/07/linux-service/</id>
    <published>2019-05-07T08:47:38.000Z</published>
    <updated>2019-06-23T14:39:36.630Z</updated>
    
    <content type="html"><![CDATA[<h2 id="service和daemon">service和daemon</h2><p>service（服务）：系统提供某些功能的一些服务(包括系统本身以及网络service)<br>daemon：实现service的程序叫做daemon</p><h2 id="daemon的分类">daemon的分类</h2><ul><li>stand alone daemon</li><li>super daemon</li></ul><h3 id="stand-alone-daemon">stand_alone daemon</h3><p>独立启动，启动并加载到内存后就一直占用内存与系统资源运行。因此对于客户端的请求响应特别快。比如WWW的daemon(httpd)，FTP的daemon(vsftpd)</p><h3 id="super-daemon">super daemon</h3><p>由一个统一daemon唤起的service，这个特殊的daemon叫做super daemon早期是inetd,后来被xinetd替代了。没有客户端请求时，service被关闭，收到客户端请求时，super daemon唤醒相应的service，请求结束后，这个service就会关闭，service反应时间会比较慢。常见的有telnetservice。<br>signal-control和interval-control，信号管理的daemon以及每隔一段时间主动执行某项job的daemon每一个service程序文件名都会加上d，d代表daemon。</p><h2 id="sysvinit-service">SysVInit service</h2><h3 id="配置文件路径">配置文件路径</h3><ul><li>/etc/rc.d/rcX.d/ (X 代表运行级别 0-6) # 不同runlevel的service存放位置</li><li>/etc/rc.d/rc.local    # 用户自定义的service</li></ul><h3 id="自定义文件示例">自定义文件示例</h3><p>创建/etc/init.d/shadowsocks_client service如下所示</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/sh</span><br><span class="line"></span><br><span class="line">### BEGIN INIT INFO</span><br><span class="line"># Provides:          shadowsocks client</span><br><span class="line"># Required-Start:    $local_fs $remote_fs $network $syslog</span><br><span class="line"># Required-Stop:     $local_fs $remote_fs $network $syslog</span><br><span class="line"># Default-Start:     2 3 4 5</span><br><span class="line"># Default-Stop:      0 1 6</span><br><span class="line"># Short-Description: shadowsocks service</span><br><span class="line"># Description:       shadowsocks service daemon</span><br><span class="line">### END INIT INFO</span><br><span class="line">start()&#123;</span><br><span class="line">　　  sslocal -c /etc/shadowsocks.json -d start</span><br><span class="line">&#125;</span><br><span class="line">stop()&#123;</span><br><span class="line">　　  sslocal -c /etc/shadowsocks.json -d stop</span><br><span class="line">&#125;</span><br><span class="line">case “$1” in</span><br><span class="line">start)</span><br><span class="line">　　　start</span><br><span class="line">　　　;;</span><br><span class="line">stop)</span><br><span class="line">　　　stop</span><br><span class="line">　　　;;</span><br><span class="line">reload)</span><br><span class="line">　　　stop</span><br><span class="line">　　　start</span><br><span class="line">　　　;;</span><br><span class="line">\*)</span><br><span class="line">　　　echo “Usage: $0 &#123;start|reload|stop&#125;”</span><br><span class="line">　　　exit 1</span><br><span class="line">　　　;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure><p>然后执行以下命令进行更新<br>~$:sudo chomod a+x /etc/init.d/shadowsocks_client<br>~$:sudo update_rc.d shadowsocks defaults</p><h3 id="运行方式">运行方式</h3><p>service shadowsocks_client start</p><h2 id="systemd">SystemD</h2><h3 id="配置文件路径-v2">配置文件路径</h3><ul><li>/etc/systemd/system系统service，不要动。大部分是软连接，指向/usr/lib/systemd/sytem</li><li>/run/systemd/systemRuntime units</li><li>/usr/local/lib/systemd/system管理员安装的System units</li><li>/usr/lib/systemd/system包管理器安装的System units(for centos)</li><li>/lib/systemd/system   包管理器安装的System units(for debian/ubuntu)</li><li>/etc/systemd/system/**.service.wants/*：此目录内的文件为链接文件，设置相依服务的链接。意思是启动了 **.service 之后，最好再加上这目录下面建议的服务。</li><li>/etc/systemd/system/vsftpd.service.requires/*：此目录内的文件为链接文件，设置相依服务的链接。意思是在启动 vsftpd.service 之前，需要事先启动哪些服务的意思。</li></ul><h3 id="自定义unit文件示例">自定义unit文件示例</h3><p>在/lib/systemd/system/创建ss_client.service，如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[Unit]</span><br><span class="line">Description=ss v4 client daemon</span><br><span class="line"> </span><br><span class="line">[Service]</span><br><span class="line">ExecStart=/usr/bin/sslocal -c /etc/shadowsocks_v4_client.json &lt;/dev/null &amp;&gt;&gt;/home/mxxmhh/.log/ss-local.log </span><br><span class="line">WorkingDirectory=/home/mxxmhh/</span><br><span class="line"># Restart=on-failure</span><br><span class="line">StartLimitBurst=2</span><br><span class="line">StartLimitInterval=30</span><br><span class="line">User=mxxmhh</span><br><span class="line">ExecReload=/bin/kill -SIGHUP $MAINPID</span><br><span class="line">ExecStop=/bin/kill -SIGINT $MAINPID</span><br><span class="line"> </span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure><p>然后执行以下命令<br>~$:sudo systemctl start ss-client.service<br>~$:sudo systemctl enable ss-client.service</p><blockquote><p>Created symlink /etc/systemd/system/multi-user.target.wants/ss-client.service → /lib/systemd/system/ss-client.service.</p></blockquote><p>执行以下命令发现报错<br>~$:sudo systemctl status ss-client.service</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ss-client.service: Start request repeated too</span><br><span class="line">ss-client.service: Failed with result &apos;exit-c</span><br></pre></td></tr></table></figure><p>根据参考文献13使用下列命令常看详细log<br>~$:journalctl -u ss-client.service<br>发现报错：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ss-client.service: Failed at step USER spawning /usr/bin/sslocal: No such proces</span><br></pre></td></tr></table></figure><p>然后根据参考文献12发现可能是自己的文件写的有问题，最后发现是user复制的时候出错了，修改之后就好了。执行以下命令加载修改后的配置文件，然后restart服务。<br>~$: sudo systemctl daemon-reload<br>~$: sudo systemctl restart ss-client.service<br>~$: sudo systemctl status ss-client.service</p><h3 id="unit文件的编写">Unit文件的编写</h3><p>每个unit都有一个配置文件，定义了这个unit启动的条件。</p><h4 id="unit格式">Unit格式</h4><p>下面是 SSH service的unit文件，service unit文件以.service 为文件名后缀。<br>~$:cat /etc/systemd/system/sshd.service</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[Unit]</span><br><span class="line">Description=OpenSSH server daemon</span><br><span class="line">[Service]</span><br><span class="line">EnvironmentFile=/etc/sysconfig/sshd</span><br><span class="line">ExecStartPre=/usr/sbin/sshd-keygen</span><br><span class="line">ExecStart=/usrsbin/sshd –D $OPTIONS</span><br><span class="line">ExecReload=/bin/kill –HUP $MAINPID</span><br><span class="line">KillMode=process</span><br><span class="line">Restart=on-failure</span><br><span class="line">RestartSec=42s</span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure><p>Unit部分仅仅有一个描述信息;Service中，ExecStartPre定义启动service之前应该运行的命令；ExecStart定义启动service的具体命令行语法；Install部分，WangtedBy 表明这个service是在多用户模式下所需要的，multi-user.target</p><h4 id="unit的配置文件区块">Unit的配置文件区块</h4><p>一个文件通常由[Unit]，[Service]（或者其他unit类型）和[Install]构成。<br>[Unit]区块通常是配置文件的第一个区块，用来定义 Unit 的元数据，以及配置与其他 Unit 的关系。它的主要字段如下。</p><ul><li>Description：简短描述</li><li>Documentation：文档地址</li><li>Requires：当前 Unit 依赖的其他 Unit，如果它们没有运行，当前 Unit 会启动失败</li><li>Wants：与当前 Unit 配合的其他 Unit，如果它们没有运行，当前 Unit 不会启动失败</li><li>BindsTo：与Requires类似，它指定的 Unit 如果退出，会导致当前 Unit 停止运行</li><li>Before：如果该字段指定的 Unit 也要启动，那么必须在当前 Unit 之后启动</li><li>After：如果该字段指定的 Unit 也要启动，那么必须在当前 Unit 之前启动</li><li>Conflicts：这里指定的 Unit 不能与当前 Unit 同时运行</li><li>Condition…：当前 Unit 运行必须满足的条件，否则不会运行</li><li>Assert…：当前 Unit 运行必须满足的条件，否则会报启动失败</li></ul><p>[Install]通常是配置文件的最后一个区块，用来定义如何启动，以及是否开机启动。它的主要字段如下。</p><ul><li>WantedBy：它的值是一个或多个 Target，当前 Unit 激活时（enable）符号链接会放入/etc/systemd/system目录下面以 Target 名 + .wants后缀构成的子目录中</li><li>RequiredBy：它的值是一个或多个 Target，当前 Unit 激活时，符号链接会放入/etc/systemd/system目录下面以 Target 名 + .required后缀构成的子目录中</li><li>Alias：当前 Unit 可用于启动的别名</li><li>Also：当前 Unit 激活（enable）时，会被同时激活的其他 Unit</li></ul><p>[Service]区块用来 Service 的配置，只有 Service 类型的 Unit 才有这个区块。它的主要字段如下。</p><ul><li>Type：定义启动时的进程行为。它有以下几种值。</li><li>Type=simple：默认值，执行ExecStart指定的命令，启动主进程</li><li>Type=forking：以 fork 方式从父进程创建子进程，创建后父进程会立即退出</li><li>Type=oneshot：一次性进程，Systemd 会等当前服务退出，再继续往下执行</li><li>Type=dbus：当前服务通过D-Bus启动</li><li>Type=notify：当前服务启动完毕，会通知Systemd，再继续往下执行</li><li>Type=idle：若有其他任务执行完毕，当前服务才会运行</li><li>ExecStart：启动当前服务的命令</li><li>ExecStartPre：启动当前服务之前执行的命令</li><li>ExecStartPost：启动当前服务之后执行的命令</li><li>ExecReload：重启当前服务时执行的命令</li><li>ExecStop：停止当前服务时执行的命令</li><li>ExecStopPost：停止当其服务之后执行的命令</li><li>RestartSec：自动重启当前服务间隔的秒数</li><li>Restart：定义何种情况 Systemd 会自动重启当前服务，可能的值包括always（总是重启）、on-success、on-failure、on-abnormal、on-abort、on-watchdog</li><li>TimeoutSec：定义 Systemd 停止当前服务之前等待的秒数</li><li>Environment：指定环境变量</li></ul><h3 id="日志">日志</h3><p>Systemd统一管理所有Unit的启动日志。带来的好处就是，可以只用journalctl一个命令，查看所有日志（内核日志和应用日志）。日志的配置文件是/etc/systemd/journald.conf。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">~$:sudo journalctl  # 查看所有日志（默认情况下 ，只保存本次启动的日志）</span><br><span class="line">~$: sudo journalctl -k    # 查看内核日志（不显示应用日志）</span><br><span class="line"><span class="meta">#</span> 查看系统本次启动的日志</span><br><span class="line">~$:sudo journalctl -b    </span><br><span class="line">~$:sudo journalctl -b -0</span><br><span class="line">~$:sudo journalctl -b -1 # 查看上一次启动的日志（需更改设置）</span><br><span class="line"><span class="meta">#</span> 查看指定时间的日志</span><br><span class="line">~$:sudo journalctl --since="2012-10-30 18:17:16"</span><br><span class="line">~$:sudo journalctl --since "20 min ago"</span><br><span class="line">~$:sudo journalctl --since yesterday</span><br><span class="line">~$:sudo journalctl --since "2015-01-10" --until "2015-01-11 03:00"</span><br><span class="line">~$:sudo journalctl --since 09:00 --until "1 hour ago"</span><br><span class="line">~$:sudo journalctl -n   # 显示尾部的最新10行日志</span><br><span class="line">~$:sudo journalctl -n 20    # 显示尾部指定行数的日志</span><br><span class="line">~$:sudo journalctl -f   # 实时滚动显示最新日志</span><br><span class="line">~$:sudo journalctl /usr/lib/systemd/systemd # 查看指定服务的日志</span><br><span class="line">~$:sudo journalctl _PID=1   # 查看指定进程的日志</span><br><span class="line">~$:sudo journalctl /usr/bin/bash    # 查看某个路径的脚本的日志</span><br><span class="line">~$:sudo journalctl _UID=33 --since today    # 查看指定用户的日志</span><br><span class="line"><span class="meta">#</span> 查看某个 Unit 的日志</span><br><span class="line">~$:sudo journalctl -u nginx.service</span><br><span class="line">~$:sudo journalctl -u nginx.service --since today</span><br><span class="line">~$:sudo journalctl -u nginx.service -f  # 实时滚动显示某个 Unit 的最新日志</span><br><span class="line">~$:journalctl -u nginx.service -u php-fpm.service --since today # 合并显示多个 Unit 的日志</span><br><span class="line"><span class="meta">#</span> 查看指定优先级（及其以上级别）的日志，共有8级</span><br><span class="line"><span class="meta">#</span> 0: emerg</span><br><span class="line"><span class="meta">#</span> 1: alert</span><br><span class="line"><span class="meta">#</span> 2: crit</span><br><span class="line"><span class="meta">#</span> 3: err</span><br><span class="line"><span class="meta">#</span> 4: warning</span><br><span class="line"><span class="meta">#</span> 5: notice</span><br><span class="line"><span class="meta">#</span> 6: info</span><br><span class="line"><span class="meta">#</span> 7: debug</span><br><span class="line">~$:sudo journalctl -p err -b</span><br><span class="line">~$:sudo journalctl --no-pager   # 日志默认分页输出，--no-pager 改为正常的标准输出</span><br><span class="line">~$:sudo journalctl -b -u nginx.service -o json  # 以 JSON 格式（单行）输出</span><br><span class="line">~$:sudo journalctl -b -u nginx.serviceqq -o json-pretty # 以 JSON 格式（多行）输出，可读性更好</span><br><span class="line">~$:sudo journalctl --disk-usage # 显示日志占据的硬盘空间</span><br><span class="line">~$:sudo journalctl --vacuum-size=1G # 指定日志文件占据的最大空间</span><br><span class="line">~$:sudo journalctl --vacuum-time=1years # 指定日志文件保存多久</span><br></pre></td></tr></table></figure><h3 id="systemctl-工具">systemctl 工具</h3><p>~$:systemctl list-units     列出正在运行的 Unit<br>~$:systemctl list-units --all   列出所有Unit，包括没有找到配置文件的或者启动失败的<br>~$:systemctl list-units --all --state=inactive      列出所有没有运行的 Unit<br>~$:systemctl list-units --failed    列出所有加载失败的 Unit<br>~$:systemctl list-units --type=service  列出所有正在运行的、类型为 service 的 Unit<br>~$:systemctl list-unit-files    列出所有配置文件<br>~$:systemctl list-unit-files --type=service     列出指定类型的配置文件<br>~$:systemctl start foo.service用来启动一个service (并不会重启现有的)<br>~$:systemctl stop foo.service用来停止一个service (并不会重启现有的)。<br>~$:systemctl restart foo.service用来停止并启动一个service。<br>~$:systemctl reload foo.service当支持时，重新装载配置文件而不中断等待操作。<br>~$:systemctl condrestart foo.service如果service正在运行那么重启它。<br>~$:systemctl status foo.service汇报service是否正在运行。<br>~$:systemctl list-unit-files --type=service用来列出可以启动或停止的service列表。<br>~$:systemctl enable foo.service在下次启动时或满足其他触发条件时设置service为启用。创建一个符号链接从/etc/systemd/system/some_target.target.wants指向/lib/systemd/system或者/etc/systemd/system。<br>~$:systemctl disable foo.service在下次启动时或满足其他触发条件时设置service为禁用<br>~$:systemctl is-enabled foo.service用来检查一个service在当前环境下被配置为启用还是禁用。<br>~$:systemctl list-unit-files --type=service输出在各个运行级别下service的启用和禁用情况<br>~$:systemctl daemon-reload当您创建新service文件或者变更设置时使用。<br>~$:systemctl isolate multi-user.target (OR systemctl isolate runlevel3.target OR telinit 3)改变至多用户运行级别。<br>~$:ls /etc/SystemD/system/*.wants/foo.service用来列出该service在哪些运行级别下启用和禁用。</p><h2 id="配置文件">配置文件</h2><p>/etc/init.d/*　# 基本上所有的service启动脚本都被放置在该目录<br>/etc/rcX.d/    # X指的是数字，从$0-6$，代表不同的run-level，是/etc/init.d/目录下service的软连接<br>/etc/systemd/system     # systemd的service文件位置，是/usr/lib/systemd/sytem的软连接<br>/etc/default    # 一些配置文件<br>/etc/*  # 各service各自的配置文件</p><h2 id="service-initctl-systemctl命令对照表">service，initctl，systemctl命令对照表</h2><p>Service 命令|UpStart initctl 命令|SystemD 命令|备注<br>—|---|<br>service foo start|initctl start|systemctl start foo.service用来启动一个service (并不会重启现有的)<br>service foo stop|initctl stop|systemctl stop foo.service用来停止一个service (并不会重启现有的)<br>service foo reload|systemctl reload foo.service当支持时，重新装载配置文件而不中断等待操作。<br>service foo restart|initctl restart|systemctl restart foo.service用来停止并启动一个service<br>service foo status|initctl status|systemctl status foo.service汇报service是否正在运行。<br>service foo reload|initctl reload|systemctl reload foo.service当支持时，重新装载配置文件而不中断等待操作。<br>service foo condrestart||systemctl condrestart foo.service如果service正在运行那么重启它。</p><h2 id="xinted">xinted</h2><p>xinted是/etc/init.d/目录中的一个脚本。<br>xinted 是inted的扩展，是super daemon，它本身管理了一系列的daemon，只有在用户调用时才由xinetd启动，他们要比独立的daemon启动晚。<br>!!!xinted默认在ubuntu中是不存在的,<br>~$:sudo apt-get install xinetd<br>/etc/xinetd.conf   #super daemon配置文件<br>/etc/xinetd.d/*    #它所管理的进程<br>/var/lib/*  各service产生的数据库<br>/var/run/*  各service的程序的pid记录处</p><h3 id="stand-alone的启动">stand alone的启动</h3><ol><li>用/etc/init.d/*启动<br>~#:/etc/init.d/cron start|stop|status|restart|reload|force-reload</li></ol><p>2.用service [service-name] (start|…)启动<br>service-name必须与/etc/init.d/相照应<br>–status-all 将所有的stand_aloneservice列出来<br>~#:service --status-all<br>~#:service cron</p><h3 id="super-daemon的启动方式">super daemon的启动方式</h3><p>super daemon本身也是一个stand alone的service，但是它所管理的其他文件就不是了。<br>查看某个service是否可用。<br>~#:grep -i ‘disable’ /etc/xinted.d/* # disable表示取消，若为yes，表示该service未开启，no表示开启</p><h4 id="示例">示例</h4><p>开启timeservice<br>~#:vim /etc/xinted.d/time<br>将disable改为no<br>重新启动xinted service<br>~#:service xinted restart<br>!!!注意是重启xinted service</p><p>查看该service的信息<br>~#:grep -i ‘time’ /etc/services<br>~#:netstat -nltp | grep ‘time port’</p><h3 id="默认值配置文件以及参数介绍">默认值配置文件以及参数介绍</h3><p>/etc/xinetd.conf<br>log_type    SYSLOG daemon info 日志文件的记录service类型<br>log_on_failure  发生错误时需要记录的信息<br>log_on_success  成功启动时的记录信息<br>cps 同一秒内的最大连接个数，若超过则暂停<br>instance    同一service的最大连接数<br>per_source  同一来源的客户端的最大连接数<br>v6only  是否运行ipv6<br>groups<br>umask</p><p>/etc/xinetd.d/<br>service <service name><br>{<br>disable 启动与否<br>id  service识别<br>server  程序文件名  这个service的启动程序<br>server_args 程序参数    设置server_args=–daemon<br>user    service所属id<br>group   用户组<br>socket_type 数据包类型  stream|dgram|raw stream使用tcp,   udp使用dgram,raw代表erver需要与ip直接交互。<br>protocol    数据包类型  tcp|udp与socket_type重复，<br>wait    连接机制    yes(single) no(multi) 一般udp为yes，tcp为no<br>instances   最大连接数<br>per_source  单用户来源  (一个数字或者NULIMTED)<br>cps 新连接限制<br>log_type    日志文件类型    以什么日志选项记载和需要记载的等级(默认为info)<br>log_on_success,log_on_failure,设置值,[PID,HOST,USERID,EXIT,DURATION]<br>PID为service启动时的pid,host为远程主机的ip，userid为登陆者的账号，EXIT为离开时记录的项目，DURATION为该用户使用此service多久。<br>env 额外环境变量设置    设置环境变量<br>port    非正规端口号    设置不同的service与对应的端口号，port与service名必须与/etc/services的值相同<br>redirect    service转址    [IP port] 将客户端的请求转到另一台主机<br>includedir  调用外部设置    表示将某个目录所有文件都放入xinetd.conf中，<br>bind    service端口锁定    运行此service的适配卡<br>interface   与bind相同<br>only_from   [0.0.0.0,192.168.1.0/24,hostname,domainname]设置为这里面的ip或者主机名才能访问，0.0.0.0表示所有主机皆能访问，如果是192.168.1.0/24则表示为C　class的域，即由(192.168.1.1~192.168.1.255)皆可登录。另外，也可选择域名，如bit.edu.cn表示运行北理工的ip登录你的主机<br>no_acess    表示的是不可登录的主机<br>acess_time  时间控制    [00:00-24:00,HH:MM-HH:MM]<br>umask   设置用户新建目录或者文件时候的属性<br>}</service></p><h2 id="参考文献">参考文献</h2><p>1.《鸟哥的LINUX私房菜》<br>2.<a href="https://askubuntu.com/questions/911525/difference-between-systemctl-init-d-and-service" target="_blank" rel="noopener">https://askubuntu.com/questions/911525/difference-between-systemctl-init-d-and-service</a><br>3.<a href="http://www.r9it.com/20180613/ubuntu-18.04-auto-start.html" target="_blank" rel="noopener">http://www.r9it.com/20180613/ubuntu-18.04-auto-start.html</a><br>4.<a href="https://www.ibm.com/developerworks/cn/linux/1407_liuming_init1/index.html" target="_blank" rel="noopener">https://www.ibm.com/developerworks/cn/linux/1407_liuming_init1/index.html</a><br>5.<a href="https://www.ibm.com/developerworks/cn/linux/1407_liuming_init2/index.html?ca=drs-" target="_blank" rel="noopener">https://www.ibm.com/developerworks/cn/linux/1407_liuming_init2/index.html?ca=drs-</a><br>6.<a href="https://www.ibm.com/developerworks/cn/linux/1407_liuming_init3/index.html?ca=drs-" target="_blank" rel="noopener">https://www.ibm.com/developerworks/cn/linux/1407_liuming_init3/index.html?ca=drs-</a><br>7.<a href="http://www.ruanyifeng.com/blog/2016/03/systemd-tutorial-commands.html" target="_blank" rel="noopener">http://www.ruanyifeng.com/blog/2016/03/systemd-tutorial-commands.html</a><br>8.<a href="https://wizardforcel.gitbooks.io/vbird-linux-basic-4e/content/150.html" target="_blank" rel="noopener">https://wizardforcel.gitbooks.io/vbird-linux-basic-4e/content/150.html</a><br>9.<a href="https://www.freedesktop.org/software/systemd/man/systemd.unit.html" target="_blank" rel="noopener">https://www.freedesktop.org/software/systemd/man/systemd.unit.html</a><br>10.<a href="https://unix.stackexchange.com/questions/206315/whats-the-difference-between-usr-lib-systemd-system-and-etc-systemd-system" target="_blank" rel="noopener">https://unix.stackexchange.com/questions/206315/whats-the-difference-between-usr-lib-systemd-system-and-etc-systemd-system</a><br>11.<a href="https://stackoverflow.com/questions/35452591/start-request-repeated-too-quickly" target="_blank" rel="noopener">https://stackoverflow.com/questions/35452591/start-request-repeated-too-quickly</a><br>12.<a href="https://superuser.com/questions/1156676/what-causes-systemd-failed-at-step-user-spawning-usr-sbin-opendkim-no-such-p" target="_blank" rel="noopener">https://superuser.com/questions/1156676/what-causes-systemd-failed-at-step-user-spawning-usr-sbin-opendkim-no-such-p</a><br>13.<a href="https://stackoverflow.com/questions/39202644/caddy-service-start-request-repeated-too-quickly" target="_blank" rel="noopener">https://stackoverflow.com/questions/39202644/caddy-service-start-request-repeated-too-quickly</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;service和daemon&quot;&gt;service和daemon&lt;/h2&gt;
&lt;p&gt;service（服务）：系统提供某些功能的一些服务(包括系统本身以及网络service)&lt;br&gt;
daemon：实现service的程序叫做daemon&lt;/p&gt;
&lt;h2 id=&quot;daem
      
    
    </summary>
    
      <category term="linux" scheme="http://mxxhcm.github.io/categories/linux/"/>
    
    
      <category term="linux" scheme="http://mxxhcm.github.io/tags/linux/"/>
    
      <category term="SystemD" scheme="http://mxxhcm.github.io/tags/SystemD/"/>
    
      <category term="serveice" scheme="http://mxxhcm.github.io/tags/serveice/"/>
    
      <category term="daemon" scheme="http://mxxhcm.github.io/tags/daemon/"/>
    
      <category term="initd" scheme="http://mxxhcm.github.io/tags/initd/"/>
    
  </entry>
  
  <entry>
    <title>linux log文件</title>
    <link href="http://mxxhcm.github.io/2019/05/07/linux-log%E6%96%87%E4%BB%B6/"/>
    <id>http://mxxhcm.github.io/2019/05/07/linux-log文件/</id>
    <published>2019-05-07T08:44:36.000Z</published>
    <updated>2019-06-20T01:47:30.578Z</updated>
    
    <content type="html"><![CDATA[<h2 id="常见的日志文件">常见的日志文件</h2><p>/var/log/cron.logcrontab调度有没有执行，有没有错误以及/etc/crontab是否正确编写<br>/var/log/lastlog所有账号最后一次的登录信息，非ASCII文件<br>/var/log/mail.log所有邮件的往来信息<br>/var/log/messages各种错误信息<br>/var/log/secure<br>/var/log/wtmp登录成功与识别的账号信息<br>/var/log/apport.log应用程序崩溃记录<br>/var/log/apt/*apt-get 安装卸载软件的日志<br>/var/log/auth.log登录认证log(与/etc/var/secure挺像)<br>/var/log/boot.log系统启动的日志<br>/var/log/btmp记录所有失败者的信息<br>/var/log/cups/* <br>/var/log/dist-upgradedist-upgrade这种更新方式的日志<br>/var/log/dmesg内核缓冲信息<br>/var/log/dpkg.log安装或dpkg命令清除软件包的日志<br>/var/log/faillog用户登录失败信息，错误登录命令也会显示<br>/var/log/fontconfig.log字体设置有关的日志<br>/var/log/fsck文件系统日志<br>/var/log/hp<br>/var/log/install<br>/var/log/kern.log内核产生的日志<br>/var/log/sambasamba存储的信息<br>/var/log/syslog系统登录信息<br>/var/log/upstart<br>/var/log/wtmp包含登录信息，找出谁正在登录进入系统以及谁用命令显示这个文件或者信息等<br>/var/log/xorg.*.log来自X的日志信息</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;常见的日志文件&quot;&gt;常见的日志文件&lt;/h2&gt;
&lt;p&gt;/var/log/cron.log	crontab调度有没有执行，有没有错误以及/etc/crontab是否正确编写&lt;br&gt;
/var/log/lastlog	所有账号最后一次的登录信息，非ASCII文件&lt;br&gt;

      
    
    </summary>
    
      <category term="linux" scheme="http://mxxhcm.github.io/categories/linux/"/>
    
    
      <category term="linux" scheme="http://mxxhcm.github.io/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>linux-启动流程</title>
    <link href="http://mxxhcm.github.io/2019/05/07/linux-%E5%90%AF%E5%8A%A8%E6%B5%81%E7%A8%8B/"/>
    <id>http://mxxhcm.github.io/2019/05/07/linux-启动流程/</id>
    <published>2019-05-07T08:41:27.000Z</published>
    <updated>2019-05-18T09:40:46.238Z</updated>
    
    <content type="html"><![CDATA[<h2 id="linux的启动流程">Linux的启动流程</h2><p>BIOS   MBR    boot loader    boot sector</p><h2 id="bios">BIOS</h2><p>BIOS(Basic Input Ouput System)是一套程序,它被写死到主板上面的一个内存芯片，这个内存芯片没有电时也能将数据记录下来，那就是一个ROM(Read Only Memory)。BIOS是系统开机时首先会去读取的一个小程序，它控制着开机时候的各项硬件参数的取得，它掌握了系统硬件的详细信息以及开机设备的选择，BIOS程序代码也会被适度修改，但是如果写在ROM中是无法修改的，现在多把BIOS写入Flash Memory 或者EEPROM中。</p><p>BIOS通过硬件的INT13中断功能来读取MBR的，所以只要BIOS能检测到磁盘那么他就能够通过INT13这条信道来读取该磁盘的第一个扇区内的MBR，这样就能够执行boot loader</p><h2 id="boot-loader">Boot Loader</h2><p>boot loader的最主要功能就是认识操作系统的文件格式并且加载该操作系统的内核到内存中执行。不同操作系统的文件类型不同，所以boot loader也是不同的，那么如果通过一个MBR来安装多操作系统呢。</p><h2 id="boot-sector">Boot Sector</h2><p>对于文件系统来说，每个文件系统都会有保留一个引导扇区(boot sector)提供给操作系统来安装boot loader。<br>每个操作系统默认会安装一个boot loader到它的文件系统中。对于Linux来说，我们可以选择将boot loader安装到MBR，也可以不选择，那样boot loader只会安装在它自己的文件系统中的即是(boot sector)。但是Windows操作系统会默认直接将boot loader安装在MBR以及boot sector中，所以说安装双系统时，最好先装Windows，再装Linux，否则反过来的话，那么Windows的boot loader可能就会覆盖掉Linux的boot loader.<br>但是，系统的MBR只有一个，所以，如何执行boot sector中的boot loader呢，那就需要谈到boot loader的功能了。<br>boot loader的功能<br>提供菜单<br>加载内核文件<br>转交其他loader<br>我们可以通过MBR中的boot loader选择其他的loader，这样就可以选择其他的操作系统运行了。<br>通过boot loader的管理读取了内核文件之后，那么就要进行工作了，重新检测硬件等。<br>但是从某些版本之后，内核是可以动态加载内核模块的，这些模块被放在/lib/modules/目录内，模块放置到磁盘根目录内，因此，启动过程中内核必须要挂载根目录，这样才能动态读取内核模块提供加载驱动程序的功能。<br>一般来说，非必要的功能可以编译成模块的内核功能，许多Linux会将内核编译成模块。USB，SATA,SCSI等设备的驱动程序都是通过模块的方式存在的。<br>那么问题来了，内核是不认识SATA硬盘的，所以根目录无法挂载，更无法通过根目录下的/lib/modules来驱动SATA硬盘了。这时候，就用到了虚拟文件系统(/boot/initrd)来管理。<br>虚拟文件系统能够通过boot loader加载到内存中，解压缩被当成一个根目录，从而通过该程序加载启动过程中所最需要的内核模块,通常是USB,RAID,LVM,SCSI等文件系统以及硬盘的驱动程序。<br>需要initrd的原因是因为启动时无法挂载根目录，如果根目录能被挂载，那么就不需要了，根目录再USB,SATA,SCSI等磁盘，或者文件系统比较特殊，为LVM，RAID等，那么是需要<br>载入这些模块之后，initd就会帮助内核重新调用/sbin/init进行后续正常的启动</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;linux的启动流程&quot;&gt;Linux的启动流程&lt;/h2&gt;
&lt;p&gt;BIOS   MBR    boot loader    boot sector&lt;/p&gt;
&lt;h2 id=&quot;bios&quot;&gt;BIOS&lt;/h2&gt;
&lt;p&gt;BIOS(Basic Input Ouput System
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>linux cpu信息查看</title>
    <link href="http://mxxhcm.github.io/2019/05/07/linux-cpu%E4%BF%A1%E6%81%AF%E6%9F%A5%E7%9C%8B/"/>
    <id>http://mxxhcm.github.io/2019/05/07/linux-cpu信息查看/</id>
    <published>2019-05-07T08:30:27.000Z</published>
    <updated>2019-06-12T03:16:52.798Z</updated>
    
    <content type="html"><![CDATA[<h2 id="查看cpu核数和cpu信息">查看cpu核数和cpu信息</h2><p>~$:lscpu</p><blockquote><p>Architecture:          x86_64<br>CPU op-mode(s):        32-bit, 64-bit<br>Byte Order:            Little Endian<br>CPU(s):                16<br>On-line CPU(s) list:   0-15<br>Thread(s) per core:    2<br>Core(s) per socket:    8<br>Socket(s):             1<br>NUMA node(s):          1<br>Vendor ID:             AuthenticAMD<br>CPU family:            23<br>Model:                 8<br>Model name:            AMD Ryzen 7 2700X Eight-Core Processor<br>Stepping:              2<br>CPU MHz:               3921.420<br>CPU max MHz:           3700.0000<br>CPU min MHz:           2200.0000<br>BogoMIPS:              7385.61<br>Virtualization:        AMD-V<br>L1d cache:             32K<br>L1i cache:             64K<br>L2 cache:              512K<br>L3 cache:              8192K<br>NUMA node0 CPU(s):     0-15<br>Flags:                 fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb hw_pstate sme ssbd ibpb vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt sha_ni xsaveopt xsavec xgetbv1 xsaves clzero irperf xsaveerptr arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif overflow_recov succor smca</p></blockquote><p>总核数 = 物理CPU个数 X 每颗物理CPU的核数<br>总逻辑CPU数 = 物理CPU个数 X 每颗物理CPU的核数 X 超线程数<br>拿我做测试的机器来说，一个cpu，每个cpu八核，每个核两个超线程。</p><h2 id="查看物理cpu个数">查看物理CPU个数</h2><p>~$:cat /proc/cpuinfo| grep “physical id”| sort| uniq |wc -l</p><blockquote><p>1</p></blockquote><h2 id="查看每个物理cpu中core的个数-即核数">查看每个物理CPU中core的个数(即核数)</h2><p>~$:cat /proc/cpuinfo| grep “cpu cores”</p><blockquote><p>8</p></blockquote><h2 id="查看逻辑cpu的个数">查看逻辑CPU的个数</h2><p>~$:cat /proc/cpuinfo| grep “processor”| wc -l</p><blockquote><p>processor: 0<br>processor: 1<br>processor: 2<br>processor: 3<br>processor: 4<br>processor: 5<br>processor: 6<br>processor: 7<br>processor: 8<br>processor: 9<br>processor: 10<br>processor: 11<br>processor: 12<br>processor: 13<br>processor: 14<br>processor: 15</p></blockquote><h2 id="参考文献">参考文献</h2><p>1.鸟哥的Linux私房菜</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;查看cpu核数和cpu信息&quot;&gt;查看cpu核数和cpu信息&lt;/h2&gt;
&lt;p&gt;~$:lscpu&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Architecture:          x86_64&lt;br&gt;
CPU op-mode(s):        32-bit, 6
      
    
    </summary>
    
      <category term="linux" scheme="http://mxxhcm.github.io/categories/linux/"/>
    
    
      <category term="linux" scheme="http://mxxhcm.github.io/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>linux jobs nohup bg fg ...</title>
    <link href="http://mxxhcm.github.io/2019/05/07/linux-jobs-nohup-fg/"/>
    <id>http://mxxhcm.github.io/2019/05/07/linux-jobs-nohup-fg/</id>
    <published>2019-05-07T08:19:29.000Z</published>
    <updated>2019-06-17T11:26:52.617Z</updated>
    
    <content type="html"><![CDATA[<h2 id="nohup">nohup</h2><p>nohup　[command parameters] [&amp;] nohup不挂断地运行命令。<br>nohup命令忽略所有挂断（SIGHUP）信号，有&amp;表示在后台执行，没有&amp;表示在机前台执行，即使脱机或者注销系统后仍然会执行，输出为nohup.out</p><h2 id="none">&amp;</h2><p>在后台运行。<br>一般nohup和&amp;会在一起使用。即nohup command &amp;，表示在后台不挂断的执行command命令<br>STDOUT以及STDERR都会被显示在屏幕上，可以采用数据流重定向将其输入文件<br>tar -cvj -f ~/my.bak/etc20161006.tar.bz2 /etc &gt; ~/tmp/log.txt 2&gt;&amp;1 &amp;<br>这样stdout以及stderr会被输入进~/tmp/log.txt</p><h2 id="示例">示例</h2><p>~$:nohup sslocal -c /etc/shadowsocks_v6.json &lt;/dev/null &amp;&gt;&gt;~/.log/ss-local.log &amp;</p><h2 id="jobs">jobs</h2><p>jobs -l 查看运行的后台进程，当打开该进程的终端关闭时，就无法看到使用jobs查看该程序了。需要使用ps命令<br>jobs [-lsr] 查看目前后台的jobs<br>-l 列出所有的后台jobs，包含pid<br>-s 列出停止的后台jobs，<br>-r 列出正在运行的jobs,</p><h2 id="fg-bg-ctrl-z">fg, bg, ctrl+z</h2><p>fg(foreground)将后台的工作拿到前台<br>fg %jobnumber<br>fg +/- [jobnumber]表示第几个后台工作，+表示最后一个被丢入后台，-表示最后第二个被丢入后台，最后第三个以及以上不显示</p><p>bg继续后台运行某个程序</p><p>ctrl+z挂起程序，将正在工作的程序放入后台(避免被ctrl+c终止,而非系统的后台)</p><h2 id="参考文献">参考文献</h2><p>1.<a href="https://baike.baidu.com/item/nohup/5683841?fr=aladdin" target="_blank" rel="noopener">https://baike.baidu.com/item/nohup/5683841?fr=aladdin</a><br>2.<a href="https://www.cnblogs.com/baby123/p/6477429.html" target="_blank" rel="noopener">https://www.cnblogs.com/baby123/p/6477429.html</a><br>3.<a href="https://www.cnblogs.com/hf8051/p/4494735.html" target="_blank" rel="noopener">https://www.cnblogs.com/hf8051/p/4494735.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;nohup&quot;&gt;nohup&lt;/h2&gt;
&lt;p&gt;nohup　[command parameters] [&amp;amp;] nohup不挂断地运行命令。&lt;br&gt;
nohup命令忽略所有挂断（SIGHUP）信号，有&amp;amp;表示在后台执行，没有&amp;amp;表示在机前台执行，即使脱
      
    
    </summary>
    
      <category term="linux" scheme="http://mxxhcm.github.io/categories/linux/"/>
    
    
      <category term="linux" scheme="http://mxxhcm.github.io/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>ubuntu 编译安装gcc</title>
    <link href="http://mxxhcm.github.io/2019/05/06/linux-gcc%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85/"/>
    <id>http://mxxhcm.github.io/2019/05/06/linux-gcc编译安装/</id>
    <published>2019-05-06T06:17:40.000Z</published>
    <updated>2019-06-12T02:45:33.982Z</updated>
    
    <content type="html"><![CDATA[<h2 id="下载相应版本的安装包">下载相应版本的安装包</h2><p>国科大源：<a href="https://mirrors.ustc.edu.cn/gnu/gcc/" target="_blank" rel="noopener">https://mirrors.ustc.edu.cn/gnu/gcc/</a><br>官网源：<a href="http://ftp.gnu.org/gnu/gcc/" target="_blank" rel="noopener">http://ftp.gnu.org/gnu/gcc/</a><br>我选择的是官方源，执行以下命令下载：<br>~$:wget <a href="http://ftp.gnu.org/gnu/gcc/gcc-7.3.0.tar.gz" target="_blank" rel="noopener">http://ftp.gnu.org/gnu/gcc/gcc-7.3.0.tar.gz</a></p><h2 id="解压">解压</h2><p>~$:tar xvf gcc-7.3.0.tar.gz<br>~$:sudo cp -r gcc-7.3.0 /usr/local/src/<br>~$:cd /usr/local/src/gcc-7.3.0/</p><h2 id="创建安装目录">创建安装目录</h2><p>~$:sudo mkdir /usr/local/gcc-7.3.0<br>~$:sudo mkdir /usr/local/src/gcc-7.3.0/build<br>~$:cd /usr/local/src/gcc-7.3.0/build</p><h2 id="配置">配置</h2><p>~$:sudo …/configure --prefix=/usr/local/gcc-7.3.0/ --enable-threads=posix --disable-multilib --enable-languages=c,c++<br>~$:sudo make -j8<br>~$:sudo make install</p><h2 id="修改gcc版本">修改gcc版本</h2><p>~$:sudo update-alternativess --install /usr/bin/cc cc /usr/local/gcc-4.6.0/bin/gcc-4.6 30<br>~$:sudo update-alternativess --install /usr/bin/c++ c++ /usr/local/gcc-4.6.0/bin/g+±4.6 30</p><p>~$:sudo update-alternativess --config cc</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;下载相应版本的安装包&quot;&gt;下载相应版本的安装包&lt;/h2&gt;
&lt;p&gt;国科大源：&lt;a href=&quot;https://mirrors.ustc.edu.cn/gnu/gcc/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://mirrors.us
      
    
    </summary>
    
      <category term="linux" scheme="http://mxxhcm.github.io/categories/linux/"/>
    
    
      <category term="gcc" scheme="http://mxxhcm.github.io/tags/gcc/"/>
    
      <category term="linux" scheme="http://mxxhcm.github.io/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>linux 扩展boot分区</title>
    <link href="http://mxxhcm.github.io/2019/05/04/linux-boot%E5%88%86%E5%8C%BA%E6%89%A9%E5%B1%95/"/>
    <id>http://mxxhcm.github.io/2019/05/04/linux-boot分区扩展/</id>
    <published>2019-05-04T04:06:18.000Z</published>
    <updated>2019-05-06T16:22:27.708Z</updated>
    
    <content type="html"><![CDATA[<p>扩展linux的/boot分区</p><h2 id="使用gpared或者fdisk创建一个新的partition">使用gpared或者fdisk创建一个新的partition</h2><h2 id="find-the-uuid-of-the-new-partition">find the uuid of the new partition</h2><p>使用命令<br>~$:ls -l /dev/disk/by-uuid/<br>获得分区的uuid<br>lrwxrwxrwx 1 root root 10 11月 24 14:34 19d6c114-8859-4209-aef9-60ee3cc108c1 -&gt; …/…/sda9<br>lrwxrwxrwx 1 root root 10 11月 24 14:34 1C48-1828 -&gt; …/…/sda2<br>lrwxrwxrwx 1 root root 10 11月 24 14:34 2840620D4061E254 -&gt; …/…/sda4<br>lrwxrwxrwx 1 root root 11 11月 24 14:34 66ab484d-0bbc-41cb-b2ca-8f436a330e2b -&gt; …/…/sda10<br>lrwxrwxrwx 1 root root 10 11月 24 14:34 71640978-4b7b-49aa-9a3e-ef22c994a183 -&gt; …/…/sda6<br>lrwxrwxrwx 1 root root 10 11月 24 14:34 8856E16256E1518C -&gt; …/…/sdb1<br>lrwxrwxrwx 1 root root 10 11月 24 14:34 99f1b75b-eb7b-41bb-9aa8-3c5ab2446f01 -&gt; …/…/sda7<br>lrwxrwxrwx 1 root root 10 11月 24 14:34 B4CEF361CEF31A76 -&gt; …/…/sdb2<br>lrwxrwxrwx 1 root root 10 11月 24 14:34 B836469636465592 -&gt; …/…/sda1<br>lrwxrwxrwx 1 root root 10 11月 24 14:34 C14D581BDA18EBFA -&gt; …/…/sda5<br>lrwxrwxrwx 1 root root 10 11月 24 14:34 e9b32a21-5e8a-4c53-9982-a31cd67c464e -&gt; …/…/sda8</p><h2 id="更新配置文件-etc-fstab">更新配置文件/etc/fstab</h2><p>通过改变uuid将/boot目录挂在到新的挂载点上<br>from<br>UUID=99f1b75b-eb7b-41bb-9aa8-3c5ab2446f01 /boot           ext4    defaults        0       2<br>to<br>UUID=66ab484d-0bbc-41cb-b2ca-8f436a330e2b /boot           ext4    defaults        0       2<br>here we can use the device name /dev/sda10 but it may change if we add some other devices, uuid is unique so that it won’t change.</p><h2 id="重启">重启</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;扩展linux的/boot分区&lt;/p&gt;
&lt;h2 id=&quot;使用gpared或者fdisk创建一个新的partition&quot;&gt;使用gpared或者fdisk创建一个新的partition&lt;/h2&gt;
&lt;h2 id=&quot;find-the-uuid-of-the-new-partitio
      
    
    </summary>
    
      <category term="linux" scheme="http://mxxhcm.github.io/categories/linux/"/>
    
    
      <category term="linux" scheme="http://mxxhcm.github.io/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>pytorch tensorflow常用函数对应</title>
    <link href="http://mxxhcm.github.io/2019/05/04/pytorch-tensorflow%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0%E5%AF%B9%E5%BA%94/"/>
    <id>http://mxxhcm.github.io/2019/05/04/pytorch-tensorflow常用函数对应/</id>
    <published>2019-05-04T02:39:44.000Z</published>
    <updated>2019-05-08T14:17:58.044Z</updated>
    
    <content type="html"><![CDATA[<h2 id="对应">对应</h2><table><thead><tr><th style="text-align:center">tensorflow</th><th style="text-align:center">pytorch</th></tr></thead><tbody><tr><td style="text-align:center">tensor.shape</td><td style="text-align:center">tensor.size()</td></tr><tr><td style="text-align:center"><a href="https://github.com/mxxhcm/myown_code/blob/master/tf/some_ops/tf_maximum.py" target="_blank" rel="noopener">tf.maximum</a></td><td style="text-align:center"><a href="https://github.com/mxxhcm/myown_code/blob/master/pytorch/pytorch_test/torch_max.py" target="_blank" rel="noopener">torch.max</a></td></tr><tr><td style="text-align:center"><a href="https://github.com/mxxhcm/myown_code/blob/master/tf/some_ops/tf_multinominal.py" target="_blank" rel="noopener">tf.multinomial</a></td><td style="text-align:center"><a href="https://github.com/mxxhcm/myown_code/blob/master/pytorch/pytorch_test/torch_distribution.py" target="_blank" rel="noopener">torch.distributions.Categorical</a></td></tr></tbody></table><h2 id="参考文献">参考文献</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;对应&quot;&gt;对应&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&quot;text-align:center&quot;&gt;tensorflow&lt;/th&gt;
&lt;th style=&quot;text-align:center&quot;&gt;pytorch&lt;/th&gt;
&lt;/tr&gt;
&lt;/th
      
    
    </summary>
    
      <category term="pytorch" scheme="http://mxxhcm.github.io/categories/pytorch/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/tags/tensorflow/"/>
    
      <category term="pytorch" scheme="http://mxxhcm.github.io/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>计算机硬件信息</title>
    <link href="http://mxxhcm.github.io/2019/05/01/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A1%AC%E4%BB%B6%E4%BF%A1%E6%81%AF/"/>
    <id>http://mxxhcm.github.io/2019/05/01/计算机硬件信息/</id>
    <published>2019-05-01T04:31:18.000Z</published>
    <updated>2019-05-06T16:22:27.712Z</updated>
    
    <content type="html"><![CDATA[<h2 id="ssd">SSD</h2><h3 id="物理接口">物理接口</h3><p>常见的物理接口，就是和主板相连的接口形状有SATA和M.2(NGFF)。<br>M.2接口也叫NGFF，有两种接口模式，socket2和socket3。socket2对应的接口是bkey，对应的是传输模式为SATA，对传输模式为SATA。而socket3对应的接口是mkey，走的是PCIE。</p><h3 id="总线-bus-方式-协议通道">总线(bus)方式（协议通道）</h3><p>目前市面上SSD的总线有两种类型PCI-E和SATA。<br>PCIE是用来取代SATA的新总线接口。PCIE总线的上层协议可以是NVME，也可以是ACHI。比如著名的sm951，既有NVME协议的也有ACHI协议的版本。[4]</p><h3 id="上层协议-逻辑设备接口标准">上层协议（逻辑设备接口标准）</h3><p>SSD的传输协议有NVME, IDE和AHCI。NVME是最新的高性能和优化协议，是用来取代AHCI的，NVME支持PCI-E，但是支持PCI-E的SSD不一定支持NVME协议。<br>NVME需要硬盘和主板M.2插槽都支持。<br>SATA采用AHCI协议，也支持IDE协议，是为寻道旋转磁盘而不是闪存设计的。</p><h3 id="总结">总结</h3><p>M.2是物理接口形式，SATA可以指的是接口，也可以指的是总线方式。M.2接口也可以走SATA总线，本质上还是sata硬盘，只不过用的是m.2的接口，只有走PCIE总线的使用Nvme协议的m.2的固态硬盘才是真正跟stata硬盘有区别的。[2]<br>如下图所示，是所有接口，<br><img src="/2019/05/01/计算机硬件信息/ssd.jpg" alt="ssd"><br>上图来源见参考文献[3]。</p><h2 id="cpu">CPU</h2><p>CPU后缀名字介绍</p><h3 id="笔记本后缀">笔记本后缀</h3><p>Y超低压处理器<br>U代表低电压<br>M代表标压<br>H高电压不可拆卸<br>X代表高性能<br>Q代表4核心至高性能处理器</p><h3 id="台式机后缀">台式机后缀</h3><p>X至高性能处理器<br>E嵌入式工程级处理器<br>S低电压处理器<br>K不锁倍频处理器<br>T超低电压处理器<br>P屏蔽集显处理器</p><h2 id="gpu">GPU</h2><p>显卡的话，好像也没啥要说的了。。。</p><h2 id="写在最后">写在最后</h2><p>好吧，看了很多电脑，神舟现在缩水很厉害，把蓝天的p系列模具的散热管去掉了很多。买了gx9之后，还是有点后悔，看上了蓝天准系统，但是太贵了，总共要13000了，i7-8700+ rtx2070，自己暂时也完全发挥不了它的性能。就先这样子把。以后有钱了再说～。</p><h2 id="参考文献">参考文献</h2><p>1.<a href="https://www.jianshu.com/p/6db2a47fdf60" target="_blank" rel="noopener">https://www.jianshu.com/p/6db2a47fdf60</a><br>2.<a href="https://www.zhihu.com/question/52811023/answer/132388287" target="_blank" rel="noopener">https://www.zhihu.com/question/52811023/answer/132388287</a><br>3.<a href="https://www.zhihu.com/question/52811023/answer/527580986" target="_blank" rel="noopener">https://www.zhihu.com/question/52811023/answer/527580986</a><br>4.<a href="https://www.zhihu.com/question/52811023/answer/132430870" target="_blank" rel="noopener">https://www.zhihu.com/question/52811023/answer/132430870</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;ssd&quot;&gt;SSD&lt;/h2&gt;
&lt;h3 id=&quot;物理接口&quot;&gt;物理接口&lt;/h3&gt;
&lt;p&gt;常见的物理接口，就是和主板相连的接口形状有SATA和M.2(NGFF)。&lt;br&gt;
M.2接口也叫NGFF，有两种接口模式，socket2和socket3。socket2对应的接口是b
      
    
    </summary>
    
      <category term="工具" scheme="http://mxxhcm.github.io/categories/%E5%B7%A5%E5%85%B7/"/>
    
    
      <category term="SSD" scheme="http://mxxhcm.github.io/tags/SSD/"/>
    
      <category term="CPU" scheme="http://mxxhcm.github.io/tags/CPU/"/>
    
      <category term="GPU" scheme="http://mxxhcm.github.io/tags/GPU/"/>
    
  </entry>
  
  <entry>
    <title>github .gitignore</title>
    <link href="http://mxxhcm.github.io/2019/04/29/git-gitignore/"/>
    <id>http://mxxhcm.github.io/2019/04/29/git-gitignore/</id>
    <published>2019-04-29T08:03:21.000Z</published>
    <updated>2019-06-26T09:12:16.934Z</updated>
    
    <content type="html"><![CDATA[<h2 id="gitignore介绍">gitignore介绍</h2><p>.gitignore是一个隐藏文件，用来指定push的时候忽略哪些文件和文件夹。<br>比如忽略所有的__pychche__文件夹<br>**/__pycache__/<br>这里一定要加上/否则就会把它当做一个文件来处理</p><h2 id="删除git服务器上已有的在-gitignore的文件">删除git服务器上已有的在.gitignore的文件</h2><p>但是.gitignore对于已经提交到git服务器的文件是无法删掉的，它在提交时只能忽略本地尚未同步到服务器的gitignore中出现的文件。<br>拿.idea举个例子。<br>在最开始的时候，没有写.gitignore文件，就把所有的python文件上传到了git，包括.idea文件，这时候，可以先在本地把.idea文件删了，然后commit一下，就把git上的.idea文件删了。这时候写.gitignore文件，以后就不会提交.idea文件了。<br>执行以下命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">find . -name '**idea' | xargs git rm -rf</span><br><span class="line"><span class="meta">#</span> 或者find . -name '*idea' | xargs git rm -rf</span><br><span class="line">git add .</span><br><span class="line">git commit -m "deleta *idea"</span><br><span class="line">git push</span><br></pre></td></tr></table></figure><p>这里首先使用find找到当前目录下所有.idea文件夹，然后使用管道命令将其删除，再提交到git。<br>接下来在.gitignore文件中添加一行：<br>**/.idea/<br>然后再次提交到git的时候就不会同步.idea文件了。</p><h2 id="加注释">加注释</h2><p>.gitignore文件的注释使用#号开头即可。</p><h2 id="参考文献">参考文献</h2><p>1.<a href="https://segmentfault.com/q/1010000000720031" target="_blank" rel="noopener">https://segmentfault.com/q/1010000000720031</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;gitignore介绍&quot;&gt;gitignore介绍&lt;/h2&gt;
&lt;p&gt;.gitignore是一个隐藏文件，用来指定push的时候忽略哪些文件和文件夹。&lt;br&gt;
比如忽略所有的__pychche__文件夹&lt;br&gt;
**/__pycache__/&lt;br&gt;
这里一定要加上/
      
    
    </summary>
    
      <category term="git" scheme="http://mxxhcm.github.io/categories/git/"/>
    
    
      <category term="git" scheme="http://mxxhcm.github.io/tags/git/"/>
    
  </entry>
  
  <entry>
    <title>reinforcement learning an introduction 第5章笔记</title>
    <link href="http://mxxhcm.github.io/2019/04/29/reinforcement-learning-an-introduction-%E7%AC%AC5%E7%AB%A0%E7%AC%94%E8%AE%B0/"/>
    <id>http://mxxhcm.github.io/2019/04/29/reinforcement-learning-an-introduction-第5章笔记/</id>
    <published>2019-04-29T07:53:02.000Z</published>
    <updated>2019-08-04T06:58:54.818Z</updated>
    
    <content type="html"><![CDATA[<h2 id="mc-methods">MC Methods</h2><p>这章主要介绍了MC算法，MC算法通过采样，估计state-value function或者action value function。为了找到最好的policy，需要让policy不断的进行探索，但是我们还需要找到最好的action，减少exploration。这两个要求是矛盾的，这一章主要介绍了两种方法来尽量满足这两个要求。一种是on-policy的方法，使用soft policy，即有一定概率随机选择action，其余情况下选择最好的action。这种情况下学习到的policy不是greedy的，同时也能进行一定的exploration。一种是off-policy的方法，这种方法使用两个不同的policy，一个用来采样的behaviour policy，一个用来评估的target policy。target policy是一个deterministic policy，而behaviour policy用来exploration。<br>MC方法通过采样估计值函数有三个优势，从真实experience中学习，从仿真环境中学习，以及每个state value的计算独立于其他state。<br>MC和DP不一样的是，它不需要环境的信息，只需要experience即可，不管是从真实交互还是从仿真环境中得到的state,action,reward序列都行。从真实交互中学习不需要环境的信息，从仿真环境中学习需要一个model，但是这个model只用于生成sample transition，并不需要像DP那样需要所有transition的完整概率分布。在很多情况下，生成experience sample要比显示的得到概率分布容易很多。<br>MC基于average sample returns估计值函数。为了保证returns是可用的，这里定义蒙特卡洛算法是episodic的，即所有的experience都有一个terminal state。只有在一个episode结束的时候，value estimate和policy才会改变。蒙塔卡洛算法可以在episode和episode实现增量式，不能在step和step之间实现增量式。(Monte Carlo methods can thus be incremental in an episode-by-episode sense, but not in a step-by-step online sense.)<br>在一个state采取action得到的return取决于同一个episode后续状态的action，因为所有的action都是在不断学习中采取，从早期state的角度来看，这个问题是non-stationary的。为了解决non-stationary问题，采用GPI中的idea。DP从已知的MDP中计算value function，蒙特卡洛使用MDP的sample returns学习value function。然后value function和对应的policy交互获得好的value和policy。<br>这一章就是把DP中的各种想法推广到了MC上，解决prediction和control问题，DP使用的是整个MDP，而MC使用的是MDP的采样。</p><h2 id="mc-prediction">MC Prediction</h2><p>Prediction problem就是估计value function，value function又分为state value function和action value function。这里会分别给出state value function和action value function的估计方法。</p><h3 id="state-value-function">State value function</h3><p>从state value function说起。最简单的想法就是使用experience估计value function，通过对每个state experience中return做个average。</p><h4 id="first-visti-mc-method">First visti MC method</h4><p>这里主要介绍两个算法，一个叫做first visit MC method，另一个是every visit MC method。比如要估计策略$\pi$下的$v(s)$，使用策略$\pi$采样一系列经过$s$的episodes，$s$在每一个episode中出现一次叫做一个visit，一个$s$可能在一个episode中出现多次。First visit就是只取第一次visit估计$v(s)$，every visit就是每一次visit都用。<br>下面给出first visit的算法：<br>算法1 <strong>First visit MC preidction</strong><br><strong>输入</strong> 被评估的policy $\pi$<br><strong>初始化</strong>:<br>$\qquad V(s)\in R,\forall s \in S$<br>$\qquad Returns(s) \leftarrow empty list,\forall s \in S$<br><strong>Loop</strong> for each episeode:<br>$\qquad$生成一个episode<br>$\qquad G\leftarrow 0$<br>$\qquad$<strong>Loop</strong> for each step, $t= T-1,T-2, \cdots, 1$<br>$\qquad\qquad G\leftarrow G + \gamma R_t$<br>$\qquad\qquad$ IF $S_t$没有在$S_0, \cdots , S_{t-1}$中出现过<br>$\qquad\qquad\qquad Returns(S_t).apppend(G)$<br>$\qquad\qquad\qquad V(S_t)\leftarrow average(Returns(S_t))$<br>$\qquad\qquad END IF$<br>Every visit算法的话，不用判断$S_t$是否出现。当$s$的visit趋于无穷的时候，first vist和every visit算法$v_{\pi}(s)$都能收敛。First visit中，每一个return都是$v_{\pi}(s)$的一个独立同分布估计。根据大数定律，估计平均值（$average(Returns(S_0),\cdots, average(Returns(S_t)$）的序列收敛于它的期望。每一个average都是它自己的一个无偏估计，标准差是$\frac{1}{\sqrt{n}}$。every visit的收敛更难直观的去理解，但是它二次收敛于$v_{\pi}(s)$。<br>补充一点：<br>大数定律：无论抽象分布如何，均值服从正态分布。<br>中心极限定理：样本大了，抽样分布近似于整体分布。</p><p>这里再次对比一下DP和MC，在扑克牌游戏中，我们知道环境的所有信息，但是我们不知道摸到下一张牌的概率，比如我们手里有很多牌了，我们知道下一张摸到什么牌会赢，但是我们不知道这件事发生的概率。使用MC可以采样获得，所以说，即使有时候知道环境信息，MC方法可能也比DP方法好。</p><h4 id="mc-backup-diagram">MC backup diagram</h4><p>能不能推广DP中的backup图到MC中？什么是backup图？backup图顶部是一个root节点，表示要被更新的节点，下面是所有的transitions，leaves是对于更新有用的reward或者estimated values。<br>MC中的backup图，root节点是一个state，下面是一个episode中的所有transtion轨迹，以terminal state为终止节点。DP backup diagram展示了所有可能的transitions，而MC backup diagram只展示了采样的那个episode；DP backup diagram只包含一步的transitions，而MC backup diagram包含一个episode的所有序列。<br><img src="/2019/04/29/reinforcement-learning-an-introduction-第5章笔记/" alt="mc backup"><br><img src="/2019/04/29/reinforcement-learning-an-introduction-第5章笔记/" alt="dp backup page 59"></p><h4 id="mc的特点">MC的特点</h4><p>DP中每个state的估计都依赖于它的后继state，而MC中每个state value的计算都不依赖于任何其他state value（MC算法不进行bootstrap），所以可以单独估计某一个state或者states的一个子集。而且估计单个state的计算复杂度和states的数量无关，我们可以只取感兴趣的states子集进行评估，这是MC的第三个优势。前两个优势是从actural experience中学习和从simulated的experience中学习。</p><h3 id="action-value-function">Action value function</h3><p>如果没有model的话，需要估计state-action value而不是state value。有model的话，只有state value就可以确定policy，选择使reward和next_state value加起来最大的action即可。没有model的话，只有state value是不够的，因为不知道下一个state是什么。而使用action value，就可以确定policy，选择$q$值最大的那个action value，取相应的action即可。<br>所以这一节的目标是学习action value function。有一个问题是许多state-action可能一次也没有被访问过，如果$\pi$是deterministic的，每一个state只输出一个action，其他action的MC估计没有returns进行平均，就无法进行更新。所以，我们需要估计每一个state对应的所有action，这是exploration问题。<br>对于action value的policy evaluation，必须保证continual exploration。一种实现方式是指定episode开始的state-action pair，每一个pair都有大于$0$的概率被选中,这就保证了每一个action-pair在无限个episode中会被访问无限次，这叫做exploring starts。这种假设有时候有用，但是在某些时候，我们无法控制环境产生的experience，可行的方法是使用stochastic policy。</p><h2 id="mc-control">MC Control</h2><p>MC control使用的还是GPI的想法，估计当前policy的action value，基于action value改进policy，不断迭代。考虑经典的policy iteration，执行一次完全的iterative policy evaluation，再执行一次完全的policy improvement，不断迭代。对于policy evaluation，每次evaluation都使用多个episodes的experience，每次action value都会离true value function更近。假设我们有无限个exploring starts生成的episodes，满足这些条件时，对于任意$\pi_k$都会精确计算出$q_{\pi_k}$。进行policy improvement时，只要对于当前的action value function进行贪心即可，即：<br>$$\pi(s) = arg\ max_a q(s,a)\tag{1}$$<br>第$4$章给出了证明，即policy improvement theorem。在每一轮improvement中，对所有的$s\in S$，执行：<br>\begin{align*}<br>q_{\pi_k}(s,\pi_{k+1}(s)) &amp;=q_{\pi_k}(s, argmax_a q_{\pi_k}(s,a))\\<br>&amp;=max_a q_{\pi_k}(s,a)\\<br>&amp;\ge q_{\pi_k}(s, \pi_k(s))\\<br>&amp;\ge v_{\pi_k}(s)\\<br>\end{align*}<br>MC算法的收敛保证需要满足两个假设，一个是exploring start，一个是policy evaluation需要无限个episode的experience。但是现实中，这两个条件是不可能满足的，我们需要替换掉这些条件近似接近最优解。</p><h3 id="mc-control-without-infinte-episodes">MC Control without infinte episodes</h3><p>无限个episodes的条件比较容易去掉，在DP方法中也有这些问题。在DP和MC任务中，都有两种方法去掉无限episode的限制，第一种方法是像iterative policy evaluation一样，规定一个误差的bound，在每一次evaluation迭代，逼近$q_{\pi_k}$，通过足够多的迭代确保误差小于bound，可能需要很多个episode才能达到这个bound。第二种是进行不完全的policy evaluation，和DP一样，使用小粒度的policy evaluation，可以只执行iterative policy evaluation的一次迭代，也可以执行一次单个state的improvement和evaluation。对于MC方法来说，很自然的就想到基于一个episode进行evaluation和improvement。每经历一个episode，执行该episode内相应state的evaluation和improvement。也就是说一个是规定每次迭代的bound，一个是规定每次迭代的次数。</p><h4 id="伪代码">伪代码</h4><p>算法2 <strong>First visit MCES</strong><br><strong>初始化</strong><br>$\qquad$任意初始化$\pi(s)\in A(s), \forall s\in S$<br>$\qquad$任意初始化$Q(s, a)\in R, \forall s\in S, \forall a \in A(s)$<br>$\qquad$Returns(s,a)$\leftarrow$ empty list, $\forall s\in S, \forall a \in A(s)$<br><strong>Loop forever(for each episode)</strong><br>$\qquad$随机选择满足$S_0\in S, A_0\in A(S_0)$的state-action$(S_0,A_0)$，满足概率大于$0$<br>$\qquad$从$S_0,A_0$生成策略$\pi$下的一个episode，$S_0,A_0,R_1,\cdots,S_{T-1},A_{T-1},R_T$<br>$\qquad G\leftarrow 0$<br>$\qquad$<strong>Loop for each step of episode</strong>,$t=T-1,T-2,\cdots,0$<br>$\qquad\qquad G\leftarrow \gamma G+R_{t+1}$<br>$\qquad\qquad$如果$S_t,A_t$没有在$S_0,A_0,\cdots, S_{t-1},A_{t-1}$中出现过<br>$\qquad\qquad\qquad$Returns($S_t,A_t$).append(G)<br>$\qquad\qquad\qquad Q(S_t,A_t) \leftarrow average(Returns(S_t, A_t)$<br>$\qquad\qquad\qquad \pi(S_t) \leftarrow argmax_a Q(S_t,a)$<br>这个算法一定会收敛到全局最优解，因为如果收敛到一个suboptimal policy，value function在迭代过程中会收敛到该policy的true value function，接下来的policy improvement会改进该suboptimal policy。</p><h2 id="on-policy-mc-control-without-es">On-policy MC Control without ES</h2><p>上节主要是去掉了无穷个episode的限制，这节需要去掉ES的限制，解决方法是需要agents一直能够去选择所有的actions。目前有两类方法实现，一种是on-policy，一种是off-policy。</p><h3 id="on-policy和off-policy">on-policy和off-policy</h3><p>On-policy算法中，用于evaluation或者improvement的policy和用于决策的policy是相同的，而off-policy算法中，evaluation和improvement的policy和决策的policy是不同的。</p><h3 id="varepsilon-soft和-varepsilon-greedy">$\varepsilon$ soft和$\varepsilon$ greedy</h3><p>在on-policy算法中，policy一般是soft的，整个policy整体上向一个deterministic policy偏移。<br>在$\varepsilon$ soft算法中，只要满足$\pi(a|s)\gt 0,\forall s\in S, a\in A$即可。<br>在$\varepsilon$ greedy算法中，用$\frac{\varepsilon}{|A(s)|}$的概率选择non-greedy的action，使用$1 -\varepsilon + \frac{\varepsilon}{|A(s)|}$的概率选择greedy的action。<br>$\varepsilon$ greedy是$\varepsilon$ soft算法中的一类，可以看成一种特殊的$\varepsilon$ soft算法。<br>本节介绍的on policy方法使用$\varepsilon$ greedy算法。</p><h3 id="on-policy-first-visit-mc">On-policy first visit MC</h3><p>本节介绍的on policy MC算法整体的思路还是GPI，首先使用first visit MC估计当前policy的action value function。去掉exploring starting条件之后，为了保证exploration，不能直接对所有的action value进行贪心，使用$\varepsilon$ greedy算法保持exploration。<br>算法3 <strong>On policy first visit MC Control</strong><br>$\varepsilon \gt 0$<br><strong>初始化</strong><br>$\qquad$用任意$\varepsilon$ soft算法初始化$\pi$<br>$\qquad$任意初始化$Q(s, a)\in R, \forall s\in S, \forall a \in A(s)$<br>$\qquad$Returns(s,a) $\leftarrow$ empty list, $\forall s\in S, \forall a \in A(s)$<br><strong>Loop forever(for each episode)</strong><br>$\qquad$根据policy $\pi$生成一个episode，$S_0,A_0,R_1,\cdots,S_{T-1},A_{T-1},R_T$<br>$\qquad G\leftarrow 0$<br>$\qquad$<strong>Loop for each step of episode</strong>,$t=T-1,T-2,\cdots,0$<br>$\qquad\qquad G\leftarrow \gamma G+R_{t+1}$<br>$\qquad\qquad$如果$S_t,A_t$没有在$S_0,A_0,\cdots, S_{t-1},A_{t-1}$中出现过<br>$\qquad\qquad\qquad$Returns($S_t,A_t$).append(G)<br>$\qquad\qquad\qquad Q(S_t,A_t) \leftarrow average(Returns(S_t, A_t)$<br>$\qquad\qquad\qquad A^{*}\leftarrow argmax_a Q(S_t,a)$<br>$\qquad\qquad\qquad$<strong>For all</strong> $a \in A(S_t) : $<br>$\qquad\qquad\qquad\qquad\pi(a|S_t)\leftarrow \begin{cases}1-\varepsilon+\frac{\varepsilon}{|A(S_t)|}\qquad if\ a = A^{*}\\ \frac{\varepsilon}{|A(S_t)|}\qquad a\neq A^{*}\end{cases}$</p><p>对于任意的$\varepsilon$ soft policy $\pi$，相对于$q_{\pi}$的$\varepsilon$ greedy算法至少和$\pi$一样好。用$\pi’$表示$\varepsilon$ greedy policy，对于$\forall s\in S$，都满足policy improvement theorem的条件：<br>\begin{align*}<br>q_{\pi}(s,\pi’(s))&amp;=\sum_a\pi’(a|s)q_{\pi}(s,a)\\<br>&amp;=\frac{\varepsilon}{|A(s)|} \sum_aq_{\pi}(s,a) + (1- \varepsilon) max_a q_{\pi}(s,a) \tag{2}\\<br>&amp;\ge \frac{\varepsilon}{|A(s)|} \sum_aq_{\pi}(s,a) + (1-\varepsilon) \sum_a\frac{\pi(a|s) - \frac{\varepsilon}{|A(s)|}}{1-\varepsilon}q_{\pi}(s,a) \tag{3}\\<br>&amp;=\frac{\varepsilon}{|A(s)|} \sum_aq_{\pi}(s,a) - \frac{\varepsilon}{|A(s)|} \sum_aq_{\pi}(s,a) + \sum_a \pi(a|s)\sum_aq_{\pi}(s,a)\\<br>&amp;=v(s)<br>\end{align*}<br>式子2到式子3是怎么变换的，我有点没看明白！！！（不懂）。后来终于想明白了，式子3的第二项分子服从的是$\pi(a|s)$，而式子2的第二项这个$a$是新的$\pi’(a|s)$。<br>接下来证明，当$\pi$和$\pi’$都是optimal $\varepsilon$ policy的时候，可以取到等号。这个我看这没什么意思，就不证明了。。在p102。</p><h2 id="off-policy-prediction-via-importance-sampling">Off-policy Prediction via Importance Sampling</h2><p>所有的control方法都要面临一个问题：一方面需要选择optimal的action估计action value，另一方面需要exploration，不能一直选择optimal action，那么该如何控制这两个问题之间的比重。on-policy方法采样的方法是学习一个接近但不是optimal的policy保持exploriation。off-policy的方法使用两个policy，一个用于采样的behavior policy，一个用于evaluation的target policy。用于学习target policy的data不是target policy自己产生的，所以叫做off-policy learning。</p><h3 id="on-policy-vs-off-policy">on-policy vs off-policy</h3><p>on policy更简单，off policy使用两个不同的policy，所以variance更大，收敛的更慢，但是off-policy效果更好，更通用。On-policy可以看成off-policy的特例，target policy和behaviour policy是相同的。Off-policy可以使用非学习出来的data，比如人工生成的data。</p><h3 id="off-policy-prediction-problem">off-policy prediction problem</h3><p>对于prediction problem，target policy和behaviour policy都是固定的。$\pi$是target policy，$b$是behaviour policy，我们要使用$b$生成的episode去估计$q_{\pi}$或者$v_{\pi}$。为了使用$b$生成的episodes估计$\pi$，需要满足一个假设，policy $\pi$中采取的action在$b$中也要能有概率被采取，即$\pi(a|s)\gt 0$表明$b(a|s) \gt 0$，这是coverage假设。<br>在control问题中，target policy通常是相对于当前action value的deterministic greedy policy，最后target policy是一个deterministic optimal policy而behaviour policy通常是$\varepsilon$ greedy的探索策略。</p><h3 id="importance-sampling和importance-sampling-ratio">importance sampling和importance sampling ratio</h3><p>很多off policy方法使用importance sampling，利用一个distribution的samples估计另一个distribution的value function。Importance sampling通过计算trajectoried在target和behaviour policy中出现的概率比值对returns进行加权，这个相对概率称为importance sampling ratio。给定以$S_t$为初始状态的sate-action trajectory，它在任何一个policy $\pi$中发生的概率如下：<br>\begin{align*}<br>&amp;Pr\{A_t, S_{t+1},A_{t+1},\cdots,S_T|A_{t:T-1}\sim \pi,S_t\}\\<br>=&amp;\pi(A_t|S_t)p(S_{t+1}|S_t,A_t)\pi(A_{t+1}|S_{t+1})\cdots p(S_T|S_{T-1},A_{T-1})\\<br>=&amp;\prod_{k=t}^{T-1}\pi(A_k|S_k)p(S_{k+1}|S_k,A_k)<br>\end{align*}<br>其中$p$是状态转换概率，imporrance sampling计算如下：<br>$$\rho_{t:T-1}=\frac{\prod_{k=t}^{T-1} \pi(A_k|S_k)p(S_{k+1}|S_k,A_k)}{\prod_{k=t}^{T-1} b(A_k|S_k)p(S_{k+1}|S_k,A_k)}=\prod_{k=t}^{T-1}\frac{\pi(A_k|S_k)}{b(A_k|S_k}\tag{2}$$<br>因为p跟policy无关，所以可以直接消去。importance sampling ratio只和policies以及sequences有关。<br>根据behaviour policy的returns $G_t$，我们可以得到一个Expectation，即$\mathbb{E}[G_t|S_t=s]=v_b(s)$，显然，这是b的value function而不是$\pi$的value function，这个时候就用到了importance sampling，ratio $\rho_{t:T-1}$对b的returns进行转换，得到了另一个期望：<br>$$\mathbb{E}[\rho_{t:T-1}G_t|S_t=s]=v_{\pi}(s)\tag{3}$$</p><h3 id="符号定义">符号定义</h3><p>假设我们想要从policy b 中的一些episodes中估计$v_{\pi}(s)$，</p><ul><li>用$t$表示episode中的每一步，有些不同的是，$t$在不同episode之间是连续的，比如第$1$个episode有$100$个timesteps，第$2$个episode的timsteps从$101$开始。</li><li>用$J(s)$表示state $s$在不同episodes中第一次出现的$t$。</li><li>用$T(t)$表示从$t$所在那个episode的terminal timestep。</li><li>用$\left\{G_t\right\}_{t\in J(s)}$表示所有state $s$的return list。</li><li>用$\left\{\rho_{t:T(t)-1}\right\}_{t\in J(s)}$表示相应的importance ratio。</li></ul><h3 id="importance-sampling">importance sampling</h3><p>有两种importance sampling方法估计$v_{\pi}(s)$，一种是oridinary importance sampling，一种是weighted importance sampling。</p><h4 id="oridinary-importance-sampling">oridinary importance sampling</h4><p>直接对多个结果进行平均<br>$$V(s) = \frac{\sum_{t\in J(s)}\rho_{t:T(t)-1} G_t}{|J(s)|}\tag{4}$$</p><h4 id="weighted-importance-sampling">weighted importance sampling</h4><p>对多个结果进行加权平均<br>$$V(s) = \frac{\sum_{t\in J(s)}\rho_{t:T(t)-1} G_t}{\sum_{t\in J(s)}\rho_{t:T(t)-1}}\tag{5}$$</p><h4 id="异同点">异同点</h4><p>为了比较这两种importance sampling的异同，考虑state s只有一个returns的first vist MC方法，在加权平均中，ratio会约分约掉，这个returns的expectation是$v_b(s)$而不是$v_{\pi}(s)$，是一个有偏估计；而普通平均，returns的expectation还是$v_{\pi}(s)$，是一个无偏估计，但是可能会很极端，比如ratio是$10$，就说明$v_{\pi}(s)$是$v_b(s)$的$10$倍，可能与实际相差很大。<br>在fisrt visit算法中，就偏差和方差来说。普通平均的偏差是无偏的，而加权平均的偏差是有偏的（逐渐趋向$0$）。普通平均的方差是unbounded，因为ratio可以是unbounded，而加权平均对于每一个returns来说，权重最大是$1$。事实上，假定returns是bounded，即使ratios的方差是infinite，加权平均的方差也会趋于$0$。实践中，加权平均有更小的方差，通常更多的被采用。<br>在every visit算法中，普通平均和加权平均都是有偏的，随着样本的增加，偏差也趋向于$0$。在实践中，因为every visit不需要记录哪个状态是否被记录过，所以要比first visit常用。</p><h3 id="无穷大方差">无穷大方差</h3><p><img src="/2019/04/29/reinforcement-learning-an-introduction-第5章笔记/figure_5_4.png" alt="example of oridinary importance ratio"><br>考虑一个例子。只有一个non-terminal state s，两个ation，left和right，right action是deterministic transition到termination，left action有$0.9$的概率回到s，有$0.1$的概率到termination。left action回到termination会产生$+1$的reward，其他操作的reward是$0$。所有target policy策略下的episodes都会经过一些次回到state s然后到达terminal state，总的returns是$1(\gamma = 1)$。使用behaviour policy等概率选择left和right action。<br>这个例子中returns的真实期望是$1$。first visit中weighted importance sampling中return的期望是$1$，因为behaviour policy中选择right的action 在target policy中概率为$0$，不满足之前假设的条件，所以没有影响。而oridinary importance sampling的returns期望也是$1$，但是可能经过了几百万个episodes之后，也不一定收敛到$1$。<br>接下来我们证明oridinary importance sampling中returns的variance是infinite。<br>$$Var(X) = \mathbb{E}\left[(X-\bar{X})^2\right] = \mathbb{E}\left[X^2-2\bar{X}X +\bar{x}^2\right]= \mathbb{E}\left[X^2\right]-\bar{X}^2 \tag{6}$$<br>如果mean是finite，只有当random variable的平方的Expectation为infinte时variance是infinte。所以，我们需要证明：<br>$$\mathbb{E}_b\left[\left(\prod_{t=0}^{T-1}\frac{\pi(A_t|S_t)}{b(A_t|S_t)}G_0\right)^2\right] \tag{7}$$<br>是infinte的。<br>这里我们按照一个episode一个episode的进行计算。但是需要注意的是，behaviour policy可以选择right action，而target policy只有left action，当behaviour policy选择right的话，ratio是$0$。我们只需要考虑那些一直选择left action回到state s，然后通过left action到达terminal state的episodes。按照下式计算期望，注意这个和上面用oridinary important ratio估计$v_{\pi}(s)$可不一样，上面是用采样估计$v_{\pi}(s)$，这个是计算真实的$v_{\pi}(s)$的期望，不对，是它的平方的期望。<br>\begin{align*}<br>\mathbb{E}_b\left[\left( \prod_{t=0}^{T-1}\frac{\pi(A_t|S_t)}{b(A_t|S_t)}G_0\right)^2\right] = &amp; \frac{1}{2}\cdot 0.1 \left(\frac{1}{0.5}\right)^2\tag{长度为1的episode}\\<br>&amp;+\frac{1}{2}\cdot 0.9\cdot\frac{1}{2}\cdot 0.1 \left(\frac{1}{0.5}\frac{1}{0.5}\right)^2\tag{长度为2的episode}\\<br>&amp;+\frac{1}{2}\cdot 0.9\cdot \frac{1}{2} \cdot 0.9 \frac{1}{2}\cdot 0.1 \left(\frac{1}{0.5}\frac{1}{0.5}\frac{1}{0.5}\right)^2\tag{长度为3的episode}\\<br>&amp;+ \cdots\\<br>=&amp;0.1 \sum_{k=0}^{\infty}0.9^k\cdot 2^k \cdot 2\\<br>=&amp;0.2 \sum_{k=0}^{\infty}1.8^k\\<br>=&amp;\infty \tag{8}\<br>\end{align*}</p><h3 id="incremental-implementation">Incremental Implementation</h3><p>Monte Carlo prediction可以增量式实现，用episode-by-episode bias。<br>在on-policy算法中，$V_t$的估计通过直接对多个episode的$G_t$进行平均得到。<br>$$V_n(s) = \frac{G_1 + G_2 + \cdots + G_{n-1}}{n - 1} \tag{9}$$<br>其中$V_n(s)$表示在第$n$个epsisode估计的state $s$的value function，$n-1$表示采样得到的总共$n-$个episode，$G_1$表示每个episode中第一次遇到$s$时的Return。<br>在第$n+1$个episodes估计$V(s)$时：<br>\begin{align*}<br>V_{n+1}(s) &amp;= \frac{G_1 + G_2 + \cdots + G_n}{n}\\<br>nV_{n+1}(s)&amp;= G_1 + G_2 + \cdots + G_{n - 1} + G_n\tag{上式两边同时乘上n}\\<br>(n-1)V_n(s)&amp;= G_1 + G_2 + \cdots + G_{n - 1}\tag{用n-1代替n}\\<br>nV_{n+1}(s)&amp;= G_1 + G_2 + \cdots + G_{n - 1} + G_n\tag{分解V_{n+1}(s)}\\<br>&amp;= (G_1 + G_2 + \cdots + G_{n - 1}) + G_n\\<br>&amp;= (n-1)V_n(s) + G_n\\<br>\frac{nV_{n+1}(s)}{n}&amp;= \frac{(n-1)V_n(s) + G_n}{n}\tag{上式两边同时除以n}\\<br>V_{n+1}(s)&amp;= \frac{(n-1)V_n(s) + G_n}{n}\\<br>&amp; = V_n(s) +\frac{G_n-V_n(s)}{n} \tag{10}<br>\end{align*}<br>这个更新规则的一般形式如下：<br>$$NewEstimate \leftarrow OldEstimate + StepSize \left[Target - OldEstimate\right] \tag{11}$$<br>表达式$\left[Target - OldEstimate\right]$是一个estimate error，通过向&quot;Target&quot;走一步减小error。这个&quot;Target&quot;给定了更新的方向，当然也有可能是noisy，在式子$10$中，target是第$n$个episode中state s的return。式子$10$的更新规则中StepSize$\frac{1}{n}$是在变的，一般我们叫它步长或者学习率，用$\alpha$表示。<br>在off-policy算法中，odrinary importance sampling和weighted importance sampling要分开。因为odirinary importance sampling只是对ratio缩放后的不同returns做了平均，还可以使用上面的公式。而对于weighted imporatance sampling，假设一系列episodes的returns是$G_1,G_2,\cdots, G_{n-1}$，对应的权重为$W_i$（比如$W_i=\rho_{t_i:T(t_i)-1}$），有：<br>$$V_n = \frac{\sum_{k=1}^{n-1}W_kG_k}{\sum_{k=1}^{n-1}W_k} \tag{11}$$<br>用$C_n$表示前$n$个episode returns的权重和，即$C_n=\sum_{k=1}^nW_k$，$V_n$的更新规则如下：<br>\begin{align*}<br>V_{n+1}&amp;=\frac{\sum_{k=1}^{n}W_kG_k}{\sum_{k=1}^{n}W_k}\\<br>&amp;=\frac{\sum_{k=1}^{n-1}W_kG_k + W_nG_n}{\sum_{k=1}^{n}W_k}\\<br>&amp;=\frac{1}{\sum_{k=1}^{n}W_k} \cdot \left(\sum_{k=1}^{n-1}W_kG_k + W_nG_n\right)\\<br>&amp;=\frac{1}{\sum_{k=1}^{n}W_k} \cdot \left(\frac{\sum_{k=1}^{n-1}W_kG_k}{\sum_{k=1}^{n-1}W_k}(\sum_{k=1}^{n-1}W_k) + W_nG_n\right)\\<br>&amp;=\frac{1}{\sum_{k=1}^{n}W_k} \cdot \left(V_n\cdot(\sum_{k=1}^{n-1}W_k) + W_nG_n\right)\\<br>&amp;=\frac{1}{\sum_{k=1}^{n}W_k} \cdot \left(V_n\cdot(\sum_{k=1}^{n-1}W_k + W_n - W_n) + W_nG_n\right)\\<br>&amp;=\frac{1}{\sum_{k=1}^{n}W_k} \cdot \left(V_n\cdot(\sum_{k=1}^{n}W_k - W_n) + W_nG_n\right)\\<br>&amp;=\frac{1}{\sum_{k=1}^{n}W_k} \cdot \left(V_n\cdot(\sum_{k=1}^{n}W_k) + W_nG_n - W_nV_n\right)\\<br>&amp;=\frac{V_n\cdot(\sum_{k=1}^{n}W_k)}{\sum_{k=1}^{n}W_k} + \frac{W_nG_n-W_nV_n}{\sum_{k=1}^{n}W_k}\\<br>&amp;=V_n + \frac{W_n}{C_n}(G_n-V_n)\\<br>\end{align*}<br>其中$C_0=0, C_{n+1} = C_n + W_{n+1}$，事实上，在$W_k=1$的情况下，即$\pi=b$时，上面的公式就变成了on-policy的公式。接下来给出一个episode-by-episode的MC  policy evaluation incremental algorithm，使用的是weighted importance sampling。</p><h3 id="off-policy-mc-prediction-算法">Off-policy MC Prediction 算法</h3><p>算法 4 Off-policy MC prediction(policy evaluation)<br>输入: 一个任意的target policy $\pi$<br>初始化，$Q(s,a)\in \mathbb{R}, C(s,a) = 0, \forall s\in S, a\in A(s)$<br><strong>Loop</strong> forever (for each episode)<br>$\qquad$$b\leftarrow$ 任意覆盖target policy $\pi$的behaviour policy<br>$\qquad$用behaviour policy $b$生成一个episode，$S_0,A_0,R_1,\cdots, S_{T-1},A_{T-1},R_T$<br>$\qquad$$G\leftarrow 0$<br>$\qquad$$W\leftarrow 1$<br>$\qquad$<strong>FOR</strong> $t \in T-1,T-2,\cdots, 0$并且$W\neq 0$<br>$\qquad\qquad$$G\leftarrow G+\gamma R_{t+1}$<br>$\qquad\qquad$$W\leftarrow = W\cdot \frac{\pi(A_t|S_t)}{b(A_t|S_t)}$！！！原书中这个是放在最后一行的，我怎么觉得应该放在这里。。<br>$\qquad\qquad$$C(S_t, A_t)\leftarrow C(S_t, A_t)+W$<br>$\qquad\qquad$$Q(S_t, A_t)\leftarrow Q(S_t, A_t)+ \frac{W}{C(S_t,A_t)}(G_t-Q(S_t,A_t))$<br>$\qquad$<strong>END FOR</strong><br><strong>思考：这里怎么把它转换为first-visit的算法</strong></p><h2 id="off-policy-mc-control">Off-policy MC Control</h2><p>这一节给出一个off-policy的MC control算法，target policy是greedy算法，而behaviour policy是soft算法，在不同的episode中可以采用不同的behaviour policy。<br>算法 5 Off-policy MC control<br>初始化，$Q(s,a)\in \mathbb{R}, C(s,a) = 0, \forall s\in S, a\in A(s), \pi(s)\leftarrow arg max_aQ(s, a)$<br><strong>Loop</strong> forever (for each episode)<br>$\qquad$$b\leftarrow$ 任意覆盖target policy $\pi$的behaviour policy<br>$\qquad$用behaviour policy $b$生成一个episode，$S_0,A_0,R_1,\cdots, S_{T-1},A_{T-1},R_T$<br>$\qquad$$G\leftarrow 0$<br>$\qquad$$W\leftarrow 1$<br>$\qquad$<strong>for</strong> $t \in T-1,T-2,\cdots, 0$并且$W\neq 0$<br>$\qquad\qquad$$G\leftarrow G+\gamma R_{t+1}$<br>$\qquad\qquad$$C(S_t, A_t)\leftarrow C(S_t, A_t)+W$<br>$\qquad\qquad$$Q(S_t, A_t)\leftarrow Q(S_t, A_t)+ \frac{W}{C(S_t,A_t)}(G_t-Q(S_t, A_t)$<br>$\qquad\qquad\pi(s)\leftarrow arg max_aQ(S_t,a)$<br>$\qquad\qquad$<strong>if</strong> $A_t\neq\pi(S_t)$ then<br>$\qquad\qquad\qquad$break for循环<br>$\qquad\qquad$<strong>end if</strong><br>$\qquad\qquad$$W\leftarrow = W\cdot \frac{1}{b(A_t|S_t)}$这个为什么放最后一行，我能理解要进行一下if判断，但是放在这里importance ratio不就不对了吗。。<br>$\qquad$<strong>end for</strong></p><h2 id="discounting-aware-importance-sampling">Discounting-aware Importance Sampling</h2><p>这一节介绍了discounting的importance sampling，假设有$100$个steps的一个episode，$\gamma=0$，其实它的returns在第一步以后就确定了，后面的$99$步已经没有影响了，因为$\gamma=0$，这里就介绍了discount importance sampling。<br>…</p><h2 id="per-decision-importance-sampling">Per-decision Importance Sampling</h2><p>根据每一个Reward确定进行importance sampling，而不是根据每一个returns。<br>…</p><h2 id="summary">Summary</h2><p>MC相对于DP的好处</p><ol><li>model-free</li><li>sample比较容易</li><li>很容易focus在一个我们需要的subset上</li><li>不进行bootstrap</li></ol><p>在MC control算法中，估计的是action-value fucntion，因为action value function能够在不知道model dynamic的情况下改进policy。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;mc-methods&quot;&gt;MC Methods&lt;/h2&gt;
&lt;p&gt;这章主要介绍了MC算法，MC算法通过采样，估计state-value function或者action value function。为了找到最好的policy，需要让policy不断的进行探索，但是我
      
    
    </summary>
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="蒙特卡洛" scheme="http://mxxhcm.github.io/tags/%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B/"/>
    
  </entry>
  
  <entry>
    <title>ubuntu NVIDIA 驱动安装</title>
    <link href="http://mxxhcm.github.io/2019/04/26/linux-NVIDIA%E9%A9%B1%E5%8A%A8%E5%AE%89%E8%A3%85/"/>
    <id>http://mxxhcm.github.io/2019/04/26/linux-NVIDIA驱动安装/</id>
    <published>2019-04-26T13:03:02.000Z</published>
    <updated>2019-08-05T13:43:42.559Z</updated>
    
    <content type="html"><![CDATA[<h2 id="方法1-命令行安装">方法1.命令行安装</h2><h3 id="步骤">步骤</h3><p>卸载原有驱动<br>~$:sudo apt purge nvidia*<br>禁用nouveau<br>~$:sudo vim /etc/modprobe.d/blacklist.conf<br>在文件最后添加<br>blacklist nouveau<br>更新内核<br>~$:sudo update-initramfs -u<br>使用如下命令，如果没有输出，即已经关闭了nouveau<br>~$:lsmod | grep nouveau<br>关闭X service<br>~$:sudo service lightdm stop<br>接下来执行如下语句即可：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install build-essential pkg-config xserver-xorg-dev linux-headers-`uname -r` sudo apt-get install mesa-common-dev</span><br><span class="line">sudo apt-get install freeglut3-dev</span><br><span class="line">sudo chmod a+x NVIDIA-Linux-x86_64-375.66.run</span><br><span class="line">sudo sh NVIDIA-Linux-x86_64-375.66.run -no-opengl-files</span><br><span class="line">sudo reboot</span><br></pre></td></tr></table></figure><h2 id="方法2-图形界面">方法2.图形界面</h2><h2 id="方法3-apt安装">方法3.apt安装</h2><h3 id="添加apt源">添加apt源</h3><p>~$:sudo add-apt-repository ppa:graphics-drivers/ppa<br>~$:sudo apt update</p><h3 id="apt安装">apt安装</h3><p>~$:sudo ubuntu-drivers devices<br>~$:sudo ubuntu-drivers autoinstall</p><!-- ### 更新grub~$:sudo vim /etc/default/grub将"splash"改为"splash acpi_osi=linux"~$:sudo update-grub--><h2 id="安装cuda-9-0">安装cuda 9.0</h2><p>到NVIDIA官网下载cuda 9.0的runfile，然后执行<br>~$:sudo sh cuda*.run</p><h3 id="测试报错">测试报错</h3><blockquote><p>Error: unsupported compiler: 7.4.0. Use --override to override this check.</p></blockquote><p>安装gcc低版本<br>~$:sudo apt install gcc-6</p><p>从CUDA 4.1版本开始，支持gcc 4.5。gcc 4.6和4.7不受支持。<br>从CUDA 5.0版本开始，支持gcc 4.6。gcc 4.7不受支持。<br>从CUDA 6.0版本开始，支持gcc 4.7。<br>从CUDA 7.0版本开始，支持gcc 4.8，在Ubuntu 14.04和Fedora 21上支持4.9。<br>从CUDA 7.5版开始，支持gcc 4.8，在Ubuntu 14.04和Fedora 21上支持4.9。<br>从CUDA 8版本开始，Ubuntu 16.06和Fedora 23支持gcc 5.3。<br>从CUDA 9版本开始，Ubuntu 16.04，Ubuntu 17.04和Fedora 25支持gcc 6。<br>使用update-alternatives修改默认gcc版本<br>~$:sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g+±6 50<br>~$:sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-6 50</p><p>然后继续安装：<br>~$:sudo sh cuda*.run</p><p>cuda安装在/usr/local/cuda-9.0 目录下<br>卸载的话进入/usr/loca/cuda-9.0/bin 找到uninstall_cuda_9.0.pl运行卸载。</p><h3 id="import-tensorflow-报错">import tensorflow 报错</h3><blockquote><p>ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory<br>Failed to load the native TensorFlow runtime.</p></blockquote><p>配置cuda环境变量<br>在bashrc文件中加入<br>export PATH=/usr/local/cuda/bin${PATH:+:${PATH}}<br>export LD_LIBRARY_PATH=/usr/local/cuda/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}<br>export CUDA_HOME=/usr/local/cuda<br>执行<br>~$:source ~/.bashrc</p><p>继续报错<br>然后我才发现我没有装cudnn，按照参考文献[1]安装cudnn即可。<br>解压cudnn<br>~$:tar -xvf cudnn-x.x-linuz-x64-vx.x.tar.gz<br>然后执行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo cp cuda/include/cudnn.h /usr/local/cuda/include/</span><br><span class="line">sudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64/</span><br><span class="line">sudo chmod a+r /usr/local/cuda/include/cudnn.h</span><br><span class="line">sudo chmod a+r /usr/local/cuda/lib64/libcudnn*</span><br></pre></td></tr></table></figure><p>即可</p><h2 id="版本对应">版本对应</h2><h3 id="显卡">显卡</h3><p>RTX 20系列显卡，需要使用cuda 10</p><h3 id="pytorch">pytorch</h3><p>而pytorch目前不支持cuda 10.1，所以只能使用cuda 10.0。</p><h3 id="tensorflow">tensorflow</h3><p>tensorflow 13.1 – cuda 10.0  – cudnn 7.3</p><h2 id="参考文献">参考文献</h2><p>1.<a href="http://gwang-cv.github.io/2017/07/26/Faster-RCNN+Ubuntu16.04+Titan%20XP+CUDA8.0+cudnn5.0/" target="_blank" rel="noopener">http://gwang-cv.github.io/2017/07/26/Faster-RCNN+Ubuntu16.04+Titan XP+CUDA8.0+cudnn5.0/</a><br>2.<a href="https://docs.nvidia.com/deeplearning/sdk/cudnn-install/index.html#axzz4qYJp45J2" target="_blank" rel="noopener">https://docs.nvidia.com/deeplearning/sdk/cudnn-install/index.html#axzz4qYJp45J2</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;方法1-命令行安装&quot;&gt;方法1.命令行安装&lt;/h2&gt;
&lt;h3 id=&quot;步骤&quot;&gt;步骤&lt;/h3&gt;
&lt;p&gt;卸载原有驱动&lt;br&gt;
~$:sudo apt purge nvidia*&lt;br&gt;
禁用nouveau&lt;br&gt;
~$:sudo vim /etc/modprobe.d/
      
    
    </summary>
    
      <category term="linux" scheme="http://mxxhcm.github.io/categories/linux/"/>
    
    
      <category term="linux" scheme="http://mxxhcm.github.io/tags/linux/"/>
    
      <category term="ubuntu" scheme="http://mxxhcm.github.io/tags/ubuntu/"/>
    
      <category term="显卡驱动" scheme="http://mxxhcm.github.io/tags/%E6%98%BE%E5%8D%A1%E9%A9%B1%E5%8A%A8/"/>
    
  </entry>
  
  <entry>
    <title>hexo常见问题（常见问题）</title>
    <link href="http://mxxhcm.github.io/2019/04/26/hexo-%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/"/>
    <id>http://mxxhcm.github.io/2019/04/26/hexo-常见问题/</id>
    <published>2019-04-26T12:36:32.000Z</published>
    <updated>2019-05-25T16:07:43.920Z</updated>
    
    <content type="html"><![CDATA[<h2 id="问题1">问题1</h2><p>Error: pandoc exited with code 7: pandoc: Unknown extension: smart</p><blockquote><p>INFO  Start processing<br>FATAL Something’s wrong. Maybe you can find the solution here: <a href="http://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">http://hexo.io/docs/troubleshooting.html</a><br>Error: pandoc exited with code 7: pandoc: Unknown extension: smart<br>at ChildProcess.<anonymous> (/home/mxxmhh/github/blog/node_modules/hexo-renderer-pandoc/index.js:94:20)<br>at emitTwo (events.js:126:13)<br>at ChildProcess.emit (events.js:214:7)<br>at maybeClose (internal/child_process.js:925:16)<br>at Socket.stream.socket.on (internal/child_process.js:346:11)<br>at emitOne (events.js:116:13)<br>at Socket.emit (events.js:211:7)<br>at Pipe._handle.close [as _onclose] (net.js:567:12)</anonymous></p></blockquote><h3 id="解决方法">解决方法</h3><p>卸载pandoc<br>~$:npm un hexo-renderer-pandoc --save</p><h2 id="问题2">问题2</h2><p>部分公式无法解析。<br>是因为markdown和mathjax的解析有一些冲突，按照参考文献$1$中进行修改即可，原因见[2]。<br>修改node_modules/kramed/lib/rules/inline.js文件，将第11行替换成&quot;escape: /^\([`*[]()#$+-.!_&gt;])/&quot;，将第19行替换成&quot;em: /<sup>\b_((?:__|[\s\S])+?)_\b|</sup>*((?😗*|[\s\S])+?)*(?!*)/&quot;。（不用加双引号）<br>第一次修改是去掉\的转义。<br>第二次修改是去掉下划线转义。</p><h2 id="问题3">问题3</h2><p>Ubuntu 16.04直接使用命令安装nodejs，版本太老，需要使用源代码安装<br>~$:sudo apt install nodejs npm<br>上述命令可以在Ubuntu 18.04直接使用。</p><h2 id="问题4">问题4</h2><p>昨天发现博客的一些公式不能渲染，刚开始的时候以为是没有修改kramed文件，就是和问题2一样，后来发现不是，是^符号后面没有设置好。比如</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$a^&#123;x+y&#125;b$</span><br></pre></td></tr></table></figure><p>应该写成</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$a^&#123;x+y&#125; b$</span><br></pre></td></tr></table></figure><p>或者</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$a\^&#123;x+y&#125;b$</span><br></pre></td></tr></table></figure><p>就是要多一个空格才行，否则就会解析错误。<br>还有就是\可能没转义成功，就用\\去代替\吧。</p><h2 id="参考文献">参考文献</h2><p>1.<a href="https://hexo-guide.readthedocs.io/zh_CN/latest/theme/%5BNexT%5D%E9%85%8D%E7%BD%AEMathJax.html" target="_blank" rel="noopener">https://hexo-guide.readthedocs.io/zh_CN/latest/theme/[NexT]配置MathJax.html</a><br>2.<a href="https://shomy.top/2016/10/22/hexo-markdown-mathjax/" target="_blank" rel="noopener">https://shomy.top/2016/10/22/hexo-markdown-mathjax/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;问题1&quot;&gt;问题1&lt;/h2&gt;
&lt;p&gt;Error: pandoc exited with code 7: pandoc: Unknown extension: smart&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;INFO  Start processing&lt;br&gt;
F
      
    
    </summary>
    
      <category term="hexo" scheme="http://mxxhcm.github.io/categories/hexo/"/>
    
    
      <category term="工具" scheme="http://mxxhcm.github.io/tags/%E5%B7%A5%E5%85%B7/"/>
    
      <category term="hexo" scheme="http://mxxhcm.github.io/tags/hexo/"/>
    
      <category term="常见问题" scheme="http://mxxhcm.github.io/tags/%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>hexo 安装</title>
    <link href="http://mxxhcm.github.io/2019/04/26/hexo-%E5%AE%89%E8%A3%85/"/>
    <id>http://mxxhcm.github.io/2019/04/26/hexo-安装/</id>
    <published>2019-04-26T10:56:46.000Z</published>
    <updated>2019-05-12T03:53:43.943Z</updated>
    
    <content type="html"><![CDATA[<h2 id="安装">安装</h2><h3 id="安装git">安装git</h3><p>~\$:sudo apt install git</p><h3 id="安装nodejs">安装nodejs</h3><h4 id="ubuntu-16-04安装">ubuntu 16.04安装</h4><p>注意在ubuntu 16.04安装的时候，一直报错，</p><blockquote><p>ERROR Local hexo not found in ~/mxxhcm/mxxhcm.github.io<br>ERROR Try running: ‘npm install hexo --save’</p></blockquote><p>其实就是安装的nodejs版本太老了。</p><p>在官网下载linux 64位nodejs安装包<br>解压之后放在/usr/local/nodejs目录下。<br>然后在PATH环境变量中添加/usr/local/nodejs/bin即可（在.bashrc文件中修改即可）。<br>使用以下命令查看nodejs版本<br>~\$:node -v</p><h4 id="ubuntu-18-04安装">ubuntu 18.04安装</h4><p>在ubuntu 18.04可以直接使用以下命令安装。<br>安装nodejs<br>~\$:sudo apt install nodejs<br>安装npm<br>~\$:sudo apt install npm</p><h3 id="安装hexo">安装hexo</h3><p>~\$:sudo npm install -g hexo-cli</p><h2 id="配置">配置</h2><p>以下二选一<br>创建文件夹<br>~\$:git clone your repo<br>或者直接<br>~\$:hexo init your repo</p><p>安装依赖包<br>~\$:npm install<br>解决问题<br>参见<a href="https://mxxhcm.github.io/2019/04/26/hexo-%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/">参考文献</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;安装&quot;&gt;安装&lt;/h2&gt;
&lt;h3 id=&quot;安装git&quot;&gt;安装git&lt;/h3&gt;
&lt;p&gt;~\$:sudo apt install git&lt;/p&gt;
&lt;h3 id=&quot;安装nodejs&quot;&gt;安装nodejs&lt;/h3&gt;
&lt;h4 id=&quot;ubuntu-16-04安装&quot;&gt;ubuntu
      
    
    </summary>
    
      <category term="hexo" scheme="http://mxxhcm.github.io/categories/hexo/"/>
    
    
      <category term="工具" scheme="http://mxxhcm.github.io/tags/%E5%B7%A5%E5%85%B7/"/>
    
      <category term="hexo" scheme="http://mxxhcm.github.io/tags/hexo/"/>
    
  </entry>
  
  <entry>
    <title>pytorch Module.children() vs Module.modules()</title>
    <link href="http://mxxhcm.github.io/2019/04/25/pytorch-Module-children-vs-Module-modules/"/>
    <id>http://mxxhcm.github.io/2019/04/25/pytorch-Module-children-vs-Module-modules/</id>
    <published>2019-04-25T13:06:46.000Z</published>
    <updated>2019-05-08T14:16:21.716Z</updated>
    
    <content type="html"><![CDATA[<h2 id="module-modules">Module.modules()</h2><p>modules()会返回所有的模块，包括它自己。<br>如下代码所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = nn.Sequential(nn.Linear(<span class="number">5</span>, <span class="number">3</span>), nn.Sequential(nn.Linear(<span class="number">3</span>, <span class="number">2</span>)))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> module <span class="keyword">in</span> model.modules():</span><br><span class="line">    print(module)</span><br></pre></td></tr></table></figure><p>输出如下：</p><blockquote><p>Sequential(<br>(0): Linear(in_features=5, out_features=3, bias=True)<br>(1): Sequential(<br>(0): Linear(in_features=3, out_features=2, bias=True)<br>)<br>)<br>Linear(in_features=5, out_features=3, bias=True)<br>Sequential(<br>(0): Linear(in_features=3, out_features=2, bias=True)<br>)<br>Linear(in_features=3, out_features=2, bias=True)</p></blockquote><p>可以看出来，上面总共含有四个modules。</p><h2 id="module-children">Module.children()</h2><p>而children()不会返回它自己。<br>如下代码所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = nn.Sequential(nn.Linear(<span class="number">5</span>, <span class="number">3</span>), nn.Sequential(nn.Linear(<span class="number">3</span>, <span class="number">2</span>)))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> child <span class="keyword">in</span> model.children():</span><br><span class="line">    print(child)</span><br></pre></td></tr></table></figure><p>输出如下：</p><blockquote><p>Linear(in_features=5, out_features=3, bias=True)<br>Sequential(<br>(0): Linear(in_features=3, out_features=2, bias=True)<br>)</p></blockquote><p>可以看出来，上面只给出了Sequential里面的modules。</p><h3 id="完整代码">完整代码</h3><p><a href="https://github.com/mxxhcm/myown_code/blob/master/pytorch/tutorials/module_vs_children.py" target="_blank" rel="noopener">https://github.com/mxxhcm/myown_code/blob/master/pytorch/tutorials/module_vs_children.py</a></p><h2 id="参考文献">参考文献</h2><p>1.<a href="https://discuss.pytorch.org/t/module-children-vs-module-modules/4551/2" target="_blank" rel="noopener">https://discuss.pytorch.org/t/module-children-vs-module-modules/4551/2</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;module-modules&quot;&gt;Module.modules()&lt;/h2&gt;
&lt;p&gt;modules()会返回所有的模块，包括它自己。&lt;br&gt;
如下代码所示：&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td cl
      
    
    </summary>
    
      <category term="pytorch" scheme="http://mxxhcm.github.io/categories/pytorch/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="pytorch" scheme="http://mxxhcm.github.io/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>python defaultdict</title>
    <link href="http://mxxhcm.github.io/2019/04/25/python-defaultdict/"/>
    <id>http://mxxhcm.github.io/2019/04/25/python-defaultdict/</id>
    <published>2019-04-25T02:24:36.000Z</published>
    <updated>2019-05-06T16:22:27.708Z</updated>
    
    <content type="html"><![CDATA[<h2 id="使用defaultdict创建字典的值默认类型">使用defaultdict创建字典的值默认类型</h2><h3 id="使用defaultdict创建值类型为dict的字典">使用defaultdict创建值类型为dict的字典</h3><p>如下示例</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"></span><br><span class="line">ddd = defaultdict(dict)</span><br><span class="line">print(ddd)</span><br><span class="line"></span><br><span class="line">m = ddd[<span class="string">'a'</span>]</span><br><span class="line">m[<span class="string">'step'</span>] = <span class="number">1</span></span><br><span class="line">m[<span class="string">'exp'</span>] = <span class="number">3</span></span><br><span class="line">print(type(m))</span><br><span class="line">print(ddd)</span><br><span class="line"></span><br><span class="line">m = ddd[<span class="string">'b'</span>]</span><br><span class="line">m[<span class="string">'step'</span>] = <span class="number">1</span></span><br><span class="line">m[<span class="string">'exp'</span>] = <span class="number">3</span></span><br><span class="line">print(ddd)</span><br></pre></td></tr></table></figure><p>上述代码创建了一个dict，dict的value类型还是一个dict</p><blockquote><p>defaultdict(class ‘dict’&amp;gt , {})<br>&amp;lt class ‘dict’&amp;gt<br>defaultdict(&amp;lt class ‘dict’&amp;gt , {‘a’: {‘step’: 1, ‘exp’: 3}})<br>defaultdict(&amp;lt class ‘dict’&amp;gt , {‘a’: {‘step’: 1, ‘exp’: 3}, ‘b’: {‘step’: 1, ‘exp’: 3}})</p></blockquote><h3 id="使用defaultdict创建值类型为list的dict">使用defaultdict创建值类型为list的dict</h3><p>如下示例</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"></span><br><span class="line">ddl = defaultdict(list)</span><br><span class="line">print(ddl)</span><br><span class="line">m = ddl[<span class="string">'a'</span>]</span><br><span class="line">print(type(m))</span><br><span class="line">m.append(<span class="number">3</span>)</span><br><span class="line">m.append(<span class="string">'hhhh'</span>)</span><br><span class="line">print(ddl)</span><br></pre></td></tr></table></figure><p>上述代码创建了一个dict，dict的value类型是一个list，输出如下</p><blockquote><p>defaultdict(&amp;lt class ‘list’&amp;gt , {})<br>&amp;lt class ‘list’&amp;gt<br>defaultdict(&amp;lt class ‘list’&amp;gt , {‘a’: [3, ‘hhhh’]})</p></blockquote><h3 id="代码">代码</h3><p>点击获得<a href="https://github.com/mxxhcm/myown_code/blob/master/tools/python/defaultdict_test.py" target="_blank" rel="noopener">完整代码</a></p><h2 id="参考文献">参考文献</h2><p>1.<a href="http://www.cnblogs.com/dancesir/p/8142775.html" target="_blank" rel="noopener">http://www.cnblogs.com/dancesir/p/8142775.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;使用defaultdict创建字典的值默认类型&quot;&gt;使用defaultdict创建字典的值默认类型&lt;/h2&gt;
&lt;h3 id=&quot;使用defaultdict创建值类型为dict的字典&quot;&gt;使用defaultdict创建值类型为dict的字典&lt;/h3&gt;
&lt;p&gt;如下示例&lt;/p
      
    
    </summary>
    
      <category term="python" scheme="http://mxxhcm.github.io/categories/python/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>hexo 博客迁移教程</title>
    <link href="http://mxxhcm.github.io/2019/04/23/hexo-blog%E8%BF%81%E7%A7%BB%E6%95%99%E7%A8%8B/"/>
    <id>http://mxxhcm.github.io/2019/04/23/hexo-blog迁移教程/</id>
    <published>2019-04-23T12:29:40.000Z</published>
    <updated>2019-05-12T03:53:50.935Z</updated>
    
    <content type="html"><![CDATA[<h2 id="hexo博客迁移">hexo博客迁移</h2><p>详细内容见参考文献</p><h2 id="参考文献">参考文献</h2><p>1.<a href="https://www.jianshu.com/p/fceaf373d797" target="_blank" rel="noopener">https://www.jianshu.com/p/fceaf373d797</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;hexo博客迁移&quot;&gt;hexo博客迁移&lt;/h2&gt;
&lt;p&gt;详细内容见参考文献&lt;/p&gt;
&lt;h2 id=&quot;参考文献&quot;&gt;参考文献&lt;/h2&gt;
&lt;p&gt;1.&lt;a href=&quot;https://www.jianshu.com/p/fceaf373d797&quot; target=&quot;_blan
      
    
    </summary>
    
      <category term="hexo" scheme="http://mxxhcm.github.io/categories/hexo/"/>
    
    
      <category term="工具" scheme="http://mxxhcm.github.io/tags/%E5%B7%A5%E5%85%B7/"/>
    
      <category term="hexo" scheme="http://mxxhcm.github.io/tags/hexo/"/>
    
  </entry>
  
  <entry>
    <title>python multiprocessing</title>
    <link href="http://mxxhcm.github.io/2019/04/23/python-multiprocessing-vs-threading/"/>
    <id>http://mxxhcm.github.io/2019/04/23/python-multiprocessing-vs-threading/</id>
    <published>2019-04-23T07:46:14.000Z</published>
    <updated>2019-07-25T06:31:58.572Z</updated>
    
    <content type="html"><![CDATA[<h2 id="multiprocessing-vs-multithread">multiprocessing vs multithread</h2><p>多个threads可以在一个process中。同一个process中的所有threads共享相同的memory。而不同的processes有不同的memory areas，每一个都有自己的variables，进程之间为了通信，需要使用其他的channels，比如files, pipes和sockets等。thread比process更容易创建和管理，thread之间的交流比processes之间的交流更快。<br>这一节首先介绍一些GIL，然后介绍两个python的package，一个是threading，一个是multiprocessing。threading主要提供了多线程的实现。multiprocessing 主要提供了多进程的实现，当然也有多线程实现。</p><h2 id="gil">GIL</h2><p>thread有一个东西，叫做GIL(Global Interpreter Lock)，阻止同一个process中不同threads的同时运行，所以python多线程并不是多线程。举个例子，如果你有8个cores，使用8个threads，CPU的利用率不会达到800%，也不会快8倍。它会使用100%CPU，速度和原来相同，甚至会更慢，因为需要对多个threads进行调度。当然，有一些例外，如果大量的计算不是使用python运行的，而是使用一些自定义的C code进行GIL handling，就会得到你想要的性能。对于网络服务器或者GUI应用来说，大部分的事件都在等待，而不是在计算，这个时候就可以使用多个thread，相当于把他们都放在后台运行，而不需要终止相应的主线程。<br>如果想用纯python代码进行大量的CPU计算，使用threads并不能起到什么作用。使用process就没有GIL的问题，每个process有自己的GIL。这个时候需要在多线程和多进程之间做个权衡，因为进程之间的通信比线程之间通信的代价大得多。</p><h2 id="cpython的gil实现">CPython的GIL实现</h2><p>CPython 2.7中GIL是这样一行代码：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">static</span> PyThread_type_lock interpreter_lock = <span class="number">0</span>; <span class="comment">/* This is the GIL */</span></span><br></pre></td></tr></table></figure><p>在Unix类系统中，PyThread_type_lock是标准的C lock mutex_t的别名。它的初始化方式如下：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">void</span></span><br><span class="line">PyEval_InitThreads(<span class="keyword">void</span>)</span><br><span class="line">&#123;</span><br><span class="line">    interpreter_lock = PyThread_allocate_lock();</span><br><span class="line">    PyThread_acquire_lock(interpreter_lock);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>解释器中执行python的C代码必须持有这个lock。GIL的作用就是让你的程序足够简单：一个thread执行python代码，其他N个thread sleep或者等待I/O。或者可以等待threading.Lock或者其他同步操作。<br>那么什么时候threads进程切换呢？当一个thread sleep或者等待I/O的时候，其他thread请求GIL，执行相应的代码。这种任务叫做cooperative multitasking。还有一种是preemptive multitasking：在python2中一个thread不间断的执行1000个bytecode，或者python3中不间断的执行15 ms，然后放弃GIL让另一个thread运行。接下来举两个例子。</p><h2 id="cooperative-multithread">cooperative multithread</h2><p>在网络I/O中，具有很强的不确定性，当一个thread请求网络I/O时，它释放GIL，这样子其他thread可以获得GIL继续执行，等到I/O完成时，该thread请求GIL继续执行。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">do_connect</span><span class="params">()</span>:</span></span><br><span class="line">    s = socket.socket()</span><br><span class="line">    s.connect((<span class="string">'python.org'</span>, <span class="number">80</span>))  <span class="comment"># drop the GIL</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">    t = threading.Thread(target=do_connect)</span><br><span class="line">    t.start()</span><br></pre></td></tr></table></figure><p>在上面的例子中，同一时刻只能有一个thread执行python代码，但是一旦thread开始connect，它就drop GIL，另一个thread可以执行。但是所有的threads都可以drop GIL，也就是多个thread可以一起并行的等待sockets连接。<br>具体python在connect socket的时候是怎么drop GIL的，我们可以看一下socketmodule的c代码：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* s.connect((host, port)) method */</span></span><br><span class="line"><span class="keyword">static</span> PyObject *</span><br><span class="line">sock_connect(PySocketSockObject *s, PyObject *addro)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">sock_addr_t</span> addrbuf;</span><br><span class="line">    <span class="keyword">int</span> addrlen;</span><br><span class="line">    <span class="keyword">int</span> res;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* convert (host, port) tuple to C address */</span></span><br><span class="line">    getsockaddrarg(s, addro, SAS2SA(&amp;addrbuf), &amp;addrlen);</span><br><span class="line"></span><br><span class="line">    Py_BEGIN_ALLOW_THREADS</span><br><span class="line">    res = connect(s-&gt;sock_fd, addr, addrlen);</span><br><span class="line">    Py_END_ALLOW_THREADS</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* error handling and so on .... */</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>其中Py_BEGIN_ALLOW_THREADS宏就是drop GIL，它的定义如下：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">PyThread_release_lock(interpreter_lock);</span><br></pre></td></tr></table></figure><p>同样，Py_END_ALLOW_THREADS宏是请求GIL。thread可以在这里block，等待GIL被释放，申请GIL继续执行。</p><h2 id="preemptive-multithread">preemptive multithread</h2><p>除了自动释放GIL外，还可以强制的释放GIL。python代码的执行有两步，第一步将python源代码编译成二进制的bytecode；第二步，python interpreter的main loop，一个叫做PyEval_EvalFrameEx()的函数，读取bytecode，并且一个一个的执行。<br>在多线程的模式下，interpreter强制周期性的drop GIL。如下所示，是thread判断是否释放GIl的代码：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (;;) &#123;</span><br><span class="line">    <span class="keyword">if</span> (--ticker &lt; <span class="number">0</span>) &#123;</span><br><span class="line">        ticker = check_interval;</span><br><span class="line">    </span><br><span class="line">        <span class="comment">/* Give another thread a chance */</span></span><br><span class="line">        PyThread_release_lock(interpreter_lock);</span><br><span class="line">    </span><br><span class="line">        <span class="comment">/* Other threads may run now */</span></span><br><span class="line">    </span><br><span class="line">        PyThread_acquire_lock(interpreter_lock, <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    bytecode = *next_instr++;</span><br><span class="line">    <span class="keyword">switch</span> (bytecode) &#123;</span><br><span class="line">        <span class="comment">/* execute the next instruction ... */</span> </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>默认设置下是1000个bytecode。所有的threads周期性的获取GIL，然后释放。在python3下，所有thread获得15ms的GIL，而不是1000个bytecode。</p><h2 id="python的thread-safety">python的thread safety</h2><p>但是，如果买票等之类的，必须保证操作的atomic，否则就会出现问题。对于sort() operation来说，它是atomic，所以无序担心。看下面一个code snippet</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">n = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">foo</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">global</span> n</span><br><span class="line">    n += <span class="number">1</span></span><br></pre></td></tr></table></figure><p>我们查看foo对应的bytecode：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> dis</span><br><span class="line"></span><br><span class="line">print(dis.dis(foo))</span><br><span class="line"></span><br><span class="line"><span class="comment">#   7           0 LOAD_GLOBAL              0 (n)</span></span><br><span class="line"><span class="comment">#               2 LOAD_CONST               1 (1)</span></span><br><span class="line"><span class="comment">#               4 INPLACE_ADD</span></span><br><span class="line"><span class="comment">#               6 STORE_GLOBAL             0 (n)</span></span><br><span class="line"><span class="comment">#               8 LOAD_CONST               0 (None)</span></span><br><span class="line"><span class="comment">#              10 RETURN_VALUE</span></span><br></pre></td></tr></table></figure><p>可以看出，foo有6个bytecode，如果在第三个bytecode处，强制释放了GIL锁，其他thread改了n的值，等到切回这个thread的时候，就会出错。。所以，为了保证不出问题，需要手动加一个lock，保证不会在这个时候释放GIL。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"></span><br><span class="line">n = <span class="number">0</span></span><br><span class="line">lock = threading.Lock()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">foo</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">global</span> n</span><br><span class="line">    <span class="keyword">with</span> lock:</span><br><span class="line">        n += <span class="number">1</span></span><br></pre></td></tr></table></figure><p>当然，如果operation本身就是atomic的话，就不需要了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">l = [<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">2</span>]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">foo</span><span class="params">()</span>:</span></span><br><span class="line">    l.sort()</span><br></pre></td></tr></table></figure><h2 id="threading">threading</h2><p>threading是python多线程的一个package。</p><h3 id="threading-thread">threading.Thread</h3><h4 id="代码示例">代码示例</h4><p><a href="thread_Thread.py">代码地址</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="threading-lock">threading.Lock</h3><h4 id="代码示例-v2">代码示例</h4><p><a href="threading_Lock.py">代码地址</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line">hhhh = <span class="number">100</span></span><br><span class="line">lock = threading.Lock()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_number</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">global</span> hhhh</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">        <span class="keyword">with</span> lock:</span><br><span class="line">            hhhh += <span class="number">1</span></span><br><span class="line">            print(<span class="string">"add: "</span>, hhhh)</span><br><span class="line">            time.sleep(<span class="number">0.015</span>)</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">subtract_number</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">global</span> hhhh</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">        <span class="keyword">with</span> lock:</span><br><span class="line">            hhhh -= <span class="number">1</span></span><br><span class="line">            print(<span class="string">"subtract:"</span>, hhhh)</span><br><span class="line">            time.sleep(<span class="number">0.015</span>)</span><br><span class="line"> </span><br><span class="line">job_list = []</span><br><span class="line">job_list.append(threading.Thread(target=subtract_number, args=()))</span><br><span class="line">job_list.append(threading.Thread(target=add_number, args=()))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> job_list:</span><br><span class="line">    t.start()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> job_list:</span><br><span class="line">    t.join()</span><br><span class="line"> </span><br><span class="line">print(<span class="string">"Done"</span>)</span><br></pre></td></tr></table></figure><h2 id="multiprocessing">multiprocessing</h2><h3 id="概述">概述</h3><p>方法| 并行|是否直接阻塞|目标函数|函数返回值|适用场景<br>–|--|–|--|–<br>mp.Pool.apply|否|是|只能有一个函数|函数返回值|<br>mp.Pool.apply_async|是|否，调用join()进行阻塞|可以相同可以不同|返回AysncResult对象|<br>mp.Pool.map|是|是|目标函数相同，参数不同|所有processes完成后直接返回有序结果|<br>mp.Pool.map_async|是|否，调用join()阻塞|不知道。。|返回AysncResult对象|<br>mp.Process|是|否|可以相同可以不同|无直接返回值|适用于线程数量比较小</p><p>mp.Pool适用于线程数量远大于cpu数量，mp.Process适用于线程数量小于或者等于cpu数量的场景。<br>mp.Pool.apply   适用于非并行，调用apply()直接阻塞，process执行结束后直接返回结果。<br>mp.Pool.apply_async 适用于并行，异步执行，目标函数可以相同可以不同，返回AysncResult对象，因为AsyncResult对象是有序的，所以调用get得到的结果也是有序的。调用join()进行阻塞，调用get()方法获得返回结果，get()方法也是阻塞方法。<br>mp.Pool.map     适用于并行，异步，目标函数相同，参数不同。调用map()函数直接阻塞，等待所有processes完成后直接返回有序结果。<br>mp.Pool.map_async   也是调用join()和get()都能阻塞。<br>mp.Process  适用于并行，异步，目标函数可以相同可以不同，返回的结果需要借助mp.Queue()等工具，mp.Queue()存储的结果是无序的，mp.Manager()存储的结果是有序的。无序的结果可以使用特殊方法进行排序。</p><h3 id="统计cpu数量">统计cpu数量</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cpus = mp.cpu_count()</span><br></pre></td></tr></table></figure><h3 id="实现并行的几种常用方法">实现并行的几种常用方法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 方式1</span></span><br><span class="line">pool.apply_async</span><br><span class="line"><span class="comment"># 方式2</span></span><br><span class="line">pool.map</span><br><span class="line"><span class="comment"># 方式3</span></span><br><span class="line">mp.Process</span><br></pre></td></tr></table></figure><h3 id="retrieve并行结果">retrieve并行结果</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 方式1</span></span><br><span class="line">results_obj = [pool.apply_async(f, args=(x,)) <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">3</span>)]</span><br><span class="line">results = [result_obj.get() <span class="keyword">for</span> result_obj <span class="keyword">in</span> results_obj]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 方式2</span></span><br><span class="line">results = pool.map(f, range(<span class="number">7</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 方式3</span></span><br><span class="line">output = Queue()</span><br><span class="line">pool.Process(target=f, args=(output))</span><br></pre></td></tr></table></figure><h2 id="mp-pool">mp.Pool</h2><h3 id="简介">简介</h3><p>指定占用的CPU核数，进程的个数可以多于CPU的核数，Pool会负责调用。如果CPU核数小于进程数，一般遵循FIFO的原则进行调用。</p><h3 id="api">API</h3><ul><li>Pool.apply,</li><li>Pool.apply_async,</li><li>Pool.map,</li><li>Pool.map_async。</li></ul><h4 id="python-apply">python apply</h4><p>在老版本的python中，调用具有任意参数的function要使用apply函数，</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">apply(f, args, kwargs)</span><br></pre></td></tr></table></figure><p>甚至在2.7版本中还存在apply函数，但是基本上不怎么用了，3版本中已经没有了这种形式，现在都是直接使用函数名：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">f(*args, **kwargs)</span><br></pre></td></tr></table></figure><h4 id="mp-pool-apply-vs-mp-pool-apply-async">mp.Pool.apply vs mp.Pool.apply_async</h4><p>multiprocessing.Pool中也有类似的interface。Pool.apply和python内置的apply挺像的，只不过Pool.apply会在一个单独的process执行，并且该函数会阻塞直到进程调用结束，所以Pool.apply不能异步执行。可以使用apply_async使用多个workers并行处理。<br>Pool.apply_async和apply基本一样，只不过它会在调用后立即返回一个AsyncResult对象，不用等到进程结束再返回。然后使用get()方法获得函数调用的返回值，get()方法会阻塞直到process结束。也就是说Pool.apply(func, args, kwargs)和pool.apply_async(func, args, kwargs).get()等价。Pool.apply_async可以调用很多个不同的函数。<br>Pool.apply_async返回值是无序的。</p><h4 id="mp-pool-map-vs-mp-pool-map-async">mp.Pool.map vs mp.Pool.map_async</h4><p>Pool.map应用于同一个函数的不同参数，它的返回值顺序和调用顺序是一致的。Pool.map(func, iterable)和Pool.map_async(func, iterable).get()是一样的。</p><h4 id="mp-pool-map-vs-mp-pool-apply">mp.Pool.map vs mp.Pool.apply</h4><p>Pool.apply(f, args): f函数仅仅被process pool中的一个worker执行。<br>Pool.map(f, iterable): 将iterable分割成多个单独的task，就是相当于同一个函数，给定不同的参数，每一组是一个task，然后使用pool中所有的processes执行这些taskes。所以map也能实现并行处理，而且是有序结果。</p><h4 id="mp-pool-map-vs-mp-pool-apply-async">mp.Pool.map vs mp.Pool.apply_async</h4><p>Pool.map返回的结果是有序的；<br>Pool.apply_async返回的结果是无序的。<br>Pool.map处理相同的函数，不同的参数；</p><blockquote><p>pool.map() is a completely different kind of animal, because it distributes a bunch of arguments to the same function (asynchronously), across the pool processes, and then waits until all function calls have completed before returning the list of results.<br>Pool.apply_async处理不同的参数。</p></blockquote><h3 id="retrieve-return-value">retrieve return value</h3><p>Pool.apply()会直接返回结果。<br>Pool.apply_async()会返回一个AsyncResult，然后使用get()方法获得结果。</p><h3 id="其他问题">其他问题</h3><p>pool.map传递多个参数，或者重复参数，使用他的另一个版本，pool.starmap()<br>如下示例，<a href>代码地址</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Pool</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> repeat</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(string, x)</span>:</span></span><br><span class="line">    print(string)</span><br><span class="line">    <span class="keyword">return</span> x*x</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    <span class="keyword">with</span> Pool(processes=<span class="number">4</span>) <span class="keyword">as</span> pool:</span><br><span class="line">        number = <span class="number">10</span></span><br><span class="line">        s = <span class="string">"hello"</span></span><br><span class="line">        print(pool.starmap(f, zip(repeat(s), range(number))))</span><br></pre></td></tr></table></figure><h3 id="使用流程">使用流程</h3><ol><li>创建Pool进程池，指定cpu核数<br>pool = Pool(cpu_core)</li><li>使用apply_async添加进程<br>processes = [p1, p2, p3]<br>results = []<br>for p in processes:<br>results.append(pool.apply_async(p, args=()))</li><li>关闭进程池<br>pool.close()</li><li>等待所有进程执行完毕<br>pool.join()</li><li>访问结果<br>for res in results:<br>print(res.get())</li></ol><h3 id="代码示例-v3">代码示例</h3><p><a href>代码地址</a></p><h2 id="mp-process">mp.Process</h2><h3 id="简介-v2">简介</h3><p>每个进程占用一个CPU核。</p><h3 id="retrieve结果">retrieve结果</h3><p>使用mp.Queue()或者mp.Pipe()等对象记录结果。Queue()不保证结果的顺序和task的执行顺序一致。</p><h3 id="使用流程-v2">使用流程</h3><h3 id="代码示例-v4">代码示例</h3><p><a href="Process.py">代码地址</a></p><h2 id="mp-pool-vs-mp-process">mp.Pool vs mp.Process</h2><ol><li>Pool会负责对cpu进行调度，即tasks数量可以远大于worker数量，一个worker占用一个cpu核。而Process的task必须小于worker，每个worker只能运行一个task。</li><li>如果执行多个task的时候，Process一定会使用多个seperate workes，但是对于Pool来说，可能会使用同一个worker去执行多个task。如下示例，p1和p2一定是两个wrokers运行两个process，而pool中，pool中有两个worker，foo可以是第一个worker也可以是第二个worker运行的process解决的，而bar也可以是这两个中任意一个worker解决的，这种情况发生在foo已经运行结束了，两个worker都是空闲的，给bar任意分配一个worker。</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">foo</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bar</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">p1 = Process(target=foo, args=())</span><br><span class="line">p2 = Process(target=bar, args=())</span><br><span class="line"></span><br><span class="line">p1.start()</span><br><span class="line">p2.start()</span><br><span class="line">p1.join()</span><br><span class="line">p2.join()</span><br><span class="line"></span><br><span class="line">pool = Pool(processes=<span class="number">2</span>)             </span><br><span class="line">r1 = pool.apply_async(foo)</span><br><span class="line">r2 = pool.apply_async(bar)</span><br></pre></td></tr></table></figure><h3 id="代码示例-v5">代码示例</h3><p><a href="Pool_Process.py">代码地址</a></p><h2 id="join方法">join方法</h2><h3 id="简介-v3">简介</h3><p>用来阻塞当前进程，直到该进程执行完毕，再继续执行后续代码。</p><h3 id="代码示例-v6">代码示例</h3><p><a href="https://github.com/mxxhcm/myown_code/blob/master/tools/py_process_thread/mp/mp_join.py" target="_blank" rel="noopener">代码地址</a><br>可以看出来，调用join()函数的时候，会等子进程执行完之后再继续执行；而不使用join()函数的话，在子进程开始执行的时候，就会继续向后执行了。</p><h2 id="参考文献">参考文献</h2><p>1.<a href="https://www.cnblogs.com/lipijin/p/3709903.html" target="_blank" rel="noopener">https://www.cnblogs.com/lipijin/p/3709903.html</a><br>2.<a href="https://www.ellicium.com/python-multiprocessing-pool-process/" target="_blank" rel="noopener">https://www.ellicium.com/python-multiprocessing-pool-process/</a><br>3.<a href="https://stackoverflow.com/questions/8533318/multiprocessing-pool-when-to-use-apply-apply-async-or-map" target="_blank" rel="noopener">https://stackoverflow.com/questions/8533318/multiprocessing-pool-when-to-use-apply-apply-async-or-map</a>&lt;mp Pool apply, apply_async, map用法&gt;<br>4.<a href="https://stackoverflow.com/questions/31711378/python-multiprocessing-how-to-know-to-use-pool-or-process" target="_blank" rel="noopener">https://stackoverflow.com/questions/31711378/python-multiprocessing-how-to-know-to-use-pool-or-process</a>&lt;mp Process和Pool.map获得不同目标函数process的结果，对mp.Process无序结果进行排序&gt;<br>5.<a href="https://stackoverflow.com/questions/18176178/python-multiprocessing-process-or-pool-for-what-i-am-doing" target="_blank" rel="noopener">https://stackoverflow.com/questions/18176178/python-multiprocessing-process-or-pool-for-what-i-am-doing</a>&lt;mp Pool.apply_async, Process不同函数的多process&gt;<br>6.<a href="https://stackoverflow.com/questions/10415028/how-can-i-recover-the-return-value-of-a-function-passed-to-multiprocessing-proce" target="_blank" rel="noopener">https://stackoverflow.com/questions/10415028/how-can-i-recover-the-return-value-of-a-function-passed-to-multiprocessing-proce</a>&lt;获得传递给mp Process函数返回值的方法&gt;<br>7.<a href="https://docs.python.org/3/library/multiprocessing.html#sharing-state-between-processes" target="_blank" rel="noopener">https://docs.python.org/3/library/multiprocessing.html#sharing-state-between-processes</a><br>8.<a href="https://sebastianraschka.com/Articles/2014_multiprocessing.html" target="_blank" rel="noopener">https://sebastianraschka.com/Articles/2014_multiprocessing.html</a><br>9.<a href="https://opensource.com/article/17/4/grok-gil" target="_blank" rel="noopener">https://opensource.com/article/17/4/grok-gil</a>&lt;GIL解释&gt;</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;multiprocessing-vs-multithread&quot;&gt;multiprocessing vs multithread&lt;/h2&gt;
&lt;p&gt;多个threads可以在一个process中。同一个process中的所有threads共享相同的memory。而不同的p
      
    
    </summary>
    
      <category term="python" scheme="http://mxxhcm.github.io/categories/python/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="Pool" scheme="http://mxxhcm.github.io/tags/Pool/"/>
    
      <category term="Process" scheme="http://mxxhcm.github.io/tags/Process/"/>
    
      <category term="multiprocessing" scheme="http://mxxhcm.github.io/tags/multiprocessing/"/>
    
      <category term="threading" scheme="http://mxxhcm.github.io/tags/threading/"/>
    
  </entry>
  
  <entry>
    <title>Asynchronous Methods for Deep Reinforcement Learning</title>
    <link href="http://mxxhcm.github.io/2019/04/19/a3c/"/>
    <id>http://mxxhcm.github.io/2019/04/19/a3c/</id>
    <published>2019-04-19T10:11:56.000Z</published>
    <updated>2019-05-31T13:29:13.132Z</updated>
    
    <content type="html"><![CDATA[<h2 id="摘要">摘要</h2><p>DQN使用experience replay buffer来稳定学习过程。本文提出一个异步框架来代替buffer，稳定学习过程。这个框架同时适用于on-policy和off-policy环境，也能应用于离散动作空间和连续的动作空间，既能训练前馈智能体，也能训练循环智能体。</p><h2 id="introduction">Introduction</h2><p>强化学习算法一般都是online的，而online学习是不稳定的，并且online更新通常都是强相关的。DQN通过引入experience replay buffer解决了这个问题，但是DQN只能应用在off policy算法上。DQN通过引入replay buffer取得了很大成功，但是replay buffer还有以下的几个缺点：</p><ul><li>在每一步交互的时候使用了更多的内存和计算资源</li><li>它只能应用在off policy的算法上，也就是说权重的更新可能会使用到很久之前的数据。</li></ul><p>这篇文章提出不使用replay buffer，而是使用异步的框架，同时在多个相同的环境中操作多个智能体（每个环境中一个智能体）并行的采集数据。这种并行性也能将智能体的数据分解成更稳定的过程（即和experience replay buffer起到了相同的作用），因为在给定的一个时间步，智能体可能会experience很多个不同的states。<br>这个框架既可以应用在on policy算法，如Sarsa，n-step methods和actor-critc等方法上，也可以应用在off policy算法如Q-learning上。</p><h2 id="异步框架">异步框架</h2><p>作者给出了一个框架，将够将on-policy search的actor-critic方法以及off-policy value-based的Q-learning方法都包括进去。<br>具体的，使用一台机器上的多CPU线程，这样子可以避免在不同机器上传递参数和梯度的消耗。然后，多个并行的actor-learner可能会探索环境的不同部分，每个actor-learner可以设置不同的exploration policy。不同的thread运行不同的exploration policy，多个actor-learner并行执行online update可能比单智能体更新在时间上更不相关。所以这里使用了不同的探索策略取代了DQN中buffer稳定学习过程的作用。<br>除了稳定学习过程之外，多个actor-learner还可以减少训练时间，此外，不使用buffer以后还可以使用on-policy的方法进行训练。</p><p><strong>总的来说，下面要介绍的四个算法，前面三个算法都使用了target network，第四个A3C算法没有使用target network。最重要的是所有四个算法都使用了多个actor-learner进行训练，并且使用累计的梯度进行更新（相当于batch的作用）。总共出现了三类参数，一类是network参数，一类是target network参数，一类是thread-specific（每个线程的参数）的参数。thread-specific参数是每个线程自己持有的，通过更新每个线程的参数更新network的参数，然后使用network的参数更新target network的参数，target network参数比network参数更新的要慢很多。<br>A3C算法的实质就是在多个线程中同步训练。分为主网络和线程中的网络，主网络不需要训练，主要用来存储和传递参数，每个线程中的网络用来训练参数。总的来说，多个线程同时训练提高了效率，另一方面，减小了数据之间的相关性，比如，线程$1$和$2$中都用主网络复制来的参数计算梯度，但是同一时刻只能有一个线程更新主网络的参数，比如线程$1$更新主网络的参数，那么线程$2$利用原来主网络参数计算的梯度会更新在线程$1$更新完之后的主网络参数上。</strong></p><h3 id="异步的one-step-q-learning">异步的one-step Q-learning</h3><ul><li>每个thread都和它自己的环境副本进行交互，在每一个时间步计算Q-learning loss的梯度。</li><li>通过使用不同的exploration策略，可以改进性能，这里实现exploration policy不同的方式就是使用$\epsilon$的不同取值实现。</li><li>使用一个共享的更新的比较缓慢的target network，就是和DQN中的target network一样。</li><li>同时也使用多个时间步上的累计梯度，和batch挺像的，这就减少了multi actor learner重写其他更新的可能性，同时也在计算效率和数据效率方面做了一个权衡。</li></ul><h4 id="伪代码">伪代码</h4><p><strong>Algorithm 1</strong> 异步的one-step Q-learning－－每个actor-learn线程的伪代码<br>用$\theta,\theta^{-}$表示全局共享参数，计数器$T=0$，<br>初始化线程时间步计数器$t\leftarrow 0$，<br>初始化target network权重$\theta^{-} \leftarrow 0$,<br>初始化network梯度$d\theta\leftarrow 0$，<br>初始化，得到初始状态$s$，<br><strong>repeat</strong><br>$\qquad$使用$\epsilon-$greedy策略采取action $a$，<br>$\qquad$接收下一个状态$s’$和reward $r$，<br>$\qquad$设置target value，$y=\begin{cases}r,&amp;for\ terminal\ s’ \\ r+\gamma max_{a’}Q(s’,a’;\theta^{-}), &amp;for\ non-terminal\ s’\end{cases}$<br>$\qquad$累计和$\theta$相关的梯度：$d\theta \leftarrow d\theta+\frac{\partial (y-Q(s,a;\theta))^2}{\partial \theta}$<br>$\qquad s\leftarrow s’$<br>$\qquad T\leftarrow T+1, t\leftarrow t+1$<br>$\qquad$<strong>if</strong> $T\ \ mod\ \ I_{target} ==0 $，那么<br>$\qquad\qquad$更新target network $\theta^{-}\leftarrow 0$<br>$\qquad$<strong>end if</strong><br>$\qquad$<strong>if</strong> $t\ \ mod\ \ I_{AsyncUpdate} ==0$或者$s$是terminal state，那么<br>$\qquad\qquad$使用$d\theta$异步更新$\theta$<br>$\qquad\qquad$将累计梯度$d\theta\leftarrow 0$<br>$\qquad$<strong>end if</strong><br><strong>until</strong> $T\ge T_{max}$</p><h3 id="异步的one-step-sarsa">异步的one-step Sarsa</h3><h4 id="概述">概述</h4><ul><li>和算法$1$很像，$Q-learning$计算target value使用$r+\gamma max_{a’}Q(s’,a’;\theta^{-})$，而Sarsa计算target value使用$r+\gamma Q(s’,a’;\theta^{-})$，即Q-learning的bahaviour policy和评估的策略是不一样的，而Sarsa的behaviour policy和评估策略是一样的。</li><li>使用target network，</li><li>同时使用多个时间步的累计梯度更新用来稳定学习过程。</li></ul><h4 id="伪代码-v2">伪代码</h4><p>和算法$1$很像。</p><h3 id="异步的n-step-q-learning">异步的n-step Q-learning</h3><h4 id="概述-v2">概述</h4><ul><li>计算$n-step$的return</li><li>在计算一次更新的时候，使用exploration policy采样到$t_{max}$步或者到terminal state。然后累加从上次更新到$t_{max}$时间步的reward。</li><li>然后计算$n-step$更新对于上次更新之后所有state-action的梯度。</li><li>使用单个时间步中的累计梯度进行更新。</li><li>使用了target network。</li></ul><h4 id="伪代码-v3">伪代码</h4><p><strong>Algorithm 2</strong> 异步的n-step Q-learning算法－－每个actor-learner线程的伪代码<br>用$\theta,\theta^{-}$表示全局共享的network参数和target network参数，用$T=0$表示全局共享计数器。<br>初始化线程步计数器$t\leftarrow 1$，<br>初始化target network参数$\theta^{-}\leftarrow \theta$<br>初始化每个线程的参数参数$\theta^{-}\leftarrow \theta$<br>初始化网络梯度$d\theta\leftarrow 0$<br><strong>repeat</strong><br>$\qquad$重置累计梯度$d\theta\leftarrow0$<br>$\qquad$同步每个线程的参数$\theta’=\theta$<br>$\qquad t_{start}=t$<br>$\qquad$得到$s_t$<br>$\qquad$<strong>repeat</strong><br>$\qquad\qquad$根据基于$Q(s_t,a;\theta’)$的$\epsilon-greedy$策略执行动作$a_t$，<br>$\qquad\qquad$接收下一个状态$s_{t+1}$和reward $r_t$，<br>$\qquad\qquad T\leftarrow T+1, t\leftarrow t+1$<br>$\qquad$ <strong>until</strong> terminal $s_t$或者$t-t_{start}==t_{max}$<br>$\qquad$设置奖励$R=\begin{cases}0,&amp;for\ terminal\ s_t\max_aQ(s_t,a;\theta^{-}), &amp;for\ non-terminal\ s_t\end{cases}$<br>$\qquad$<strong>for</strong> $i\in{t-1,\cdots,t_{start}}$ do<br>$\qquad\qquad R\leftarrow r_i+\gamma R$<br>$\qquad\qquad$累计和$\theta’$相关的梯度：$d\theta \leftarrow d\theta+\frac{\partial (R-Q(s_t,a;\theta’))^2}{\partial \theta’}$<br>$\qquad$<strong>end for</strong><br>$\qquad$使用$d\theta$异步更新$\theta$.<br>$\qquad$<strong>if</strong>$\quad T\quad mod\quad I_{target}==0$那么<br>$\qquad\qquad\theta^{-}\leftarrow \theta$<br>$\qquad$<strong>end if</strong><br><strong>until</strong> $T\gt T_{max}$</p><h3 id="异步的advantage-actor-critic">异步的advantage actor-critic</h3><h4 id="概述-v3">概述</h4><ul><li>A3C算法，是一个on-policy的actor-critic方法，使用值函数$V(s_t;\theta_v)$辅助学习policy $\pi(a_t|s_t;\theta)$，同时这里使用$n-step$的returns更新policy和value function。</li><li>每隔$t_{max}$个action更新一次或者到了terminal state更新一次。</li><li>Actor的更新方向为$\nabla_{\theta’}log\pi(a_t|s_t;\theta’)A(s_t,a_t;\theta,\theta_v)$，其中$A$是advantage function的一个估计，通过$\sum_{i=0}^{k-1} \gamma^ir_{t+i}+\gamma^kV(s_{t+k};\theta_v) - V(s_t;\theta_v)$计算。</li><li>这里同样使用并行的actor-learner和累计的梯度用来稳定学习。$\theta$和$\theta_v$在实现上通常共享参数。</li><li>添加entropy正则项鼓励exploration。包含了正则化项的的objective function的梯度为$\nabla_{\theta’}log\pi(a_t|s_t;\theta’)(R_t-V(s_t;\theta_v))+\beta\nabla_{\theta’}H(\pi(s_t;\theta’))$。这里的$R$就是上面的$\sum_{i=0}^{k-1}\gamma^ir_{t+i}+\gamma^kV(s_{t+k};\theta_v) - V(s_t;\theta_v)$。</li><li>Critic的更新方向通过最小化loss来实现，这里的loss指的是TD-error，即$\sum_{i=0}^{k-1}\gamma^ir_{t+i} + \gamma^kV(s_{t+k};\theta_v) - V(s_t;\theta_v)$。</li><li>没有使用target network。</li></ul><h4 id="伪代码-v4">伪代码</h4><p><strong>Algorithm 3</strong> A3C－－每个actor-learn线程的伪代码<br>用$\theta,\theta_v$表示全局共享参数，用$T=0$表示全局共享计数器，<br>用$\theta’,\theta’_v$表示每个线程中的参数<br>初始化线程步计数器$t\leftarrow 1$，<br><strong>repeat</strong><br>$\qquad$重置梯度$d\theta\leftarrow 0,d\theta_v\leftarrow 0$，<br>$\qquad$同步线程参数$\theta’=\theta,\theta’_v=\theta_v$<br>$\qquad t_{start}=t$<br>$\qquad$得到状态$s_t$，<br>$\qquad$<strong>repeat</strong><br>$\qquad\qquad$根据策略$\pi(a_t|s_t;\theta’)$执行动作$a_t$，<br>$\qquad\qquad$接收下一个状态$s_{t+1}$和reward $r_t$，<br>$\qquad\qquad T\leftarrow T+1, t\leftarrow t+1$<br>$\qquad$ <strong>until</strong> terminal $s_t$或者$t-t_{start}==t_{max}$<br>$\qquad$设置奖励$R=\begin{cases}0,&amp;for\ terminal\ s_t\\ V(s_t,\theta’_v), &amp;for\ non-terminal\ s_t\end{cases}$<br>$\qquad$<strong>for</strong> $i\in{t-1,\cdots,t_{start}}$ do<br>$\qquad\qquad R\leftarrow r_i+\gamma R$<br>$\qquad\qquad$累计和$\theta’$相关的梯度：$d\theta \leftarrow d\theta+\frac{\partial (y-Q(s,a;\theta))^2}{\partial \theta}$<br>$\qquad\qquad$累计和$\theta’_v$相关的梯度：$d\theta_v \leftarrow d\theta_v+\frac{\partial (R-V(s_i;\theta’_v))^2}{\partial \theta’_v}$<br>$\qquad$<strong>end for</strong><br>$\qquad$使用$d\theta$异步更新$\theta$，使用$d\theta_v$异步更新$\theta_v$.<br><strong>until</strong> $T\ge T_{max}$</p><h3 id="优化方法">优化方法</h3><p>作者尝试了三种不同的优化方法，带有momentum的SGD，带有共享statistics的RMSProp以及不带shared statistics的RMSProp。</p><h2 id="实验">实验</h2><h3 id="优化细节">优化细节</h3><p>作者在异步框架中测试了两个优化算法SGD和RMSProp，并且因为效率原因没有使用线程锁。</p><h3 id="设置">设置</h3><ul><li>Atari环境中，每个实验使用$16$个actor-learner线程。</li><li>所有方法都每隔$5$个actions更新一次，并且使用共享的RMSProp进行优化。</li><li>三个异步的value-based算法使用每隔$40000$帧更新的共享target network，</li><li>使用了DQN中action repeat of $4$.</li><li>网络架构和DQN一样</li><li>基于值的方法只有一个线性输出层，每个输出单元代表一个action的值。</li><li>actor-critic方法有两个输出层，一个softmax表示选择某一个action的概率，一个线性输出代表值函数。</li><li>所有实验使用的$\gamma=0.99$，RMSProp的衰减因子$\alpha = 0.99$。</li><li>Value-based方法采用的exploration rate $\epsilon$有三个取值$\epsilon_1,\epsilon_2,\epsilon_3$，相应的概率为$0.4,0.3,0.3$，它们的值在前$4$百万帧中从$1$退火到$0.1,0.01,0.5$。</li><li>A3C使用了entropy进行正则化，entropy项的权重为$\beta=0.01$</li><li>初始学习率从分布$LogUniform(10^{-4},10^{-2})$中进行采样，在训练过程中退火到$0$。</li></ul><h2 id="代码">代码</h2><h3 id="代码地址">代码地址</h3><p><a href="https://github.com/ikostrikov/pytorch-a3c" target="_blank" rel="noopener">https://github.com/ikostrikov/pytorch-a3c</a></p><h3 id="问题">问题</h3><p>如果直接git下来运行的话，会出问题，需要在main()下加上这样一句</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mp.set_start_method(<span class="string">"forkserver"</span>)</span><br></pre></td></tr></table></figure><p>可能是因为Unix系统默认的多进程方式是fork，这里只要不设置为fork,设置为其他两种方式spawn, forkserver都行。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;摘要&quot;&gt;摘要&lt;/h2&gt;
&lt;p&gt;DQN使用experience replay buffer来稳定学习过程。本文提出一个异步框架来代替buffer，稳定学习过程。这个框架同时适用于on-policy和off-policy环境，也能应用于离散动作空间和连续的动作空间，既
      
    
    </summary>
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="A3C" scheme="http://mxxhcm.github.io/tags/A3C/"/>
    
  </entry>
  
  <entry>
    <title>Distral Robust Multitask Reinforcement Learning</title>
    <link href="http://mxxhcm.github.io/2019/04/18/distral/"/>
    <id>http://mxxhcm.github.io/2019/04/18/distral/</id>
    <published>2019-04-18T03:04:22.000Z</published>
    <updated>2019-05-17T00:57:01.586Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章给出了多任务学习和迁移学习的一个框架。</p><h2 id="摘要">摘要</h2><p>使用共享网络参数的multitas learning，然后在不同的任务之间迁移可以提高效率。但是这种方法有几个缺点：</p><ul><li>不同任务的gradient可能相互干扰，让学习变得不稳定，甚至很低效。</li><li>另一个问题是不同任务的reward scheme不同，可能会让其中的某一个reward占主导地位。</li></ul><p>本文提出了Distral方法，并不是使用共享参数，而是使用一个提炼的policy去学习不同任务的公共行为。每一个worker解决它自己的任务，但是需要加一个限制条件，每一个单独的策略需要离提炼出的共享策略足够近，而提炼的共享策略需要在所有单独策略的质心上，通过优化一个联合的目标函数，这两个过程都可以实现。</p><h2 id="introduction">Introduction</h2><p>由于DRL需要的训练时间和训练数据很多，现在的DRL问题逐渐向单独的智能体（同时或者连续的）解决多个相关问题移动。由于巨大的计算开销，这个方向的研究需要设计出非常鲁棒的，与具体任务无关的算法。直观上来说，因为不同的相关任务有共同的结构，所以我们觉得它们一块应该能够促进学习，然而事实上，在实践中我们并不能总是得到这个结论。所以，multitask和transfer learning可以需要解决一个问题，在多个任务上训练会对单个任务的训练产生负面影响。所以就需要使用一些方法来结局这个问题。事实上，其他任务的梯度可能会被当做做一年，干扰学习，甚至极端情况下，其中一个任务可能会主导其他的任务。<br>在这篇论文中，作者提出了一种multitask和transfer RL算法，给出一些算法实例，它能够高效的跨任务共享behaviour structure。除了在grid world领域的一些指导性illustration(例证)，作者还在DeepMind Lab $3D$环境中详细分析了最终的算法和A3C baseline的比较。作者验证了Distral算法学习的很快，而且能够达到很好的收敛性能，而且对超参数很鲁棒，比multitask A3C baselines要稳定的多。</p><h2 id="distral-distill-and-transfer-learning">Distral: Distill and Transfer Learning</h2><p>作者给出了一个同时训练多个任务的框架，叫做Distral。如下图所示，作者给出了从四个任务中提取shared policy的一个例子。该方法用一个shared policy去提取task-specific的polices之间的common behaviour和representation，然后又用这个shared policy使用KL散度去正则化task-specific polices。这里使用KL散度的作用相当于shaping reward，鼓励exploration。最后，这些相关任务中的common knowledge都被提炼到shared policy中去了，然后可以迁移到其他任务中去。<br><img src="/2019/04/18/distral/figure1.png" alt="figure1"></p><h3 id="数学框架">数学框架</h3><p>一个multitask RL任务中，假设有$n$个任务，折扣因子为$\gamma$，它们的state space和action space是相同的，但是每个任务$i$的状态转换概率$p_i(s’|s,a)$和奖励函数$R_i(s,a)$是不同的，用$\pi_i$表示第$i$个任务的stochastic polices。给定从一些初始状态开始的state和action联合分布的轨迹，用$\pi_i$表示dynamics和polices。<br>作者通过优化一个expected return和policy regularization组成的目标函数将学习不同任务的policy联系起来。用$\pi_0$表示要提取的shared policy，然后通过使用$\pi_0$和$\pi_i$的KL散度$\mathbb{E}_{\pi_i}\left[\sum_{t\ge 0}\gamma^tlog\frac{\pi_i(a_t|s_t)}{\pi_0(s_t|a_t)}\right]$ 正则化使所有的策略 $\pi_i$ 向 $\pi_0$ 移动。此外，作者还使用了一个带折扣因子的entropy正则化项鼓励exploration。系统越混乱，entropy越大，所以exploration越多，采取的动作越随机，entropy就越大。最后总的优化目标就变成了：<br>\begin{align*}<br>J(\pi_0, {\pi_i}_{i=1}^n) &amp;=\sum_i\mathbb{E}_{\pi_i}\left[\sum_{t\ge 0}\gamma^tR_i(s_t,a_t) -c_{KL}\gamma^t log\frac{\pi_i(a_t|s_t)}{\pi_0(a_t|s_t)}-c_{Ent}\gamma^t log\pi_i(a_t|s_t)\right]\\<br>&amp;=\sum_i\mathbb{E}_{\pi_i}\left[\sum_{t\ge 0}\gamma^tR_i(s_t,a_t) - c_{KL}\gamma^tlog{\pi_i(a_t|s_t)} + c_{KL}\gamma^tlog{\pi_0(a_t|s_t)} - c_{Ent}\gamma^tlog\pi_i(a_t|s_t)\right]\\<br>&amp;=\sum_i\mathbb{E}_{\pi_i}\left[\sum_{t\ge 0}\gamma^tR_i(s_t,a_t) + c_{KL}\gamma^tlog{\pi_0(a_t|s_t)} - (c_{Ent}\gamma^t + c_{KL}\gamma^t)log\pi_i(a_t|s_t)\right]\\<br>&amp;=\sum_i\mathbb{E}_{\pi_i}\left[\sum_{t\ge 0}\gamma^tR_i(s_t,a_t) +\frac{\gamma^t \alpha}{\beta}log{\pi_0(a_t|s_t)}-\frac{\gamma^t}{\beta}log\pi_i(a_t|s_t)\right], \tag{1}<br>\end{align*}<br>其中$c_{KL},c_{Ent}\ge 0$是控制KL散度正则化项和entropy正则化项大小的超参数，$\alpha = \frac{c_{KL}}{c_{KL}+c_{Ent}},\beta = \frac{1}{c_{KL}+c_{Ent}}。log\pi_0(a_t|s_t)$可以看成reward shaping，鼓励大概率的action；而entropy项$-log\pi_i(a_t|s_t)$鼓励exploration。在这个公式中，设置所有任务的正则化系数$c_{KL}$和$c_{Ent}$都是相同的，如果不同任务的reward scale不同，可以根据具体情况给相应任务设定相应系数。</p><h3 id="soft-q-learing">Soft Q-Learing</h3><p>这一节在表格形式的情况下使用和EM算法类似的策略优化目标函数－－固定$\pi_0$优化$\pi_i$，固定$\pi_i$然后优化$\pi_0$。当$\pi_0$固定的时候，式子(1)可以分解成每个任务的最大化问题，即优化每个任务的entropy正则化return，return使用的是正则化reward $R’_i(s,a) = R_i(s,a) + \frac{\alpha}{\beta}log\pi_0(a|s)$，正则化后的return可以使用G-learning来优化（这里说的应该是原来的return和R什么都没，这里都加上了正则化）。根据Soft Q-Learning(G-learning)的证明，给定$\pi_0$，我们能得到以下的关系：<br>$$\pi_i(a_t|s_t) = \pi_0^{\alpha} (a_t|s_t)e^{\beta Q_i(a_t|s_t)-\beta V(s_t)} = \pi_0^{\alpha} (a_t|s_t)e^{\beta A_i(a_t|s_t)} \tag{2}$$<br>其中$A_i(s,a) = Q_i(s,a)-V_i(s)$是advantage function，$\pi_0$可以看成是一个policy prior，<strong>需要注意的是这里多了一个指数$\alpha \lt 1$，这是多出来的entropy项的影响，soften了$\pi_0$对$\pi_i$的影响</strong>。$V$和$Q$是新定义的一种state value和action value，使用推导的softened Bellman公式更新：<br>$$V_i(s_t) = \frac{1}{\beta} log\sum_{a_t}\pi_0^{\alpha} (a_t|s_t)e^{\beta Q_i(s_t,a_t)} \tag{3}$$<br>$$Q_i(s_t,a_t) = R_i(s_t, a_t)+ \gamma \sum_{s_t}p_i(s_{t+1}|s_t,a_t)V_i(s_{t+1}) \tag{4}$$<br>这个Bellman update公式是softened的，因为state value $V_i$在actions上的max操作被温度$\beta$倒数上的soft-max操作代替了，当$\beta\rightarrow\infty$时，就变成了max 操作，这里有些不明白。为什么呢？这个我不理解有什么关系，这是这篇文章给出的解释。<strong>按照我的理解，这个和我们平常使用Bellman 期望公式或者最优等式没有什么关系，只是给了一种新的更新Q值和V值的方法。实际上，这两个公式都是根据推导给出的定义。</strong><br>还有一点：$\pi_0$是学出来的，而不是手动选出来的。式子(1)中和$\pi_0$相关的只有：<br>$$\frac{\alpha}{\beta}\sum_i\mathbb{E_{\pi_i}}\left[\sum_{t\ge 0}\gamma^tlog\pi_0(a_t|s_t) \right]\tag{5}$$<br>可以看出来，这是使用$\pi_0$去拟合一个混合的带折扣因子$\gamma$的state-action分布，每个$i$代表一个任务，可以使用最大似然估计来求解，如果是非表格情况的话，可以使用stochastic gradient ascent进行优化，但是需要注意的是本文中作者使用的目标函数多了一个KL散度。另一个区别是本文的distilled policy可以作为下一步要优化的task policy的反馈。<br>多加一个entropy正则项的意义？如果不加entropy正则化，也就是式子$(2)$中的$\alpha = 1$，考虑$n=1$时的例子，式子$(5)$在$\pi_0=\pi_1$的时候最大，KL散度为$0$，目标函数退化成了一个没有正则化项的expected return，最终策略$\pi_1$会收敛到一个局部最优值。**和TRPO的一个比较？？？未完待续。。。。**如果$\alpha\lt 1$，式(1)中有一个额外的entropy项。这样即使$\pi_0=\pi_1$，$KL(\pi_1||\pi_0)=0$，因为有entropy项，也无法通过greedy策略最大化式子$(1)$。式子$1$的entropy正则化系数变成了$\beta’=\frac{\beta}{1-\alpha} = \frac{1}{c_{Ent}}$（第一个等号是为什么？？是因为$\pi_0=\pi_1$，然后就可以将$\pi_0,\pi_i$的系数合并了），最优的策略就是$\beta’$处的Boltzmann policy。添加这个entropy项可以保证策略不是greedy的，通过调整$c_{Ent}$的大小可以调整exploration。<br>最开始的时候，exploration是在multitask任务上加的，如果有多个任务，一个很简单，而其他的很复杂，如果先遇到了简单任务，没有加entropy的话，最后就会收敛到最简单任务的greedy策略，这样子就无法充分探索其他任务的，导致陷入到次优解。对于single-task的RL来说，在A3C中提出用entropy取应对过早的收敛，作者在这里推广到了multitask任务上。</p><h3 id="policy-gradient-and-a-better-parameterization">Policy Gradient and a Better Parameterization</h3><p>上面一节讲的是表格形式的计算，给定$\pi_0$，首先求解出$\pi$对应的$V$和$Q$，然后写出$\pi_i$的解析。但是如果我们用神经网络等函数去拟合$V$和$Q$，$V$和$Q$的求解特别慢，这里使用梯度下降同时优化task polices和distilled policy。这种情况下，$\pi_i$的梯度更新通过求带有entropy正则化的return即可求出来，并且可以放在如actor-critic之类的框架中。<br>每一个$\pi_i$都用一个单独的网络表示，$\pi_0$也用一个单独的网络表示，用$\theta_0$表示$\pi_0$的参数，对应的policy表示为：<br>$$\hat{\pi_0}(a_t|s_t) = \frac{e^{(h_{\theta_0}(a_t|s_t))} }{\sum_{a’}e^{h_{\theta_0}(a’|s_t)}} \tag{6}$$<br>使用参数为$\theta_i$的神经网络表示$Q$值，用$f_{\theta_i}$表示第$i$个策略$\pi$的$Q$值，用$Q$表示$V$，再估计$A=Q-V$的值：<br>$$\hat{A}_i(a_t|s_t) = f_{\theta_i}(a_t|s_t) - \frac{1}{\beta}log\sum_a\hat{\pi}_0^{\alpha} (a|s_t)e^{\beta f_{\theta_i}(a|s_t)} \tag{7}$$<br>将式子$(7)$代入式子$(2)$得第$i$个任务的policy可以参数化为：<br>\begin{align*}<br>\hat{\pi}_i(a_t|s_t)<br>&amp; = \hat{\pi}_0^{\alpha} (a_t|s_t)e^{\left(\beta \hat{Q}_i(a_t|s_t)-\beta \hat{V}(s_t)\right)}\\<br>&amp; = \hat{\pi}_0^{\alpha} (a_t|s_t)e^{\left(\beta \hat{A}_i(a_t|s_t)\right)}\\<br>&amp; = \hat{\pi}_0^{\alpha} (a_t|s_t)e^{\left(\beta \left(f_{\theta_i}(a_t|s_t) - \frac{1}{\beta}log\sum_a\hat{\pi}_0^{\alpha}(a|s_t)e^{\beta f_{\theta_i}(a|s_t)}\right)\right)}\\<br>&amp; = \hat{\pi}_0^{\alpha} (a_t|s_t)e^{\left(\beta f_{\theta_i}(a_t|s_t) - log\sum_a\hat{\pi}_0^{\alpha}(a|s_t)e^{\beta f_{\theta_i}(a|s_t)}\right)}\\<br>&amp; = \left(\frac{e^{(h_{\theta_0}(a_t|s_t))} }{\sum_{a’}e^{h_{\theta_0}(a’|s_t)}}\right)^{\alpha}e^{\left(\beta f_{\theta_i}(a_t|s_t) - log\sum_a\hat{\pi}_0^{\alpha}(a|s_t)e^{\beta f_{\theta_i}(a|s_t)}\right)}\\<br>&amp; = \left(\frac{e^{(h_{\theta_0}(a_t|s_t))} }{\sum_{a’}e^{h_{\theta_0}(a’|s_t)}}\right)^{\alpha}<br>\cdot<br>\frac{e^{\beta f_{\theta_i}(a_t|s_t)} }   {e^{log\sum_a\hat{\pi}_0^{\alpha}(a|s_t)e^{\beta f_{\theta_i}(a|s_t)}}}\\<br>&amp; = \frac{\left(e^{(h_{\theta_0}(a_t|s_t))}\right)^{\alpha}}  {\left(\sum_{a’}e^{h_{\theta_0}(a’|s_t)}\right)^{\alpha}}<br>\cdot<br>\frac{e^{\beta f_{\theta_i}(a_t|s_t)}}   {e^{log\sum_a\hat{\pi}_0^{\alpha}(a|s_t)e^{\beta f_{\theta_i}(a|s_t)}}}\\<br>&amp; = \frac{e^{\alpha \cdot(h_{\theta_0}(a_t|s_t))}}   {\left(\sum_{a’}e^{h_{\theta_0}(a’|s_t)}\right)^{\alpha}}<br>\cdot<br>\frac{e^{\beta f_{\theta_i}(a_t|s_t)}}{e^{log\sum_a\hat{\pi}_0^{\alpha}(a|s_t)e^{\beta f_{\theta_i}(a|s_t)}}}\\<br>&amp; = \frac{e^{(\alpha h_{\theta_0}(a_t|s_t))}}  {\left(\sum_{a’}e^{h_{\theta_0}(a’|s_t)}\right)^{\alpha}}<br>\cdot<br>\frac{e^{\beta f_{\theta_i}(a_t|s_t)}}{\sum_a\hat{\pi}_0^{\alpha}(a|s_t)e^{\beta f_{\theta_i}(a|s_t)}}\\<br>&amp; = \frac{e^{(\alpha h_{\theta_0}(a_t|s_t))} \cdot e^{\beta f_{\theta_i}(a_t|s_t) }}<br>{\left(\sum_{a’}e^{h_{\theta_0}(a’|s_t)}\right)^{\alpha} \cdot {\sum_a\hat{\pi}_0^{\alpha}(a|s_t) e^{\beta f_{\theta_i}(a|s_t)}}}\\<br>&amp; = \frac{e^{(\alpha h_{\theta_0}(a_t|s_t) + \beta f_{\theta_i}(a_t|s_t)) }}<br>{\left(\sum_{a’}e^{h_{\theta_0}(a’|s_t)}\right)^{\alpha} \cdot {\sum_a\hat{\pi}_0^{\alpha}(a|s_t) e^{\beta f_{\theta_i}(a|s_t)}}}(Why to blow equation???)\\<br>&amp; = \frac{e^{(\alpha h_{\theta_0}(a_t|s_t) + \beta f_{\theta_i}(a_t|s_t))}}  {\sum_{a’}e^{(\alpha h_{\theta_0}(a’|s_t) + \beta f_{\theta_i}(a’|s_t))}}<br>\end{align*}<br>所以：<br>$$\hat{\pi}_i(a_t|s_t) = \hat{\pi}_0^{\alpha}(a_t|s_t)e^{(\beta\hat{A}_i(a_t|s_t))}=\frac{e^{(\alpha h_{\theta_0}(a_t|s_t) + \beta f_{\theta_i}(a_t|s_t))}}{\sum_{a’}e^{(\alpha h_{\theta_0}(a’|s_t) + \beta f_{\theta_i}(a’|s_t))}} \tag{8}$$<br>这可以看成policy的一个两列架构，一列是提取的shared policy，一列是将$\pi_0$应用到task $i$上需要做的一些修改。<br>使用参数化的$\pi_0, \pi_i$，首先推导策略相对于$\pi_i$的梯度（policy gradient的推导，这里是直接应用了)：<br>\begin{align*}<br>\nabla_{\theta_i}J&amp; = \mathbb{E}_{\hat{\pi}_i}\left[\left(\sum_{t\gt 1} \nabla_{\theta_i}log{\hat{\pi}}_i(a_t|s_t)\right) \left(\sum_{u\ge 1}\gamma^u \left(R^{reg}_i(a_u,s_u)\right)\right) \right]\\<br>&amp; = \mathbb{E}_{\hat{\pi}_i}\left[\sum_{t\gt 1} \nabla_{\theta_i}log\hat{\pi}_i(a_t|s_t)\left(\sum_{u\ge t}\gamma^u \left(R^{reg}_i(a_u,s_u)\right)\right) \right] \tag{9}\\<br>\end{align*}<br>其中$R_i^{reg}(s,a) = R_i(s,a) + \frac{\alpha}{\beta}log\hat{\pi}_0(a|s) - \frac{1}{\beta}log\hat{\pi}_i(a|s)$是正则化后的reward。注意，这里$\mathbb{E}_{\hat{\pi}_i}\left[\nabla_{\theta_i}log\hat{\pi}_i(a_t|s_t)\right] = 0$，因为log-derivative trick。如果有一个value baseline，那么为了减少梯度的方差，可以从正则化后的returns中减去它。<br>关于$\theta_0$的梯度如下：<br>\begin{align*}<br>\nabla_{\theta_0}J<br>&amp; = \mathbb{E}_{\hat{\pi}_i}<br>\left[<br>\sum_{t\gt 1} \nabla_{\theta_i}log\hat{\pi}_i(a_t|s_t)<br>\left(\sum_{u\ge 1}\gamma^u<br>\left(<br>R^{reg}_i(a_u,s_u)<br>\right)<br>\right)<br>\right]\\<br>&amp; \qquad +\frac{\alpha}{\beta}\sum_i\mathbb{E}_{\hat{\pi}_i}<br>\left[<br>\sum_{t\ge 1}\gamma^t\sum_{a’_t}<br>\left(<br>\hat{\pi}_i(a’_t|s_t)-\hat{\pi}_0(a’_t|s_t)<br>\right)<br>\nabla_{\theta_0}h_{\theta_0}(a’_t|s_t)<br>\right] \tag{10}<br>\end{align*}<br>第一项和$\pi_i$一样，第二项是让$\hat{\pi}_i,\hat{\pi}_0$的概率尽可能接近。如果不使用KL散度的话，这里就不会有第二项了。KL正则是为了让$\pi_0$在$\pi_i$的质心上，即$\hat{\pi}_0(a’_t|s_t) = \frac{1}{n}\sum_i\hat{\pi}_i(a’_t|s_t)$，最后第二项就为$0$了，可以快速的将公共信息迁移到新任务上。<br>和ADMM,EASGD等在参数空间上进行优化不同的是，Distral是在策略空间上进行优化，这样子在语义上更有意义，对于稳定学习过程很重要。<br>本文的方法通过添加了entropy正则化和KL正则化，使得算法可以分开控制每个任务迁移的信息大小和exploration程序。</p><h2 id="算法">算法</h2><p>上面给出的框架可以对不同的目标函数，算法和架构进行组合，然后生成一系列算法实例。</p><ul><li>KL散度和entropy：当$\alpha=0$时，只有entorpy，不同任务之间没有耦合，在不同任务中进行迁移。当$\alpha=1$时，只有KL散度，不同任务之间有耦合，在不同任务中进行迁移，但是如果$\pi_i,\pi_0$很像的话，会过早的停止探索。当$0\lt \alpha \lt 1$时，KL散度和entropy都有。</li><li>迭代优化还是联合优化：可以选择同时优化$\pi_0,\pi_i$，也可以固定其中一个，优化另一个。迭代优化和actor-mimic以及policy-distilled有一些相似，但是Distral是迭代进行的，$\pi_0$会对$\pi_i$的优化提供反馈。尽管迭代优化可能会很慢，但是从actor-mimic等的结果来看，可能它会更稳定。</li><li>Separate还是two-column参数化：这里的意思是$\pi_i$是否使用式子(8)中的$\pi_0$，如果用的话，$\pi_0$中提取到的信息可以立刻用到$\pi_i$上，transfer可以更快。但是如果transfer的太快的话，可能会抑制在单个任务上exploration的有效性。。</li></ul><p>这里作者给出了使用到的一些算法组合，如下表和下图所示。这里作者和三个A3C baseline(三种架构)做了比较，作者做实验的时候，试了两种A3C，第一个是原始的A3C，第二个是A3C的变种，最后发现这两种A3C没啥差别，在实验部分就选择了原始的A3C作比较。</p><p><img src="/2019/04/18/distral/figure2.png" alt="figure2"><br><img src="/2019/04/18/distral/table.png" alt="table"></p><h3 id="algorithm">Algorithm</h3><ul><li>A3C: 在每个任务上单独使用A3C训练的policy</li><li>A3C_multitask: 使用A3C同时在所有任务上训练得到的policy</li><li>A3C_2col: 使用了式子(8)中的two-column架构A3C在每个任务上训练的policy</li><li>KL_1col: $\pi_0,\pi_i$分别用一个网络来表示，令$\alpha=1$，即只有KL散度的式子(1)进行优化，</li><li>KL+ent_1col: 和KL_1col一样，只不过包括了KL散度和entropy项，并设置$\alpha = 0.5$。</li><li>KL_2col: 和KL_1col一样，但是使用了式子(8)中的two-column架构</li><li>KL+ent_2col: 和KL+ent_1col一样，只是使用了two-column架构。</li></ul><h2 id="实验">实验</h2><p>总共有两个实验，第一个是在grid world上使用soft Q-learning和policy distilltion的迭代优化，第二个是七个算法在三个3D部分可观测环境上的评估。</p><h3 id="环境">环境</h3><h4 id="grid-world">Grid world</h4><p>这个实验是在一些简单的grid world上进行的，每一个任务通过一个随机选择的goal location进行区分。<br>每一个MDP的state由map location, previous action和previous reward组成。一个Distral智能体通过KL正则化的目标函数进行训练，优化算法在Soft Q-learing和policy distilltion之间进行迭代。每次soft Q-learing 的展开长度是$10$。</p><h4 id="3d环境">3D环境</h4><p>这个实验使用了三个第一人称的$3D$环境。所有的智能体都是用pytorch/tensorflow实现的，每个任务有$32$个workers，使用异步的RMSProp进行学习。每个网络由CNN和LSTM组成，在不同的算法和实验中都是相同的。作者尝试了三个$\beta$和三个学习率$\epsilon$，每一组超参数跑了四次，其他超参数和单任务的A3C都一样的，对于KL+ent 1col和KL+ent 2col算法，$\alpha$被固定为$0.5$。</p><h5 id="maze">Maze</h5><p>八个任务，每个任务都是一个随机放置reward和goal的迷宫。作者给出了$7$个算法的学习曲线，每一个学习曲线是选出最好的$\beta,\epsilon$在$8$个任务跑$4$次的平均值。Distral学习的很快，并且超过了三个A3C baselies，而two-column算法比one-column学习的要快，不带entropy的Distral要比带entropy学得快，但是最终得分要低，这可能是没有充分explration的原因。<br>multitask A3C和two-column A3C学习的不稳定，有时候学的好，有时候学的不好，有时候刚开始就不好了。而Distral对于超参数也很鲁棒。</p><h5 id="navigation">Navigation</h5><p>四个任务，比Maze难度要大。</p><h5 id="laser-tag">Laser-tag</h5><p>DeepMind Lab中的八个任务，最好的baseline是单独在每个任务上训练的A3C。</p><h2 id="discussion">Discussion</h2><p>有两个idea这里需要强调一下。在优化过程中，使用KL散度正则化使$\pi_i$向$\pi_0$移动，使用$\pi_0$正则化$\pi_i$。<br>另一个就是在深度神经网络中，它们的参数没有意义，所以作者不是在参数空间进行的正则化，而是在策略空间进行正则化，这样子更有语义意义。</p><h2 id="参考文献">参考文献</h2><p>1.<a href="https://papers.nips.cc/paper/7036-distral-robust-multitask-reinforcement-learning.pdf" target="_blank" rel="noopener">https://papers.nips.cc/paper/7036-distral-robust-multitask-reinforcement-learning.pdf</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;这篇文章给出了多任务学习和迁移学习的一个框架。&lt;/p&gt;
&lt;h2 id=&quot;摘要&quot;&gt;摘要&lt;/h2&gt;
&lt;p&gt;使用共享网络参数的multitas learning，然后在不同的任务之间迁移可以提高效率。但是这种方法有几个缺点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;不同任务的gradient可
      
    
    </summary>
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="论文" scheme="http://mxxhcm.github.io/tags/%E8%AE%BA%E6%96%87/"/>
    
  </entry>
  
  <entry>
    <title>linux 终端快速访问某个目录</title>
    <link href="http://mxxhcm.github.io/2019/04/15/linux-%E7%BB%88%E7%AB%AF%E5%BF%AB%E9%80%9F%E8%AE%BF%E9%97%AE%E6%9F%90%E4%B8%AA%E7%9B%AE%E5%BD%95/"/>
    <id>http://mxxhcm.github.io/2019/04/15/linux-终端快速访问某个目录/</id>
    <published>2019-04-15T10:49:57.000Z</published>
    <updated>2019-05-12T04:04:35.072Z</updated>
    
    <content type="html"><![CDATA[<h2 id="动机">动机</h2><p>在写博客的过程中，每次在终端中进入该目录，都要输好长的命令，在想着有没有什么简单的方法。后来就在网上找到了。</p><h2 id="方法">方法</h2><p>利用alias命令进行重命名<br>这里给出一个具体的例子，我的博客文件存放在/home/mxxmhh/github/blog/source/_posts下，<br>在/home/mxxmhh/.bashrc文件中添加如下一行即可(当然也可以在其他配置文件中添加)：<br>alias posts='cd /home/mxxmhh/github/blog/source/_posts’<br>然后执行<br>~\$:source /home/mxxmhh/.bashrc<br>即可。<br>接下来可在终端输入<br>~\$:posts<br>直接访问该目录。</p><h2 id="参考文献">参考文献</h2><p>1.<a href="https://www.cnblogs.com/wlsphper/p/6782625.html" target="_blank" rel="noopener">https://www.cnblogs.com/wlsphper/p/6782625.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;动机&quot;&gt;动机&lt;/h2&gt;
&lt;p&gt;在写博客的过程中，每次在终端中进入该目录，都要输好长的命令，在想着有没有什么简单的方法。后来就在网上找到了。&lt;/p&gt;
&lt;h2 id=&quot;方法&quot;&gt;方法&lt;/h2&gt;
&lt;p&gt;利用alias命令进行重命名&lt;br&gt;
这里给出一个具体的例子，我的博客
      
    
    </summary>
    
      <category term="linux" scheme="http://mxxhcm.github.io/categories/linux/"/>
    
    
      <category term="linux" scheme="http://mxxhcm.github.io/tags/linux/"/>
    
      <category term="ubuntu" scheme="http://mxxhcm.github.io/tags/ubuntu/"/>
    
  </entry>
  
  <entry>
    <title>python 类和函数的属性</title>
    <link href="http://mxxhcm.github.io/2019/04/14/python-%E7%B1%BB%E5%92%8C%E5%87%BD%E6%95%B0%E7%9A%84%E5%B1%9E%E6%80%A7/"/>
    <id>http://mxxhcm.github.io/2019/04/14/python-类和函数的属性/</id>
    <published>2019-04-14T06:49:41.000Z</published>
    <updated>2019-05-06T16:22:27.708Z</updated>
    
    <content type="html"><![CDATA[<h2 id="函数和类的默认属性">函数和类的默认属性</h2><p>这里主要介绍类和函数的一些属性。<br>__dict__用来描述对象的属性。对于类来说，它内部的变量就是它的数量，注意，不是它的member variable，但是对于函数来说不是。对于类来说，而对于类对象来说，输出的是整个类的属性，而__dict__输出的是self.variable的内容。</p><p>python中的函数有很多特殊的属性（包括自定义的函数和库函数）</p><ul><li><strong>doc</strong>  输出用户定义的关于函数的说明</li><li><strong>name</strong> 输出函数名字</li><li><strong>module</strong> 输出函数所在模块的名字</li><li><strong>dict</strong> 输出函数中的字典</li></ul><p>示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">myfunc</span><span class="params">()</span>:</span></span><br><span class="line">   <span class="string">'this func is to test the __doc__'</span></span><br><span class="line">   myfunc.func_attr = <span class="string">"attr"</span></span><br><span class="line">   print(<span class="string">"hhhh"</span>)</span><br><span class="line"> </span><br><span class="line">myfunc.func_attr1 = <span class="string">"first1"</span></span><br><span class="line">myfunc.func_attr2 = <span class="string">"first2"</span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">  print(myfunc.__doc__)</span><br><span class="line">  print(myfunc.__name__)</span><br><span class="line">  print(myfunc.__module__)</span><br><span class="line">  print(myfunc.__dict__)</span><br></pre></td></tr></table></figure><p>输出：</p><blockquote><p>this func is to test the <strong>doc</strong><br>myfunc<br><strong>main</strong><br>{‘func_attr1’: ‘first1’, ‘func_attr2’: ‘first2’}</p></blockquote><p>类也有很多特殊的属性（包括自定义的类和库中的类）</p><ul><li><strong>doc</strong>  输出用户定义的类的说明</li><li><strong>module</strong> 输出类所在模块的名字</li><li><strong>dict</strong> 输出类中的字典</li></ul><p>示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyClass</span>:</span></span><br><span class="line">  <span class="string">"""This is my class __doc__"""</span></span><br><span class="line">  class_name = <span class="string">"cllll"</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, test=None)</span>:</span></span><br><span class="line">     self.test = test</span><br><span class="line">  <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">  print(MyClass.__dict__)</span><br><span class="line">  print(MyClass.__doc__)</span><br><span class="line">  print(MyClass.__module__)</span><br></pre></td></tr></table></figure><p>输出：</p><blockquote><p>{‘<strong>module</strong>’: ‘<strong>main</strong>’, ‘<strong>doc</strong>’: ‘This is my class <strong>doc</strong>’, ‘class_name’: ‘cllll’, ‘<strong>init</strong>’: &lt;function MyClass.<strong>init</strong> at 0x7f1349d44510&gt;, ‘<strong>dict</strong>’: &lt;attribute ‘<strong>dict</strong>’ of ‘MyClass’ objects&gt;, ‘<strong>weakref</strong>’: &lt;attribute ‘<strong>weakref</strong>’ of ‘MyClass’ objects&gt;}<br>This is my class <strong>doc</strong><br><strong>main</strong></p></blockquote><p>类的对象的属性</p><ul><li><strong>doc</strong>  输出用户定义的类的说明</li><li><strong>module</strong> 输出类对象所在模块的名字</li><li><strong>dict</strong> 输出类对象中的字典</li></ul><p>示例</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"> <span class="number">1</span> <span class="class"><span class="keyword">class</span> <span class="title">MyClass</span>:</span></span><br><span class="line"> <span class="number">2</span>   <span class="string">"""This is my class __doc__"""</span></span><br><span class="line"> <span class="number">3</span>   class_name = <span class="string">"cllll"</span></span><br><span class="line"> <span class="number">4</span>   <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, test=None)</span>:</span></span><br><span class="line"> <span class="number">5</span>      self.test = test</span><br><span class="line"> <span class="number">6</span>   <span class="keyword">pass</span></span><br><span class="line"> <span class="number">7</span> </span><br><span class="line"> <span class="number">8</span> <span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line"> <span class="number">9</span> </span><br><span class="line"><span class="number">10</span>   cl = MyClass()</span><br><span class="line"><span class="number">11</span>   print(cl.__dict__)</span><br><span class="line"><span class="number">12</span>   print(cl.__doc__)</span><br><span class="line"><span class="number">13</span>   print(cl.__module__)</span><br></pre></td></tr></table></figure><p>输出</p><blockquote><p>{‘test’: None}<br>This is my class <strong>doc</strong><br><strong>main</strong></p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;函数和类的默认属性&quot;&gt;函数和类的默认属性&lt;/h2&gt;
&lt;p&gt;这里主要介绍类和函数的一些属性。&lt;br&gt;
__dict__用来描述对象的属性。对于类来说，它内部的变量就是它的数量，注意，不是它的member variable，但是对于函数来说不是。对于类来说，而对于类对
      
    
    </summary>
    
      <category term="python" scheme="http://mxxhcm.github.io/categories/python/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>python zip和enumerate</title>
    <link href="http://mxxhcm.github.io/2019/04/13/python-zip%E5%92%8Cenumerate/"/>
    <id>http://mxxhcm.github.io/2019/04/13/python-zip和enumerate/</id>
    <published>2019-04-13T06:59:12.000Z</published>
    <updated>2019-05-06T16:22:27.708Z</updated>
    
    <content type="html"><![CDATA[<h2 id="zip-function">zip function</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">a = np.zeros((<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>))</span><br><span class="line">b = np.zeros((<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>))</span><br><span class="line">c = np.zeros((<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>))</span><br><span class="line">d = zip(a,b,c)   </span><br><span class="line">print(list(d))        </span><br><span class="line">d = list(zip(a,b,c))</span><br><span class="line">e,f,g = d</span><br></pre></td></tr></table></figure><p>这里d是一个什么呢，是多个tuple，数量是min(len(a),len(b),len©)，每一个element是一个tuple，这个tuple的内容为(a[0],b[0],c[0])，…<br>打印出list(d)是一个list，这个list的长度为min(len(a),len(b),len©)每一个element是一个tuple，tuple的形状是((2,2),(2,2),(2,2))<br>用zip的话，就是看一下它的len，然后在第一维上对他们进行拼接，形成多个新的元组。<br>例子</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a = (<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">b = (<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">c = (<span class="number">4</span>,<span class="number">5</span>)</span><br><span class="line">d = zip(a,b,c)</span><br><span class="line">print(list(c))</span><br></pre></td></tr></table></figure><blockquote><p>[(2,3),(3,4),(4,5)]</p></blockquote><p>相当于吧tuple a和tuple b分别当做一个list的一个元组，然后结合成一个新的tuple的list，</p><h2 id="enumerate-iterable-start-0">enumerate(iterable, start=0)</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">seasons = [<span class="string">'Spring'</span>, <span class="string">'Summer'</span>, <span class="string">'Fall'</span>, <span class="string">'Winter'</span>]</span><br><span class="line">print(list(enumerate(seasons)))</span><br><span class="line">print(list(enumerate(seasons, start=<span class="number">1</span>)))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> enumerate(seansons):</span><br><span class="line">   print(i)</span><br></pre></td></tr></table></figure><blockquote><p>[(0, ‘Spring’), (1, ‘Summer’), (2, ‘Fall’), (3, ‘Winter’)]<br>[(1, ‘Spring’), (2, ‘Summer’), (3, ‘Fall’), (4, ‘Winter’)]<br>(0, ‘Spring’)<br>(1, ‘Summer’)<br>(2, ‘Fall’)<br>(3, ‘Winter’)</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;zip-function&quot;&gt;zip function&lt;/h2&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;
      
    
    </summary>
    
      <category term="python" scheme="http://mxxhcm.github.io/categories/python/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>python time</title>
    <link href="http://mxxhcm.github.io/2019/04/13/python-time/"/>
    <id>http://mxxhcm.github.io/2019/04/13/python-time/</id>
    <published>2019-04-13T06:52:30.000Z</published>
    <updated>2019-05-06T16:22:27.708Z</updated>
    
    <content type="html"><![CDATA[<h2 id="time-time-library-datetime-library-panda-timestamp">time（time library,datetime library,panda.Timestamp()）</h2><p>import time</p><h3 id="获得当前时间">获得当前时间</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">time.time()        <span class="comment">#获得当前timestamp</span></span><br></pre></td></tr></table></figure><h3 id="time-localtime-timestamp">time.localtime(timestamp)</h3><p>得到一个struct_time<br>time.struct_time(tm_year=2018…)</p><h3 id="将struct-time转换成string">将struct time转换成string</h3><blockquote><p>Convert a tuple or struct_time representing a time as returned by gmtime() or localtime() to a string as specified by the format argument.If t is not provided,the current time as returned by localtime() is used.</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line">time.strftime(format,t)        <span class="comment">#将一个struct_time表示为一个格式化字符串</span></span><br><span class="line">time.strftime(<span class="string">"%Y-%m-%d %H:%M:%S"</span>,time.localtime())</span><br></pre></td></tr></table></figure><h3 id="将一个string类型的事件转换成struct-time">将一个string类型的事件转换成struct time</h3><blockquote><p>Parse a string representing a time accroding to a format.The return value is a struct_time as returned by gmtime() or localtime()</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line">time.strptime(<span class="string">"a string representing a time"</span>,<span class="string">"a format"</span>)    <span class="comment">#将某个format表示的time转化为一个struct_time()</span></span><br><span class="line">time.strptime(<span class="string">"2014-02-01 00:00:00"</span>,<span class="string">"%Y-%m-%d %H:%M:%S"</span>)</span><br></pre></td></tr></table></figure><h3 id="time-mktime">time.mktime()</h3><p>将时间t转换成timestamp</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;time-time-library-datetime-library-panda-timestamp&quot;&gt;time（time library,datetime library,panda.Timestamp()）&lt;/h2&gt;
&lt;p&gt;import time&lt;/p&gt;
&lt;h
      
    
    </summary>
    
      <category term="python" scheme="http://mxxhcm.github.io/categories/python/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>python文件和目录操作(os和shutil)</title>
    <link href="http://mxxhcm.github.io/2019/04/13/python-file-dir/"/>
    <id>http://mxxhcm.github.io/2019/04/13/python-file-dir/</id>
    <published>2019-04-13T06:51:26.000Z</published>
    <updated>2019-05-06T16:22:27.708Z</updated>
    
    <content type="html"><![CDATA[<h2 id="文件和目录操作-os库和shutil库">文件和目录操作（os库和shutil库）</h2><p>import os</p><h3 id="查看信息">查看信息</h3><p>不是函数，而是属性<br>os.linesep   #列出当前平台的行终止符<br><a href="http://os.name" target="_blank" rel="noopener">os.name</a>    #列出当前的平台信息</p><h3 id="列出目录">列出目录</h3><p>file_dir_list = os.listdir(parent_dir)    #列出某个目录下的文件和目录，默认的话为当前目录<br>parent_dir 是一个目录<br>file_dir_list是一个list</p><p>os.path.exists(pathname)    #判断pathname是否存在<br>os.path.isdir(pathname)    #判断pathname是否是目录<br>os.path.isfile(pathname)    #判断pathname是否是文件<br>os.path.isabs(pathname)    #判断pathname是否是绝对路径</p><p>os.path.basename(pathname)    # 列出pathname的dir<br>os.path.dirname(pathname)        # 列出pathname的file name<br>os.path.split(pathname)    #将pathname分为dir和filename<br>os.path.split(pathname)    #将pathname的扩展名分离出来</p><p>os.path.join(“dir_name”,“file_name”)    # 拼接两个路径</p><p>os.getcwd()    #获得当前路径<br>os.chdir(pathname)    #改变当前路径</p><h3 id="创建和删除">创建和删除</h3><p>os.mkdir(pathname)    #创建新目录<br>os.rmdir(pathname)    #删除目录<br>os.makedirs(&quot;/home/mxxhcm/Documents/&quot;)    #创建多级目录<br>os.removedirs()    #删除多个目录<br>os.remove(file_pathname)    #删除文件</p><p>os.rename(old_pathname,new_pathname)    #重命名</p><h3 id="打开文件">打开文件</h3><p>对于open文件来说，共有三种模式，分别为w,a,r<br>r的话，为只读，读取一个不存在的文件，会报错<br>r+的话，为可读写，读取一个不存在的文件，会报错<br>a的话，为追加读，读取一个不存在的文件，会创建该文件<br>w的话，为写入文件，读取一个不存在的文件，会创建改文件，打开一个存在的同名文件，会删除该文件，创建一个新的文件</p><h3 id="读取文件">读取文件</h3><p>fp = open(file_path_name,“r+”)</p><h4 id="read-将文件读到一个字符串中">read()将文件读到一个字符串中</h4><p>file_str = fp.read()<br>fp.read()会返回一个字符串，包含换行符</p><h4 id="readline">readline()</h4><p>for file_str in fp:<br>print(file_str)<br>这里的file_str是一个str类型变量</p><h4 id="readlines-将文件读到一个列表中">readlines()将文件读到一个列表中</h4><p>list(fp)<br>file_list = fp.readlines()<br>filt_list是一个list变量</p><h3 id="关闭文件">关闭文件</h3><p>fp.close()<br>或者<br>with open(file_pathname, “r”) as f:<br>file_str = fp.read()<br>当跳出这个语句块的时候，文件已经别关闭了。</p><h3 id="复制文件">复制文件</h3><p>shutil.move(‘test’,‘test_move’)    # 递归的将文件或者目录移动到另一个位置。如果目标位置是一个目录，移动到这个目录里，如果目标已经存在而且不是一个目录，可能会用os.rename()重命名<br>shutil.copyfile(src,dst) #复制文件内容，metadata没有复制<br>shutil.copymode(src,dst) #copy权限。文件内容，owner和group不变。<br>shutil.copystat(src,dst)    #copy权限，各种时间以及flags位。文件内容，owner，group不变<br>shutil.copy(src,dst)    #copy file,权限为也会被copied<br>shutil.copy2(src,dst)  #和先后调用shutil.copy()和shutil.copystat()函数一样<br>shutil.copytree(src,dst,symlinks=False,ignore=None)  #递归的将str目录结构复制到dst，dst位置必须不存在，目录的权限和时间用copystat来复制，文件的赋值用copy2()来复制<br>shutil.rmtree(path[,ignore_errors[,onerror]])   #删除一个完整的目录，无论目录是否为空</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;文件和目录操作-os库和shutil库&quot;&gt;文件和目录操作（os库和shutil库）&lt;/h2&gt;
&lt;p&gt;import os&lt;/p&gt;
&lt;h3 id=&quot;查看信息&quot;&gt;查看信息&lt;/h3&gt;
&lt;p&gt;不是函数，而是属性&lt;br&gt;
os.linesep   #列出当前平台的行终止符&lt;b
      
    
    </summary>
    
      <category term="python" scheme="http://mxxhcm.github.io/categories/python/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>python regex</title>
    <link href="http://mxxhcm.github.io/2019/04/13/python-regex/"/>
    <id>http://mxxhcm.github.io/2019/04/13/python-regex/</id>
    <published>2019-04-13T06:50:41.000Z</published>
    <updated>2019-06-06T07:47:18.093Z</updated>
    
    <content type="html"><![CDATA[<h2 id="regex-examples">regex examples</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> regex <span class="keyword">as</span> re</span><br><span class="line"></span><br><span class="line"><span class="comment"># 找出每一行的数字</span></span><br><span class="line">string = <span class="string">"""a9apple1234</span></span><br><span class="line"><span class="string">2banana5678</span></span><br><span class="line"><span class="string">a3coconut9012"""</span></span><br><span class="line">pattern = <span class="string">"[0-9]+"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># search</span></span><br><span class="line">result = re.search(pattern, string)</span><br><span class="line">print(type(result))</span><br><span class="line">print(result[<span class="number">0</span>])</span><br><span class="line">print(result.group(<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># match</span></span><br><span class="line"><span class="comment"># 即使设置了MULTILINE模式，也只会匹配string的开头而不是每一行的开头</span></span><br><span class="line">result = re.match(pattern, string, re.S| re.M)  </span><br><span class="line">print(type(result))</span><br><span class="line"><span class="comment"># print(result[0])</span></span><br><span class="line"><span class="comment"># print(result.group(0))</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># findall</span></span><br><span class="line">result = re.findall(pattern, string)</span><br><span class="line">print(type(result))</span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure><h2 id="语法">语法</h2><p>.   匹配除了newline的任意character，如果要匹配newline，需要添加re.DOTALL flag<br>*  重复至少$0$次<br>+  重复至少$1$次<br>?   重复$0$次或者$1$次<br>{}  重复多少次，如a{3,5}表示重复$3-5$次<br>[]  匹配方括号内的内容,如[1-9]表示匹配$1-9$中任意一个<br>^   matching the start of the string<br>$   matching the end os the string<br>+,*.?    都是贪婪匹配，如果加一个?为非贪婪匹配<br>+?,*?,??    为非贪婪匹配<br>()  匹配括号内的正则表达式，表示一个group的开始和结束<br>|   或<br>\number<br>\b  匹配empty string<br>\B<br>\d  匹配数字<br>\D  匹配非数字<br>\s  匹配空白符[ \t\n\r\f\v]<br>\S  匹配非空白符<br>\w  匹配unicode<br>\W<br>\A<br>\Z</p><h2 id="模块">模块</h2><ul><li>re.compile(patern, flags=0)</li><li>re.match(pattern, string, flags=0)</li><li>re.fullmatch(pattern,string,flags=0)</li><li>re.search(pattern, string, flags=0)</li><li>re.split(pattern, string, maxflit=0, flags=0)</li><li>re.findall(pattern,string,flags=0)</li><li>re.sub(pattern,repl,string,count=0,flags=0)</li><li>re.subn(pattern,repl,string,count=0,flags=0)</li></ul><h3 id="flags">flags</h3><blockquote><p>flags can be re.DEBUG, re.I, re.IGNORECASE, re.L, re.LOCALE, re.M, re.MULTILINE, re.S, re.DOTALL, re.U, re.UNICODE, re.X, re.VERBOSE</p></blockquote><ul><li>re.I(re.IGNORECASE) 忽略大小写</li><li>re.L(re.LOCALE)</li><li>re.M(re.MULTILINE) 多行模式，设置以后.匹配newline。指定re.S时，’^'匹配string的开始和each line的开始(紧跟着each newline); '$'匹配string的结束和each line的结束($在newline之前，immediately preceding each newline)。如果不指定的话, '^‘只匹配string的开始,’$'只匹配string的结束和immediately before the newline (if any) at the end of the string，对应inline flag (?m).</li><li>re.S(re.DOTALL)</li><li>re.U(re.UNICODE)</li><li>re.X(re.VERBOSE)</li><li>re.DEBUG</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> regex <span class="keyword">as</span> re</span><br><span class="line"></span><br><span class="line">print(re.I)</span><br><span class="line">print(re.IGNORECASE)</span><br><span class="line">print(re.L)</span><br><span class="line">print(re.LOCALE)</span><br><span class="line">print(re.M)</span><br><span class="line">print(re.MULTILINE)</span><br><span class="line">print(re.S)</span><br><span class="line">print(re.DOTALL)</span><br><span class="line">print(re.U)</span><br><span class="line">print(re.UNICODE)</span><br><span class="line">print(re.X)</span><br><span class="line">print(re.VERBOSE)</span><br><span class="line">print(re.DEBUG)</span><br><span class="line"></span><br><span class="line">print(re.M <span class="keyword">is</span> re.MULTILINE)</span><br><span class="line">print(re.I <span class="keyword">is</span> re.IGNORECASE)</span><br></pre></td></tr></table></figure><p>re.M例子</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">text = <span class="string">"""First line.</span></span><br><span class="line"><span class="string">Second line.</span></span><br><span class="line"><span class="string">Third line."""</span></span><br><span class="line"></span><br><span class="line">pattern = <span class="string">"^.*$"</span>  <span class="comment"># 匹配从开始到结束的任何字符</span></span><br><span class="line"><span class="comment"># 默认情况下， . 不匹配newlines，所以默认情况下不会有任何匹配结果，因为$之前有newline，而.不能匹配</span></span><br><span class="line"><span class="comment"># re.search(pattern, text) is None  # Nothing matches!</span></span><br><span class="line">print(re.search(pattern, text))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果设置MULTILINE模式, $匹配每一行的结尾，这个时候第一行就满足要求了，设置MULTILINE模式后，$匹配string的结尾和每一行的结尾（each newline之前)</span></span><br><span class="line">print(re.search(pattern, text, re.M).group())</span><br><span class="line"><span class="comment"># First line.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果同时设置MULTILINE和DOTALL模式, .能够匹配newlines，所以第一行和第二行的newline都匹配了，在贪婪模式下，就匹配了整个字符串。</span></span><br><span class="line">print(re.search(pattern, text, re.M | re.S).group())</span><br><span class="line"><span class="comment"># First line.</span></span><br><span class="line"><span class="comment"># Second line.</span></span><br><span class="line"><span class="comment"># Third line.</span></span><br></pre></td></tr></table></figure><h3 id="re-compile-patern-flags-0">re.compile(patern, flags=0)</h3><p>将一个正则表达式语句编译成一个正则表达式对象，可以调用正则表达式的match()和search()函数进行matching。</p><blockquote><p>complie a regular expression pattern into a regular expression object,which can be used for matching using its match() and search()</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">str = <span class="string">"https://abc https://dcdf https://httpfn https://hello"</span></span><br><span class="line"><span class="keyword">import</span> regex <span class="keyword">as</span> re</span><br><span class="line"></span><br><span class="line">prog = re.compile(pattern)</span><br><span class="line">results = prog.match(string)</span><br><span class="line"><span class="comment"># 上面两行等价于下面一行</span></span><br><span class="line"></span><br><span class="line">results = re.match(pattern, string)</span><br></pre></td></tr></table></figure><h3 id="re-match-pattern-string-flags-0-or-re-fullmatch-pattern-string-flags-0">re.match(pattern, string, flags=0) or re.fullmatch(pattern,string,flags=0)</h3><p>在给定的string开始位置进行查找，返回一个match object。<strong>即使设置了MULTILINE mode, re.match()也只会在string的开始而不是each line的每一行开始匹配。</strong></p><h3 id="re-search-pattern-string-flags-0">re.search(pattern, string, flags=0)</h3><p>在给定的string任意位置进行查找，返回一个match object。</p><blockquote><p>locat a match anywhere in string</p></blockquote><h3 id="search-vs-match">search() vs. match()</h3><p>re.macth()在string的开头查找，而re.search在string的任意位置查找，他们都返回match object对象。如果不匹配，返回None。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">import re</span><br><span class="line"></span><br><span class="line">match1 = re.match(&quot;cd&quot;, &quot;abcdef&quot;)     # match</span><br><span class="line">match2 = re.search(&quot;cd&quot;, &quot;abcdef&quot;)    # search</span><br><span class="line">print(match1)</span><br><span class="line">print(match2)</span><br><span class="line">print(match2.group(0))</span><br><span class="line"># None</span><br><span class="line"># &lt;regex.Match object; span=(2, 4), match=&apos;cd&apos;&gt;</span><br><span class="line"># cd</span><br><span class="line"></span><br><span class="line">with open(&quot;content.txt&quot;, &quot;r&quot;) as f:</span><br><span class="line">    s = f.read()</span><br><span class="line">match3 = re.match(&quot;cd&quot;, s)     # match</span><br><span class="line">match4 = re.search(&quot;cd&quot;, s)</span><br><span class="line">print(match3)</span><br><span class="line">print(match4)</span><br><span class="line"># None</span><br><span class="line"># &lt;regex.Match object; span=(4, 6), match=&apos;cd&apos;&gt;</span><br></pre></td></tr></table></figure><h3 id="re-findall-pattern-string-flags-0">re.findall(pattern,string,flags=0)</h3><p>查找字符string所有匹配pattern的字符</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> regex <span class="keyword">as</span> re</span><br><span class="line">str = <span class="string">"https://abc https://dcdf https://httpfn https://hello"</span></span><br><span class="line">p2 = <span class="string">"https.+? "</span>    <span class="comment"># pay attention to space here</span></span><br><span class="line">results = re.findall(p2,str)</span><br></pre></td></tr></table></figure><blockquote><p>['<a href="https://abc" target="_blank" rel="noopener">https://abc</a> ', '<a href="https://dcdf" target="_blank" rel="noopener">https://dcdf</a> ', '<a href="https://httpfn" target="_blank" rel="noopener">https://httpfn</a> ']    # pay attention to the last ,since the end of str is \n</p></blockquote><h3 id="re-split-pattern-string-maxflit-0-flags-0">re.split(pattern, string, maxflit=0, flags=0)</h3><p>按照patten对string进行分割</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> regex <span class="keyword">as</span> re</span><br><span class="line">str = <span class="string">"https://abc https://dcdf https://httpfn https://hello"</span></span><br><span class="line">p1 = <span class="string">" "</span></span><br><span class="line">results = re.split(p1,str)</span><br></pre></td></tr></table></figure><blockquote><p>[‘<a href="https://abc" target="_blank" rel="noopener">https://abc</a>’, ‘<a href="https://dcdf" target="_blank" rel="noopener">https://dcdf</a>’, ‘<a href="https://httpfn" target="_blank" rel="noopener">https://httpfn</a>’, ‘<a href="https://hello" target="_blank" rel="noopener">https://hello</a>’]</p></blockquote><h3 id="re-sub-pattern-repl-string-count-0-flags-0">re.sub(pattern,repl,string,count=0,flags=0)</h3><h3 id="re-subn-pattern-repl-string-count-0-flags-0">re.subn(pattern,repl,string,count=0,flags=0)</h3><h3 id="none">…</h3><h2 id="正则表达式对象-regular-express-object">正则表达式对象(regular express object)</h2><p>class re.RegexObject<br>只有re.compile()函数会产生正则表达式对象，正则</p><blockquote><p>only re.compile() will create a direct regular express object,<br>it’s a special class which design for re.compile().<br>正则表达式对象支持下列方法和属性</p></blockquote><ul><li>match(string[,pos[,endpos]])</li><li>search(string[,pos[,endpos]])</li><li>findall(string[,pos[,endpos]])</li><li>split(string,maxsplit=0)</li><li>sub()</li><li>flags</li><li>groups</li><li>groupindex</li><li>pattern</li></ul><h3 id="match-string-pos-endpos">match(string[,pos[,endpos]])</h3><h3 id="search-string-pos-endpos">search(string[,pos[,endpos]])</h3><h3 id="findall-string-pos-endpos">findall(string[,pos[,endpos]])</h3><h3 id="split-string-maxsplit-0">split(string,maxsplit=0)</h3><h3 id="sub">sub()</h3><h3 id="flags-v2">flags</h3><h3 id="groups">groups</h3><h3 id="groupindex">groupindex</h3><h3 id="pattern">pattern</h3><h2 id="匹配对象-match-objects">匹配对象(match objects)</h2><p>class re.MatchObject<br>匹配是否成功</p><blockquote><p>match objects have a boolean value of True.</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">match = re.search(pattern, string)</span><br><span class="line">if match:</span><br><span class="line">   processs(match)</span><br></pre></td></tr></table></figure><p>MatchObject支持以下方法和属性</p><ul><li>group([group1,…])</li><li>groups([default=None])</li><li>groupdict(default=None)</li><li>start([group])</li><li>end([group])</li><li>span([group])</li><li>pos</li><li>endpos</li><li>lstindex</li><li>lastgroup</li><li>re</li><li>string</li></ul><h3 id="group-group1">group([group1,…])</h3><p>group的话pattern需要多个()</p><h3 id="groups-default">groups([default])</h3><p>返回一个元组</p><blockquote><p>return a tuple containing all the subgroups of the match.</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">re.match(<span class="string">r"(\d+)\.(\d+)"</span>,<span class="string">"24.1632"</span>)</span><br><span class="line">m.groups()</span><br></pre></td></tr></table></figure><blockquote><p>(‘24’,‘1632’)</p></blockquote><p>show default</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">m = re.match(<span class="string">r"(\d+)\.?(\d+)?"</span>, <span class="string">"24"</span>)</span><br><span class="line">m.groups()      <span class="comment"># Second group defaults to None.</span></span><br></pre></td></tr></table></figure><blockquote><p>(‘24’, None)</p></blockquote><p>change default to 0</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">m.groups(<span class="string">'0'</span>)  <span class="comment"># Now, the second group defaults to '0'.</span></span><br><span class="line">(<span class="string">'24'</span>, <span class="string">'0'</span>)</span><br></pre></td></tr></table></figure><h2 id="参考文献">参考文献</h2><p>1.<a href="https://stackoverflow.com/questions/180986/what-is-the-difference-between-re-search-and-re-match" target="_blank" rel="noopener">https://stackoverflow.com/questions/180986/what-is-the-difference-between-re-search-and-re-match</a><br>2.<a href="https://devdocs.io/python~3.7/library/re" target="_blank" rel="noopener">https://devdocs.io/python~3.7/library/re</a><br>3.<a href="https://mail.python.org/pipermail/python-list/2014-July/674576.html" target="_blank" rel="noopener">https://mail.python.org/pipermail/python-list/2014-July/674576.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;regex-examples&quot;&gt;regex examples&lt;/h2&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;
      
    
    </summary>
    
      <category term="python" scheme="http://mxxhcm.github.io/categories/python/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="正则表达式" scheme="http://mxxhcm.github.io/tags/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/"/>
    
  </entry>
  
  <entry>
    <title>python数组初始化</title>
    <link href="http://mxxhcm.github.io/2019/04/13/python-%E6%95%B0%E7%BB%84%E5%88%9D%E5%A7%8B%E5%8C%96/"/>
    <id>http://mxxhcm.github.io/2019/04/13/python-数组初始化/</id>
    <published>2019-04-13T06:49:35.000Z</published>
    <updated>2019-06-09T03:10:21.669Z</updated>
    
    <content type="html"><![CDATA[<h2 id="array-initialize">array initialize</h2><p>array_one_dimension =  [ 0 for i in range(cols)]<br>array_multi_dimension  = [[0 for i in range(cols)] for j in range(rows)]</p><h2 id="numpy">numpy</h2><ul><li>numpy.array()</li><li>numpy.zeros()</li><li>numpy.empty()</li></ul><p>返回np.ndarray数组</p><h3 id="np-ndarray属性">np.ndarray属性</h3><p>ndarray.shape        #array的shape<br>ndarray.ndim            #array的维度<br>ndarray.size            #the number of ndarray in array<br>ndarray.dtype        #type of the number in array<br>ndarray.itemsize        #size of the element in array<br>array[array &gt; 0].size    #统计一个数组有多少个非零元素，不论array的维度是多少</p><h3 id="numpy-array">numpy.array()</h3><p>np.array(object,dtype=None,copy=True,order=False,subok=False,ndim=0)</p><h3 id="numpy-zeros">numpy.zeros()</h3><p>np.zeros(shape,dtype=float,order=‘C’)</p><h3 id="numpy-empty">numpy.empty()</h3><p>np.empty(shape,dtype=float,order=‘C’)</p><h3 id="numpy-random-randn-shape">numpy.random.randn(shape)</h3><p>np.random.randn(3,4)</p><h3 id="numpy-arange">numpy.arange()</h3><h3 id="numpy-linspace">numpy.linspace()</h3>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;array-initialize&quot;&gt;array initialize&lt;/h2&gt;
&lt;p&gt;array_one_dimension =  [ 0 for i in range(cols)]&lt;br&gt;
array_multi_dimension  = [[0 for i i
      
    
    </summary>
    
      <category term="python" scheme="http://mxxhcm.github.io/categories/python/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="numpy" scheme="http://mxxhcm.github.io/tags/numpy/"/>
    
  </entry>
  
  <entry>
    <title>python2和python3中的dict</title>
    <link href="http://mxxhcm.github.io/2019/04/13/python-dict/"/>
    <id>http://mxxhcm.github.io/2019/04/13/python-dict/</id>
    <published>2019-04-13T06:46:26.000Z</published>
    <updated>2019-05-06T16:22:27.708Z</updated>
    
    <content type="html"><![CDATA[<h2 id="python2和python3的dict">python2和python3的dict</h2><h3 id="将object转换为dict">将object转换为dict</h3><p>vars([object]) -&gt; dictionary</p><h3 id="python2-dict">python2 dict</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">m_dict = &#123;&apos;a&apos;: 10, &apos;b&apos;: 20&#125;</span><br><span class="line"></span><br><span class="line">values = m_dict.values()</span><br><span class="line">print(type(values))</span><br><span class="line">print(values)</span><br><span class="line">print(&quot;\n&quot;)</span><br><span class="line"></span><br><span class="line">items = m_dict.items()</span><br><span class="line">print(type(items))</span><br><span class="line">print(items)</span><br><span class="line">print(&quot;\n&quot;)</span><br><span class="line"></span><br><span class="line">keys = m_dict.keys()</span><br><span class="line">print(type(keys))</span><br><span class="line">print(keys)</span><br><span class="line">print(&quot;\n&quot;)</span><br><span class="line"></span><br><span class="line">l_values = list(values)</span><br><span class="line">print(type(l_values))</span><br><span class="line">print(l_values)</span><br><span class="line"></span><br><span class="line">输出：</span><br></pre></td></tr></table></figure><h3 id="python3-dict">python3 dict</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">m_dict = &#123;&apos;a&apos;: 10, &apos;b&apos;: 20&#125;</span><br><span class="line"></span><br><span class="line">values = m_dict.values()</span><br><span class="line">print(type(values))</span><br><span class="line">print(values) print(&quot;\n&quot;)</span><br><span class="line"></span><br><span class="line">items = m_dict.items()</span><br><span class="line">print(type(items))</span><br><span class="line">print(items)</span><br><span class="line">print(&quot;\n&quot;)</span><br><span class="line"></span><br><span class="line">keys = m_dict.keys()</span><br><span class="line">print(type(keys))</span><br><span class="line">print(keys)</span><br><span class="line">print(&quot;\n&quot;)</span><br><span class="line"></span><br><span class="line">l_values = list(values)</span><br><span class="line">print(type(l_values))</span><br><span class="line">print(l_values)</span><br></pre></td></tr></table></figure><p>输出：</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;python2和python3的dict&quot;&gt;python2和python3的dict&lt;/h2&gt;
&lt;h3 id=&quot;将object转换为dict&quot;&gt;将object转换为dict&lt;/h3&gt;
&lt;p&gt;vars([object]) -&amp;gt; dictionary&lt;/p&gt;
&lt;
      
    
    </summary>
    
      <category term="python" scheme="http://mxxhcm.github.io/categories/python/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>python中的深复制和浅复制</title>
    <link href="http://mxxhcm.github.io/2019/04/13/python-%E6%B7%B1%E5%A4%8D%E5%88%B6%E5%92%8C%E6%B5%85%E5%A4%8D%E5%88%B6/"/>
    <id>http://mxxhcm.github.io/2019/04/13/python-深复制和浅复制/</id>
    <published>2019-04-13T06:43:31.000Z</published>
    <updated>2019-05-06T16:22:27.708Z</updated>
    
    <content type="html"><![CDATA[<h2 id="简单赋值-浅拷贝-深拷贝">简单赋值，浅拷贝，深拷贝</h2><h3 id="简单赋值">简单赋值</h3><h4 id="str">str</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = <span class="string">'hello'</span></span><br><span class="line">b = <span class="string">'hello'</span></span><br><span class="line">c = a</span><br><span class="line">print(id(a),id(b),id(c))</span><br></pre></td></tr></table></figure><blockquote><p>2432356754632  2432356754632  2432356754632</p></blockquote><p>这里打印出a，b，c的id是一样的，因为他们全是指向’hello’这个字符串在内存中的地址</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = <span class="string">'world'</span></span><br><span class="line">print(id(a),id(b),id(c))</span><br></pre></td></tr></table></figure><blockquote><p>2432356757376  2432356754632  2432356754632</p></blockquote><p>将a指向一个新的字符串’world’,所以变量a的地址就改变了，指向字符串’world’的地址，但是b和c还是指向字符串’hello’的地址。</p><h4 id="list">list</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = [<span class="string">'hello'</span>]</span><br><span class="line">b = [<span class="string">'hello'</span>]</span><br><span class="line">c = a</span><br><span class="line">print(id(a),id(b),id(c))</span><br></pre></td></tr></table></figure><blockquote><p>2432356788424 2432356797064 2432356788424</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">b = [<span class="string">'world'</span>]</span><br><span class="line">print(id(a),id(b),id(c))</span><br></pre></td></tr></table></figure><blockquote><p>2432356798024 2432356797064 2432356788424</p></blockquote><h4 id="结论">结论</h4><p>简单赋值是先给一个变量分配内存，然后把变量的地址赋值给一个变量名。<br>对于一些不可变的类型，比如str，int等，某一个值在内存中的地址是固定的，如果用赋值操作直接指向一个值的话，那么变量名指向的就是这个值在内存中地址。<br>比如a=‘hello’,b=‘hello’,这样a和b的id是相同的，都指向内存中hello的地址<br>对于一些可变的类型，比如list，因为他是可变的，所以如果用赋值操作指向同一个值的话，那么这几个变量的地址也不一样<br>比如a =[‘hello’],b=[‘hello’],这样a和b的id是不同的，虽然他们指向的值是一样的，</p><h3 id="浅拷贝">浅拷贝</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">a = [<span class="string">'hello'</span> , [<span class="number">123</span>] ]</span><br><span class="line">b = a[:]</span><br><span class="line">a = [<span class="string">'hello'</span> , [<span class="number">123</span>] ]</span><br><span class="line">b = a[:]</span><br><span class="line">print(a,b)</span><br><span class="line">print(id(a),id(b))</span><br><span class="line">print(id(a[<span class="number">0</span>]),id(a[<span class="number">1</span>]))</span><br><span class="line">print(id(b[<span class="number">0</span>]),id(b[<span class="number">1</span>]))</span><br></pre></td></tr></table></figure><blockquote><p>[‘hello’, [123]] [‘hello’, [123]]<br>2432356775368 2432356775432 2432356754632 2432356774984<br>2432356754632 2432356774984</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;a[<span class="number">0</span>] = <span class="string">'world'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(a,b)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(id(a),id(b))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(id(a[<span class="number">0</span>]),id(a[<span class="number">1</span>]))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(id(b[<span class="number">0</span>]),id(b[<span class="number">1</span>]))</span><br></pre></td></tr></table></figure><blockquote><p>[‘world’, [123]] [‘hello’, [123]]<br>2432356775368 2432356775432<br>2432356756424 2432356774984<br>2432356754632 2432356774984</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a[<span class="number">1</span>].append(<span class="number">3</span>)</span><br><span class="line">print(a,b)</span><br><span class="line">print(id(a),id(b))</span><br><span class="line">print(id(a[<span class="number">0</span>]),id(a[<span class="number">1</span>]))</span><br><span class="line">print(id(b[<span class="number">0</span>]),id(b[<span class="number">1</span>]))</span><br></pre></td></tr></table></figure><blockquote><p>[‘world’, [123, 3]] [‘hello’, [123, 3]]<br>2432356775368 2432356775432<br>2432356756424 2432356774984<br>2432356754632 2432356774984</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">a[<span class="number">1</span>] = [<span class="number">123</span>]</span><br><span class="line">print(a,b)</span><br><span class="line">print(id(a),id(b))</span><br><span class="line">print(id(a[<span class="number">0</span>]),id(a[<span class="number">1</span>]))</span><br><span class="line">print(id(b[<span class="number">0</span>]),id(b[<span class="number">1</span>]))</span><br><span class="line">``` </span><br><span class="line">&gt; [<span class="string">'world'</span>, [<span class="number">123</span>]] [<span class="string">'hello'</span>, [<span class="number">123</span>, <span class="number">3</span>]]</span><br><span class="line"><span class="number">2432356775368</span> <span class="number">2432356775432</span></span><br><span class="line"><span class="number">2432356756424</span> <span class="number">2432356822984</span></span><br><span class="line"><span class="number">2432356754632</span> <span class="number">2432356774984</span></span><br><span class="line"></span><br><span class="line"><span class="comment">### 深拷贝</span></span><br><span class="line">``` python</span><br><span class="line"><span class="keyword">from</span> copy <span class="keyword">import</span> deepcopy</span><br><span class="line">a = [<span class="string">'hello'</span>,[<span class="number">123</span>,<span class="number">234</span>]</span><br><span class="line">b = deepcopy(a)</span><br></pre></td></tr></table></figure><p>a，b以及a，b中任何元素（除了str，int等类型）的地址都是不一样的</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;简单赋值-浅拷贝-深拷贝&quot;&gt;简单赋值，浅拷贝，深拷贝&lt;/h2&gt;
&lt;h3 id=&quot;简单赋值&quot;&gt;简单赋值&lt;/h3&gt;
&lt;h4 id=&quot;str&quot;&gt;str&lt;/h4&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=
      
    
    </summary>
    
      <category term="python" scheme="http://mxxhcm.github.io/categories/python/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>python special method</title>
    <link href="http://mxxhcm.github.io/2019/04/13/python-special-method/"/>
    <id>http://mxxhcm.github.io/2019/04/13/python-special-method/</id>
    <published>2019-04-13T06:41:38.000Z</published>
    <updated>2019-05-06T16:22:27.708Z</updated>
    
    <content type="html"><![CDATA[<h2 id="结论">结论</h2><p>print(object)就是调用了类对象object的__repr__()函数<br>如下代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Tem</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">     <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__repr__</span><span class="params">(self)</span>:</span></span><br><span class="line">     <span class="keyword">return</span> <span class="string">"tem class"</span></span><br></pre></td></tr></table></figure><p>声明类对象</p><blockquote><blockquote><blockquote><p>Tem tem<br>下面两行代码的功能是一样的。</p></blockquote></blockquote><blockquote><blockquote><p>print(tem)<br>print(repr(tem))</p></blockquote></blockquote></blockquote><h2 id="基本的自定义方法">基本的自定义方法</h2><h3 id="object-new">object.<strong>new</strong></h3><h3 id="object-init">object.<strong>init</strong></h3><h3 id="object-repr-和object-str">object.__repr__和object.<strong>str</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Tem</span><span class="params">(object)</span>:</span></span><br><span class="line">  <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TemStr</span><span class="params">(object)</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__str__</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">'foo'</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TemRepr</span><span class="params">(object)</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__repr__</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">'foo'</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TemStrRepr</span><span class="params">(object)</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__repr__</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">'foo'</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__str__</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">'foo_str'</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>: </span><br><span class="line">   tem = Tem() </span><br><span class="line">   print(str(tem)) </span><br><span class="line">   print(repr(tem)) </span><br><span class="line">   tem_str = TemStr() </span><br><span class="line">   print(str(tem_str)) </span><br><span class="line">   print(repr(tem_str)) </span><br><span class="line">   tem_repr = TemRepr() </span><br><span class="line">   print(str(tem_repr)) </span><br><span class="line">   print(repr(tem_repr)) </span><br><span class="line">   tem_str_repr = TemStrRepr() </span><br><span class="line">   print(str(tem_str_repr)) </span><br><span class="line">   print(repr(tem_str_repr))</span><br></pre></td></tr></table></figure><p>单独重载__repr__，<strong>str__也会调用__repr</strong>，<br>但是单独重载__str__,__repr__不会调用它。<br>__repr__面向的是程序员，而__str__面向的是普通用户。它们都用来返回一个字符串，这个字符串可以是任何字符串，我觉得这个函数的目的就是将对象转化为字符串。</p><h3 id="object-bytes">object.<strong>bytes</strong></h3><h2 id="自定义属性方法">自定义属性方法</h2><h3 id="object-getattr-self-name">object.<strong>getattr</strong>(self, name)</h3><h3 id="object-setattr-self-name">object.<strong>setattr</strong>(self, name)</h3><h2 id="比较">比较</h2><h3 id="object-eq-self-others">object.<strong>eq</strong>(self, others)</h3><h3 id="object-lt-self-others">object.<strong>lt</strong>(self, others)</h3><h3 id="object-le-self-others">object.<strong>le</strong>(self, others)</h3><h3 id="object-ne-self-others">object.<strong>ne</strong>(self, others)</h3><h3 id="object-gt-self-others">object.<strong>gt</strong>(self, others)</h3><h3 id="object-ge-self-others">object.<strong>ge</strong>(self, others)</h3><h2 id="特殊属性">特殊属性</h2><h3 id="object-dict">object.<strong>dict</strong></h3><h3 id="instance-class">instance.<strong>class</strong></h3>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;结论&quot;&gt;结论&lt;/h2&gt;
&lt;p&gt;print(object)就是调用了类对象object的__repr__()函数&lt;br&gt;
如下代码&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;
      
    
    </summary>
    
      <category term="python" scheme="http://mxxhcm.github.io/categories/python/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>gym介绍</title>
    <link href="http://mxxhcm.github.io/2019/04/12/gym%E4%BB%8B%E7%BB%8D/"/>
    <id>http://mxxhcm.github.io/2019/04/12/gym介绍/</id>
    <published>2019-04-12T08:54:44.000Z</published>
    <updated>2019-05-06T16:22:27.708Z</updated>
    
    <content type="html"><![CDATA[<h2 id="简介">简介</h2><p>强化学习中最主要的两类对象是“智能体”和“environment”，然后有一些概念：“reward”、“return”、“state”、“action”、“value”、“policy”、“predict”、“control”等。这些概念把智能体和environment联系了起来。<br>总结起来，有这样几条关系：</p><ol><li>environment会对智能体采取的action做出回应。当智能体执行一个行为时，它需要根据environment本身的动力学来更新environment，也包括更新智能体状态，同时给以智能体一个反馈信息：即时奖励(immediate reward)。</li><li>对于智能体来说，它并不知道整个environment的所有信息，只能通过观测(observation)来获得所需要的信息，它能观测到的信息取决于问题的设置；同样因为智能体需要通过action与environment进行交互，智能体能采取哪些action，也要由智能体和environment协商好。因此environment要确定智能体的观测空间和action空间。</li><li>智能体还需要有一个决策功能，该功能根据当前observation来判断下一时刻该采取什么action，也就是决策过程。</li><li>智能体能执行一个确定的action。（这个刚开始还没想明白，智能体执行什么action干嘛，一般我们写代码不都是env.step(action)，后来才想到是action本身就是智能体自己执行的，只不过代码是这么写，因为environment需要根据这个action，去更新智能体的状态以及environment的状态。）</li><li>智能体应该能从与environment的交互中学到知识，进而在与environment交互时尽可能多的获取reward，最终达到最大化累积奖励(accumate reward)的目的。</li><li>environment应该给智能体设置一个（些）终止条件，即当智能体处在这个状态或这些状态之一时，交互结束，即产生一个完整的Episode。随后重新开始一个Episode或者退出交互。</li></ol><h2 id="自己实现一个environment">自己实现一个environment</h2><p>如果用代码表示上述关系，可以定义为如下式子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Environment</span><span class="params">(object)</span>:</span></span><br><span class="line">  self.aget_state <span class="comment">#</span></span><br><span class="line">  self.states <span class="comment"># 所有可能的状态集合</span></span><br><span class="line">  self.observation_space <span class="comment"># 智能体的observation space</span></span><br><span class="line">  self.action_space <span class="comment"># 智能体体的action space</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># 给出智能体的immediate reward</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">reward</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># 根据智能体的动作，更新环境</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">step</span><span class="params">(self, action)</span>:</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># 当前回合是否结束</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">is_episode_end</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># 生成智能体的obs</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">obs_for_agent</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Agent</span><span class="params">(object)</span>:</span></span><br><span class="line">   self.env = env <span class="comment"># 智能体依附于某一个环境</span></span><br><span class="line">   self.obs <span class="comment"># 智能体的obs</span></span><br><span class="line">   self.reward  <span class="comment"># 智能体获得的immediate reward</span></span><br><span class="line"></span><br><span class="line">   <span class="comment"># 根据当前的obs生成action</span></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">policy</span><span class="params">(self, obs)</span>:</span></span><br><span class="line">      self.action</span><br><span class="line"></span><br><span class="line">   <span class="comment"># 智能体观测到obs和reward</span></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">observe</span><span class="params">(self)</span>:</span></span><br><span class="line">     self.obs = </span><br><span class="line">     self.reward =</span><br></pre></td></tr></table></figure><h2 id="gym">gym</h2><p>gym库在设计environment和智能体的交互时基本上也是按照这几条关系来实现自己的规范和接口的。gym库的核心在文件core.py里，这里定义了两个最基本的类Env和Space。<br>Env类是所有environment类的基类，Space类是所有space类的基类。</p><h2 id="spaces">Spaces</h2><p>Space是一个抽象类，其中包含以下函数，以下几个全是abstract函数，需要在子类中实现</p><ul><li><strong>init</strong>(self, shape=None, dtype=None) 函数初始化shape和dtype以及初始化numpy随机数RandomState()对象。</li><li>sample(self) 函数进行采样，实际上是调用了numpy的随机函数。</li><li>seed(self, seed) 设置numpy随机数种子，这里使用的是RandomState对象，生成随机数，种子一定的情况下，采样的过程是一定的。</li><li>contains(self, x) 函数判断某个对象x是否是这个space中的一个member。</li><li>to_jsonable(self, sample_n)</li><li>from_jsonable(self, sample_n)</li></ul><p>从Space基类派生出几个常用的Space子类，其中最主要的是Discrete类和Box类，其余的还有MultiBinary类，MultiDiscrete类，Tuple类等，每个子类重新实现了__repr__和__eq__以及几乎所有Space类中的函数。<br>最常见的Discrete和Box类，Discrete对应于一维离散空间，Box对应于多维连续空间。它们既可以应用在action space中，也可以用在state space，可以根据具体场景选择。<br>Discrete声明的时候需要给定一个整数，然后整个类的取值在${0, 1, \cdots, n-1}$之间。然后使用sample()函数采样，实际调用的是numpy的randint()进行采样，得到一个整数值。<br>示例</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> gym <span class="keyword">import</span> spaces</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.Discrete</span></span><br><span class="line"><span class="comment"># 取值是&#123;0, 1, ..., n - 1&#125;</span></span><br><span class="line">print(<span class="string">"=================="</span>)</span><br><span class="line">dis = spaces.Discrete(<span class="number">8</span>)</span><br><span class="line">print(dis.shape)</span><br><span class="line">print(dis.n)</span><br><span class="line">print(dis)</span><br><span class="line">dis.seed(<span class="number">4</span>)</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">    print(dis.sample())</span><br></pre></td></tr></table></figure><p>输出结果是：</p><blockquote><p>==================<br>() # shape是None<br>8  # n为8<br>Discrete(8) # repr()函数的值<br>2<br>6<br>7<br>5<br>1</p></blockquote><p>而Box类应用于连续空间，有两种初始化方式，一种是给出最小值，最大值和shape，另一种是直接给出最小值矩阵和最大值矩阵。然后使用sample()函数采样，实际上调用的是numpy的uniform()函数。<br>示例</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> gym <span class="keyword">import</span> spaces</span><br><span class="line"><span class="comment"># 2.Box</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line">print(<span class="string">"=================="</span>)</span><br><span class="line"><span class="comment"># def __init__(self, low=None, high=None, shape=None, dtype=None):</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Two kinds of valid input:</span></span><br><span class="line"><span class="string">    Box(low=-1.0, high=1.0, shape=(3,4)) # low and high are scalars, and shape is provided</span></span><br><span class="line"><span class="string">    Box(low=np.array([-1.0,-2.0]), high=np.array([2.0,4.0])) # low and high are arrays of the same shape</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">box = spaces.Box(low=<span class="number">3.0</span>, high=<span class="number">4</span>, shape=(<span class="number">2</span>,<span class="number">2</span>))</span><br><span class="line">print(box) </span><br><span class="line">box.seed(<span class="number">4</span>)</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">    print(box.sample())</span><br></pre></td></tr></table></figure><p>输出结果是：</p><blockquote><p>==================<br>Box(2, 2) # repr()函数的值<br>[[3.9670298 3.5472322]<br>[3.9726844 3.714816 ]]<br>[[3.6977289 3.2160895]<br>[3.9762745 3.0062304]]</p></blockquote><p>这里给出一个应用场景，例如要描述一个$4\times 4$的网格世界，它一共有16个状态，每一个状态只需要用一个数字来描述即可，这样可以用Discrete(16)对象来表示这个问题的state space。<br>对于经典的小车爬山的问题，小车的state是用两个变量来描述，一个是小车对应目标旗杆的水平距离，另一个是小车的速度，因此environment要描述小车的state需要2个连续的变量。由于小车的state对智能体是完全可见的，因此小车的state space即是小车的observation space，此时不能用Discrete来表示，要用Box类，Box空间定义了多维空间，每一个维度用一个最小值和最大值来约束。同时小车作为智能体可以执行的action有3个：左侧加速、不加速、右侧加速。因此action space可以用Discrete来描述。最终，该environment类的观测空间和行为空间描述如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Env</span><span class="params">(obejct)</span>:</span></span><br><span class="line">  self.min_position = <span class="number">-1.2</span></span><br><span class="line">  self.max_position = <span class="number">0.6</span></span><br><span class="line">  self.max_speed = <span class="number">0.07</span></span><br><span class="line">  self.goal_position = <span class="number">0.5</span> </span><br><span class="line">  self.low = np.array([self.min_position, -self.max_speed])</span><br><span class="line">  self.high = np.array([self.max_position, self.max_speed])</span><br><span class="line">  self.action_space = spaces.Discrete(<span class="number">3</span>)  <span class="comment"># action space,是离散的</span></span><br><span class="line">  self.observation_space = spaces.Box(self.low, self.high) <span class="comment"># 状态空间是连续的</span></span><br></pre></td></tr></table></figure><h2 id="env">Env</h2><h3 id="组成">组成</h3><p>OpenAI官方在gym.core.Env类中给出了如下的说明<br>The main OpenAI Gym class. It encapsulates an environment with arbitrary behind-the-scenes dynamics. An environment can be partially or fully observed.</p><p>用户需要知道的方法主要有下面五个：<br>The main API methods that users of this class need to know are:</p><ul><li>step</li><li>reset</li><li>render</li><li>close</li><li>seed</li></ul><p>用户需要知道的属性主要有下面三个：<br>And set the following attributes:</p><ul><li>action_space: The Space object corresponding to valid actions</li><li>observation_space: The Space object corresponding to valid observations</li><li>reward_range: A tuple corresponding to the min and max possible rewards</li></ul><p>Note: a default reward range set to [-inf,+inf] already exists. Set it if you want a narrower range.</p><p>The methods are accessed publicly as “step”, “reset”, etc… The non-underscored versions are wrapper methods to which we may add functionality over time.</p><p>智能体主要通过环境的几个方法进行交互，用户如果要编写自己的环境的话，需要实现seed, reset, step, close, render等函数。</p><h3 id="step函数执行一个时间步的更新">step函数执行一个时间步的更新。</h3><p>Accepts an action and returns a tuple (observation, reward, done, info).<br>输入参数：<br>action (object):智能体执行的动作<br>返回值：</p><ul><li>observation (object): agent’s observation of the current environment</li><li>reward (float) : amount of reward returned after previous action</li><li>done (boolean): whether the episode has ended, in which case further step() calls will return undefined results</li><li>info (dict): contains auxiliary diagnostic information (调试信息, and sometimes learning)</li></ul><h3 id="reset函数重置">reset函数重置</h3><p>重置环境并返回初始的observation.<br>Returns: observation (object): 返回环境的初始observation</p><h3 id="reder函数绘制">reder函数绘制</h3><p>Renders the environment.</p><h3 id="close函数回收garbge">close函数回收garbge</h3><p>在使用完之后调用close函数清理内存</p><h3 id="seed函数设置环境的随机数种子">seed函数设置环境的随机数种子</h3><p>使用seed函数设置随机数种子，使得结果可以复现。</p><h3 id="使用env类">使用Env类</h3><p>在使用Env类的时候，一种是使用gym中自带的已经注册了的类，另一种是使用自己编写的类。<br>第一种的话，使用如下语句注册：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> gym</span><br><span class="line">env = gym.make(<span class="string">"registered_env_name"</span>)</span><br></pre></td></tr></table></figure><p>另一种自己编写的环境类是和普通的python 类对象声明一样。</p><h2 id="some-issues">Some issues</h2><p>1.&gt;gym.error.DeprecatedEnv: Env PongDeterministic-v4 not found (valid versions include [‘PongDeterministic-v3’, ‘PongDeterministic-v0’])<br>gym版本太老了，升级一下就行[2]。这个是gym$0.7.0$遇到的问题。<br>2.&gt;UserWarning: WARN: &lt;class ‘envs.AtariRescale42x42’&gt; doesn’t implement ‘observation’ method. Maybe it implements deprecated ‘_observation’ method.<br>这个是gym版本太新了，apis进行了重命名。这个是gym$0.12.0$遇到的问题。<br>上面两个问题都是在测试github上的一个<img src="https://github.com/ikostrikov/pytorch-a3c/" alt="A3C">代码遇到的。最后装了$0.9$版本的gym就没有警告了。（测试了一下，装$0.10$版本的也不行）</p><h2 id="参考文献">参考文献</h2><p>1.<a href="https://github.com/openai/gym" target="_blank" rel="noopener">https://github.com/openai/gym</a><br>2.<a href="https://github.com/ikostrikov/pytorch-a3c/issues/36" target="_blank" rel="noopener">https://github.com/ikostrikov/pytorch-a3c/issues/36</a><br>3.<a href="https://github.com/openai/roboschool/issues/169" target="_blank" rel="noopener">https://github.com/openai/roboschool/issues/169</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;简介&quot;&gt;简介&lt;/h2&gt;
&lt;p&gt;强化学习中最主要的两类对象是“智能体”和“environment”，然后有一些概念：“reward”、“return”、“state”、“action”、“value”、“policy”、“predict”、“control”等。这些
      
    
    </summary>
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="gym" scheme="http://mxxhcm.github.io/tags/gym/"/>
    
  </entry>
  
  <entry>
    <title>reinforcement learning an introduction 第4章笔记</title>
    <link href="http://mxxhcm.github.io/2019/04/07/reinforcement-learning-an-introduction-%E7%AC%AC4%E7%AB%A0%E7%AC%94%E8%AE%B0/"/>
    <id>http://mxxhcm.github.io/2019/04/07/reinforcement-learning-an-introduction-第4章笔记/</id>
    <published>2019-04-07T15:46:17.000Z</published>
    <updated>2019-08-30T03:40:06.869Z</updated>
    
    <content type="html"><![CDATA[<h2 id="原理">原理</h2><p>Policy iteration有两种方式实现，一种是使用两个数组，一个保存原来的值，一个用来进行更新，这种方法是雅克比方法，或者叫同步的方法，因为他可以并行的进行。<br>In-place的方法是高斯赛德尔方法。就是用来解方程组的迭代法。</p><h2 id="dynamic-programming">Dynamic Programming</h2><p>DP指的是给定环境的模型，通常是一个MDP，计算智能体最优策略的一类算法。经典的DP算法应用场景有限，因为它需要环境的模型，以及很高的计算代价，但是DP的思路是很重要的。其他的许多算法都是在减少计算代价和环境信息的前提下尽可能获得和DP接近的性能。<br>通常我们假定环境是一个有限(finite)的MDP，也就是state, action, reward都是有限的。尽管DP可以应用于连续(continuous)的state和action space，但是只能应用在几个特殊的场景上。一个常见的做法是将连续state和action quantize(量化)，然后使用有限MDP。<br>DP关键在于使用value function寻找好的policy，在找到了满足Bellman optimal equation的optimal value function之后，可以找到optimal policy，参见<a href="https://mxxhcm.github.io/2018/12/21/reinforcement-learning-an-introduction-%E7%AC%AC3%E7%AB%A0%E7%AC%94%E8%AE%B0/">第三章推导</a>：<br>Bellman optimal equation:<br>\begin{align*}<br>v_{*}(s) &amp;= max_a\mathbb{E}\left[R_{t+1}+\gamma v_{*}(S_{t+1})|S_t=s,A_t=a\right] \\<br>&amp;= max_a \sum_{s’,r} p(s’,r|s,a){*}\left[r+\gamma v_{*}(s’)\right]  \tag{1}<br>\end{align*}</p><p>\begin{align*}<br>q_{*}(s,a) &amp;= \mathbb{E}\left[R_{t+1}+\gamma max_{a’}q_{*}(S_{t+1},a’)|S_t=s,A_t = a\right]\\<br>&amp;= \sum_{s’,r} p(s’,r|s,a) \left[r + \gamma max_a q_{*}(s’,a’)\right] \tag{2}<br>\end{align*}</p><h2 id="policy-evaluation-prediction">Policy Evaluation(Prediction)</h2><p>给定一个policy，计算state value function的过程叫做policy evaluation或者prediction problem。<br>根据$v(s)$和它的后继状态$v(s’)$之间的关系：<br>\begin{align*}<br>v_{\pi}(s) &amp;= \mathbb{E}_{\pi}[G_t|S_t = s]\\<br>&amp;= \mathbb{E}_{\pi}\left[R_{t+1}+\gamma G_{t+1}|S_t = s\right]\\<br>&amp;= \sum_a \pi(a|s)\sum_{s’}\sum_rp(s’,r|s,a) \left[r + \gamma \mathbb{E}_{\pi}\left[G_{t+1}|S_{t+1}=s’\right]\right] \tag{3}\\<br>&amp;= \sum_a \pi(a|s)\sum_{s’,r}p(s’,r|s,a) \left[r + \gamma v_{\pi}(s’) \right] \tag{4}\\<br>\end{align*}<br>只要$\gamma \lt 1$或者存在terminal state，那么$v_{\pi}$的必然存在且唯一。这个我觉得是迭代法解方程的条件。数值分析上有证明。<br>如果环境的转换概率$p$是已知的，可以列出方程组，直接求解出每个状态$s$的$v(s)$。这里采用迭代法求解，随机初始化$v_0$，使用式子$(4)$进行更新：<br>\begin{align*}<br>v_{k+1}(s) &amp;= \mathbb{E}\left[R_{t+1} + \gamma v_k(S_{t+1})\ S_t=s\right]\\<br>&amp;= \sum_a \pi(a|s)\sum_{s’,r}p(s’,r|s,a) \left[r + \gamma v_k(s’) \right] \tag{5}<br>\end{align*}<br>直到$v_k=v_{\pi}$到达fixed point，Bellman equation满足这个条件。当$k\rightarrow \infty$时收敛到$v_{\pi}$。这个算法叫做iterative policy evaluation。<br>在每一次$v_k$到$v_{k+1}$的迭代过程中，所有的$v(s)$都会被更新，$s$的旧值被后继状态$s’$的旧值加上reward替换，正如公式$(5)$中体现的那样。这个目标值被称为expected update，因为它是基于所有$s’$的期望计算出来的（利用环境的模型），而不是通过对$s’$采样计算的。<br>在实现iterative policy evaluation的时候，每一次迭代，都需要重新计算所有$s$的值。这里有一个问题，就是你在每次更新$s$的时候，使用的$s’$如果在本次迭代过程中已经被更新过了，那么是使用更新过的$s’$，还是使用没有更新的$s’$，这就和迭代法中的雅克比迭代以及高斯赛德尔迭代很像，如果使用更新后的$s’$，这里我们叫它in-place的算法，否则就不是。具体那种方法收敛的快，还是要看应用场景的，并不是in-place的就一定收敛的快，这是在数值分析上学到的。<br>下面给出in-place版本的iterative policy evation算法伪代码。<br><strong>iterative policy evation 算法</strong><br><strong>输入</strong>需要evaluation的policy $\pi$<br>给出算法的参数：阈值$\theta\gt 0$，当两次更新的差值小于这个阈值的时候，就停止迭代，随机初始化$V(s),\forall s\in S^{+}$，除了$V(terminal) = 0$。<br><strong>Loop</strong><br>$\qquad \delta \leftarrow 0$<br>$\qquad$ <strong>for</strong> each $s\in S$<br>$\qquad\qquad v\leftarrow V(s)$ （保存迭代之前的$V(s)$）<br>$\qquad\qquad V(s)\leftarrow\sum_a \pi(a|s)\sum_{s’,r}p(s’,r|s,a) \left[r + \gamma v_k(s’) \right] $<br>$\qquad\qquad \nabla \leftarrow max(\delta,|v-V(s)|)$<br>$\qquad$<strong>end for</strong><br><strong>until</strong> $\delta \lt \theta$</p><h2 id="policy-improvement">Policy Improvement</h2><p>为什么要进行policy evaluation，或者说为什么要计算value function？<br>其中一个原因是为了找到更好的policy。假设我们已经知道了一个deterministic的策略$\pi$，但是在其中一些状态，我们想要知道是不是有更好的action选择，如$a\neq \pi(s)$的时候，是不是这个改变后的策略会更好。好该怎么取评价，这个时候就可以使用值函数进行评价了，在某个状态，我们选择$a \neq \pi(s)$，在其余状态，依然遵循策略$\pi$。用公式表示为：<br>\begin{align*}<br>q_{\pi}(s,a) &amp;= \mathbb{E}\left[R_{t+1}+\gamma v_{\pi}(S_{t+1})|S_t=s,A_t = a\right]\\<br>&amp;=\sum_{s’,r}p(s’,r|s,a)\left[r+\gamma v_{\pi}(s’)\right] \tag{6}<br>\end{align*}<br>那么，这个值是是比$v(s)$要大还是要小呢？如果比$v(s)$要大，那么这个新的策略就比$\pi$要好。<br>用$\pi$和$\pi’$表示任意一对满足下式的deterministic policy：<br>$$q_{\pi}(s,\pi’(s)) \ge v_{\pi}(s) \tag{7}$$<br>那么$\pi’$至少和$\pi$一样好。可以证明，任意满足$(7)$的$s$都满足下式：<br>$$v_{\pi’}(s) \ge v_{\pi}(s) \tag{8}$$<br>对于我们提到的$\pi$和$\pi’$来说，除了在状态$s$处，$v_{\pi’}(s) = a \neq v_{\pi}(s)$，在其他状态处$\pi$和$\pi’$是一样的，都有$q_{\pi}(s,\pi’(s)) = v_{\pi}(s)$。而在状态$s$处，如果$q_{\pi}(s,a) \gt v_{\pi}(s)$，注意这里$a=\pi’(s)$，那么$\pi’$一定比$\pi$好。<br>证明：<br>\begin{align*}<br>v_{\pi}(s) &amp;\le q_{\pi}(s,\pi’(s))\\<br>&amp; = \mathbb{E}\left[R_{t+1} + \gamma v_{\pi}(S_{t+1})|S_t = s, A_t = \pi’(s) \right]\\<br>&amp; = \mathbb{E}_{\pi’}\left[R_{t+1} + \gamma v_{\pi}(S_{t+1})|S_t = s \right]\\<br>&amp; \le \mathbb{E}_{\pi’}\left[R_{t+1} + \gamma q_{\pi}(S_{t+1},\pi’(S_{t+1}))|S_t = s \right]\\<br>&amp; = \mathbb{E}_{\pi’}\left[ R_{t+1} + \gamma \mathbb{E}_{\pi’}\left[R_{t+2} +\gamma v_{\pi}(S_{t+2})|S_{t+1}, A_{t+1}=\pi’(S_{t+1})|S_t = s \right]\right]\\<br>&amp; = \mathbb{E}_{\pi’}\left[ R_{t+1} + \gamma R_{t+2} +\gamma^2 v_{\pi}(S_{t+2})|S_t = s \right]\\<br>&amp; \le \mathbb{E}_{\pi’}\left[ R_{t+1} + \gamma R_{t+2} +\gamma^2 R_{t+3}  +\gamma^3 v_{\pi}(S_{t+3})|S_t = s \right]\\<br>&amp; \le \mathbb{E}_{\pi’}\left[ R_{t+1} + \gamma R_{t+2} +\gamma^2 R_{t+3}  +\gamma^3 R_{t+4} + \cdots |S_t = s \right]\\<br>&amp;=v_{\pi’}(s)<br>\end{align*}<br>所以，在计算出一个policy的value function的时候，很容易我们就直到某个状态$s$处的变化是好还是坏。扩展到所有状态和所有action的时候，在每个state，根据$q_{\pi}(s,a)$选择处最好的action，这样就得到了一个greedy策略$\pi’$，给出如下定义：<br>\begin{align*}<br>\pi’(s’) &amp;= argmax_{a} q_{\pi}(s,a)\\<br>&amp; = argmax_{a} \mathbb{E}\left[R_{t+1} + \gamma v_{\pi}(S_{t+1} |S_t=a,A_t=a)\right] \tag{9}\\<br>&amp; = argmax_{a} \sum_{s’,r}p(s’,r|s,a)\left[r+v_{\pi}(s’) \right]<br>\end{align*}<br>可以看出来，该策略的定义一定满足式子$(7)$，所以$\pi’$比$\pi$要好或者相等，这就叫做policy improvement。当$\pi’$和$\pi$相等时，，根据式子$(9)$我们有：<br>\begin{align*}<br>v_{\pi’}(s’)&amp; = max_{a} \mathbb{E}\left[R_{t+1} + \gamma v_{\pi’}(S_{t+1} |S_t=a,A_t=a)\right] \tag{9}\\<br>&amp; = max_{a} \sum_{s’,r}p(s’,r|s,a)\left[r+v_{\pi’}(s’) \right]<br>\end{align*}<br>这和贝尔曼最优等式是一样的？？？殊途同归！！！<br>但是，需要说的一点是，目前我们假设的$\pi$和$\pi’$是deterministic，当$\pi$是stochastic情况的时候，其实也是一样的。只不过，原来我们每次选择的是使得$v_{\pi}$最大的action。对于stochastic的情况来说，输出的是每个动作的概率，可能有几个动作都能使得value function最大，那就让这几个动作的概率一样大，比如是$n$个动作，都是$\frac{1}{n}$。</p><h2 id="policy-iteration">Policy Iteration</h2><p>我们已经讲了Policy Evaluation和Policy Improvement，Evalution会计算出一个固定$\pi$的value function，Improvment会根据value function改进这个policy，然后计算出一个新的policy $\pi’$，对于新的策略，我们可以再次进行Evaluation，然后在Improvement，就这样一直迭代，对于有限的MDP，我们可以求解出最优的value function和policy。这就是Policy Iteration算法。</p><p><strong>Policy Iteration算法</strong><br><strong>1.初始化</strong><br>$V(s)\in R,\pi(s) in A(s)$<br>$\qquad$<br><strong>2.Policy Evaluation</strong><br><strong>Loop</strong><br>$\qquad\Delta\leftarrow 0 $<br>$\qquad$ <strong>For</strong> each $s\in S$<br>$\qquad\qquad v\leftarrow V(s)$<br>$\qquad\qquad V(s)\leftarrow \sum_{s’,r}p(s’,r|s,a)\left[r+\gamma V(s’)\right]$<br>$\qquad\qquad \Delta \leftarrow max(\Delta, |v-V(s)|) $<br><strong>until</strong> $\Delta \lt \theta$<br><strong>3.Policy Improvement</strong><br>$policy-stable\leftarrow true$<br><strong>For</strong> each $s \in S$<br>$\qquad old_action = \pi(s)$<br>$\qquad \pi(s) = argmax_a \sum_{s’,a’}p(s’,r|s,a)\left[r+\gamma V(s’)\right]$<br>$\qquad If\ old_action \neq \pi(s), policy-stable\leftarrow false$<br><strong>If policy-stable</strong>，停止迭代，返回$V$和$\pi$，否则回到2.Policy Evalution继续执行。</p><h2 id="value-iteration">Value Iteration</h2><p>从Policy Iteration算法中我们可以看出来，整个算法分为两步，第一步是Policy Evaluation，第二步是Policy Improvement。而每一次Policy Evaluation都要等到Value function收敛到一定程度才结束，这样子就会非常慢。一个替代的策略是我们尝试每一次Policy Evaluation只进行几步的话，一种特殊情况就是每一个Policy Evaluation只进行一步，这种就叫做Value Iteration。给出如下定义：<br>\begin{align*}<br>v_{k+1}(s) &amp;= max_a \mathbb{E}\left[R_{t+1} + \gamma v_k(S_{t+1})| S_t=s, A_t = a\right]\\<br>&amp;= max_a \sum_{s’,r}p(s’,r|s,a) \left[r+\gamma v_k(s’)\right] \tag{10}<br>\end{align*}<br>它其实就是把两个步骤给合在了一起，原来分开是：<br>\begin{align*}<br>v_{\pi}(s) &amp;= \mathbb{E}\left[R_{t+1} + \gamma v_k(S_{t+1})| S_t=s, A_t = a\right]\\<br>&amp;= \sum_{s’,r}p(s’,r|s,a) \left[r+\gamma v_k(s’)\right]\\<br>v_{\pi’}(s) &amp;= max_a \sum_{s’,r}p(s’,r|s,a) \left[r+\gamma v_{\pi}(s’)\right]\\<br>\end{align*}<br>另一种方式理解式$(10)$可以把它看成是使用贝尔曼最优等式进行迭代更新，Policy Evaluation用的是贝尔曼期望等式进行更新。下面给出完整的Value Iteration算法</p><p><strong>Value Iteration 算法</strong><br><strong>初始化</strong><br>阈值$\theta$，以及随机初始化的$V(s), s\in S^{+}$，$V(terminal)=0$。<br><strong>Loop</strong><br>$\qquad v\leftarrow V(s)$<br>$\qquad$<strong>Loop</strong> for each $s\in S$<br>$\qquad\qquad V(s) = max_a\sum_{s’,r}p(s’,r|s,a)\left[r+\gamma V(s’)\right]$<br>$\qquad\qquad\Delta \leftarrow max(Delta, |v-V(s)|)$<br><strong>until</strong> $\Delta \lt \theta$<br><strong>返回</strong> 输出一个策略$\pi\approx\pi_{*}$，这里书中说是deterministic，我觉得都可以，$\pi$也可以是stochastic的，最后得到的$\pi$满足:<br>$\pi(s) = argmax_a\sum_{s’,r}p(s’,r|s,a)\left[r+\gamma V(s’)\right]$</p><h2 id="asychronous-dynamic-programming">Asychronous Dynamic Programming</h2><p>之前介绍的这些DP方法，在每一次操作的时候，都有对所有的状态进行处理，这就很耗费资源。所以这里就产生了异步的DP算法，这类算法在更新的时候，不会使用整个的state set，而是使用部分state进行更新，其中一些state可能被访问了很多次，而另一些state一次也没有被访问过。<br>其中一种异步DP算法就是在plicy evalutaion的过程中，只使用一个state。<br>使用DP算法并不代表一定能减少计算量，他只是减少在策略没有改进之前陷入无意义的evaluation的可能。尽量选取那些重要的state用来进行更新。<br>同时，异步DP方便进行实时的交互。在使用异步DP更新的时候，同时使用一个真实场景中的agent经历进行更新。智能体的experience可以被用来确定使用哪些state进行更新，DP更新后的值也可以用来指导智能体的决策。</p><h2 id="generalized-policy-iteration">Generalized Policy Iteration</h2><p>之前介绍了三类方法，Policy Iteration,Value iteration以及Asychronous DP算法，它们都有两个过程在不断的迭代进行。一个是evaluation，一个是improvement，这类算法统一的被称为Generalized Policy Iteration(GPI)，可以根据不同的粒度进行细分。基本上所有的算法都是GPI，policy使用value function进行改进，value function朝着policy的真实值函数改进，如果value function和policy都稳定之后，那么说他们都是最优的了。<br>GPI中evalution和improvemetnt可以看成既有竞争又有合作。竞争是因为evaluation和improment的方向通常是相对的，policy改进意味着value function不适用于当前的policy,value function更新意味着policy不是greedy的。然后长期来说，他们共同作用，想要找到最优的值函数和policy。<br>GPI可以看成两个目标的交互过程，这两个目标不是正交的，改进一个目标也会使用另一个目标有所改进，直到最后，这两个交互过程使得总的目标变成最优的。</p><h2 id="efficiency-of-dynamic-programming">Efficiency of Dynamic Programming</h2><p>用$n$和$k$表示MDP的状态数和动作数，DP算法保证在多项式时间内找到最优解，即使策略的总数是$k^n$个。<br>DP比任何在policy space内搜索的算法要快上指数倍，因为policy space搜索需要检查每一个算法。Linear Programming算法也可以用来解MDP问题，在某些情况下最坏的情况还要比DP算法快，但是LP要比只适合解决state数量小的问题。而DP也能处理states很大的情况。</p><h2 id="summary">Summary</h2><ul><li>使用贝尔曼公式更新值函数，可以使用backup diagram看他们的直观表示。</li><li>基本上所有的强化学习算法都可以看成GPI(generalized policy iteraion)，先评估某个策略，然后改进这个策略，评估新的策略…这样子循环下去，直到收敛，找到一个不在变化的最优值函数和策略。<br>GPI不一定是收敛的，本章介绍的这些大多都是收敛的，但是还有一些没有被证明收敛。</li><li>可以使用异步的DP算法。</li><li>所有的DP算法都有一个属性叫做bootstrapping，即基于其他states的估计更新每一个state的值。因为每一个state value的更新都需要用到他们的successor state的估计。</li></ul><blockquote><p>They update estimates onthe basis of other estimates。</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;原理&quot;&gt;原理&lt;/h2&gt;
&lt;p&gt;Policy iteration有两种方式实现，一种是使用两个数组，一个保存原来的值，一个用来进行更新，这种方法是雅克比方法，或者叫同步的方法，因为他可以并行的进行。&lt;br&gt;
In-place的方法是高斯赛德尔方法。就是用来解方程组的
      
    
    </summary>
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="动态规划" scheme="http://mxxhcm.github.io/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"/>
    
  </entry>
  
  <entry>
    <title>reinforcement learning an introduction 第9章笔记</title>
    <link href="http://mxxhcm.github.io/2019/04/04/reinforcement-learning-an-introduction-%E7%AC%AC9%E7%AB%A0%E7%AC%94%E8%AE%B0/"/>
    <id>http://mxxhcm.github.io/2019/04/04/reinforcement-learning-an-introduction-第9章笔记/</id>
    <published>2019-04-04T02:14:08.000Z</published>
    <updated>2019-08-30T03:44:59.932Z</updated>
    
    <content type="html"><![CDATA[<h2 id="on-policy-prediction-with-approximation">On-policy Prediction with Approximation</h2><p>这一章讲的是利用on-policy的数据估计函数形式的值函数，on-policy就是说利用一个已知的policy $\pi$生成的experience来估计$v_{\pi}$。和之前讲的不同的是，前面几章讲的是表格形式的值函数，而这一章是使用参数为$\mathbf{w}\in R^d$的函数表示。即$\hat{v}(s,\mathbf{w})\approx v_{\pi}(s)$表示给定一个权值vector $\mathbf{w}$，state $s$的状态值。这个函数可以是任何形式的，可以是线性函数，也可以是神经网络，还可以是决策树。</p><h2 id="值函数估计">值函数估计</h2><p>目前这本书介绍的所有prediction方法都是更新某一个state的估计值函数向backed-up value（或者叫update target）值移动。我们用符号$s\mapsto u$表示一次更新。其中$s$是要更新的状态，$u$是$s$的估计值函数的update target。例如，Monte Carlo更新的value prediction是：$S_t \mapsto G_t$，TD(0)的update是：$S_t \mapsto R_{t+1} + \gamma \hat{v}(S_{t+1}, \mathbf{w}_t)$，$n$-step TD update是：$S_t \mapsto G_{t:t+n}$。在DP policy evaluation update中是：$s\mapsto E_{\pi}[R_{t+1}+\gamma\hat{v}(S_{t+1}, \mathbf{w}_t)| S_t =s]$，任意一个状态$s$被更新了，同时在其他真实experience中遇到的$S_t$也被更新了。</p><p>之前表格的更新太trivial了，更次更新$s$向$u$移动，其他状态的值都保持不变。现在使用函数实现更新，在状态$s$处的更新，可以一次性更新很多个其他状态的值。就像监督学习学习input和output之间的映射一样，我们可以把$s\mapsto g$的更新看做一个训练样本。这样就可以使用很多监督学习的方法学习这样一个函数。<br>但是并不是所有的方法都适用于强化学习，因为许多复杂的神经网络和统计学方法都假设训练集是静态不变的。然而强化学习中，学习是online的，即智能体不断地与环境进行交互产生新的数据，这就需要这个方法能够从不断增加的数据中高效的学习。<br>此外，强化学习通常需要function approximation能够处理target function不稳定的情况，即target function随着事件在不断的变化。比如，在基于GPI的control方法中，在$\pi$不断变化的情况下，我们想要学习出$q_{\pi}$。即使policy保持不变，如果使用booststrapping方法（DP和TD学习），训练样本的target value也在不断的改变，因为下一个state的value值在不断的改变。所以不能处理这些不稳定情况的方法有点不适合强化学习。</p><h2 id="预测目标-the-prediction-objective">预测目标(The Prediction Objective)</h2><p>表格形式的值函数最终都会收敛到真值，状态值之间也都是解耦的，即更新一个state不影响另一个state。<br>但是使用函数拟合，更新一个state的估计值就会影响很多个其他状态，并且不可能精确的估计所有states的值。假设我们的states比weights多的多，让一个state的估计更精确也意味着使得其他的state越不accurate。我们用一个state $s$上的分布,$\mu(s)\ge 0,\sum_s\mu(s)=1$代表对每个state上error的权重。然后使用$\mu(s)$对approximate value $\hat{v}(s,\mathbf{w})$和true value $v_{\pi}(s)$的squared error进行加权，得到Mean Squared Value Error，表示为$\bar{VE}$：<br>$$\bar{VE}(\mathbf{w}) = \sum_{s\in S}\mu(s)[v_{\pi}(s) - \hat{v}(s, \mathbf{w})]^2$$<br>通常情况下，$\mu(s)$是在state $s$处花费时间的百分比。在on-policy训练中，这叫做on-policy分布。在continuing tasks中，策略$\pi$下的on-policy分布是一个stationary distribution。<br>在episodic tasks中，on-policy分布有一些不同，因为它还取决于每个episodic的初始状态，用$h(s)$表示在一个episodic开始状态为$s$的概率，用$\eta(s)$表示在一个回合中，state $s$平均被访问的次数。<br>$$\eta(s) = h(s) + \sum_{\bar{s}}\eta(\bar{s})\sum_a\pi(a|\bar{s})p(s|\bar{s},a), forall\ s \in S$$<br>其中$\bar{s}$是$s$的前一个状态，$s$处的时间为以状态$s$开始的概率$h(s)$加上它由前一个状态$\bar{s}$转换过来消耗的时间。<br>列出一个方程组，可以解出来$\eta(s)$的期望值。然后进行归一化，得到：<br>$$\mu(s)=\frac{\eta{s}}{\sum_{s’}\eta{s’}}, \forall s \in S.$$<br>这是没有折扣因子的式子，如果有折扣因子的话，可以看成一种形式的</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;on-policy-prediction-with-approximation&quot;&gt;On-policy Prediction with Approximation&lt;/h2&gt;
&lt;p&gt;这一章讲的是利用on-policy的数据估计函数形式的值函数，on-policy就是说
      
    
    </summary>
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="函数近似" scheme="http://mxxhcm.github.io/tags/%E5%87%BD%E6%95%B0%E8%BF%91%E4%BC%BC/"/>
    
      <category term="on-policy" scheme="http://mxxhcm.github.io/tags/on-policy/"/>
    
      <category term="值函数" scheme="http://mxxhcm.github.io/tags/%E5%80%BC%E5%87%BD%E6%95%B0/"/>
    
  </entry>
  
  <entry>
    <title>引导和分区</title>
    <link href="http://mxxhcm.github.io/2019/04/03/%E5%BC%95%E5%AF%BC%E5%92%8C%E5%88%86%E5%8C%BA/"/>
    <id>http://mxxhcm.github.io/2019/04/03/引导和分区/</id>
    <published>2019-04-03T08:15:36.000Z</published>
    <updated>2019-05-06T16:22:27.708Z</updated>
    
    <content type="html"><![CDATA[<h2 id="硬盘逻辑划分">硬盘逻辑划分</h2><p>分区可以说是对硬盘的一种格式化。创建分区设置好硬盘的各项物理参数，指定了硬盘主引导记录（即Master Boot Record，一般简称为MBR）和引导记录备份的存放位置。而对于文件系统以及其他操作系统管理硬盘所需要的信息则是通过以后的高级格式化，即 Format命令来实现。面、磁道和扇区硬盘分区后，将会被划分为面（Side）、磁道（Track）和扇区（Sector）。需要注意的是，这些只是个 虚拟的概念，并不是真正在硬盘上划轨道。</p><p><strong>面，磁头，柱面</strong> 硬盘一般是由一片或几片圆形薄片叠加而成的。每个圆形薄片都有两个“面”，这两个面都可以用来存储数据的。按照面的顺序，依次称为0 面，1面，…，每个面都都有一个读写磁头，也常用0头，1头，…，按照硬盘容量和规格的不同，硬盘面数(或头数)也各有差异。每个硬盘上所有硬盘面数磁道号相同的磁道叠起来，称为一个柱面(Cylinder)。</p><p><strong>磁道，扇区</strong> 由于磁盘通过旋转磁头读取或者写入数据，磁头旋转的时候就形成了一个圆周。这样的圆周就称为一个磁道。如果磁头沿着面的半径移动，就到了另外一个磁道。根据硬盘的不同，磁道数可以从几百到数千不等；一个磁道上可以容纳数KB 的数据，而主机读写时往往并不需要一次读写那么多，于是，磁道又被划分成若干段，每段称为一个扇区。一个扇区一般存放512字节的数据。对同一磁道中的扇区进行编号：1扇区，2扇区，…<br>计算机对硬盘的读写，出于效率的考虑，以扇区为基本单位。即计算机如果只需要硬盘上存储的某个字节，也必须一次把这个字节所在的扇区中的512字节全部 读入内存，再使用所需的那个字节。为了区分每个山区，在每个扇区存取的数据前、后两端，都有一些特定的数据，这些数据构成了扇区的界限标志，标志中含有扇区的编号和其他信息。计算机凭借着这些标志来识别扇区。</p><h2 id="硬盘分区">硬盘分区</h2><p>硬盘的数据按照特点和作用可以分为$5$部分，引导区，DBR区，FAT区，DIR区和DATA区。<br>引导区常见的有MBR和GPT。<br>DBR是操作系统引导记录区<br>FAT区存放的是文件簇信息。常见的有FAT16和FAT32<br>DIR是根目录区<br>DATA区存放数据</p><h2 id="bios-uefi和mbr-gpt">BIOS,UEFI和MBR,GPT</h2><p>BIOS和UEFI是常见的引导，MBR和GPT是分区表类型。<br>BIOS(Basic Input Output System)<br>UEFI(Unifed Extensible Firmware Interface)<br>MBR(Master Boot Record)<br>GPT(GUID Partion Table)</p><h2 id="mbr">MBR</h2><p>传统的MBR，位于整个硬盘的$0$磁道$0$柱面$1$扇区，也叫主引导扇区，总计$512$个字节。MBR只占用了$446$个字节，剩下的$64$个字节用来保存硬盘的分区表(Disk Partion Talbe, DPT)，最多只有四个表项，也就是我们常遇到的最多只能设置四个主分区（或者$3$个主分区，$1$个扩展分区和无限制个数的逻辑驱动器），每个表项只有$16$个字节，每一个分区使用$4$个字节存储总扇区数，每个分区不能大于$2TB(2^{32}\times 512 bytes$)，就是$2^{32}$个扇区，每个扇区按$512$字节来算，其他$12$个字节用来存储分区的其他信息。如图所示：<br><img src="/2019/04/03/引导和分区/mbr.jpeg" alt="mbr"></p><h2 id="gpt">GPT</h2><p>GPT分区需要需要操作系统更支持，可以有任何个数个主分区，每个分区都可以大于$2$T，它是基于UEFI使用的磁盘分区架构。</p><h2 id="uefi">UEFI</h2><p>UEFI是用来取代BIOS的，UEFI启动系统引导的方法是查找硬盘分区中第一个FAT分区内的引导文件进行系统分区，不具体指定分区表区。<br>FAT分区内可以存放MBR分区表，也可以存放GPT分区表。</p><h2 id="从gpt硬盘启动">从GPT硬盘启动</h2><p>从GPT分区硬盘启动需要满足三个条件：</p><ul><li>操作系统支持，windows只有64为操作系统支持</li><li>硬盘使用GPT分区</li><li>主板使用UEFI模式</li></ul><h2 id="引导和分区类型匹配">引导和分区类型匹配</h2><h3 id="bios-mbr">BIOS + MBR</h3><p>所有系统都支持，不支持大于$2$T的硬盘。</p><h3 id="bios-gpt">BIOS + GPT</h3><p>BIOS可以使用GPT分布表，将GPT硬盘作为资料盘，但是不能用来引导系统，而且必须使用$64$位系统。</p><h3 id="uefi-legacy-mbr">UEFI(legacy) + MBR</h3><p>可以将UEFI设置为legacy(传统模式)，支持MBR启动，和BIOS+MBR一样，也可以建立FAT分区，放置UEFI启动文件。</p><h3 id="uefi-gpt">UEFI + GPT</h3><p>可以把大于$2$T的硬盘当做系统盘，必须使用$64$位系统。</p><h2 id="双系统">双系统</h2><p>安装双系统直接进windows，使用EasyUEFI/Easybcd(工具)添加linux启动项，或者使用windows命令，bcdedit进行编辑（文档参见msdn,推荐使用这种方法）。<br>双系统直接进ubuntu，使用grub引导，执行update-grub自动修改/boot/grub/grub.cfg 文件。然后重启就会发现有了这个开机启动项，见参考文献[3]。</p><p>可以参考参考文献[3]，或者参考文献[4]。</p><h2 id="参考文献">参考文献</h2><p>1.<a href="https://blog.csdn.net/hyy5801965/article/details/51136395" target="_blank" rel="noopener">https://blog.csdn.net/hyy5801965/article/details/51136395</a><br>2.<a href="https://www.cnblogs.com/zhangming-blog/articles/5392115.html" target="_blank" rel="noopener">https://www.cnblogs.com/zhangming-blog/articles/5392115.html</a><br>3.<a href="https://askubuntu.com/a/945988" target="_blank" rel="noopener">https://askubuntu.com/a/945988</a><br>4.<a href="https://askubuntu.com/a/217970" target="_blank" rel="noopener">https://askubuntu.com/a/217970</a><br>5.<a href="http://lanlingzi.cn/post/notes/2016/0313_grub_win10/" target="_blank" rel="noopener">http://lanlingzi.cn/post/notes/2016/0313_grub_win10/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;硬盘逻辑划分&quot;&gt;硬盘逻辑划分&lt;/h2&gt;
&lt;p&gt;分区可以说是对硬盘的一种格式化。创建分区设置好硬盘的各项物理参数，指定了硬盘主引导记录（即Master Boot Record，一般简称为MBR）和引导记录备份的存放位置。而对于文件系统以及其他操作系统管理硬盘所需要的
      
    
    </summary>
    
      <category term="工具" scheme="http://mxxhcm.github.io/categories/%E5%B7%A5%E5%85%B7/"/>
    
    
      <category term="BIOS" scheme="http://mxxhcm.github.io/tags/BIOS/"/>
    
      <category term="UEFI" scheme="http://mxxhcm.github.io/tags/UEFI/"/>
    
      <category term="MBR" scheme="http://mxxhcm.github.io/tags/MBR/"/>
    
      <category term="GPT" scheme="http://mxxhcm.github.io/tags/GPT/"/>
    
      <category term="引导" scheme="http://mxxhcm.github.io/tags/%E5%BC%95%E5%AF%BC/"/>
    
      <category term="分区" scheme="http://mxxhcm.github.io/tags/%E5%88%86%E5%8C%BA/"/>
    
  </entry>
  
  <entry>
    <title>reinforcement learning an introduction 第13章笔记.md</title>
    <link href="http://mxxhcm.github.io/2019/04/03/reinforcement-learning-an-introduction-%E7%AC%AC13%E7%AB%A0%E7%AC%94%E8%AE%B0/"/>
    <id>http://mxxhcm.github.io/2019/04/03/reinforcement-learning-an-introduction-第13章笔记/</id>
    <published>2019-04-03T01:46:49.000Z</published>
    <updated>2019-07-25T06:31:58.572Z</updated>
    
    <content type="html"><![CDATA[<h2 id="policy-gradient">Policy gradient</h2><p>这章介绍的是使用一个参数化策略(parameterized policy)直接给出action，而不用借助一个value funciton选择action。但是需要说一下的是，Policy gradient方法也可以学习一个Value function，但是value function是用来帮助学习policy parameters的，而不是用来选择action。我们用$\mathbf{\theta} \in R^{d’}$表示policy’s parameters vector，用$\pi(a|s, \mathbf{\theta}) = Pr[A_t = a|S_t = s, \mathbf{\theta}_t = \mathbf{\theta}]$表示environment在时刻$t$处于state $s$时，智能体根据参数为$\mathbf{\theta}$的策略$\pi$选择action $a$。<br>如果policy gradient方法使用了一个value function,它的权重用$\mathbf{w} \in R^d$表示，即$\hat{v}(s,\mathbf{w})$。</p><p>用$J(\mathbf{\theta})$表示policy parameters的标量performance measure。使用梯度上升(gradient ascent) 方法来最大化这个performance：<br>$$\mathbf{\theta}_{t+1} = \mathbf{\theta}_t + \alpha \widehat{\nabla J(\mathbf{\theta}_t}),\tag{1}$$<br>其中$\widehat{\nabla J(\mathbf{\theta}_t)} \in R^{d’}$是一个随机估计(stachastic estimate)，它的期望是performance measure对$\mathbf{\theta_t}$的梯度。不管它们是否使用value function，这种方法就叫做policy gradient方法。既学习policy，又学习value function的方法被称为actor-critic，其中actor指的是学到的policy，critic指的是学习到的value funciton,通常是state value function。</p><h2 id="policy估计和它的优势">policy估计和它的优势</h2><h3 id="参数化policy的条件">参数化policy的条件</h3><p>policy可以用任何方式参数化，只要$\pi(a|s,\mathbf{\theta}),\mathbf{\theta}\in R^{d’}$对于它的参数$\mathbf{\theta}$是可导的，即只要$\nabla_{\pi}(a|s,\mathbf{\theta})$（即：$\pi(a|s,\mathbf{\theta})$相对于$\mathbf{\theta}$的偏导数列向量）存在，并且$\forall s\in S, a\in A(s)$偏导数都是有限的即可。</p><h3 id="stochastic-policy">stochastic policy</h3><p>为了保证exploration，通常策略是stochastic，而不是deterministic，即$\forall s,a,\mathbf{\theta}, \pi(a|s,\mathbf{\theta})\in (0,1)$</p><h3 id="参数化方式的选择">参数化方式的选择</h3><h4 id="softmax">softmax</h4><p>对于有限且离散的action space，一个很自然的参数化方法就是对于每一个state-action对都计算一个参数化的数值偏好$h(s,a,\mathbf{\theta})\in R$。通过计算一个exponetial softmax，这个数值大的动作有更大的概率被选中：<br>$$\pi(a|s,\mathbf{\theta}) = \frac{e^{h(s,a,\mathbf{\theta} )}}{\sum_be^{h(s,b,\mathbf{\theta} )}}, \tag{2}$$<br>其中$b$是在state $s$下所有可能采取的动作，它们的概率加起来为$1$，这种方法叫做softmax in aciton preferences。</p><h4 id="nn和线性方法">NN和线性方法</h4><p>参数化还可以选择其他各种各样的方法，如AlphaGo中使用的NN，或者可以使用如下的线性方法：<br>$$h(s,a, \mathbf{\theta}) = \mathbf{\theta}^Tx(s,a), \tag{3}$$</p><h3 id="优势">优势</h3><p>和action value方法相比，policy gradient有多个优势。<br>第一个优势是使用action preferences的softmax，同时用$\epsilon-greedy$算法用$\epsilon$的概率随机选择action得到的策略可以接近一个deterministic policy。<br>而单单使用action values的方法并不会使得策略接近一个deterministic policy，但是action-value方法会逐渐收敛于它的true values，翻译成概率来表示就是在$0$和$1$之间的一个概率值。但是action preferences方法不收敛于任何值，它们产生optimal stochastic policy，如果optimal policy是deterministic，那么optimal action的preferences应该比其他所有suboptimal actions都要高。</p><p>第二个优势是使用action preferences方法得到的参数化策略可以使用任意的概率选择action。在某些问题中，最好的approximate policy可能是stochastic的，actor-value方法不能找到一个stochastic optimal policy，它总是根据action value值选出来一个值最大的action，但是这时候的结果通常不是最优的。</p><p>第三个优势是policy parameterization可能比action value parameterization更容易学习。当然，也有时候可能是action value更容易。这个要根据情况而定</p><p>第四个优势是policy parameterizaiton比较容易添加先验知识到policy中。</p><h2 id="policy-gradient理论">policy gradient理论</h2><p>除了上节说的实用优势之外，还有理论优势。policy parameterization学到关于参数的一个连续函数，action probability概率可以平滑的变化。然而$\epsilon-greedy$算法中，action-value改变以后，action probability可能变化很大。很大程度上是因为policy gradient方法的收敛性要比action value方法强的多。因为policy的连续性依赖于参数，使得policy gradient方法接近于gradient ascent。<br>这里讨论episodic情况。定义perfromance measure是episode初始状态的值。假设每一个episode，都从state $s_0$开始，定义：<br>$$J(\mathbf{\theta}) = v_{\pi_\mathbf{\theta}}(s_0), \tag{4}$$<br>其中$v_{\pi_\mathbf{\theta}}(s_0)$是由参数$\mathbf{\theta}$确定的策略$\pi_{\mathbf{\theta}}$的true value function。假设在episodic情况下，$\gamma=1$。</p><p>使用function approximation，一个需要解决的问题就是如何确保每次更新policy parameter，performance measure都有improvement。因为performence不仅仅依赖于action的选择，还取决于state的分布，然后它们都受policy parameter的影响。给定一个state，policy parameter对于actions，reward的影响，都可以相对直接的利用参数知识计算出来。但是policy parameter对于state 分布的影响是一个环境的函数，通常是不知道的。当梯度依赖于policy改变对于state分布的影响未知时，我们该如何估计performance相对于参数的梯度。</p><h3 id="episodic-case证明">Episodic case证明</h3><p>为了简化表示，用$\pi$表示参数为$\theta$的policy，所有的梯度都是相对于$\mathbf{\theta}$求的<br>\begin{align*}<br>\nabla v_{\pi}(s) &amp;= \nabla [ \sum_a \pi(a|s)q_{\pi}(s,a)], \forall s\in S \tag{5}\\<br>&amp;= \sum_a [\nabla\pi(a|s)q_{\pi}(s,a)], \forall s\in S \tag{6}\\<br>&amp;= \sum_a[\nabla\pi(a|s)q_{\pi}(s,a) + \pi(a|s)\nabla q_{\pi}(s,a)] \tag{7}\\<br>&amp;= \sum_a[\nabla\pi(a|s)q_{\pi}(s,a) + \pi(a|s)\nabla \sum_{s’,r}p(s’,r|s,a)(r+\gamma v_{\pi}(s’))] \tag{8}\\<br>&amp;= \sum_a[\nabla\pi(a|s)q_{\pi}(s,a) + \pi(a|s) \nabla \sum_{s’,r}p(s’,r|s,a)r + \pi(a|s)\nabla \sum_{s’,r}p(s’,r|s,a)\gamma v_{\pi}(s’))] \tag{9}\\<br>&amp;= \sum_a[\nabla\pi(a|s)q_{\pi}(s,a) + 0 + \pi(a|s)\sum_{s’}\gamma p(s’|s,a)\nabla v_{\pi}(s’) ] \tag{10}\\<br>&amp;= \sum_a[\nabla\pi(a|s)q_{\pi}(s,a) + 0 + \pi(a|s)\sum_{s’}\gamma p(s’|s,a)\\<br>&amp;\ \ \ \ \ \ \ \ \sum_{a’}[\nabla\pi(a’|s’)q_{\pi}(s’,a’) + \pi(a’|s’)\sum_{s’’}\gamma p(s’’|s’,a’)\nabla v_{\pi}(s’’))] ],  \tag{11}展开\\<br>&amp;= \sum_{x\in S}\sum_{k=0}^{\infty}Pr(s\rightarrow x, k,\pi)\sum_a\nabla\pi(a|x)q_{\pi}(x,a) \tag{12}<br>\end{align*}<br>第(5)式使用了$v_{\pi}(s) = \sum_a\pi(a|s)q(s,a)$进行展开。第(6)式将梯度符号放进求和里面。第(7)步使用product rule对q(s,a)求导。第(8)步利用$q_{\pi}(s, a) =\sum_{s’,r}p(s’,r|s,a)(r+v_{\pi}(s’)$ 对$q_{\pi}(s,a)$进行展开。第(9)步将(8)式进行分解。第(10)步对式(9)进行计算，因为$\sum_{s’,r}p(s’,r|s,a)r$是一个定制，求偏导之后为$0$。第(11)步对生成的$v_{\pi}(s’)$重复(5)-(10)步骤，得到式子(11)。如果对式子(11)中的$v_{\pi}(s)$一直展开，就得到了式子(12)。式子(12)中的$Pr(s\rightarrow x, k, \pi)$是在策略$\pi$下从state $s$经过$k$步转换到state $x$的概率，这里我有一个问题，就是为什么，$k$可以取到$\infty$，后来想了想，因为对第(11)步进行展开以后，可能会有重复的state，重复的意思就是从状态$s$开始，可能会多次到达某一个状态$x$，$k$就能取很多次，大不了$k=\infty$的概率为$0$就是了。</p><p>所以，对于$v_{\pi}(s_0)$，就有：<br>\begin{align*}<br>\nabla J(\mathbf{\theta}) &amp;= \nabla_{v_{\pi}}(s_0)\\<br>&amp;= \sum_{s\in S}( \sum_{k=0}^{\infty}Pr(s_0\rightarrow s,k,\pi) ) \sum_a\nabla_{\pi}(a|s)q_{\pi}(s,a)\\<br>&amp;=\sum_{s\in S}\eta(s)\sum_a \nabla_{\pi}(a|s)q_{\pi}(s,a)\\<br>&amp;=\sum_{s’\in S}\eta(s’)\sum_s\frac{\eta(s)}{\sum_{s’}\eta(s’)}\sum_a \nabla_{\pi}(a|s)q_{\pi}(s,a)\\<br>&amp;=\sum_{s’\in S}\eta(s’)\sum_s\mu(s)\sum_a \nabla_{\pi}(a|s)q_{\pi}(s,a)\\<br>&amp;\propto \sum_{s\in S}\mu(s)\sum_a\nabla\pi(a|s)q_{\pi}(s,a)<br>\end{align*}<br>最后，我们可以看出来performance对policy求导不涉及state distribution的导数。Episodic 情况下的策略梯度如下所示：<br>$$\nabla J(\mathbf{\theta})\propto \sum_{s\in S}\mu(s)\sum_aq_{\pi}(s,a)\nabla\pi(a|s,\mathbf{\theta}), \tag{13}$$<br>其中梯度是performacne指标$J$关于$\mathbf{\theta}$的偏导数列向量，$\pi$是参数$\mathbf{\theta}$对应的策略。在episodic情况下，比例常数是一个episode的平均长度，在continuing情况下，常数是$1$，实际上这个正比于就是一个等式。分布$\mu$是策略$\pi$下的on-policy分布。</p><h2 id="reinforce-monte-carlo-policy-gradient">REINFORCE: Monte Carlo Policy Gradient</h2><p>对于式子(1)，我们需要进行采样，让样本梯度的期望正比于performance measure对于$\mathbf{\theta}$的真实梯度。比例系数不需要确定，因为步长$\alpha$的大小是手动设置的。Policy gradient理论给出了一个正比于gradient的精确表达式，我们要做的就是选择采样方式，它的期望等于或者接近policy gradient理论给出的值。</p><h3 id="all-actions">all-actions</h3><p>使用随机变量的期望替换对随机变量求和的取值，我们可以将式子(13)进行如下变化：<br>\begin{align*}<br>\nabla J(\mathbf{\theta})&amp;\propto \sum_{s\in S}\mu(s)\nabla\pi(a|s,\mathbf{\theta})\sum_aq_{\pi}(s,a)\\<br>&amp;=\mathbb{E}_{\pi}\left[\nabla\pi(a|S_t,\mathbf{\theta})\sum_aq_{\pi}(S_t,a)\right]\tag{14}<br>\end{align*}<br>接下来，我们可以实例化该方法：<br>$$\mathbf{\theta}_{t+1} = \mathbf{\theta}_t+\alpha\sum_a\hat{q}(S_t,s,\mathbf{w})\nabla\pi(a|S_t,\mathbf{\theta}), \tag{15}$$<br>其中$\hat{q}$是$q_{\pi}$的估计值，这个算法被称为all-actions方法，因为它的更新涉及到了所有的action。然而，我们这里介绍的REINFORCE仅仅使用了$t$时刻的action $A_t$。。</p><h3 id="reinforce">REINFORCE</h3><p>和引入$S_t$的方法一样，使用随机变量的期望代替对与随机变量的可能取值进行求和，我们在式子(14)中引入$A_t$，<br>\begin{align*}<br>\nabla J(\mathbf{\theta}) &amp;= \mathbb{E}_{\pi}\left[\sum_aq_{\pi}(S_t,a)\nabla\pi(a|S_t,\mathbf{\theta})\right]\\<br>&amp; = \mathbb{E}_{\pi}\left[\sum_aq_{\pi}(S_t,a)\pi(a|S_t,\mathbf{\theta})\frac{\nabla\pi(a|S_t,\mathbf{\theta})}{\pi(a|S_t,\mathbf{\theta})}\right]\\<br>&amp; = \mathbb{E}_{\pi}\left[q_{\pi}(S_t,A_t)\frac{\nabla\pi(A_t|S_t,\mathbf{\theta})}{\pi(A_t|S_t,\mathbf{\theta})}\right]\\<br>\end{align*}</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;policy-gradient&quot;&gt;Policy gradient&lt;/h2&gt;
&lt;p&gt;这章介绍的是使用一个参数化策略(parameterized policy)直接给出action，而不用借助一个value funciton选择action。但是需要说一下的是，Pol
      
    
    </summary>
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="Policy Gradient" scheme="http://mxxhcm.github.io/tags/Policy-Gradient/"/>
    
  </entry>
  
  <entry>
    <title>DQN-ops-tensorflow-实现与解析</title>
    <link href="http://mxxhcm.github.io/2019/03/28/DQN-ops-tensorflow-%E5%AE%9E%E7%8E%B0%E4%B8%8E%E8%A7%A3%E6%9E%90/"/>
    <id>http://mxxhcm.github.io/2019/03/28/DQN-ops-tensorflow-实现与解析/</id>
    <published>2019-03-28T08:02:40.000Z</published>
    <updated>2019-05-06T16:22:27.700Z</updated>
    
    <summary type="html">
    
    </summary>
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://mxxhcm.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/tags/tensorflow/"/>
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="DQN" scheme="http://mxxhcm.github.io/tags/DQN/"/>
    
  </entry>
  
  <entry>
    <title>DQN replay buffer tensorflow 实现与解析</title>
    <link href="http://mxxhcm.github.io/2019/03/27/DQN-replay-buffer-tensorflow-%E5%AE%9E%E7%8E%B0%E4%B8%8E%E8%A7%A3%E6%9E%90/"/>
    <id>http://mxxhcm.github.io/2019/03/27/DQN-replay-buffer-tensorflow-实现与解析/</id>
    <published>2019-03-27T12:21:40.000Z</published>
    <updated>2019-05-06T16:22:27.700Z</updated>
    
    <content type="html"><![CDATA[<h2 id="代码">代码</h2><p>这个DQN的Replay Buffer实现只用到了numpy库，可以很容易的进行扩展。主要有五个函数。接下来分函数进行解析。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ReplayBuffer</span>:</span></span><br><span class="line">    <span class="comment"># config : memory_size, batch_size, history_length, state_format, screen_height, screen_width,</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, config)</span>:</span></span><br><span class="line">        self.memory_size = config.memory_size</span><br><span class="line">        self.batch_size = config.batch_size</span><br><span class="line"></span><br><span class="line">        self.screens = np.empty((self.memory_size, config.screen_height, config.screen_width), dtype=np.float16)</span><br><span class="line">        self.actions = np.empty(self.memory_size, dtype=np.uint8)</span><br><span class="line">        self.rewards = np.empty(self.memory_size, dtype=np.int8)</span><br><span class="line">        self.terminals = np.empty(self.memory_size, dtype=np.bool)</span><br><span class="line">        self.history_length = config.history_length <span class="comment"># state使用多少张screens拼接在一起，论文中是4张</span></span><br><span class="line">        self.state_format = config.state_format</span><br><span class="line">        self.dims = (config.screen_height, config.screen_width)</span><br><span class="line">        <span class="comment"># state and next_state</span></span><br><span class="line">        self.states = np.empty((self.batch_size, self.history_length)+self.dims, dtype=np.float16)</span><br><span class="line">        self.next_states = np.empty((self.batch_size, self.history_length)+self.dims, dtype=np.float16)</span><br><span class="line"></span><br><span class="line">        self.count = <span class="number">0</span>  <span class="comment"># 记录总共有多少条记录</span></span><br><span class="line">        self.current = <span class="number">0</span> <span class="comment"># 获取当前是第几条</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add</span><span class="params">(self, screen, action, reward, terminal)</span>:</span></span><br><span class="line">        self.screens[self.current] = screen</span><br><span class="line">        self.actions[self.current] = action</span><br><span class="line">        self.rewards[self.current] = reward</span><br><span class="line">        self.terminals[self.current] = terminal</span><br><span class="line">        self.count = max(self.current + <span class="number">1</span>, self.count)</span><br><span class="line">        self.current = (self.current + <span class="number">1</span>) % self.memory_size</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.count</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">clear</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.current = <span class="number">0</span></span><br><span class="line">        self.count = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">getState</span><span class="params">(self, index)</span>:</span></span><br><span class="line">        <span class="keyword">assert</span> self.count &gt; <span class="number">0</span></span><br><span class="line">        <span class="comment"># 每一个样本都要取self.history_length那么长。</span></span><br><span class="line">        <span class="keyword">if</span> index &gt;= self.history_length - <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> self.screens[index-(self.history_length - <span class="number">1</span>):index+<span class="number">1</span>, ...]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 如果当前下标比self.history_length还要小，那么就要从buffer的结尾处取了。</span></span><br><span class="line">            indexes = [(index - i )% self.count <span class="keyword">for</span> i <span class="keyword">in</span> reversed(range(self.history_length))]</span><br><span class="line">            <span class="keyword">return</span> self.screens[indexes, ...]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sample</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">assert</span> self.count &gt; self.history_length</span><br><span class="line">        indexes = []</span><br><span class="line">        <span class="keyword">while</span> len(indexes) &lt; self.batch_size:</span><br><span class="line">            <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">                index = random.randint(self.history_length, self.count + <span class="number">1</span>)    <span class="comment"># 相当于从self.histor_length之后进行采样</span></span><br><span class="line">                <span class="comment"># 如果包含current，就重新采样。（current是刚生成的样本）</span></span><br><span class="line">                <span class="keyword">if</span> index &gt; self.current <span class="keyword">and</span> self.current - self.history_length &lt;= index:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                <span class="comment"># 如果包含一个episode的结束状态，重新采样</span></span><br><span class="line">                <span class="keyword">if</span> self.terminals[(index - self.history_length):self.history_length].any():</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">            self.states[len(indexes),...] = self.getState(index - <span class="number">1</span>)</span><br><span class="line">            self.next_states[len(indexes),...] = self.getState(index)</span><br><span class="line">            indexes.append(index)</span><br><span class="line"></span><br><span class="line">        actions = self.actions[indexes]</span><br><span class="line">        rewards = self.rewards[indexes]</span><br><span class="line">        terminals = self.terminals[indexes]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.state_format == <span class="string">'NHWC'</span>:</span><br><span class="line">            <span class="keyword">return</span> np.transpose(self.states, (<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>)), actions, rewards, np.transpose(self.next_states, (<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>)),terminals</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> self.states, actions, rewards, self.next_states, terminals</span><br></pre></td></tr></table></figure><h2 id="init函数">init函数</h2><p>ReplayBuffer的init的输入参数为一个config文件，包含了创建ReplayBuffer的参数，memory_size是Buffer大小，batch_size为训练和测试的batch大小，screens, actions, rewards, terminals分别存放的是每次采样得到的screen, action, reward和terminal(当前episode是否结束)。history_length是原文中提到的连续处理四张图片的四，而不仅仅是一张。state_format指的是’NHWC’还是’NCHW’，即depth通道在第$1$维还是第$3$维，states存放的是一个tensor，shape为$(batch_size, screen_height, screen_width, history_length)$，count记录当前Buffer的大小，current记录当前experience插入的地方。</p><h2 id="add方法">add方法</h2><p>该方法实现了向ReplayBuffer中添加experience。</p><h2 id="len-方法">__len__方法</h2><p>放回Buffer当前的大小</p><h2 id="clear方法">clear方法</h2><p>清空Buffer</p><h2 id="sample方法">sample方法</h2><p>从buffer中进行采样，返回一个元组，(states, actions, rewards, next_states, terminals)</p><h2 id="getstate方法">getState方法</h2><p>给定一个index，寻找它的前history_length - 1 个screens。</p><h2 id="参考文献">参考文献</h2><p>1.<a href="https://github.com/devsisters/DQN-tensorflow" target="_blank" rel="noopener">https://github.com/devsisters/DQN-tensorflow</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;代码&quot;&gt;代码&lt;/h2&gt;
&lt;p&gt;这个DQN的Replay Buffer实现只用到了numpy库，可以很容易的进行扩展。主要有五个函数。接下来分函数进行解析。&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td cl
      
    
    </summary>
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://mxxhcm.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/tags/tensorflow/"/>
    
      <category term="replay buffer" scheme="http://mxxhcm.github.io/tags/replay-buffer/"/>
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="DQN" scheme="http://mxxhcm.github.io/tags/DQN/"/>
    
  </entry>
  
  <entry>
    <title>神经网络-dropout</title>
    <link href="http://mxxhcm.github.io/2019/03/23/dropout/"/>
    <id>http://mxxhcm.github.io/2019/03/23/dropout/</id>
    <published>2019-03-23T11:26:18.000Z</published>
    <updated>2019-05-06T16:22:27.712Z</updated>
    
    <content type="html"><![CDATA[<h2 id="dropou是干什么的">dropou是干什么的</h2><p>Dropout 是一种正则化技术，通过学习鲁棒的特征来防止过拟合。</p><h2 id="为什么会有过拟合">为什么会有过拟合</h2><p>如果输入和正确输出之间有很复杂的映射关系，而网络又有足够多的隐藏单元去正确的建模，那么通常会用很多组权重都能在训练集上得到好的结果。但是每一组权重在测试集上的结果都比训练集差，因为它们只在训练集上训练了，而没有在测试集上训练。</p><h2 id="什么是dropout">什么是dropout</h2><p>在网络中每一个隐藏单元的输出单元都有$0.5$的概率被忽略，所以每一个隐藏单元需要学会独立于其他的隐藏单元决定输出结果。</p><blockquote><p>This technique reduces complex co-adaptations of neurons, since a neuron cannot rely on the presence of particular other neurons. It is, therefore, forced to learn more robust features that are useful in conjunction with many different random subsets of the other neurons. [0]</p></blockquote><blockquote><p>On each presentation of each training case, each hidden unit is randomly omitted from the network with a probability of 0.5, so a hidden unit cannot rely on other hidden units being present.[1]</p></blockquote><blockquote><p>Dropout stops the mechanism of training neurons of any layers as a family, so reduces co-adaptability.[3]</p></blockquote><p>另一种方式可以把dropout看成对神经网络做平均。一种非常有效的减少测试误差的方法就是对一系列神经网络预测的结果取平均。理想的方式是训练很多个网络，然后分别在每个网络上进行测试，但是这样子的计算代价是很高的。随机的dropout让在合理的时间内训练大量不同的网络变得可能。当我们丢弃一个神经元的时候，它对loss函数没有任何贡献，所以在反向传播的时候，梯度为$0$，权值不会被更新。这就相当于我们对网络进行了一个下采样，训练过程的每次迭代中，采样网络的一部分进行训练，这样我们就得到了一个共享参数的集成模型。对于每一次训练，网络结构都是相同的，但是每次选择的参数都有很大可能是不同的，而且权重是共享的。</p><blockquote><p>The neurons which are “dropped out” in this way do not contribute to the forward pass and do not participate in backpropagation. So every time an input is presented, the neural network samples a different architecture, but all these architectures share weights.</p></blockquote><p>在测试的时候，使用&quot;mean networks&quot;，就是保留网络中所有的权重，但是要把激活函数的输出（activations)乘上$0.5$，因为相对训练的时候，每个神经元都有$0.5$的概率被激活，这个时候如果不乘上的话，最后就相当于测试的时候激活的神经元是训练时候的两倍。在实践中证明，这和对一系列经过dropout的网络取平均值的结果是很像的。（为什么就是两倍？）</p><blockquote><p>Dropout can also be thought of as an ensemble of models that share parameters. When we drop a neuron, it has no effect on the loss function and thus the gradient that flows through it during backpropagation is effectively zero and so its weights will not get updated. This means that we are basically subsampling a part of the neural network and we are training it on a single example. In every iteration of training, we will subsample a different part of the network and train that network on the datapoint at that point of time. Thus what we have essentially is an ensemble of models that share some parameters.[3]</p></blockquote><p>一个具有$N$个隐藏节点的网络，和一个用于计算类别标签的softmax输出层，使用mean networks就相当于对$2^N$个网络输出的标签概率做几何平均（并不是数学上的几何平均）。（为什么是几何平均？这里其实不是几何平均，只是一个等权重加权。）</p><blockquote><p>a) The authors of the referenced article don’t use the ‘geometric mean’ of the predictions, but “an equally weighted geometric mean” of them.<br>b) They propose geometric mean over arithmetic mean for giving more value to more frequent data, probably according to the understanding by them of the underlying relations.<br>If, for example, you take the arithmetic mean of ${10, 10, 100}$, you get $40$, but if you take their geometric mean you get $\sqrt[3]{10000} \approx 21.54$, meaning the ‘odd’ measurement ($100$) plays a smaller role to the mean.<br>c) Even the geometric mean might be misleading, if the data are not assigned their true ‘weight’, meaning their occurrence or probability of occurrence, while assuring that this assignment of weights is equally important for all data.<br>Hence “equally weighted geometric mean”.[2]</p></blockquote><p>如果采取dropout之后的网络输出不一样，那么mean network的输出能够保证赋值一个更高的可能性到正确标签。mean network的方根误差要比dropout网络方根误差的平均值要好，也就是说先对网络做平均然后计算误差要比先计算误差然后再平均要好。</p><p>实际上，$0.5$这个值不是固定的，可以根据不同情况进行微调。</p><h2 id="why-dropout-works">why dropout works</h2><p>其实这个和上面介绍中差不多，给出一种直观的解释。给一个例子[4]，有一个三层的神经网络，在下图中，红圈中的节点对于正确的输出起到了决定性的作用，在BP的过程中，它的权值不断增加，但是它可能在训练集上效果很好，但是测试集上很差。<br><img src="/2019/03/23/dropout/dropout_1.png" alt="dropout"><br>当采用了dropout以后，我们随意丢弃一些节点，如果把上图的关键节点丢了，那么网络必须重新学习其他的节点，才能够正确的进行分类。如下图，网络必须在另外可能没有丢弃的三个节点中选择一个用于正确分类。所以，这样子上图中的关键节点的作用就会被减轻，在新数据集上的鲁棒性可能就会更好。<br><img src="/2019/03/23/dropout/dropout_2.png" alt="dropout"></p><h2 id="实现">实现</h2><h3 id="numpy-实现">numpy 实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(x, w1, w2, w3, training=False)</span>:</span></span><br><span class="line">  z1 = np.dot(x, w1)</span><br><span class="line">  y1 = np.tanh(z1)</span><br><span class="line"></span><br><span class="line">  z2 = np.dot(y1, w2)</span><br><span class="line">  y2 = np.dot(z2)</span><br><span class="line">  <span class="comment"># dropout in layer 2 </span></span><br><span class="line">  <span class="keyword">if</span> training:</span><br><span class="line">     m2 = np.random.binomial(<span class="number">1</span>, <span class="number">0.5</span>, size=z2.shape)</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">     m2 = <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line">  y2 *= m2</span><br><span class="line">  z3 = np.dot(y2, w3)</span><br><span class="line">  y3 = z3</span><br><span class="line">  <span class="keyword">return</span> y1, y2, y3, m2</span><br></pre></td></tr></table></figure><h3 id="pytorch库">pytorch库</h3><h2 id="参考文献">参考文献</h2><p>1.<a href="https://arxiv.org/pdf/1207.0580.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1207.0580.pdf</a><br>2.<a href="https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf" target="_blank" rel="noopener">https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf</a><br>3.<a href="https://www.quora.com/What-is-dropout-in-deep-learning" target="_blank" rel="noopener">https://www.quora.com/What-is-dropout-in-deep-learning</a><br>4.<a href="https://www.quora.com/What-is-the-use-of-geometric-mean-in-dropout-neural-networks-It-says-that-by-approximating-an-equally-weighted-geometric-mean-of-the-predictions-of-an-exponential-number-of-learned-models-that-share-parameters" target="_blank" rel="noopener">https://www.quora.com/What-is-the-use-of-geometric-mean-in-dropout-neural-networks-It-says-that-by-approximating-an-equally-weighted-geometric-mean-of-the-predictions-of-an-exponential-number-of-learned-models-that-share-parameters</a><br>5.<a href="https://www.quora.com/Why-exactly-does-dropout-in-deep-learning-work" target="_blank" rel="noopener">https://www.quora.com/Why-exactly-does-dropout-in-deep-learning-work</a><br>6.<a href="https://www.quora.com/How-does-the-dropout-method-work-in-deep-learning-And-why-is-it-claimed-to-be-an-effective-trick-to-improve-your-network" target="_blank" rel="noopener">https://www.quora.com/How-does-the-dropout-method-work-in-deep-learning-And-why-is-it-claimed-to-be-an-effective-trick-to-improve-your-network</a><br>7.<a href="https://pgaleone.eu/deep-learning/regularization/2017/01/10/anaysis-of-dropout/" target="_blank" rel="noopener">https://pgaleone.eu/deep-learning/regularization/2017/01/10/anaysis-of-dropout/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;dropou是干什么的&quot;&gt;dropou是干什么的&lt;/h2&gt;
&lt;p&gt;Dropout 是一种正则化技术，通过学习鲁棒的特征来防止过拟合。&lt;/p&gt;
&lt;h2 id=&quot;为什么会有过拟合&quot;&gt;为什么会有过拟合&lt;/h2&gt;
&lt;p&gt;如果输入和正确输出之间有很复杂的映射关系，而网络又有
      
    
    </summary>
    
      <category term="机器学习" scheme="http://mxxhcm.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://mxxhcm.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="http://mxxhcm.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="http://mxxhcm.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="dropout" scheme="http://mxxhcm.github.io/tags/dropout/"/>
    
  </entry>
  
  <entry>
    <title>matplotlib笔记</title>
    <link href="http://mxxhcm.github.io/2019/03/21/python-matplotlib%E7%AC%94%E8%AE%B0/"/>
    <id>http://mxxhcm.github.io/2019/03/21/python-matplotlib笔记/</id>
    <published>2019-03-21T07:29:17.000Z</published>
    <updated>2019-07-08T02:26:38.208Z</updated>
    
    <content type="html"><![CDATA[<h2 id="show">show()</h2><h3 id="介绍">介绍</h3><p>show()函数是一个阻塞函数，调用该函数，显示当前已经绘制的图像，然后需要手动关闭打开的图像，程序才会继续执行。</p><h3 id="代码示例">代码示例</h3><p><a href="https://github.com/mxxhcm/code/blob/master/tools/matplotlib/1_show.py" target="_blank" rel="noopener">代码地址</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x = np.arange(<span class="number">0</span>,<span class="number">10</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">y1 = x**<span class="number">2</span></span><br><span class="line">y2 = <span class="number">2</span>*x +<span class="number">5</span></span><br><span class="line"></span><br><span class="line">plt.plot(x,y1)</span><br><span class="line">plt.savefig(<span class="string">"0_1.png"</span>)</span><br><span class="line">plt.show()  <span class="comment"># 调用show()会阻塞，然后关掉打开的图片，程序继续执行</span></span><br><span class="line"></span><br><span class="line">plt.plot(x,y2)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h2 id="savefig">savefig()</h2><h3 id="介绍-v2">介绍</h3><p>该文件接收一个参数，作为文件保存的路径。</p><h3 id="代码示例-v2">代码示例</h3><p><a href="https://github.com/mxxhcm/code/blob/master/tools/matplotlib/2_savefig.py" target="_blank" rel="noopener">代码地址</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x = np.arange(<span class="number">0</span>, <span class="number">10</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">y1 = x**<span class="number">2</span></span><br><span class="line">y2 = <span class="number">2</span>*x +<span class="number">5</span></span><br><span class="line"></span><br><span class="line">plt.plot(x,y1)</span><br><span class="line">plt.savefig(<span class="string">"2.png"</span>) <span class="comment"># 保存图像，名字为2.png</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h2 id="figure">figure()</h2><h3 id="介绍-v3">介绍</h3><p>figure()函数相当于生成一张画布。如果不显示调用的话，所有的图像都会绘制在默认的画布上。可以通过调用figure()函数将函数图像分开。figure()会接受几个参数，num是生成图片的序号，figsize指定图片的大小。</p><h3 id="代码示例-v3">代码示例</h3><p><a href="https://github.com/mxxhcm/code/blob/master/tools/matplotlib/3_figure.py" target="_blank" rel="noopener">代码地址</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x = np.arange(<span class="number">0</span>,<span class="number">10</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">y1 = x**<span class="number">2</span></span><br><span class="line">y2 = <span class="number">2</span>*x +<span class="number">5</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># figure</span></span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(x,y1)</span><br><span class="line"></span><br><span class="line">plt.figure(num=<span class="number">6</span>,figsize=(<span class="number">10</span>,<span class="number">10</span>))</span><br><span class="line">plt.plot(x,y2)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h2 id="imshow">imshow()</h2><h3 id="介绍-v4">介绍</h3><p>该函数用来显示图像，接受一个图像矩阵。调用完该函数之后还需要调用show()函数。</p><h3 id="代码示例-v4">代码示例</h3><p><a href="https://github.com/mxxhcm/code/blob/master/tools/matplotlib/4_image.py" target="_blank" rel="noopener">代码地址</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">img = np.random.randint(<span class="number">0</span>, <span class="number">255</span>, [<span class="number">32</span>, <span class="number">32</span>])</span><br><span class="line">print(img.shape)</span><br><span class="line"></span><br><span class="line">plt.imshow(img)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h2 id="subplot">subplot()</h2><h3 id="介绍-v5">介绍</h3><p>绘制$m\times n$个子图</p><h3 id="代码示例-v5">代码示例</h3><p><a href="https://github.com/mxxhcm/code/blob/master/tools/matplotlib/5_subplot.py" target="_blank" rel="noopener">代码地址</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x = np.arange(<span class="number">0</span>, <span class="number">10</span>, <span class="number">1</span>)</span><br><span class="line">y1 = <span class="number">2</span> * x</span><br><span class="line">y2 = <span class="number">3</span> * x</span><br><span class="line">y3 = <span class="number">4</span> * x</span><br><span class="line">y4 = <span class="number">5</span> * x</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">plt.plot(x, y1, marker=<span class="string">'s'</span>, lw=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">plt.plot(x, y2, ls=<span class="string">'-.'</span>)</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">plt.plot(x, y3, color=<span class="string">'r'</span>)</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">2</span>, <span class="number">2</span>, <span class="number">4</span>)</span><br><span class="line">plt.plot(x, y4, ms=<span class="number">10</span>, marker=<span class="string">'o'</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h2 id="subplots">subplots()</h2><h3 id="介绍-v6">介绍</h3><p>将一张图分成$m\times n$个子图。</p><h3 id="代码示例-v6">代码示例</h3><p><a href="https://github.com/mxxhcm/code/blob/master/tools/matplotlib/6_subplots.py" target="_blank" rel="noopener">代码地址</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">figure,axes = plt.subplots(<span class="number">2</span>, <span class="number">3</span>, figsize=[<span class="number">40</span>,<span class="number">20</span>])</span><br><span class="line">axes = axes.flatten()</span><br><span class="line"></span><br><span class="line">x = np.arange(<span class="number">0</span>, <span class="number">20</span>) </span><br><span class="line">y1 = pow(x, <span class="number">2</span>)</span><br><span class="line">axes[<span class="number">0</span>].plot(x, y1) </span><br><span class="line"></span><br><span class="line">y5 = pow(x, <span class="number">3</span>)</span><br><span class="line">axes[<span class="number">5</span>].plot(x, y5) </span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h2 id="ax">ax()</h2><h3 id="介绍-v7">介绍</h3><p>获得当前figure的坐标轴，用来绘制。</p><h3 id="代码示例-v7">代码示例</h3><p><a href="https://github.com/mxxhcm/code/blob/master/tools/matplotlib/7_axes.py" target="_blank" rel="noopener">代码地址</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x = np.arange(<span class="number">-3.5</span>,<span class="number">3.5</span>,<span class="number">0.5</span>)</span><br><span class="line">y1 = np.abs(<span class="number">2</span> * x)</span><br><span class="line">y2 = np.abs(x)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>,<span class="number">10</span>))</span><br><span class="line">ax = plt.gca() <span class="comment"># gca = get current axis</span></span><br><span class="line">ax.spines[<span class="string">'right'</span>].set_color(<span class="string">'none'</span>)</span><br><span class="line">ax.spines[<span class="string">'top'</span>].set_color(<span class="string">'red'</span>)</span><br><span class="line">ax.xaxis.set_ticks_position(<span class="string">"bottom"</span>)</span><br><span class="line">ax.yaxis.set_ticks_position(<span class="string">"left"</span>)</span><br><span class="line">ax.spines[<span class="string">'bottom'</span>].set_position((<span class="string">'data'</span>,<span class="number">0</span>))</span><br><span class="line">ax.spines[<span class="string">'left'</span>].set_position((<span class="string">'data'</span>,<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># both work</span></span><br><span class="line">ax.plot(x,y1,lw=<span class="number">2</span>,marker=<span class="string">'-'</span>,ms=<span class="number">8</span>)</span><br><span class="line">plt.plot(x,y2,lw=<span class="number">3</span>,marker=<span class="string">'^'</span>,ms=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># xlim and ylim</span></span><br><span class="line"><span class="comment"># ax.xlim([-3.8, 3.3])</span></span><br><span class="line"><span class="comment"># AttributeError: 'AxesSubplot' object has no attribute 'xlim'</span></span><br><span class="line">plt.xlim([<span class="number">-3.8</span>, <span class="number">3.3</span>])</span><br><span class="line">plt.ylim([<span class="number">0</span>, <span class="number">7.2</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># xlabel and ylabel</span></span><br><span class="line"><span class="comment"># ax.xlabel('x',fontsize=20)</span></span><br><span class="line"><span class="comment"># AttributeError: 'AxesSubplot' object has no attribute 'xlabel'</span></span><br><span class="line">plt.xlabel(<span class="string">'x'</span>,fontsize=<span class="number">20</span>)</span><br><span class="line">plt.ylabel(<span class="string">'y = 2x '</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># xticklabel and yticaklabel</span></span><br><span class="line"><span class="comment"># ax.xticks(x,('a','b','c','d','e','f','g','h','i','j','k','l','m','n'),fontsize=20)</span></span><br><span class="line"><span class="comment"># AttributeError: 'AxesSubplot' object has no attribute 'xticks'</span></span><br><span class="line">plt.xticks(x,(<span class="string">'a'</span>,<span class="string">'b'</span>,<span class="string">'c'</span>,<span class="string">'d'</span>,<span class="string">'e'</span>,<span class="string">'f'</span>,<span class="string">'g'</span>,<span class="string">'h'</span>,<span class="string">'i'</span>,<span class="string">'j'</span>,<span class="string">'k'</span>,<span class="string">'l'</span>,<span class="string">'m'</span>,<span class="string">'n'</span>),fontsize=<span class="number">20</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># both work</span></span><br><span class="line">ax.legend([<span class="string">'t1'</span>,<span class="string">'t2'</span>])</span><br><span class="line">plt.legend([<span class="string">'y1'</span>,<span class="string">'y2'</span>])</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h2 id="ion-和ioff">ion()和ioff()</h2><h3 id="介绍-v8">介绍</h3><p>交互式绘图，可以在一张图上不断的更新。</p><h3 id="代码示例-v8">代码示例</h3><p><a href="https://github.com/mxxhcm/code/blob/master/tools/matplotlib/8_plt_ion_ioff.py" target="_blank" rel="noopener">代码地址</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">count = 1</span><br><span class="line">flag = True</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">ax = plt.gca()</span><br><span class="line">x = np.arange(20)</span><br><span class="line">plt.figure()</span><br><span class="line">ax2 = plt.gca()</span><br><span class="line"></span><br><span class="line">while flag:</span><br><span class="line">    plt.ion()</span><br><span class="line">    y = pow(x[:count], 2)</span><br><span class="line">    temp = x[:count]</span><br><span class="line">    ax.plot(temp, y, linewidth=1)</span><br><span class="line">    plt.pause(1)</span><br><span class="line">    plt.ioff()</span><br><span class="line"></span><br><span class="line">    ax2.plot(x, x+count)</span><br><span class="line">    count += 1</span><br><span class="line">    if count &gt; 20:</span><br><span class="line">        break</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h2 id="seanborn">seanborn</h2><h3 id="介绍-v9">介绍</h3><p>对matplotlib进行了一层封装</p><h3 id="代码示例-v9">代码示例</h3><p><a href="https://github.com/mxxhcm/code/blob/master/tools/matplotlib/9_seanborn.py" target="_blank" rel="noopener">代码地址</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">values = np.zeros((<span class="number">21</span>,<span class="number">21</span>), dtype=np.int)</span><br><span class="line">fig, axes = plt.subplots(<span class="number">2</span>, <span class="number">3</span>, figsize=(<span class="number">40</span>,<span class="number">20</span>))</span><br><span class="line">plt.subplots_adjust(wspace=<span class="number">0.1</span>, hspace=<span class="number">0.2</span>)</span><br><span class="line">axes = axes.flatten()</span><br><span class="line"></span><br><span class="line"><span class="comment"># cmap is the paramter to specify color type, ax is the parameter to specify where to show the picture</span></span><br><span class="line"><span class="comment"># np.flipud(matrix), flip the column in the up/down direction, rows are preserved</span></span><br><span class="line">figure = sns.heatmap(np.flipud(values), cmap=<span class="string">"YlGnBu"</span>, ax=axes[<span class="number">0</span>])</span><br><span class="line">figure.set_xlabel(<span class="string">"cars at second location"</span>, fontsize=<span class="number">30</span>)</span><br><span class="line">figure.set_title(<span class="string">"policy"</span>, fontsize=<span class="number">30</span>)</span><br><span class="line">figure.set_ylabel(<span class="string">"cars at first location"</span>, fontsize=<span class="number">30</span>)</span><br><span class="line">figure.set_yticks(list(reversed(range(<span class="number">21</span>))))</span><br><span class="line"></span><br><span class="line">figure = sns.heatmap(np.flipud(values), ax=axes[<span class="number">1</span>])</span><br><span class="line">figure.set_ylabel(<span class="string">"cars at first location"</span>, fontsize=<span class="number">30</span>)</span><br><span class="line">figure.set_yticks(list(reversed(range(<span class="number">21</span>))))</span><br><span class="line">figure.set_title(<span class="string">"policy"</span>, fontsize=<span class="number">30</span>)</span><br><span class="line">figure.set_xlabel(<span class="string">"cars at second location"</span>, fontsize=<span class="number">30</span>)</span><br><span class="line"></span><br><span class="line">plt.savefig(<span class="string">"hello.pdf"</span>)</span><br><span class="line">plt.show()</span><br><span class="line">plt.close()</span><br></pre></td></tr></table></figure><h2 id="color">color</h2><h3 id="介绍-v10">介绍</h3><p>指定线条的颜色，用color=’'实现。常见的颜色有：‘b’, ‘g’, ‘r’, ‘c’, ‘m’, ‘y’, ‘k’, ‘w’。</p><h3 id="代码示例-v10">代码示例</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">color = [<span class="string">'b'</span>, <span class="string">'g'</span>, <span class="string">'r'</span>, <span class="string">'c'</span>, <span class="string">'m'</span>, <span class="string">'y'</span>, <span class="string">'k'</span>, <span class="string">'w'</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(color)):</span><br><span class="line">    x = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line">    y = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line">    plt.plot(x, y+i, color=color[i])</span><br><span class="line"></span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">plt.plot(range(<span class="number">10</span>), range(<span class="number">10</span>), color=<span class="string">'w'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h3 id="注意事项">注意事项</h3><p>color=‘w’，'w’是white，所以画出来的图你是看不到的。。。这困扰了我好久。。。。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;show&quot;&gt;show()&lt;/h2&gt;
&lt;h3 id=&quot;介绍&quot;&gt;介绍&lt;/h3&gt;
&lt;p&gt;show()函数是一个阻塞函数，调用该函数，显示当前已经绘制的图像，然后需要手动关闭打开的图像，程序才会继续执行。&lt;/p&gt;
&lt;h3 id=&quot;代码示例&quot;&gt;代码示例&lt;/h3&gt;
&lt;p&gt;&lt;a
      
    
    </summary>
    
      <category term="python" scheme="http://mxxhcm.github.io/categories/python/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="matplotlib" scheme="http://mxxhcm.github.io/tags/matplotlib/"/>
    
  </entry>
  
  <entry>
    <title>梯度下降和反向传播</title>
    <link href="http://mxxhcm.github.io/2019/03/18/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/"/>
    <id>http://mxxhcm.github.io/2019/03/18/梯度下降和反向传播/</id>
    <published>2019-03-18T07:19:07.000Z</published>
    <updated>2019-06-09T02:48:43.340Z</updated>
    
    <content type="html"><![CDATA[<p>梯度下降和反向传播，他们两个之间的关系？</p><h2 id="导数-偏导数-梯度-方向倒数">导数，偏导数，梯度，方向倒数</h2><h3 id="导数">导数</h3><p>定义：<br>$$f^{’}(x_0) = {\lim_{\Delta x \to 0}}\frac{\Delta y}{\Delta x} = \lim_{\Delta x \to 0}\frac{f(x_0+\Delta x)-f(x_0)}{\Delta x}$$<br>反映的是函数y=f(x)在某一点处沿x轴正方向的变化率。也能表示在x点处的斜率</p><h3 id="偏导数">偏导数</h3><p>定义：<br>$$\frac{\partial }{\partial x}f(x,y,z) = \lim_{\Delta x \to 0}\frac{f(x + \Delta x,y,z) - f(x,y,z)}{\Delta x}$$<br>导数与偏导数本质都是一样的，当自变量的变化量趋于0时，函数值的变化量与自变量变化量比值的极限，偏导数就是函数在某一点上沿坐标轴正方向上的变化率。比如函数f(x,y,z)，f(x,y,z)在某一点处可以分别求对于x，y，z轴正方向的偏导数。</p><h3 id="方向导数">方向导数</h3><p>方向导数是某一点在某一趋近方向上的导数值，是函数在这个方向上的变化率。<br>定义：三元函数u=f(x,y,z)在点P(x,y,z)沿着l方向(方向角为$\alpha,\beta,\gamma$)的方向导数定义为<br>$$\frac{\partial f}{\partial l} = \lim_{\rho \to 0}\frac{f(x+\Delta x,y+\Delta y,z+\Delta z)-f(x,y,z)}{\rho}$$</p><h3 id="梯度">梯度</h3><p>梯度是方向导数中最大的那个向量，这个向量我们就称他为梯度，因为梯度是向量，所以才有梯度上升和下降的说法。梯度方向是函数增长最快的方向，梯度反方向是函数下降最快的方向。</p><h2 id="梯度下降">梯度下降</h2><p>神经网络的训练一般是通过定义一个loss函数，然后通过优化这个loss函数，实现神经网络的训练，一般的loss函数主要是定义了训练样本的预测结果和真实结果之间的差异，比如说定义交叉熵等。<br>至于优化loss函数的方法，就是通过梯度下降法来实现，该算法从任一点开始，沿该点梯度的反方向运动一段距离，再沿新位置的梯度反方向运行一段距离 … 如此迭代。解一直朝下坡最陡的方向运动，希望能运动到函数的全局最小点，梯度下降法是寻找函数局部最优解的有效方法（这里说的是局部最优解，而不是全局最优解，但是一般我们遇到的问题都是凸问题，局部最优解就是全局最优解），至于我们为什么不直接进行求解呢，因为计算量太大，如果有几百个参数的话，是不可行的（感觉这里说的不清楚，应该更具体的描述一下）。</p><h2 id="反向传播算法">反向传播算法</h2><p>使用梯度下降算法的时候，我们需要计算函数的梯度，反向传播算法解释计算神经网络中误差函数梯度的一种方法。</p><h2 id="参考文献">参考文献</h2><p>1.<a href="https://zhuanlan.zhihu.com/p/25355758" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/25355758</a><br>2.<a href="https://www.zhihu.com/question/36301367/answer/142096153" target="_blank" rel="noopener">https://www.zhihu.com/question/36301367/answer/142096153</a><br>3.<a href="http://neuralnetworksanddeeplearning.com/" target="_blank" rel="noopener">http://neuralnetworksanddeeplearning.com/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;梯度下降和反向传播，他们两个之间的关系？&lt;/p&gt;
&lt;h2 id=&quot;导数-偏导数-梯度-方向倒数&quot;&gt;导数，偏导数，梯度，方向倒数&lt;/h2&gt;
&lt;h3 id=&quot;导数&quot;&gt;导数&lt;/h3&gt;
&lt;p&gt;定义：&lt;br&gt;
$$f^{’}(x_0) = {\lim_{\Delta x \to 0}
      
    
    </summary>
    
      <category term="机器学习" scheme="http://mxxhcm.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://mxxhcm.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="梯度下降" scheme="http://mxxhcm.github.io/tags/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/"/>
    
      <category term="反向传播" scheme="http://mxxhcm.github.io/tags/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/"/>
    
  </entry>
  
  <entry>
    <title>pandas笔记</title>
    <link href="http://mxxhcm.github.io/2019/03/18/python-pandas%E7%AC%94%E8%AE%B0/"/>
    <id>http://mxxhcm.github.io/2019/03/18/python-pandas笔记/</id>
    <published>2019-03-18T07:15:54.000Z</published>
    <updated>2019-08-16T08:59:53.844Z</updated>
    
    <content type="html"><![CDATA[<h2 id="pd-read">pd.read_***()</h2><h3 id="pd-read-csv">pd.read_csv()</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas</span><br><span class="line">pandas.read_csv(filepath_or_buffer, sep=<span class="string">', '</span>, delimiter=<span class="literal">None</span>, header=<span class="string">'infer'</span>, names=<span class="literal">None</span>, index_col=<span class="literal">None</span>, usecols=<span class="literal">None</span>, squeeze=<span class="literal">False</span>, prefix=<span class="literal">None</span>, mangle_dupe_cols=<span class="literal">True</span>, dtype=<span class="literal">None</span>, engine=<span class="literal">None</span>, converters=<span class="literal">None</span>, true_values=<span class="literal">None</span>, false_values=<span class="literal">None</span>, skipinitialspace=<span class="literal">False</span>, skiprows=<span class="literal">None</span>, nrows=<span class="literal">None</span>, na_values=<span class="literal">None</span>, keep_default_na=<span class="literal">True</span>, na_filter=<span class="literal">True</span>, verbose=<span class="literal">False</span>, skip_blank_lines=<span class="literal">True</span>, parse_dates=<span class="literal">False</span>, infer_datetime_format=<span class="literal">False</span>, keep_date_col=<span class="literal">False</span>, date_parser=<span class="literal">None</span>, dayfirst=<span class="literal">False</span>, iterator=<span class="literal">False</span>, chunksize=<span class="literal">None</span>, compression=<span class="string">'infer'</span>, thousands=<span class="literal">None</span>, decimal=<span class="string">b'.'</span>, lineterminator=<span class="literal">None</span>, quotechar=<span class="string">'"'</span>, quoting=<span class="number">0</span>, escapechar=<span class="literal">None</span>, comment=<span class="literal">None</span>, encoding=<span class="literal">None</span>, dialect=<span class="literal">None</span>, tupleize_cols=<span class="literal">None</span>, error_bad_lines=<span class="literal">True</span>, warn_bad_lines=<span class="literal">True</span>, skipfooter=<span class="number">0</span>, skip_footer=<span class="number">0</span>, doublequote=<span class="literal">True</span>, delim_whitespace=<span class="literal">False</span>, as_recarray=<span class="literal">None</span>, compact_ints=<span class="literal">None</span>, use_unsigned=<span class="literal">None</span>, low_memory=<span class="literal">True</span>, buffer_lines=<span class="literal">None</span>, memory_map=<span class="literal">False</span>, float_precision=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure><p>filepath_or_buffer: 文件路径，或者一个字符串，url等等<br>sep: str,分隔符，默认是’,'<br>delimiter: str,定界符，如果指定该参数，sep参数失效<br>delimiter_whitespace: boolean,指定是否吧空格作为分界符如果指定该参数，则delimiter失效<br>header: int or list of ints,指定列名字，默认是header=0,表示把第一行当做列名，如果header=[0,3,4],表示吧第0,3,4行都当做列名，真正的数据从第二行开始，如果没有列名，指定header=None<br>index_col: int or sequence or False,指定哪几列作为index，index_col=[0,1],表示用前两列的值作为一个index，去访问后面几列的值。<br>prefix: str,如果header为None的话，可以指定列名。<br>parse_dates: boolean or list of ints or names,or list of lists, or dict 如果是True，解析index，如果是list of ints，把每一个int代表的列都分别当做一个日期解析，如果是list of lists，将list中的list作为一个日期解析，如果是字典的话，将dict中key作为一个新的列名，value为这个新的列的值。<br>keep_date_col: boolean,如果parser_dates中是将多个列合并为一个日期的话，是否保留原始列<br>date_parser: function,用来解析parse_dates中给出的日期列，是自己写的函数，函数参数个数和一个日期的列数相同。</p><p>chunksize: 如果文件太大的话，分块读入</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data = pd.read_csv(<span class="string">"input.csv"</span>,chunksize=<span class="number">1000</span>)</span><br><span class="line"><span class="keyword">for</span>  i  <span class="keyword">in</span>  data:</span><br><span class="line">  ...</span><br></pre></td></tr></table></figure><h2 id="dataframe">DataFrame</h2><h3 id="声明一个dataframe">声明一个DataFrame</h3><p>data = pandas.DataFrame(numpy.arange(16).reshape(4,4),index=list(‘abcd’),columns=(‘wxyz’)<br>w  x  y  z<br>a  0  1  2  3<br>b  4  5  6  7<br>c  8  9  10  11<br>d  12  13  14  15<br>index 是index列的值<br>columns 是列名</p><h3 id="访问某一列">访问某一列</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data = pandas.DataFrame(numpy.arange(<span class="number">16</span>).reshape(<span class="number">4</span>,<span class="number">4</span>),index=list(<span class="string">'abcd'</span>),columns=(<span class="string">'wxyz'</span>)</span><br><span class="line">data[<span class="string">'w'</span>]</span><br><span class="line">data.w</span><br></pre></td></tr></table></figure><h3 id="写入某一列">写入某一列</h3><p>只能先访问列 再访问行<br>data[‘w’] = []   # =左右两边shape必须一样<br>data[‘w’][0]  #某一列的第0行</p><h3 id="groupby">groupby</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">data = pandas.DataFrame(np.arange(<span class="number">16</span>).reshape(<span class="number">4</span>,<span class="number">4</span>),index=list(<span class="string">'abcd'</span>),columns=(<span class="string">'wxyz'</span>))</span><br><span class="line"><span class="keyword">for</span> key,value <span class="keyword">in</span> data.groupby(<span class="string">"w"</span>):  <span class="comment"># group by 列名什么的，就是说某一列的值一样分一组</span></span><br><span class="line">  value = value.values  <span class="comment"># value是一个numpy数组</span></span><br><span class="line">  value_list = value.tolist()  <span class="comment">#将numpy数组转换为一个list</span></span><br><span class="line">  <span class="keyword">for</span> single_list <span class="keyword">in</span> value_list:</span><br><span class="line">     single_list = str(single_list)</span><br><span class="line">     ...</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;pd-read&quot;&gt;pd.read_***()&lt;/h2&gt;
&lt;h3 id=&quot;pd-read-csv&quot;&gt;pd.read_csv()&lt;/h3&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;
      
    
    </summary>
    
      <category term="python" scheme="http://mxxhcm.github.io/categories/python/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="pandas" scheme="http://mxxhcm.github.io/tags/pandas/"/>
    
  </entry>
  
  <entry>
    <title>argparse笔记</title>
    <link href="http://mxxhcm.github.io/2019/03/18/python-argparse%E7%AC%94%E8%AE%B0/"/>
    <id>http://mxxhcm.github.io/2019/03/18/python-argparse笔记/</id>
    <published>2019-03-18T07:15:41.000Z</published>
    <updated>2019-06-26T03:27:59.464Z</updated>
    
    <content type="html"><![CDATA[<h2 id="简单的例子">简单的例子</h2><h3 id="创建一个parser">创建一个parser</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">parser = argparse.ArgumentParser(description=<span class="string">'Process Intergers'</span>)</span><br></pre></td></tr></table></figure><h3 id="添加参数">添加参数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">parser.add_argument(,,)</span><br></pre></td></tr></table></figure><h3 id="解析参数">解析参数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">arglist = parser.parse_args()</span><br></pre></td></tr></table></figure><h3 id="代码示例">代码示例</h3><p>完整代码如下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_args</span><span class="params">()</span>:</span></span><br><span class="line">    parser = argparse.ArgumentParser(<span class="string">"input parameters"</span>)</span><br><span class="line">    parser.add_argument(<span class="string">"--batch_size"</span>, type=int, default=<span class="number">32</span>)</span><br><span class="line">    parser.add_argument(<span class="string">"--episodes"</span>, type=int, default=<span class="number">1</span>)</span><br><span class="line">    parser.add_argument(<span class="string">"--lr"</span>, type=float, default=<span class="number">0.01</span>)</span><br><span class="line">    parser.add_argument(<span class="string">"--momentum"</span>, type=float, default=<span class="number">0.9</span>)</span><br><span class="line">    args_list = parser.parse_args()</span><br><span class="line">    <span class="keyword">return</span> args_list</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(args_list)</span>:</span></span><br><span class="line">print(args_list.batch_size)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    args_list = parse_args()</span><br><span class="line">    main(args_list)</span><br></pre></td></tr></table></figure><h2 id="argumentparser-objects">ArgumentParser objects</h2><blockquote><p>The ArgumentParser object will hold all the information necessary to parse the command line into python data types</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">argparse</span>.<span class="title">ArgumentParser</span><span class="params">(</span></span></span><br><span class="line"><span class="class"><span class="params">prog=None,</span></span></span><br><span class="line"><span class="class"><span class="params">usage=None,</span></span></span><br><span class="line"><span class="class"><span class="params">description=None,</span></span></span><br><span class="line"><span class="class"><span class="params">epilog=None,</span></span></span><br><span class="line"><span class="class"><span class="params">parents=[],</span></span></span><br><span class="line"><span class="class"><span class="params">formatter_class=argparse.HelpFormatter,</span></span></span><br><span class="line"><span class="class"><span class="params">prefix_chars=<span class="string">'-'</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">fromfile_prefix_chars=None,</span></span></span><br><span class="line"><span class="class"><span class="params">argument_default=None,</span></span></span><br><span class="line"><span class="class"><span class="params">conflict_handler=<span class="string">'error'</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">add_help=True</span></span></span><br><span class="line"><span class="class"><span class="params">)</span></span></span><br></pre></td></tr></table></figure><h3 id="创建一个名为test-py的程序如下">创建一个名为test.py的程序如下</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">args = parser.parse_args()</span><br></pre></td></tr></table></figure><p>~#:python <a href="http://test.py" target="_blank" rel="noopener">test.py</a> -h</p><blockquote><p>usage: <a href="http://test.py" target="_blank" rel="noopener">test.py</a> [-h]<br>optional arguments:<br>-h, --help  show this help message and exit</p></blockquote><h3 id="prog参数">prog参数</h3><p>设置显示程序的名称</p><h4 id="直接使用默认显示的程序名">直接使用默认显示的程序名</h4><p>~#:python <a href="http://test.py" target="_blank" rel="noopener">test.py</a> -h</p><blockquote><p>usage: <a href="http://test.py" target="_blank" rel="noopener">test.py</a> [-h]<br>optional arguments:<br>-h, --help  show this help message and exit</p></blockquote><h4 id="使用prog参数进行设置">使用prog参数进行设置</h4><p>修改test.py的程序如下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser(prog=<span class="string">"mytest"</span>)</span><br><span class="line">args = parser.parse_args()</span><br></pre></td></tr></table></figure><p>~#:python <a href="http://test.py" target="_blank" rel="noopener">test.py</a> -h</p><blockquote><p>usage: mytest [-h]<br>optional arguments:<br>-h, --help  show this help message and exit</p></blockquote><p>usage后的名称变为我们prog参数指定的名称</p><h3 id="usage">usage</h3><h4 id="使用默认的usage">使用默认的usage</h4><h4 id="使用指定的usage">使用指定的usage</h4><h3 id="description">description</h3><h4 id="使用默认的description">使用默认的description</h4><h4 id="使用指定的description">使用指定的description</h4><h3 id="epilog">epilog</h3><h4 id="使用默认的epilog">使用默认的epilog</h4><h4 id="使用指定的epilog">使用指定的epilog</h4><h3 id="parents">parents</h3><h3 id="formatter-class">formatter_class</h3><h3 id="prefix-chars">prefix_chars</h3><p>指定其他的prefix，默认的是-，比如可以指定可选参数的前缀为+</p><h3 id="fromfile-prefix-chars">fromfile_prefix_chars</h3><h3 id="argument-default">argument_default</h3><h3 id="conflict-handler">conflict_handler</h3><p>将conflict_handler设置为resolve就可以防止override原来older arguments</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser(conflict_handler=<span class="string">'resolve'</span>)</span><br><span class="line">parser.add_argument(<span class="string">'--foo'</span>,<span class="string">'-f'</span>,help=<span class="string">"old help"</span>)</span><br><span class="line">parser.add_argument(<span class="string">'-f'</span>,help=<span class="string">"new_help"</span>)</span><br><span class="line">parser.print_help()</span><br></pre></td></tr></table></figure><h3 id="add-help">add_help</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser(add_help=<span class="literal">False</span>)</span><br><span class="line">parser.print_help()</span><br></pre></td></tr></table></figure><p>输出</p><blockquote><p>usage: [-h]<br>optional arguments:<br>-h, --help  show this help message and exit<br>将add_help设置为false</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser(add_help=<span class="literal">False</span>)</span><br><span class="line">parser.print_help()</span><br></pre></td></tr></table></figure><p>输出</p><blockquote><p>usage:</p></blockquote><h2 id="the-add-argument-method">The add_argument() method</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">ArgumentParser.add_argument(</span><br><span class="line">name <span class="keyword">or</span> flags...</span><br><span class="line">[,action],</span><br><span class="line">[,nargs],</span><br><span class="line">[,const],</span><br><span class="line">[,default],</span><br><span class="line">[,type],</span><br><span class="line">[,choices],</span><br><span class="line">[,required],</span><br><span class="line">[,help],</span><br><span class="line">[,metavar],</span><br><span class="line">[,dest]</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="例子">例子</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">'-f'</span>,<span class="string">'-foo'</span>,<span class="string">'-a'</span>, defaults=, type=, help=)</span><br><span class="line">parser.add_argument(<span class="string">'hello'</span>)</span><br><span class="line">parser.add_argument(<span class="string">'hi'</span>)</span><br><span class="line">args = parser.parse_args([<span class="string">'Hello'</span>,<span class="string">'-f'</span>,<span class="string">'123'</span>,<span class="string">'Hi'</span>])</span><br><span class="line">print(args)</span><br></pre></td></tr></table></figure><h3 id="name-or-flags">name or flags</h3><h4 id="添加可选参数">添加可选参数</h4><p>parser.add_argument(’-f’, ‘–foo’, ‘-fooo’)</p><h4 id="添加必选参数">添加必选参数</h4><p>parser.add_argument(‘bar’)</p><h4 id="调用parse-args">调用parse_args()</h4><p>当parse_args()函数被调用的时候，可选参数会被-prefix所识别，剩下的参数会被分配给必选参数的位置。如下代码中，'3’对应的就是’hello’的参数，‘this is hi’对应的就是’hi’的参数，而’123’是’-f’的参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">'-f'</span>,<span class="string">'-foo'</span>,<span class="string">'-a'</span>)</span><br><span class="line">parser.add_argument(<span class="string">'hello'</span>, type=int)</span><br><span class="line">parser.add_argument(<span class="string">'hi'</span>)</span><br><span class="line">args = parser.parse_args([<span class="string">'3'</span>,<span class="string">'-f'</span>,<span class="string">'123'</span>,<span class="string">'this is hi'</span>])</span><br><span class="line">print(args)</span><br></pre></td></tr></table></figure><p>输出</p><blockquote><p>Namespace(f=‘123’, hello=‘Hello’, hi=‘Hi’)</p></blockquote><h3 id="action">action</h3><h4 id="store-the-default-action">store,the default action</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">'--foo'</span>)</span><br><span class="line">args = parser.parse_args([<span class="string">'--foo'</span>,<span class="string">'1'</span>])</span><br><span class="line">print(args)</span><br></pre></td></tr></table></figure><p>输出</p><blockquote><p>Namespace(foo=‘1’)</p></blockquote><h4 id="store-const">store_const</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">'--foo'</span>, action=<span class="string">'store_const'</span>, const=<span class="number">42</span>)</span><br><span class="line">args = parser.parse_args([<span class="string">'--foo'</span>)</span><br><span class="line">print(args)</span><br></pre></td></tr></table></figure><p>输出</p><blockquote><p>Namespace(foo=42)</p></blockquote><h4 id="store-true-and-store-false">store_true and store_false</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">'--foo'</span>, action=<span class="string">'store_true'</span>)</span><br><span class="line">parser.add_argument(<span class="string">'--bar'</span>, action=<span class="string">'store_false'</span>)</span><br><span class="line">parser.add_argument(<span class="string">'--baz'</span>, action=<span class="string">'store_false'</span>)</span><br><span class="line">args = parser.parse_args(<span class="string">'--foo --bar'</span>.split())</span><br><span class="line">print(args)</span><br></pre></td></tr></table></figure><p>输出</p><blockquote><p>Namespace(bar=False, baz=True, foo=True)</p></blockquote><p>这里为什么是这样呢，因为默认存储的都是True，当你调用–bar,–foo参数时，会执行action操作，会把action指定的动作执行</p><h4 id="d-append">d.append</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">'--foo'</span>, action=<span class="string">'append'</span>)</span><br><span class="line">args = parser.parse_args(<span class="string">'--foo 1 --foo 2 --foo 3'</span>.split())</span><br><span class="line">print(args)</span><br></pre></td></tr></table></figure><p>输出</p><blockquote><p>Namespace(foo=[‘1’, ‘2’, ‘3’])</p></blockquote><h4 id="append-const">append_const</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">'--str'</span>, action=<span class="string">'append_const'</span>,const=str)</span><br><span class="line">parser.add_argument(<span class="string">'--int'</span>, action=<span class="string">'append_const'</span>,const=int)</span><br><span class="line">args = parser.parse_args(<span class="string">'--str --int'</span>.split())</span><br><span class="line">print(args)</span><br></pre></td></tr></table></figure><p>输出</p><blockquote><p>Namespace(int=[&lt;class ‘int’&gt;], str=[&lt;class ‘str’&gt;])</p></blockquote><h4 id="count">count</h4><p>统计一个keyword argument出现了多少次</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">'--co'</span>, <span class="string">'-c'</span>,action=<span class="string">'count'</span>)</span><br><span class="line">args = parser.parse_args([<span class="string">'-ccc'</span>])</span><br><span class="line">print(args)</span><br></pre></td></tr></table></figure><p>输出</p><blockquote><p>Namespace(co=3)</p></blockquote><h4 id="help">help</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">args = parser.parse_args(<span class="string">'--help'</span>.split())</span><br><span class="line">print(args)</span><br></pre></td></tr></table></figure><p>输出，如果是交互式环境的话，会退出python</p><blockquote><p>usage: [-h]</p></blockquote><blockquote><p>optional arguments:<br>-h, --help  show this help message and exit</p></blockquote><h4 id="version">version</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">'--version'</span>, action=<span class="string">'version'</span>,version=<span class="string">'version 3'</span>)</span><br><span class="line">args = parser.parse_args(<span class="string">'--version'</span>.split())</span><br><span class="line">print(args)</span><br></pre></td></tr></table></figure><p>输出,如果是交互式环境的话，会退出python</p><blockquote><p>version 3</p></blockquote><h3 id="nargs-指定参数个数">nargs 指定参数个数</h3><h4 id="n">N</h4><p>如果是可选参数的话，或者不指定这个参数，或者必须指定N个参数<br>如果是必选参数的话，必须指定N个参数，不能多也不能少，也不能为0个</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">'--foo'</span>,nargs=<span class="number">3</span>)</span><br><span class="line">parser.add_argument(<span class="string">'bar'</span>,nargs=<span class="number">4</span>)</span><br><span class="line">args = parser.parse_args(<span class="string">'bar 3 4 5'</span>.split())</span><br><span class="line">print(args)</span><br></pre></td></tr></table></figure><p>输出</p><blockquote><p>Namespace(bar=[‘bar’, ‘3’, ‘4’, ‘5’], foo=None)</p></blockquote><h4 id="none">?</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">'--foo'</span>,nargs=<span class="string">'?'</span>,const=<span class="string">'c'</span>,default=<span class="string">'d'</span>)</span><br><span class="line">parser.add_argument(<span class="string">'bar'</span>,nargs=<span class="string">'?'</span>,default=<span class="string">'d'</span>)</span><br><span class="line">args = parser.parse_args(<span class="string">'3'</span>.split())</span><br><span class="line">print(args)</span><br><span class="line">args = parser.parse_args(<span class="string">'3 --foo'</span>.split())</span><br><span class="line">print(args)</span><br></pre></td></tr></table></figure><p>输出</p><blockquote><p>Namespace(bar=‘3’, foo=‘d’)<br>Namespace(bar=‘3’, foo=‘c’)</p></blockquote><p>如果显式指定可选参数，但是不给它参数，那么如果有const的话，就会显示const的值，否则就会显示None</p><h4 id="none-v2">*</h4><p>nargs设置为*的话，不能直接用const=’'来设置const参数，需要使用其他方式</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">'--foo'</span>,nargs=<span class="string">'*'</span>,default=<span class="string">'d'</span>)</span><br><span class="line">parser.add_argument(<span class="string">'bar'</span>,nargs=<span class="string">'*'</span>,default=<span class="string">'d'</span>)</span><br><span class="line">args = parser.parse_args(<span class="string">'3 --foo 3 4'</span>.split())</span><br><span class="line">print(args)</span><br></pre></td></tr></table></figure><p>输出</p><blockquote><p>Namespace(bar=[‘3’], foo=[‘3’, ‘4’])</p></blockquote><h4 id="none-v3">+</h4><p>nargs设置为+，参数个数必须大于等于1</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">'--foo'</span>,nargs=<span class="string">'+'</span>,default=<span class="string">'d'</span>)</span><br><span class="line">parser.add_argument(<span class="string">'bar'</span>,nargs=<span class="string">'+'</span>,default=<span class="string">'d'</span>)</span><br><span class="line">args = parser.parse_args(<span class="string">'3 3'</span>.split())</span><br><span class="line">print(args)</span><br><span class="line">args = parser.parse_args(<span class="string">'--foo 3'</span>.split())</span><br><span class="line">print(args)</span><br></pre></td></tr></table></figure><p>输出</p><blockquote><p>Namespace(bar=[‘3’], foo=‘d’)<br>Namespace(bar=[‘3’], foo=[‘3’])</p></blockquote><h3 id="const">const</h3><h4 id="action-store-const-or-action-append-const">action=’'store_const&quot; or action=“append_const”</h4><p>the examples are in the action</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">'--foo'</span>, action=<span class="string">'store_const'</span>, const=<span class="number">42</span>)</span><br><span class="line">args = parser.parse_args([<span class="string">'--foo'</span>)</span><br><span class="line">print(args)</span><br></pre></td></tr></table></figure><p>输出</p><blockquote><p>Namespace(foo=42)</p></blockquote><h4 id="like-f-or-foo-and-nargs">like -f or --foo and nargs=’?’</h4><p>the examples are the same as examples in the nargs=’?’</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">'--foo'</span>,nargs=<span class="string">'?'</span>,const=<span class="string">'c'</span>,default=<span class="string">'d'</span>)</span><br><span class="line">parser.add_argument(<span class="string">'bar'</span>,nargs=<span class="string">'?'</span>,default=<span class="string">'d'</span>)</span><br><span class="line">args = parser.parse_args(<span class="string">'3'</span>.split())</span><br><span class="line">print(args)</span><br><span class="line">args = parser.parse_args(<span class="string">'3 --foo'</span>.split())</span><br><span class="line">print(args)</span><br></pre></td></tr></table></figure><p>输出</p><blockquote><p>Namespace(bar=‘3’, foo=‘d’)<br>Namespace(bar=‘3’, foo=‘c’)<br>如果显式指定可选参数，但是不给它参数，那么如果有const的话，就会显示const的值，否则就会显示None</p></blockquote><h3 id="default">default</h3><p>default对于可选参数来说，是有用的，当可选参数没有在command line中显示出来时被使用，但是对于必选参数来说，只有nargs=?或者*才能起作用。</p><h4 id="对于可选参数">对于可选参数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">'--foo'</span>,default=<span class="number">43</span>)</span><br><span class="line">args = parser.parse_args([])</span><br><span class="line">print(args)</span><br><span class="line">args = parser.parse_args(<span class="string">'--foo 3'</span>.split())</span><br><span class="line">print(args)</span><br></pre></td></tr></table></figure><p>输出</p><blockquote><p>Namespace(foo=‘43’)<br>Namespace(foo=‘3’)</p></blockquote><h4 id="对于必选参数">对于必选参数</h4><p>对于nargs=‘+’是会出错</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(&apos;bar&apos;,nargs=&apos;+&apos;,default=&apos;d&apos;)</span><br><span class="line">args = parser.parse_args([])</span><br><span class="line">print(args)</span><br></pre></td></tr></table></figure><p>输出</p><blockquote><p>usage: [-h] bar [bar …]<br>: error: the following arguments are required: bar</p></blockquote><p>对于nargs=‘*’或者nargs=’?'就行了</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">'bar'</span>,nargs=<span class="string">'?'</span>,default=<span class="string">'d'</span>)</span><br><span class="line">args = parser.parse_args([])</span><br><span class="line">print(args)</span><br></pre></td></tr></table></figure><p>输出</p><blockquote><p>Namespace(bar=‘d’)</p></blockquote><h3 id="type">type</h3><p>将输入的字符串参数转换为你想要的参数类型<br>对于文件类型来说，这个文件必须在当前目录存在。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">'--door'</span>,type=int)</span><br><span class="line">parser.add_argument(<span class="string">'filename'</span>,type=file)</span><br><span class="line">parser.parse_args([<span class="string">'--door'</span>,<span class="string">'3'</span>,<span class="string">'hello.txt'</span>])</span><br></pre></td></tr></table></figure><p>输出</p><blockquote><p>Namespace(door=3)<br>这里的door就是int类型的</p></blockquote><h3 id="choices">choices</h3><p>输入的参数必须在choices这个范围中，否则就会报错</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParse()</span><br><span class="line">parser.add_argument(<span class="string">'--door'</span>,type=int,choices=range(<span class="number">1</span>,<span class="number">9</span>))</span><br><span class="line">parser.parse_args([<span class="string">'--door'</span>,<span class="string">'3'</span>])</span><br></pre></td></tr></table></figure><p>输出</p><blockquote><p>Namespace(door=3)</p></blockquote><h3 id="required">required</h3><p>如果将required设置为True的话，那么这个可选参数必须要设置的</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">'-f'</span>, <span class="string">'--foo-bar'</span>, <span class="string">'--foo'</span>,required=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h3 id="help-v2">help</h3><p>help可以设置某个参数的简要介绍。<br>使用help=argparse.SUPRESS可以在help界面中不显示这个参数的介绍</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">'-f'</span>, <span class="string">'--foo-bar'</span>, <span class="string">'--foo'</span>,help=<span class="string">'fool you '</span>)</span><br><span class="line">parser.add_argument(<span class="string">'-xs'</span>, <span class="string">'--y'</span>,help=argparse.SUPPRESS)</span><br><span class="line">parser.print_help()</span><br></pre></td></tr></table></figure><p>输出</p><blockquote><p>usage: [-h] [-f FOO_BAR]</p></blockquote><blockquote><p>optional arguments:<br>-h, --help            show this help message and exit<br>-f FOO_BAR, --foo-bar FOO_BAR, --foo FOO_BAR<br>fool you</p></blockquote><h3 id="dest">dest</h3><p>dest就是在help输出时显示的optional和positional参数后跟的名字（没有指定metavar时）<br>如下,dest就是FOO<br>-foo FOO</p><h4 id="positional-argument">positional argument</h4><p>dest is normally supplied as the first argument to add_argument()</p><h4 id="可选参数">可选参数</h4><p>对于optional argument选择，–参数最长的一个作为dest，如果没有最长的，选择第一个出现的，如果没有–参数名，选择-参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">'-f'</span>, <span class="string">'--foo-bar'</span>, <span class="string">'--foo'</span>)</span><br><span class="line">parser.add_argument(<span class="string">'-xs'</span>, <span class="string">'--y'</span>)</span><br><span class="line">args = parser.parse_args(<span class="string">'-f 1 -xs 2'</span>.split())</span><br><span class="line">print(args)</span><br><span class="line">args = parser.parse_args(<span class="string">'--foo 1 --y 2'</span>.split())</span><br><span class="line">print(args)</span><br><span class="line">parser.print_help()</span><br></pre></td></tr></table></figure><p>输出</p><blockquote><p>Namespace(foo_bar=‘1’, y=‘2’)<br>Namespace(foo_bar=‘1’, y=‘2’)<br>usage: [-h] [-f FOO_BAR] [-xs Y]</p></blockquote><blockquote><p>optional arguments:<br>-h, --help            show this help message and exit<br>-f FOO_BAR, --foo-bar FOO_BAR, --foo FOO_BAR<br>-xs Y, --y Y</p></blockquote><h3 id="metavar">metavar</h3><p>如果指定metavar变量名的话，那么help输出的postional和positional参数后跟的名字就是metavar的名字而不是dest的名字</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">'-f'</span>, <span class="string">'--foo-bar'</span>, <span class="string">'--foo'</span>,metavar=<span class="string">"FOO"</span>)</span><br><span class="line">parser.add_argument(<span class="string">'-xs'</span>, <span class="string">'--y'</span>,metavar=<span class="string">'XY'</span>)</span><br><span class="line">args = parser.parse_args(<span class="string">'-f 1 -xs 2'</span>.split())</span><br><span class="line">print(args)</span><br><span class="line">args = parser.parse_args(<span class="string">'--foo 1 --y 2'</span>.split())</span><br><span class="line">print(args)</span><br><span class="line">parser.print_help()</span><br></pre></td></tr></table></figure><p>输出</p><blockquote><p>Namespace(foo_bar=‘1’, y=‘2’)<br>Namespace(foo_bar=‘1’, y=‘2’)<br>usage: [-h] [-f FOO] [-xs XY]</p></blockquote><blockquote><p>optional arguments:<br>-h, --help            show this help message and exit<br>-f FOO, --foo-bar FOO, --foo FOO<br>-xs XY, --y XY</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;简单的例子&quot;&gt;简单的例子&lt;/h2&gt;
&lt;h3 id=&quot;创建一个parser&quot;&gt;创建一个parser&lt;/h3&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span clas
      
    
    </summary>
    
      <category term="python" scheme="http://mxxhcm.github.io/categories/python/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="argparse" scheme="http://mxxhcm.github.io/tags/argparse/"/>
    
  </entry>
  
  <entry>
    <title>numpy笔记</title>
    <link href="http://mxxhcm.github.io/2019/03/18/python-numpy%E7%AC%94%E8%AE%B0/"/>
    <id>http://mxxhcm.github.io/2019/03/18/python-numpy笔记/</id>
    <published>2019-03-18T07:15:29.000Z</published>
    <updated>2019-07-12T13:24:11.729Z</updated>
    
    <content type="html"><![CDATA[<h2 id="numpy-ndarray">numpy.ndarray</h2><h3 id="attribute-of-the-np-ndarray">attribute of the np.ndarray</h3><p>ndarray.shape        #array的shape<br>ndarray.ndim            #array的维度<br>ndarray.size            #the number of ndarray in array<br>ndarray.dtype        #type of the number in array，dtype可以是’S’,int等<br>ndarray.itemsize        #size of the element in array</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array[array&gt;<span class="number">0</span>].size    <span class="comment">#统计一个数组有多少个非零元素，不论array的维度是多少</span></span><br></pre></td></tr></table></figure><h3 id="改变数组数据类型">改变数组数据类型</h3><p>将整形数组改为字符型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = numpy.zeros((<span class="number">3</span>,<span class="number">4</span>),dtype=<span class="string">'i'</span>)</span><br><span class="line">a.astype(<span class="string">'S'</span>)</span><br></pre></td></tr></table></figure><h3 id="将numpy转为list">将numpy转为list</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">a = np.zeros((<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>))</span><br><span class="line">b = a.tolist()</span><br><span class="line">print(b)</span><br><span class="line">print(len(b))</span><br><span class="line">print(len(b[<span class="number">0</span>]))</span><br><span class="line"><span class="comment"># [[[0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0]], [[0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0]], [[0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0]]]</span></span><br><span class="line"><span class="comment"># 3</span></span><br><span class="line"><span class="comment"># 4</span></span><br></pre></td></tr></table></figure><h3 id="reshape">reshape</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">a = np.zeros((<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>))</span><br><span class="line">a.reshape(<span class="number">-1</span>, <span class="number">1</span>)</span><br></pre></td></tr></table></figure><h3 id="flatten">flatten</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">a = np.zeros((<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>))</span><br><span class="line">a.flatten()</span><br></pre></td></tr></table></figure><h2 id="numpy数组初始化">numpy数组初始化</h2><ul><li>numpy.array()</li><li>numpy.zeros()</li><li>numpy.empty()</li><li>numpy.random()</li></ul><h3 id="numpy-array">numpy.array()</h3><h4 id="api">API</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">np.array(</span><br><span class="line">    object,</span><br><span class="line">    dtype=<span class="literal">None</span>,</span><br><span class="line">    copy=<span class="literal">True</span>,</span><br><span class="line">    order=<span class="literal">False</span>,</span><br><span class="line">    subok=<span class="literal">False</span>,</span><br><span class="line">    ndim=<span class="number">0</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="numpy-zeros">numpy.zeros()</h3><h4 id="api-v2">API</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">np.zeros(</span><br><span class="line">    shape,</span><br><span class="line">    dtype=float,</span><br><span class="line">    order=<span class="string">'C'</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h4 id="代码示例">代码示例</h4><p><a href>代码地址</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">np.zeros((<span class="number">3</span>, <span class="number">4</span>),dtype=<span class="string">'i'</span>)</span><br><span class="line">``` </span><br><span class="line"></span><br><span class="line"><span class="comment">### numpy.empty()</span></span><br><span class="line"><span class="comment">#### API</span></span><br><span class="line">``` python</span><br><span class="line">np.empty(</span><br><span class="line">    shape,</span><br><span class="line">    dtype=float,</span><br><span class="line">    order=<span class="string">'C'</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h4 id="代码示例-v2">代码示例</h4><p><a href>代码地址</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.empty((<span class="number">3</span>, <span class="number">4</span>),dtype=<span class="string">'f'</span>)</span><br></pre></td></tr></table></figure><h3 id="numpy-random">numpy.random</h3><h4 id="numpy-random-randn">numpy.random.randn()</h4><p>返回标准正态分布的一个样本<br>numpy.random.randn(d0, d1, …, dn)<br>例子</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.random.randn(<span class="number">3</span>,<span class="number">4</span>)</span><br></pre></td></tr></table></figure><blockquote><p>array([[ 0.47203644, -0.0869761 , -1.02814481, -0.45945482],<br>[ 0.34586502, -0.63121119,  0.35510786,  0.82975136],<br>[-2.00253326, -0.63773715, -0.82700167,  1.80724647]])</p></blockquote><h4 id="numpy-random-rand">numpy.random.rand()</h4><p>创建一个给定shape的数组，从区间[0,1)上的均匀分布中随机采样</p><blockquote><p>create an array of the given shape and populate it with random samples from a uniform disctribution over [0,1)</p></blockquote><p>numpy.random.rand(d0,d1,…,dn)<br>例子</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.random.rand(<span class="number">3</span>,<span class="number">4</span>)</span><br></pre></td></tr></table></figure><h4 id="numpy-random-random">numpy.random.random()</h4><p>返回区间[0.0, 1.0)之间的随机浮点数</p><blockquote><p>return random floats in the half-open interval [0.0,1.0)</p></blockquote><p>numpy.random.random(size=None)<br>例子</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.random.random((<span class="number">3</span>,<span class="number">4</span>))</span><br></pre></td></tr></table></figure><h5 id="note">Note</h5><p>注意，random.random()和random.rand()实现的功能都是一样的，就是输入的参数不同。见参考文献[1]。</p><h4 id="numpy-random-ranf">numpy.random.ranf()</h4><p>我觉得它和random.random()没啥区别</p><h4 id="numpy-random-randint">numpy.random.randint()</h4><blockquote><p>return random integers from low(inclusive) to high(exclusive),[low,high) if high is None,then results are from [0,low)</p></blockquote><p>numpy.random.randint(low,high=None,size=None,dtype=‘l’)<br>例子</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">np.random.randint(<span class="number">3</span>,size=[<span class="number">3</span>,<span class="number">4</span>])</span><br><span class="line">np.random.randint(<span class="number">4</span>,<span class="number">6</span>,size=[<span class="number">6</span>,<span class="number">2</span>])</span><br></pre></td></tr></table></figure><h4 id="numpy-random-randomstate">numpy.random.RandomState()</h4><blockquote><p>class numpy.random.RandomState(seed=None)</p></blockquote><p>这是一个类，给定一个种子，它接下来产生的一系列随机数都是固定的。每次需要重新产生随机数的时候，就重置种子。<br>通过一个例子来看：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">rdm = np.randrom.RandomState()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">   rdm.seed(<span class="number">3</span>)</span><br><span class="line">   print(rdm.rand())</span><br><span class="line">   print(rdm.rand())</span><br><span class="line">   print(rdm.rand())</span><br><span class="line">    print(<span class="string">"\n"</span>)</span><br><span class="line"><span class="comment"># 0.9670298390136767</span></span><br><span class="line"><span class="comment"># 0.5472322491757223</span></span><br><span class="line"><span class="comment"># 0.9726843599648843</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 0.9670298390136767</span></span><br><span class="line"><span class="comment"># 0.5472322491757223</span></span><br><span class="line"><span class="comment"># 0.9726843599648843</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 0.9670298390136767</span></span><br><span class="line"><span class="comment"># 0.5472322491757223</span></span><br><span class="line"><span class="comment"># 0.9726843599648843</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 0.9670298390136767</span></span><br><span class="line"><span class="comment"># 0.5472322491757223</span></span><br><span class="line"><span class="comment"># 0.9726843599648843</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 0.9670298390136767</span></span><br><span class="line"><span class="comment"># 0.5472322491757223</span></span><br><span class="line"><span class="comment"># 0.9726843599648843</span></span><br></pre></td></tr></table></figure><h3 id="创建bool类型数组">创建bool类型数组</h3><p>np.ones([2, 2], dtype=bool)<br>np.zeros([2, 2], dtype=bool)</p><h3 id="others">others</h3><h4 id="numpy-arange">numpy.arange()</h4><h4 id="numpy-linspace">numpy.linspace()</h4><h2 id="np-random-binomial">np.random.binomial</h2><h3 id="api-v3">API</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">numpy.random.binomial(</span><br><span class="line">n, </span><br><span class="line">p, </span><br><span class="line">size=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="介绍">介绍</h3><p>二项分布，共有三个参数，前两个是必选参数，第三个是可选参数。$n$是实验的个数，比如同时扔三枚硬币，这里就是$n=3$,$p$是为$1$的概率。$size$是总共进行多少次实验。<br>返回值是在每次试验中，trival成功的个数。如果是一个scalar，代表$size=1$，如果是一个list，代表$size\gt 1$。</p><h3 id="代码示例-v3">代码示例</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    rand = np.random.binomial(<span class="number">2</span>, <span class="number">0.9</span>)</span><br><span class="line">    print(rand)</span><br><span class="line"><span class="comment"># 可以看成扔2个硬币，每个硬币正面向上的概率是0.9,最后有几个硬币正面向上。</span></span><br><span class="line"><span class="comment"># 2</span></span><br><span class="line"><span class="comment"># 2</span></span><br><span class="line"><span class="comment"># 2</span></span><br><span class="line"><span class="comment"># 2</span></span><br><span class="line"><span class="comment"># 2</span></span><br><span class="line"><span class="comment"># 2</span></span><br><span class="line"><span class="comment"># 2</span></span><br><span class="line"><span class="comment"># 1</span></span><br><span class="line"><span class="comment"># 1</span></span><br><span class="line"><span class="comment"># 2</span></span><br><span class="line"></span><br><span class="line">rand = np.random.binomial(<span class="number">3</span>, <span class="number">0.9</span>, <span class="number">5</span>)</span><br><span class="line">print(rand)</span><br><span class="line"><span class="comment"># 可以看成扔3个硬币，每个硬币正面向上的概率是0.9,最后有几个硬币正面向上。一共进行5次实验。</span></span><br><span class="line"><span class="comment"># [2 2 3 3 2]</span></span><br></pre></td></tr></table></figure><h2 id="np-random-choice">np.random.choice</h2><h3 id="api-v4">API</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">numpy.random.choice(</span><br><span class="line">    a,  <span class="comment"># 1d array或者int，如果是一个数组，从其中生成样本；如果是一个整数，从np.arange(a)中生成样本</span></span><br><span class="line">    size=<span class="literal">None</span>,  <span class="comment"># output shape，比如是(m, n, k)的话，总共要m*n*k个样本，默认是None,返回一个样本。</span></span><br><span class="line">    replace=<span class="literal">True</span>,   <span class="comment"># 是否使用replacement，设置为False的话所有元素不重复。</span></span><br><span class="line">    p=<span class="literal">None</span>  <span class="comment"># 概率分布，相加必须等于1，默认是从一个均匀分布中采样。</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="代码示例-v4">代码示例</h3><p><a href>代码地址</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">a0 = np.random.choice([<span class="number">8</span>, <span class="number">9</span>, <span class="number">-1</span>, <span class="number">2</span>, <span class="number">0</span>], <span class="number">3</span>)</span><br><span class="line">print(a0)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 从np.arange(5)从使用均匀分布采样一个shape为4的样本</span></span><br><span class="line">a1 = np.random.choice(<span class="number">5</span>, <span class="number">4</span>)</span><br><span class="line">print(a1)</span><br><span class="line"></span><br><span class="line">a2 = np.random.choice(<span class="number">5</span>, <span class="number">8</span>, p=[<span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.5</span>, <span class="number">0.2</span>, <span class="number">0</span>])</span><br><span class="line">print(a2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># replace 设置为False，相当于np.random.permutation()</span></span><br><span class="line">a3 = np.random.choice([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">8</span>, <span class="number">9</span>], <span class="number">5</span>, replace=<span class="literal">False</span>)</span><br><span class="line">print(a3)</span><br></pre></td></tr></table></figure><h2 id="np-random-permutation">np.random.permutation</h2><h3 id="api-v5">API</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">np.random.permutation(</span><br><span class="line">    x   <span class="comment"># int或者array，如果是int，置换np.arange(x)。如果是array，make a copy，随机打乱元素。</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="简介">简介</h3><p>对输入序列进行排列组合，如果输入是多维的话，只会在第一维重新排列。</p><h3 id="代码示例-v5">代码示例</h3><p><a href>代码地址</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">a1 = np.random.permutation(<span class="number">9</span>)</span><br><span class="line">print(a1)</span><br><span class="line"></span><br><span class="line">a2 = np.random.permutation([<span class="number">1</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">9</span>, <span class="number">8</span>])</span><br><span class="line">print(a2)</span><br><span class="line"></span><br><span class="line">a3 = np.random.permutation(np.arange(<span class="number">9</span>).reshape(<span class="number">3</span>, <span class="number">3</span>))</span><br><span class="line">print(a3)</span><br></pre></td></tr></table></figure><p>参考文献<br>1.<a href="https://stackoverflow.com/questions/47231852/np-random-rand-vs-np-random-random" target="_blank" rel="noopener">https://stackoverflow.com/questions/47231852/np-random-rand-vs-np-random-random</a><br>2.<a href="https://stackoverflow.com/questions/21174961/how-to-create-a-numpy-array-of-all-true-or-all-false" target="_blank" rel="noopener">https://stackoverflow.com/questions/21174961/how-to-create-a-numpy-array-of-all-true-or-all-false</a><br>3.<a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.choice.html" target="_blank" rel="noopener">https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.choice.html</a><br>4.<a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.permutation.html" target="_blank" rel="noopener">https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.permutation.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;numpy-ndarray&quot;&gt;numpy.ndarray&lt;/h2&gt;
&lt;h3 id=&quot;attribute-of-the-np-ndarray&quot;&gt;attribute of the np.ndarray&lt;/h3&gt;
&lt;p&gt;ndarray.shape        #arr
      
    
    </summary>
    
      <category term="python" scheme="http://mxxhcm.github.io/categories/python/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="numpy" scheme="http://mxxhcm.github.io/tags/numpy/"/>
    
  </entry>
  
  <entry>
    <title>jupyter notebook笔记</title>
    <link href="http://mxxhcm.github.io/2019/03/18/jupyter-notebook%E7%AC%94%E8%AE%B0/"/>
    <id>http://mxxhcm.github.io/2019/03/18/jupyter-notebook笔记/</id>
    <published>2019-03-18T07:14:33.000Z</published>
    <updated>2019-05-06T16:22:27.708Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一-安装和运行">一、安装和运行</h2><h3 id="1-安装">1.安装</h3><h4 id="anaconda安装">Anaconda安装</h4><p>Anaconda自身已经集成了jupyter包，所以如果没有装python的话，可以选择安装Anaconda集成环境</p><h4 id="pip安装">pip安装</h4><p>~#:pip install jupyter</p><h3 id="2-运行">2.运行</h3><p>~#:jupyter notebook</p><h3 id="3-远程访问">3.远程访问</h3><h4 id="1-直接使用命令">(1).直接使用命令</h4><p>这种方法是建立了一个session，会有一个token，这个会话结束之后，这个token就无效了，需要再重现建立新的session</p><h5 id="a-在前台运行以下命令">a.在前台运行以下命令</h5><p>~#:jupyter notebook --ip=your_server_ip<br>输出如下</p><p>复制这个url到你的客户端浏览器，就可以直接访问服务器端。</p><h5 id="b-后台运行">b.后台运行</h5><p>~#:nohup jupyter notebook --ip=10.4.21.214 &amp;</p><h4 id="2-创建配置文件">(2).创建配置文件</h4><h5 id="a-服务器端设置密码">a.服务器端设置密码</h5><p>这里是使用notebook的passwd()函数生成自己设置密码的sha1哈希值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> notebook.auth <span class="keyword">import</span> passwd</span><br><span class="line">passwd()</span><br></pre></td></tr></table></figure><p>输入两边自己设置的密码，然后将哈希值复制到下面的配置文件中即可。</p><h5 id="b-服务端设置配置文件">b.服务端设置配置文件</h5><p>~#:jupyter notebook --generate-config<br>~#:vim ~/.jupyter/jupyter_notebook_config.py</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">c.NotebookApp.ip=&apos;localhost&apos;</span><br><span class="line">c.NotebookApp.password=u&apos;sha1:...&apos;</span><br><span class="line">c.NotebookApp.open_browser=False</span><br><span class="line">c.NotebookApp.port=8888(your_port)</span><br></pre></td></tr></table></figure><h5 id="c-服务器端启动">c.服务器端启动</h5><p>~#:jupyter notebook</p><h5 id="d-客户端访问">d.客户端访问</h5><p>http://your_server_ip:port<br>输入密码即可</p><h2 id="二-使用">二、使用</h2><h3 id="1-创建新的文档">1.创建新的文档</h3><h2 id="三-快捷键">三、快捷键</h2><p>Jupyter Notebook 有两种键盘输入模式。编辑模式，允许你往单元中键入代码或文本；这时的单元框线是绿色的。命令模式，键盘输入运行程序命令；这时的单元框线是灰色。</p><h3 id="1-命令模式-按键-esc-开启">1.命令模式 (按键 Esc 开启)</h3><p>Enter : 转入编辑模式<br>Shift-Enter : 运行本单元，选中下个单元<br>Ctrl-Enter : 运行本单元<br>Alt-Enter : 运行本单元，在其下插入新单元<br>Y : 单元转入代码状态<br>M :单元转入markdown状态<br>R : 单元转入raw状态<br>1 : 设定 1 级标题<br>2 : 设定 2 级标题<br>3 : 设定 3 级标题<br>4 : 设定 4 级标题<br>5 : 设定 5 级标题<br>6 : 设定 6 级标题<br>Up : 选中上方单元<br>K : 选中上方单元<br>Down : 选中下方单元<br>J : 选中下方单元<br>Shift-K : 扩大选中上方单元<br>Shift-J : 扩大选中下方单元<br>A : 在上方插入新单元<br>B : 在下方插入新单元<br>X : 剪切选中的单元<br>C : 复制选中的单元<br>Shift-V : 粘贴到上方单元<br>V : 粘贴到下方单元<br>Z : 恢复删除的最后一个单元<br>dd : 删除选中的单元<br>Shift-M : 合并选中的单元<br>Ctrl-S : 文件存盘<br>S : 文件存盘<br>L : 转换行号<br>O : 转换输出<br>Shift-O : 转换输出滚动<br>Esc : 关闭页面<br>Q : 关闭页面<br>H : 显示快捷键帮助<br>I,I : 中断Notebook内核<br>0,0 : 重启Notebook内核<br>Shift : 忽略<br>Shift-Space : 向上滚动<br>Space : 向下滚动</p><h3 id="2-编辑模式-enter-键启动">2.编辑模式 ( Enter 键启动)</h3><p>Tab : 代码补全或缩进<br>Shift-Tab : 提示<br>Ctrl-] : 缩进<br>Ctrl-[ : 解除缩进<br>Ctrl-A : 全选<br>Ctrl-Z : 复原<br>Ctrl-Shift-Z : 再做<br>Ctrl-Y : 再做<br>Ctrl-Home : 跳到单元开头<br>Ctrl-Up : 跳到单元开头<br>Ctrl-End : 跳到单元末尾<br>Ctrl-Down : 跳到单元末尾<br>Ctrl-Left : 跳到左边一个字首<br>Ctrl-Right : 跳到右边一个字首<br>Ctrl-Backspace : 删除前面一个字<br>Ctrl-Delete : 删除后面一个字<br>Esc : 进入命令模式<br>Ctrl-M : 进入命令模式<br>Shift-Enter : 运行本单元，选中下一单元<br>Ctrl-Enter : 运行本单元<br>Alt-Enter : 运行本单元，在下面插入一单元<br>Ctrl-Shift-- : 分割单元<br>Ctrl-Shift-Subtract : 分割单元<br>Ctrl-S : 文件存盘<br>Shift : 忽略<br>Up : 光标上移或转入上一单元<br>Down :光标下移或转入下一单元</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;一-安装和运行&quot;&gt;一、安装和运行&lt;/h2&gt;
&lt;h3 id=&quot;1-安装&quot;&gt;1.安装&lt;/h3&gt;
&lt;h4 id=&quot;anaconda安装&quot;&gt;Anaconda安装&lt;/h4&gt;
&lt;p&gt;Anaconda自身已经集成了jupyter包，所以如果没有装python的话，可以选择安装A
      
    
    </summary>
    
      <category term="工具" scheme="http://mxxhcm.github.io/categories/%E5%B7%A5%E5%85%B7/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="jupyter" scheme="http://mxxhcm.github.io/tags/jupyter/"/>
    
  </entry>
  
  <entry>
    <title>h5py笔记</title>
    <link href="http://mxxhcm.github.io/2019/03/18/python-hdf5%E7%AC%94%E8%AE%B0/"/>
    <id>http://mxxhcm.github.io/2019/03/18/python-hdf5笔记/</id>
    <published>2019-03-18T07:12:03.000Z</published>
    <updated>2019-06-13T02:06:17.974Z</updated>
    
    <content type="html"><![CDATA[<h2 id="python包安装">python包安装</h2><p>~$:pip install h5py</p><h2 id="简介">简介</h2><h3 id="创建和打开h5py文件">创建和打开h5py文件</h3><p>f = h5py.File(“pathname”,“w”)<br>w     create file, truncate if exist<br>w- or x  create file,fail if exists<br>r         readonly, file must be exist r+        read/write,file must be exist<br>a        read/write if exists,create othrewise (default)</p><h3 id="删除一个dataset或者group">删除一个dataset或者group</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">del</span> group[<span class="string">"dataset_name/group_name"</span>]</span><br></pre></td></tr></table></figure><h2 id="dataset">dataset</h2><h3 id="什么是dataset">什么是dataset</h3><p>datasets和numpy arrays挺像的</p><h3 id="创建一个dataset">创建一个dataset</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">f = h5py.File(<span class="string">"pathname"</span>,<span class="string">"w"</span>)</span><br><span class="line">f.create_dataset(<span class="string">"dataset_name"</span>, (<span class="number">10</span>,), dtype=<span class="string">'i'</span>)</span><br><span class="line">f.create_dataset(<span class="string">"dataset_name"</span>, (<span class="number">10</span>,), dtype=<span class="string">'c'</span>)</span><br></pre></td></tr></table></figure><p>第一个参数是dataset的名字, 第二个参数是dataset的shape, dtype参数是dataset中元素的类型。</p><h3 id="如何访问一个dataset">如何访问一个dataset</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dataset = f[<span class="string">"dataset_name"</span>]                           <span class="comment"># acess like a python dict</span></span><br><span class="line">dataset = f.create_dateset(<span class="string">"dataset_name"</span>)  <span class="comment"># or create a new dataset</span></span><br></pre></td></tr></table></figure><h3 id="dataset的属性">dataset的属性</h3><p><a href="http://dataset.name" target="_blank" rel="noopener">dataset.name</a>        #输出dataset的名字<br>dataset.tdype        #输出dataset中elements的type<br>dataset.shape        #输出dataset的shape<br>dataset.value<br>dataset doesn’t hava attrs like keys,values,items,etc…</p><h3 id="给h5py-dataset复制numpy-array">给h5py dataset复制numpy array</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">array = np.zero((<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">h[<span class="string">'array'</span>] = array        <span class="comment"># in h5py file, you need't to explicit declare the shape of array, just assign it an object of numpy array</span></span><br></pre></td></tr></table></figure><h2 id="group">group</h2><h3 id="什么是group">什么是group</h3><p>group和字典挺像的</p><h3 id="创建一个group">创建一个group</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">group = f.create_group(<span class="string">"group_name"</span>)    <span class="comment">#在f下创建一个group</span></span><br><span class="line">group.create_group(<span class="string">"group_name"</span>)        <span class="comment">#在group下创建一个group</span></span><br><span class="line">group.create_dataset(<span class="string">"dataset_name"</span>)    <span class="comment">#在group下创建一个dataset</span></span><br></pre></td></tr></table></figure><h3 id="访问一个group-the-same-as-dataset">访问一个group(the same as dataset)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">group = f[<span class="string">"group_name"</span>]                           <span class="comment"># acess like a python dict</span></span><br><span class="line">group = f.create_dateset(<span class="string">"group_name"</span>)  <span class="comment"># or create a new group</span></span><br></pre></td></tr></table></figure><h3 id="group的属性和方法">group的属性和方法</h3><p><a href="http://group.name" target="_blank" rel="noopener">group.name</a>        #输出group的名字<br>以下内容分为python2和python3版本</p><h4 id="python-2-版本">python 2 版本</h4><p>group.values()    #输出group的value<br>group.keys()        #输出gorup的keys<br>group.items()    #输出group中所有的item，包含group和dataste</p><h4 id="python-3-版本">python 3 版本</h4><p>list(group.keys())<br>list(group.values())<br>list(group.items())</p><h2 id="属性">属性</h2><h3 id="设置dataset属性">设置dataset属性</h3><p>dataset.attrs[“attr_name”]=“attr_value”    #设置attr<br>print(dataset.attrs[“attr_name”])                #访问attr</p><h3 id="设置group属性">设置group属性</h3><p>group.attrs[“attr_name”]=“attr_value”    #设置attr<br>print(group.attrs[“attr_name”])                #访问attr</p><h2 id="numpy-and-h5py">numpy and h5py</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">f = h5py.File(pathname,<span class="string">"r"</span>)</span><br><span class="line"></span><br><span class="line">data = f[<span class="string">'data'</span>]    <span class="comment"># type 是dataset</span></span><br><span class="line">data = f[<span class="string">'data'</span>][:] <span class="comment">#type是numpy ndarray</span></span><br><span class="line">f.close()</span><br></pre></td></tr></table></figure><h2 id="参考文献">参考文献</h2><p>1.<a href="http://docs.h5py.org/en/latest/index.html" target="_blank" rel="noopener">http://docs.h5py.org/en/latest/index.html</a><br>2.<a href="https://stackoverflow.com/questions/31037088/discovering-keys-using-h5py-in-python3" target="_blank" rel="noopener">https://stackoverflow.com/questions/31037088/discovering-keys-using-h5py-in-python3</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;python包安装&quot;&gt;python包安装&lt;/h2&gt;
&lt;p&gt;~$:pip install h5py&lt;/p&gt;
&lt;h2 id=&quot;简介&quot;&gt;简介&lt;/h2&gt;
&lt;h3 id=&quot;创建和打开h5py文件&quot;&gt;创建和打开h5py文件&lt;/h3&gt;
&lt;p&gt;f = h5py.File(“pat
      
    
    </summary>
    
      <category term="python" scheme="http://mxxhcm.github.io/categories/python/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="h5py" scheme="http://mxxhcm.github.io/tags/h5py/"/>
    
  </entry>
  
  <entry>
    <title>MongoDB笔记</title>
    <link href="http://mxxhcm.github.io/2019/03/18/MongoDB%E7%AC%94%E8%AE%B0/"/>
    <id>http://mxxhcm.github.io/2019/03/18/MongoDB笔记/</id>
    <published>2019-03-18T07:06:56.000Z</published>
    <updated>2019-05-06T16:22:27.704Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一-数据库的安装">一、数据库的安装</h2><p>自行下载安装包并安装</p><h2 id="二-数据库的运行和连接-以及以下简单的使用">二、数据库的运行和连接,以及以下简单的使用</h2><h3 id="1-windows命令行下连接">1.windows命令行下连接</h3><h4 id="1-设置数据库存放目录">（1）设置数据库存放目录</h4><p>~#:md D:/data/db</p><h4 id="2-运行mongodb服务">（2）运行mongodb服务</h4><p>~#:mongod</p><h4 id="3-连接mongodb数据库">（3）连接mongodb数据库</h4><p>~#:mongo (database_name)<br>如果不输入数据库名会默认连接到mongodb自带的一个数据库test，如果指定了数据库名就会连接到该数据库</p><h3 id="2-使用python代码中连接到数据库">2.使用python代码中连接到数据库</h3><h4 id="1-导入python-pacakge">（1）导入python pacakge</h4><p>使用pip安装即可<br>import pymongo</p><h4 id="2-连接到mongodb">（2）连接到mongodb</h4><p>connection = MongoClient(‘localhost’, 27017)</p><h4 id="3-连接到某个数据库">（3）连接到某个数据库</h4><p>db = connection.test  #连接到test数据库<br>db现在指向的是test这个数据库</p><h4 id="4-指向某个collection">（4）指向某个collection</h4><p>collection = db.collection_one</p><h4 id="5-查看collection中的内容">（5）查看collection中的内容</h4><p>items = collection.find()<br>print(items[‘key’])</p><h3 id="3-一些简单的操作">3.一些简单的操作</h3><h4 id="1-切换数据库">（1）切换数据库</h4><p>~#:use database_name</p><h4 id="2-查看所有的数据库">（2）查看所有的数据库</h4><p>~#:show databases;</p><h4 id="3-查看所有的collection">（3）查看所有的collection</h4><p>~#:show collections;</p><h2 id="三-crud操作">三.CRUD操作</h2><h3 id="1-id的构成-12-bytes-hex">1.id的构成 12 bytes hex</h3><p>4+3+2+3<br>timestamp + mac address + pid + counter<br>timestamp是unix timestamp，mac address 是 mongd运行的网卡mac address，pid是process id，</p><h3 id="2-create-document">2. create document</h3><h4 id="1-create-one-document-insertone">（1）create one document(insertOne)</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">db.collection_one.insertOne(&#123;&quot;key_one&quot;:&quot;value&quot;,&quot;key_two&quot;:&quot;value&quot;&#125;)</span><br></pre></td></tr></table></figure><h4 id="2-create-many-documents-有order-insertmany">（2）create many documents（有order,insertMany）</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">db.collection_one.insertMany(</span><br><span class="line">[</span><br><span class="line">&#123;&quot;key_one&quot;:&quot;value&quot;,&quot;key_two&quot;:&quot;value&quot;&#125;,</span><br><span class="line">&#123;&quot;key_one&quot;:&quot;value&quot;,&quot;key_two&quot;:&quot;value&quot;&#125;,</span><br><span class="line">&#123;&quot;key_one&quot;:&quot;value&quot;,&quot;key_two&quot;:&quot;value&quot;&#125;</span><br><span class="line">])</span><br></pre></td></tr></table></figure><h4 id="3-create-many-documents-无order-insertmany">（3）create many documents（无order,insertMany）</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">db.collection_one.insertMany(</span><br><span class="line">db.collection_one.insertMany(</span><br><span class="line">[</span><br><span class="line">&#123;&quot;key_one&quot;:&quot;value&quot;,&quot;key_two&quot;:&quot;value&quot;&#125;,</span><br><span class="line">&#123;&quot;key_one&quot;:&quot;value&quot;,&quot;key_two&quot;:&quot;value&quot;&#125;,</span><br><span class="line">&#123;&quot;key_one&quot;:&quot;value&quot;,&quot;key_two&quot;:&quot;value&quot;&#125;</span><br><span class="line">] , &#123;&quot;ordered&quot;:false&#125;)</span><br></pre></td></tr></table></figure><h4 id="4-upsert">（4）upsert</h4><p>第一个参数是一个filter选择合适的 document，第二个参数是一个更新操作for the documents were selected，第三个参数是 that if there is no matching result,if the value of upsert is true,then insert a new document,else do nothing.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">db.collection_one.insertMany(</span><br><span class="line">db.movieDetails.updateOne(&#123; name:&quot;mxxhcm&quot;&#125;, &#123; \$set:&#123;lover:&quot;mahuihui&quot;&#125; &#125; , &#123;upsert : true&#125;)</span><br></pre></td></tr></table></figure><h4 id="5-有无order的区别">（5）有无order的区别</h4><p>有order的话遇到inset错误就会停下来，没有order的话在插入document的时候，遇到错误会跳过该条语句执行下一条语句。</p><h3 id="3-read-documents-query-documents">3.read documents(query documents)</h3><p>link:<br><a href="https://docs.mongodb.com/manual/reference/operator/query/" target="_blank" rel="noopener">https://docs.mongodb.com/manual/reference/operator/query/</a></p><h4 id="1-查找document">（1）查找document</h4><p>查找collection_one这个collection中所有的document</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">db.collection_one.find()</span><br></pre></td></tr></table></figure><p>查找collection_one这个collection中满足{}中条件的collection，{}中的条件需要满足anded</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">db.collection_one.find(&#123;&#125;)</span><br></pre></td></tr></table></figure><p>pretty()表示以规范的格式展现出来查询结果</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">db.collection_one.find().pretty()</span><br></pre></td></tr></table></figure><p>findOne表示只展示出第一条结果</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">db.collection_one.findOne()</span><br></pre></td></tr></table></figure><p>满足{}中条件的第一条结果</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">db.collection_one.findOne(&#123;&#125;)</span><br></pre></td></tr></table></figure><h4 id="2-对document进行计数">（2）对document进行计数</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">db.collection_one.count()</span><br></pre></td></tr></table></figure><h4 id="3-设置查找的条件-equality-match">（3）设置查找的条件(equality match)</h4><h5 id="a-scalar-equality-match">a.scalar equality match</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">db.collection_one.find(&#123;&quot;key&quot;:&quot;value&quot;,&quot;key&quot;,&quot;value&quot;&#125;)</span><br></pre></td></tr></table></figure><h5 id="b-nested-documents-equality-match">b.nested documents equality match</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">db.collection_one.find(&#123;&quot;key.key2.key3&quot;:&quot;value&quot;&#125;)</span><br></pre></td></tr></table></figure><h5 id="c-equality-matches-on-arrays">c.equality matches on arrays</h5><h6 id="entire-array-value-match">entire array value match</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">db.collection_one.find(&#123;key:[value1,value2]&#125;)</span><br></pre></td></tr></table></figure><h6 id="any-array-element-fileds-match-a-specfic-value">any array element fileds match a specfic value</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">db.collection_one.find(&#123;key:&quot;value2&quot;&#125;)</span><br></pre></td></tr></table></figure><h6 id="a-specfiec-element-fields-match-a-specfic-value">a specfiec element fields match a specfic value</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">db.collection_one.find(&#123;key.0:&quot;value1&quot;&#125;)</span><br></pre></td></tr></table></figure><h4 id="4-cursor">（4）cursor</h4><h4 id="5-projection">（5）projection</h4><p>by default,mongodb return all fields in all matching documents for query.<br>Projection are supplied as the second argument<br>db.collection_one.find({“key1”:“value”,“key2”,“value”},{“key1”:1,“key2”:1,“key3”:0,“key4”:0}).pretty()</p><h4 id="6-comparison-operation">（6）comparison operation</h4><p>$eq<br>$gt<br>$gte<br>$lt<br>$lte<br>$ne<br>$in<br>$nin</p><h5 id="a-在某个范围内">a.在某个范围内</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">db.movieDetails.find(&#123; runtime : &#123; \$gt: 70,  \$lte:100 &#125; &#125;).pretty()</span><br></pre></td></tr></table></figure><h5 id="b-不等于-ne">b.不等于($ne)</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">db.movieDetails.find(&#123; rated : &#123; \$ne:&quot;unrated&quot; &#125; &#125;).pretty()</span><br></pre></td></tr></table></figure><h5 id="c-在-in">c.在($in)</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">db.movieDetails.find(&#123;rated : &#123; \$in : [&quot;G&quot;,&quot;PG&quot;,&quot;PG-13&quot;] &#125;  &#125;).pretty()</span><br></pre></td></tr></table></figure><h4 id="7-element-operator">（7）element operator</h4><h5 id="a-存在某个filed-exists">a.存在某个filed($exists)</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">db.movieDetail.find( &#123; filed_name : &#123; \$exists: true|false &#125; &#125; ).pretty()</span><br></pre></td></tr></table></figure><h5 id="b-某个字段的类型-type">b.某个字段的类型($type)</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">db.movieDetail.find( &#123; filed_name : &#123; \$type :&quot;string&quot;&#125; &#125;).pretty()</span><br></pre></td></tr></table></figure><h4 id="8-logical-operator">（8）logical operator</h4><p>$or<br>$and<br>$not<br>$nor</p><h5 id="a-逻辑或-or">a.逻辑或($or)</h5><p>$or需要数组作为参数</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">db.movieDetails.find( &#123; \$or: [ &#123; field_one : &#123;\$type : &quot;string&quot;&#125; &#125; , &#123;field_two : &#123;\$exist: &quot;name&quot; &#125; &#125; ] &#125; ).pretty()</span><br></pre></td></tr></table></figure><h5 id="b-逻辑与-and">b.逻辑与($and)</h5><p>$and操作支持我们在同一个filed指定多个约束条件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">db.movieDetails.find(&#123; \$and: [ &#123;field_one: &#123;\$ne :null&#125; &#125; , &#123; field_one: &#123;\$gt:60, \$lte: 100&#125; &#125; ] &#125;).pretty()</span><br></pre></td></tr></table></figure><h4 id="9-regex-operator">（9）regex operator</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">db.movieDetails.find(&#123; &quot;awards.text&quot;: &#123; \$regex: /^Won\s/&#125;  &#125;).pretty()</span><br></pre></td></tr></table></figure><h4 id="10-array-operator">（10）array operator</h4><p>$all<br>$size<br>$elementMatch</p><h5 id="a-all">a.all</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">db.movieDetails.find(&#123;genres : &#123;\$all :[&quot;comedy&quot;,&quot;crime&quot;,&quot;drama&quot;]&#125; &#125;).pretty()</span><br><span class="line">db.movieDetails.find(&#123;genres :  [&quot;comedy&quot;,&quot;crime&quot;,&quot;drama&quot;]  &#125;).pretty()</span><br></pre></td></tr></table></figure><p>上面两个式子是有区别的，第一个式子会匹配genres中包含&quot;comedy&quot;,“crime”,“drama&quot;的document<br>而第二个只会匹配genres为&quot;comedy”,“crime”,&quot;drama&quot;的document。</p><h5 id="b-size">b.size</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">db.movieDetails.find(&#123;country : &#123;\$size : 3&#125; &#125;).pretty()</span><br></pre></td></tr></table></figure><h5 id="c-elementmatch">c.elementMatch</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">db.movieDetails.find(&#123; boxOffice: &#123; country: &quot;UK&quot;, revenue: &#123; \$gt: 15 &#125; &#125; &#125;)</span><br></pre></td></tr></table></figure><h3 id="9-update-documents">9.update documents</h3><p>link:<br><a href="https://docs.mongodb.com/manual/reference/operator/update/" target="_blank" rel="noopener">https://docs.mongodb.com/manual/reference/operator/update/</a></p><h4 id="0-some-update-operator">（0）some update operator</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">db.movieDetails.updateOne( &#123; name : &quot;mxxhcm&quot; &#125; , &#123; \$inc : &#123; age: 1&#125; &#125;)</span><br></pre></td></tr></table></figure><h4 id="1-updateone">（1）updateOne</h4><h5 id="a-update-for-scalar-fields">a.update for scalar fields</h5><p>$set</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">db.movieDetails.updateOne( &#123; name : &quot;mxxhcm&quot; &#125; , &#123; \$set : &#123; age: 19&#125; &#125;)</span><br></pre></td></tr></table></figure><p>$unset</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">db.movieDetails.updateOne( &#123; name : &quot;mxxhcm&quot; &#125; , &#123; \$unset : &#123; age: 19&#125; &#125;)</span><br></pre></td></tr></table></figure><p>$inc<br>age后是在原来的age上加的数值</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">db.movieDetails.updateOne( &#123; name : &quot;mxxhcm&quot; &#125; , &#123; \$set : &#123; age: 19&#125; &#125;)</span><br></pre></td></tr></table></figure><p>updateOne has two arguments, the first one is a selector,the second argument is how we want to update the document.</p><h5 id="b-update-for-array-fields">b.update for array fields</h5><p>$push</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">db.movieDetails.updateOne(&#123;name:&quot;mxxhcm&quot;&#125; , &#123;\$push: &#123; reviews: &#123; key1:value,key2:value...&#125;  &#125;  &#125; )</span><br><span class="line">db.movieDetails.updateOne(&#123;name:&quot;mxxhcm&quot;&#125; , &#123;\$push: &#123; reviews:</span><br><span class="line">                                                                                                &#123; \$each: [&#123; key1:value,key2:value...&#125; ,                                                                                                                        &#123;key1:value,key2:value...&#125; ]  &#125;  </span><br><span class="line">                                                                                              &#125;   &#125; )</span><br><span class="line">db.movieDetails.updateOne(&#123;name:&quot;mxxhcm&quot;&#125; , &#123;\$push: &#123; reviews:</span><br><span class="line">                                                                                                &#123; \$each: [&#123; key1:value,key2:value...&#125; ,                                                                                                                        &#123;key1:value,key2:value...&#125; ] ,                                                                                                             \$slice:3 &#125;  </span><br><span class="line">                                                                                              &#125;   &#125; )</span><br><span class="line">db.movieDetails.updateOne(&#123;name:&quot;mxxhcm&quot;&#125; , &#123;\$push: &#123; reviews:</span><br><span class="line">                                                                                                &#123; \$each: [&#123; key1:value,key2:value...&#125; ,                                                                                                                        &#123;key1:value,key2:value...&#125; ] ,                                                                                                             \$position:0,  </span><br><span class="line">                                                                                                  \$slice:3 &#125;  </span><br><span class="line">                                                                                              &#125;   &#125; )</span><br></pre></td></tr></table></figure><h4 id="2-updatemany">（2）updateMany</h4><p>the same as updateOne</p><h4 id="3-replaceone">（3）replaceOne</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">db.movieDetail.replcaeOne(&#123;&#125;,&#123;&#125;)</span><br></pre></td></tr></table></figure><p>the first argument is a filter,the second argument is the thing that replace what the filter choose,it can be a document,or a variable point to a document.</p><h3 id="10-using-mongdb-by-pymongo">10. using mongdb by pymongo</h3><p>见代码</p><h4 id="1-sort-skip-limit">（1）sort，skip，limit</h4><p>sort &gt; skip &gt; limit</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cursor.sort(&apos;student_id&apos;,pymongo.ASCENDING).skip(4).limit(3)</span><br><span class="line">in python file:</span><br><span class="line">cursor.sort(  [ (&apos;student_id&apos;,pymongo.ASCENDING) , (&apos;score&apos;,pymongo.DESCENDING) ] ).skip(4).limit(3)</span><br><span class="line">in mongo shell:</span><br><span class="line">cursor.sort(  [ &#123;&apos;student_id&apos;:1&#125;, &#123;&apos;score&apos;,-1)&#125; ] ).skip(4).limit(3)</span><br></pre></td></tr></table></figure><p>####（2）find,find_one,cursors<br>####（3）project<br>####（4）regex<br>####（5）insert<br>####（6）update<br>####（7）<br>There is a intervening between find and update,so maybe you find and update is not the same one.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;一-数据库的安装&quot;&gt;一、数据库的安装&lt;/h2&gt;
&lt;p&gt;自行下载安装包并安装&lt;/p&gt;
&lt;h2 id=&quot;二-数据库的运行和连接-以及以下简单的使用&quot;&gt;二、数据库的运行和连接,以及以下简单的使用&lt;/h2&gt;
&lt;h3 id=&quot;1-windows命令行下连接&quot;&gt;1.windo
      
    
    </summary>
    
      <category term="数据库" scheme="http://mxxhcm.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
    
      <category term="数据库" scheme="http://mxxhcm.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
      <category term="MongoDB" scheme="http://mxxhcm.github.io/tags/MongoDB/"/>
    
      <category term="非关系型数据库" scheme="http://mxxhcm.github.io/tags/%E9%9D%9E%E5%85%B3%E7%B3%BB%E5%9E%8B%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>神经网络-激活函数</title>
    <link href="http://mxxhcm.github.io/2019/03/14/activation/"/>
    <id>http://mxxhcm.github.io/2019/03/14/activation/</id>
    <published>2019-03-14T03:45:46.000Z</published>
    <updated>2019-05-06T16:22:27.712Z</updated>
    
    <content type="html"><![CDATA[<h2 id="激活函数的一些问题">激活函数的一些问题</h2><h3 id="为什么要使用non-linear激活函数不使用linear激活函数？">为什么要使用non-linear激活函数不使用linear激活函数？</h3><p><img src="/2019/03/14/activation/fnn.png" alt="fnn"><br>给定一个如图所示的前馈神经网络。有一个输入层，一个隐藏层，一个输出层。输入是$2$维的，有$4$个隐藏单元，输出是$2$维的。<br>则：$ \hat{f}(x) = \sigma(w_1x+b_1)w_2 + b_2$<br>这里$\sigma$是一个线性的激活函数，不妨设$\sigma(x) = x$。<br>那么就有：<br>\begin{align*}<br>\hat{f}(x) &amp;= \sigma(w_1x+b_1)w_2 + b_2\<br>&amp;= (w_1x+b_1)w_2 + b_2\<br>&amp;= w_1w_2x + w_2b1 + b_2\<br>&amp;= (w_1w_2) x + (w_2b1 + b_2)\<br>&amp;= w’ x + b’<br>\end{align*}<br>因此，当使用线性激活函数的时候，我们可以把一个多层感知机模型化简成一个线性模型。当使用线性激活函数时，增加网络的深度没有用，使用线性激活函数的十层感知机和一层感知机没有区别，并不能增加网络的表达能力。因为任意两个仿射函数的组合还是仿射函数。</p><h3 id="为什么relu激活函数是non-linear的？">为什么ReLU激活函数是non-linear的？</h3><p>ReLU的数学表达形式如下：<br>$$g(x) = max(0, x)$$<br>首先考虑一下什么是linear function,什么是non-linear function。在微积分上，平面内的任意一条直线是线性函数，否则就是非线性函数。<br>考虑这样一个例子，输入数据的维度为$1$，输出数据的维度也为$1$，用$g(ax+b)$表示ReLU激活函数。如果我们使用两个隐藏单元，那么$h_1(x) = g(x)+g(-x)$可以用来表示$f(x)=|x|$，而函数$|x|$是一个非线性函数，函数图像如下所示。<br><img src="/2019/03/14/activation/absolute.png" alt="f(x)=|x|"><br>我们还可以用ReLU逼近二次函数$f(x) = x^2$，如使用函数$h_2(x) = g(x) + g(-x) + g(2x-2) + g(2x+2)$逼近二次函数，对应的图像如下。<br><img src="/2019/03/14/activation/quadratic.png" alt="h_2(x)"><br>使用的项越多，最后近似出来的图像也就和我们要逼近的二次函数越像。<br>同理，可以使用ReLU激活函数去逼近任意非线性函数。</p><h3 id="为什么relu比sigmod还有tanh激活函数要好？">为什么ReLU比sigmod还有tanh激活函数要好？</h3><p>ReLU收敛的更快，因为梯度更大。<br>当CNN的层数越来越深的时候，实验表明，使用ReLU的CNN要比使用sigmod或者tanh的CNN训练的更容易，更快收敛。<br>为什么会这样，目前有两种理论，见参考文献[4]。<br>第一个，$tanh(x)$有梯度消散问题(vanishing gradient)。当$x$趋向于$\pm\infty$时，$tanh(x)$的导数趋向于$0$。如下图所示。</p><blockquote><p>Vanishing gradients occur when lower layers of a DNN have gradients of nearly 0 because higher layer units are nearly saturated at -1 or 1, the asymptotes of the tanh function. Such vanishing gradients cause slow optimization convergence, and in some cases the final trained network converges to a poor local minimum.</p></blockquote><blockquote><p>One way ReLUs improve neural networks is by speeding up training. The gradient computation is very simple (either 0 or 1 depending on the sign of x). Also, the computational step of a ReLU is easy: any negative elements are set to 0.0 – no exponentials, no multiplication or division operations.</p></blockquote><p><img src="/2019/03/14/activation/tanh.png" alt="tanh(x)"><br>ReLU是non-saturating nonlinearity的激活函数，sigmod和tanh是saturating nonlinearity激活函数，会将输出挤压到一个区间内。</p><blockquote><p>f是non-saturating 当且仅当$|lim_{z\rightarrow -\infty} f(z)| \rightarrow + \infty$或者$|lim_{z\rightarrow +\infty} f(z)| \rightarrow + \infty$</p></blockquote><p>tanh和sigmod将输入都挤压在某一个很小的区间内，比如(0,1)，输入发生很大的变化，经过激活函数以后变化很小，经过好几层之后，基本上就没有差别了。而当网络很深的时候，反向传播主要集中在后几层，而输入层附近的权值没办法好好学习。而对于ReLU来说，任意深度的神经网络，都不存在梯度消失。</p><p>第二种理论是说有一些定理能够证明，在某些假设条件下，局部最小就是全局最小。如果使用sigmod或者tanh激活函数的时候，这些假设不能成立，而使用ReLU的话，这些条件就会成立。</p><h3 id="为什么发生了梯度消失以后训练结构很差？">为什么发生了梯度消失以后训练结构很差？</h3><p>我的想法是，</p><h2 id="参考文献">参考文献</h2><p>1.<a href="https://stats.stackexchange.com/a/391971" target="_blank" rel="noopener">https://stats.stackexchange.com/a/391971</a><br>2.<a href="https://stats.stackexchange.com/a/299933" target="_blank" rel="noopener">https://stats.stackexchange.com/a/299933</a><br>3.<a href="https://stats.stackexchange.com/a/141978" target="_blank" rel="noopener">https://stats.stackexchange.com/a/141978</a><br>4.<a href="https://stats.stackexchange.com/a/335972" target="_blank" rel="noopener">https://stats.stackexchange.com/a/335972</a><br>5.<a href="https://stats.stackexchange.com/a/174438" target="_blank" rel="noopener">https://stats.stackexchange.com/a/174438</a><br>6.<a href="https://stats.stackexchange.com/questions/391968/relu-vs-a-linear-activation-function" target="_blank" rel="noopener">https://stats.stackexchange.com/questions/391968/relu-vs-a-linear-activation-function</a><br>7.<a href="https://stats.stackexchange.com/questions/141960/why-are-rectified-linear-units-considered-non-linear" target="_blank" rel="noopener">https://stats.stackexchange.com/questions/141960/why-are-rectified-linear-units-considered-non-linear</a><br>8.<a href="https://stats.stackexchange.com/questions/299915/how-does-the-rectified-linear-unit-relu-activation-function-produce-non-linear" target="_blank" rel="noopener">https://stats.stackexchange.com/questions/299915/how-does-the-rectified-linear-unit-relu-activation-function-produce-non-linear</a><br>9.<a href="https://stats.stackexchange.com/questions/226923/why-do-we-use-relu-in-neural-networks-and-how-do-we-use-it/226927#226927" target="_blank" rel="noopener">https://stats.stackexchange.com/questions/226923/why-do-we-use-relu-in-neural-networks-and-how-do-we-use-it/226927#226927</a><br>10.<a href="https://www.zhihu.com/question/264163033" target="_blank" rel="noopener">https://www.zhihu.com/question/264163033</a><br>11.<a href="http://ai.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf" target="_blank" rel="noopener">http://ai.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;激活函数的一些问题&quot;&gt;激活函数的一些问题&lt;/h2&gt;
&lt;h3 id=&quot;为什么要使用non-linear激活函数不使用linear激活函数？&quot;&gt;为什么要使用non-linear激活函数不使用linear激活函数？&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/2019/03/
      
    
    </summary>
    
      <category term="机器学习" scheme="http://mxxhcm.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://mxxhcm.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="http://mxxhcm.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="激活函数" scheme="http://mxxhcm.github.io/tags/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/"/>
    
      <category term="神经网络" scheme="http://mxxhcm.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="ReLU" scheme="http://mxxhcm.github.io/tags/ReLU/"/>
    
      <category term="tanh" scheme="http://mxxhcm.github.io/tags/tanh/"/>
    
      <category term="sigmod" scheme="http://mxxhcm.github.io/tags/sigmod/"/>
    
  </entry>
  
  <entry>
    <title>CNN</title>
    <link href="http://mxxhcm.github.io/2019/03/13/cnn/"/>
    <id>http://mxxhcm.github.io/2019/03/13/cnn/</id>
    <published>2019-03-13T07:21:27.000Z</published>
    <updated>2019-07-13T12:29:42.230Z</updated>
    
    <content type="html"><![CDATA[<h2 id="cnn">CNN</h2><h3 id="图片的表示">图片的表示</h3><p>图像在计算机中是一堆按顺序排列的顺子，数值为0到255。0表示最暗，255表示最亮。我们可以把这堆数字用一个长长的一维数组来表示，但是这样会失去平面结构的信息，为保留该结构信息，我们通常会选择矩阵的表示方式，用一个nn的矩阵来表示一个图像。对于黑白颜色的灰度图来说，我们只需要一个nn的矩阵表示即可。对于一个彩色图像，我们会选择RGB颜色模型来表示。<br>在彩色图像中，我们需要用三个矩阵去表示一张图，也可以理解为一个三维张量，每一个矩阵叫做这张图片的一个channel。这个三维张量可以表示为(width,length,depth),一张图片就可以用这样一个张量来表示。</p><h3 id="卷积神经网络-cnn">卷积神经网络(CNN)</h3><h4 id="作用">作用</h4><p>让权重在不同位置共享</p><h4 id="filter和stride">filter和stride</h4><p>filter又叫做kernel或者feature detector。filter会对输入的局部区域进行处理，filter处理的局部区域的范围叫做filter size。比如说一个filter的大小为(3,3),那么这个filter会一次处理width=3，length = 3的区域。卷积神经网络会用filter对整个输入进行扫描，一次移动的多少叫做stride。filter处理一次的输出为一个feature map。</p><h4 id="depth">depth</h4><p>对于filter来说，我们一般说它的大小为（3，3）只说了它在平面的大小，但是输入的图片一般是一个RGB的三维张量，对于deepth这一个维度，如果为1的话，那么filter是（3,3），但是如果deepth大于1的话，这个filter的deepth维度一般是和张量中的deepth维度一样的。<br>deepth=1时，filter=（3,3），处理输入中33 个节点的值<br>deepth=2时，filter=（3,3），会处理输入中332个节点的值<br>deepth=n时，filter=（3,3），会处理输入中$33\times n$个节点的值</p><h4 id="zero-paddings">zero paddings</h4><p>因为经过filter处理后，输入的矩阵维度会变小，所以，如果经过很多层filter处理后，就会变得越来越少，因此，为了解决这个问题，提出了zero paddings，zero padding是在filter要处理的输入上，在输入的最外层有选择的加上一行（列）或多行（列）0，从而保持输入经过filter处理之后形状不变。</p><h4 id="feature-map">feature map</h4><p>一个filter的输出就是一个feature map，该feature map的width和height为：$(input_size + 2\times padding_size - filter_size)/stride + 1$<br>一个filter可以提取一个feature，得到一个feature map，为了提取多个feature，需要使用多个filters，最后可以得到多个feature map。</p><p>所以说，feature map是一类值，因为它对应的是一个filter，给定不同的输入images，一个feature map可以有不同的取值。这个问题是我在看ZFNet中遇到的，因为它在原文中说<br>“For a given feature map, we show the top 9 activations”。给定一个feature map，这里应该是在所有样本中选择最大的$9$个activations对应的images。<br>“the strongest activation (across all training examples) within a given feature map”。给定一个feature map，在所有样本中选择一个最强的activation。</p><h4 id="activate-function">activate function</h4><p>一般使用非线性激活函数relu对feature map进行变化</p><h4 id="pooling">pooling</h4><h5 id="maxpooling">maxpooling</h5><p>它基本上采用一个filter和一个同样长度的stride通常是（2,2）和2，然后把它应用到输入中，输出filter卷积计算的每个区域中的最大数字，这个pooling是在各个维度上分别进行的。<br>比如一个 22422464的input，经过一个（2,2）的maxpooling会输出一个11211232的张量</p><h5 id="averagepooling">averagepooling</h5><h4 id="fc-layers">fc layers</h4><h2 id="alexnet-2012">Alexnet(2012)</h2><p>论文名称：ImageNet Classification with Deep Convolutional Neural Networks<br>论文地址：<a href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" target="_blank" rel="noopener">http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf</a></p><h3 id="概述">概述</h3><p>作者提出了一个卷积神经网络架构对Imagenet中$1000$类中的$120$万张图片进行分类。网络架构包含$5$个卷积层，$3$个全连接层，和一个$1000$-way的softmax层，整个网络共有$6000$万参数，$65000$个神经元。作者提出了一些方法提高性能和减少训练的时间，并且介绍了一些防止过拟合的技巧。最后在imagenet测试集上，跑出$37.5%$的top-1 error以及$17.0%$的top-5 error。<br>本文主要的contribution：</p><ol><li>给出了一个benchmark－Imagenet</li><li>提出了一个CNN架构</li><li>ReLU激活函数</li><li>dropout的使用</li><li>数据增强，四个角落和中心的crop以及对应的horizontial 翻转。</li></ol><h3 id="问题">问题</h3><p>1.数据集太小，都是数以万计的，需要更大的数据集。</p><h3 id="创新">创新</h3><h4 id="relu非线性激活函数">ReLU非线性激活函数</h4><h5 id="作用-v2">作用</h5><p>作者说实验表明ReLU可以加速训练过程。</p><h5 id="saturating-nonlinearity">saturating nonlinearity</h5><p>一个饱和的激活函数会将输出挤压到一个区间内。</p><blockquote><p>A saturating activation function squeezes the input.</p></blockquote><p><strong>定义</strong><br>f是non-saturating 当且仅当$|lim_{z\rightarrow -\infty} f(z)| \rightarrow + \infty$或者$|lim_{z\rightarrow +\infty} f(z)| \rightarrow + \infty$<br>f是saturating 当且仅当f不是non-saturating<br><strong>例子</strong><br>ReLU就是non-saturating nonlinearity的激活函数，因为$f(x) = max(0, x)$，如下图所示。<br><img src="/2019/03/13/cnn/relu.png" alt="relu"><br>当$x$趋于无穷时，$f(x)$也趋于无穷。<br>sigmod和tanh是saturating nonlinearity激活函数，如下图所示。<br><img src="/2019/03/13/cnn/sigmod.png" alt="sigmo"><br><img src="/2019/03/13/cnn/tanh.png" alt="tanh"></p><h4 id="多块gpu并行">多块GPU并行</h4><p>作者使用了两块GPU一块运行，每个GPU中的参数个数是一样的，在一些特定层中，两个GPU中的参数信息可以进行通信。</p><h4 id="overlapping-pooling">Overlapping Pooling</h4><p>就是Pooling kernel的size要比stride大。比如一个$12\times 12$的图片，用$5\times 5$的pooling kernel，步长为$3$，步长要比kernel核小，即$3$比$5$小。<br>为什么这能减小过拟合？</p><ul><li>可能是减小了Pooling过程中信息的丢失。</li></ul><blockquote><p>If the pooling regions do not overlap, the pooling regions are disjointed and if that is the case, more information is lost in each pooling layer. If some overlap is allowed the pooling regions overlap with some degree and less spatial information is lost in each layer.[4]</p></blockquote><h4 id="数据增强">数据增强</h4><p>目的：防止过拟合</p><h5 id="裁剪和翻转">裁剪和翻转</h5><p>输入是$256\times 256 \times 3$的图像。<br>训练：对每张图片都提取多个$224\times 224$大小的patch，这样子总共就多产生了$(256-224)\times (256-224) = 1024$个样本，然后对每个patch做一个水平翻转，就有$1024\times 2 = 2048$个样本。<br>测试：通过对每张图片裁剪五个（四个角落加中间）$224\times 224$的patches，并且对它们做翻转，也就是有$10$个patches，网络对十个patch的softmax层输出做平均作为预测结果。</p><h5 id="在图片上调整rgb通道的密度">在图片上调整RGB通道的密度</h5><p>使用PCA对RGB值做主成分分析。对于每张训练图片，加上主成分，其大小正比于特征值乘上一个均值为$0$，方差为$0.1$的高斯分布产生的随机变量。对于一张图片$x,y$点处的像素值$I_{xy}=[I_{xy}^R, I_{xy}<sup>G,I_{xy}</sup>B]^T$，加上$[\bold{p_1},\bold{p_2},\bold{p_3}][\alpha_1\lambda_1,\alpha_2\lambda_2,\alpha_3\lambda_3]$，其中$[\bold{p_1},\bold{p_2},\bold{p_3}]$是特征向量，$\lambda_i$是特征值，$\alpha_i$就是前面说的随机变量。</p><h4 id="dropout">Dropout</h4><p>通过学习鲁棒的特征防止过拟合。<br>在训练的时候，每个隐藏单元的输出有$p$的概率被设置为$0$，在该次训练中，如果这个神经元的输出被设置为$0$，它就对loss函数没有贡献，反向传播也不会被更新。对于一层有$N$个神经单元的全连接层，总共有$2^N$种神经元的组合结果，这就相当于训练了一系列共享参数的模型。<br>在测试的时候，所有隐藏单元的输出都不丢弃，但是会乘上$p$的概率，相当于对一系列集成模型取平均。具体可见<a href="https://mxxhcm.github.io/2019/03/23/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-dropout/">dropout</a><br>在该模型中，作者在三层全连接层的前两层输出上加了dropout。</p><h4 id="局部响应归一化-local-response-normalizaiton">局部响应归一化(Local Response Normalizaiton)</h4><p>事实上，后来发现这个东西没啥用。但是这里还是给出一个公式。</p><p>$$ b^i_{x,y} = \frac{a^i_{x,y}}{(k+\alpha \sum<sup>{min(N-1,\frac{i+n}{2})}_{j=max(0,\frac{i-n}{2})}(a</sup>j_{x,y})^2)^{\beta}}$$<br>其中$a^i_{x,y}$是在点$(x,y)$处使用kernel $i$之后，在经过ReLU激活函数。$k,n,\alpha,\beta$是超参数。</p><blockquote><p>It seems that these kinds of layers have a minimal impact and are not used any more. Basically, their role have been outplayed by other regularization techniques (such as dropout and batch normalization), better initializations and training methods.</p></blockquote><h3 id="整体架构">整体架构</h3><h4 id="目标函数">目标函数</h4><p>多峰logistic回归。</p><h4 id="并行框架">并行框架</h4><p>下图是并行的架构，分为两层，上面一层用一个GPU，下面一层用一个GPU，它们只在第三个卷积层有交互。<br><img src="/2019/03/13/cnn/alexnet.png" alt="alexnet"></p><h4 id="简化框架">简化框架</h4><p>下图是简化版的结构，不需要使用两个GPU。<br><img src="/2019/03/13/cnn/alexnet_simple.png" alt="alexnet_simple"></p><h4 id="数据流-简化框架">数据流（简化框架）</h4><p>输入是$224\times 224 \times 3$的图片，第一层是$96$个stride为$4$的$11\times 11\times 3$卷积核构成的卷积层，输出经过max pooling(步长为2，kernel size为3)输入到第二层；第二层有$256$个$5\times 5\times 96$个卷积核，输出经过max pooling(步长为2，kernel size为3)输入到第三层；第三层到第四层，第四层到第五层之间没有经过pooling和normalization)，第三层有384个$3\times 3\times 256$个卷积核，第四层有$384$个$3\times 3\times 384$个卷积核，第五层有$256$个$3\times 3\times 384$个卷积核。然后接了两个$2048$个神经元的全连接层和一个$1000$个神经元的全连接层。</p><h3 id="实验">实验</h3><h4 id="datasets">Datasets</h4><p>ILSVRC-2010</p><h4 id="baselines">Baselines</h4><ul><li>Sparse coding</li><li>SIFT+FV</li><li>CNN</li></ul><h4 id="metric">Metric</h4><ul><li>top-1 error rate</li><li>top-5 error rate</li></ul><h3 id="代码">代码</h3><p>pytorch实现<br><a href="https://github.com/mxxhcm/myown_code/blob/master/CNN/alexnet.py" target="_blank" rel="noopener">https://github.com/mxxhcm/myown_code/blob/master/CNN/alexnet.py</a></p><h2 id="maxout-networks">Maxout networks</h2><p>论文名称：Maxout Networks<br>下载地址：<a href="https://arxiv.org/pdf/1302.4389.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1302.4389.pdf</a></p><h2 id="nin">NIN</h2><p>论文名称：Network In Network<br>论文地址：<a href="https://arxiv.org/pdf/1312.4400.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1312.4400.pdf</a></p><h3 id="摘要">摘要</h3><p>这篇文章作者使用更复杂的micro神经网络代替CNN，用一个mlp实例化micro nn。CNN中的filter用的是generalized linear model(GLM)。本文使用nonlinear的FA，作者用一个multi layers perceptron 取代GLM。通过和cnn类似的操作对input进行sliding得到feature maps，然后传入下一层，deep NIN通过堆叠多层类似的结构生成。同时作者使用average pooling取代最后的fullcy connected layer。<br>本文的两个contribution是：</p><ol><li>使用MLP代替CNN中linear model，引入$1\times 1$的filter</li><li>使用average pooling代替fully connected layer。</li></ol><p>在传统的CNN中，一个concept的不同variation可能需要多个filters，这样子会让下一层的的计算量太大。高层CNN的filters对应input的区域更大，高层的concept是通过对底层的concepts进行组合得到的。这里作者在每一层都对local patch进行组合，而不是在高层才开始进行组合，在每一层中，micro network计算更加local patches更abstract的特征。</p><h3 id="network-in-network">Network in Network</h3><h4 id="mlp-convolution-layers">MLP convolution layers</h4><p>为什么使用MLP代替GLP？</p><ol><li>MLP和CNN的结构兼容，可以使用BP进行训练；</li><li>MLP本身就是一个deep model，满足feature复用的想法。</li></ol><p>如下图所示，是MLP CNN和GLP CNN的区别。<br><img src="/2019/03/13/cnn/mlp_vs_linear.png" alt="mvl_vs_glp"></p><p>MLP的公式如下。<br><img src="/2019/03/13/cnn/equ.png" alt="equ"><br>从cross channel(feature maps)的pooling角度来看，上面的公式相当于在一个正常的conv layer上进行多次的parametric pooling，每一个pooling layer对输入的feature map进行线性加权，经过一个relu层之后在下一层继续进行pooling。Cross channel pooled的feature maps在接下来的层中多次进行cross channel pooling。这个cross channel pooling的结构的作用是学习复杂的cross channel信息。<br>其实整个cross channel的paramteric pooling结构相当于一个普通的卷积加上了多个$1\times 1$的卷积，如下图所示：<br><img src="/2019/03/13/cnn/11filter.png" alt="11filter"></p><h4 id="global-average-pooling">Global average pooling</h4><p>FC layers证明是容易过拟合的，dropout被提出来正则化fc layers的参数。<br>本文提出的global average pooling取代了CNN的fc layers，直接在最后一个mlpconv layer中对应于分类任务中的每个类别生成一个feature map。然后用在feature maps上的average pooling代替fc layers，然后把它送入softmax layer。原来的CNN是将feature map reshape成一个一维向量，现在是对每一个feature map进行一个average pooling，有多少个feature map就有多少个pooling，相当于一个feature map对应与一个类型。<br>这样做有以下几个好处：</p><ol><li>在fc layers上的global average pooling让feature map和categories对应起来，feature map可以看成类别的置信度。</li><li>直接进行average pooling不用优化fc layer的参数，也就没有过拟合问题。</li><li>global average pooling对全局信息进行了加和，对于input的spatial信息更加鲁邦。</li></ol><h4 id="nin-v2">NIN</h4><p>如下图所示，是NIN的整体架构。<br><img src="/2019/03/13/cnn/nin.png" alt="nin"><br>下图是一个具体参数化的示例<br><img src="/2019/03/13/cnn/instance.png" alt="instance"></p><h3 id="实验-v2">实验</h3><h2 id="overfeat-2013">OverFeat(2013)</h2><p>论文名称：OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks<br>论文地址：<a href="https://arxiv.org/pdf/1312.6229.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1312.6229.pdf</a></p><h3 id="概述-v2">概述</h3><p>本文提出了一个可用于classification, localization和detection等任务的CNN框架。<br>ImageNet数据集中大部分选择的是几乎填满了整个image中心的object，image中我们感兴趣的objects的大小和位置也可能变化很大。为了解决这个问题，作者提出了三个方法：</p><ol><li>用sliding window和multiple scales在image的多个位置apply ConvNet。即使这样，许多window中可能包含能够完美识别object类型的一部分，比如一个狗头。。。最后的结果是classfication很好，但是localization和detection结果很差。</li><li>训练一个网络不仅仅预测每一个window的category distribution，还预测包含object的bounding box相对于window的位置和大小。</li><li>在每个位置和大小累加每个category的evidence</li></ol><h3 id="vision任务">Vision任务</h3><p>classification，localization和detection。classification和localization通常只有一个很大的object，而detection需要找到很多很小的objects。<br>classification任务中，每个image都有一个label对应image中主要的object的类型。为了找到正确的label，每个图片可以猜$5$次（图片中可能包含了没有label的数据）。localization任务中，不仅要给出label，还需要找到这个label对应的bouding box，bounding box和groundtruth至少要有$50$匹配，label和bounding box也需要匹配。detection和localization不同的是，detection任务中可以有任何数量的objects，false positive会使用mean average precison measure。localization任务可以看成classification到detection任务的一个中间步。</p><h3 id="fcn">FCN</h3><p>用卷积层代替全连接层。具体是什么意思呢。<br>alexnet中，有5层卷积层，3层全连接层。假设第五层的输出是$5\times 5 \times 512$，$512$是output channels number，$5\times 5$是第五层的feature maps的大小。如果使用全连接的话，假设第六层的输出单元是$N$个，第六层权重总共是$(5\times 5\times 512) * (N)$，对于一个训练好的网络，图片的输入大小是固定的，因为第六层是一个全连接层，输入的大小是需要固定的。如果输入一个其他大小的图片，网络就会出错，所以就有了Fully Convolutional networks，它可以处理不同大小的输入图片。<br>如下所示，使用某个大小的image训练的网络，在classifier处用卷积层替换全连接层，如果使用全连接层，首先将$(5, 5, out_channels)$的feature map进行flatten $5\times 5\times out_channels$，然后经过三层全连接，最后输出一个softmax的结果。而fcn使用卷积层代替全连接，使用$N$个$5\times 5$的卷积核，直接得到$1\tims 1 \times N$的结果，最后得到一个$1\times 1\times C$的输出，$C$代表图像类别，$N$代表全连接层中隐藏节点的数量。<br><img src="/2019/03/13/cnn/fcn.png" alt="fcn"><br>事实上，FCN和全连接的本质上都是一样的，只不过一个进行了flatten，一个直接对feature map进行操作，直接对feature map操作可以处理不同大小的输入，而flatten不行。<br>当输入图片大小发生变化时，输出大小也会改变，但是网络并不会出错，如下所示：<br><img src="/2019/03/13/cnn/fcn2.png" alt="fcn2"><br>最后输出的结果是$2\times 2 \times C$的结果，可以直接对它们取平均，最后得到一个$1\times 1\times C$的分类结果。</p><h3 id="offset-max-pooling">offset Max pooling</h3><p>我们之前做max pooling的时候，设$kernel_size=3, stride_size=1$，如果feature map是$3$的倍数，那么只有一个pooling的结果，但是如果不是$3$的倍数，max pooling会很多个结果，比如有个$20\times 20$的feature map，在$x,y$上做max pooling分别有三种结果，分别从$x,y$的位置$0$开始，位置$1$开始，位置$2$开始，排列组合有$9$中情况，这九种情况的结果是不同的。<br>如下图所示，在一维的长为$20$的pixels上做maxpooling，有三种情况。<br><img src="/2019/03/13/cnn/offset_maxpooling.png" alt="offset_maxpooling"></p><h3 id="overfeat">overfeat</h3><p>这两个方法中，fcn是在输入图片上进行的window sliding，而offset maxpooling是在feature map进行的window sliding，这两个方法结合起来就是overfeat，要比alexnet直接在输入图片上进行window sliding 要好。</p><h3 id="classification">Classification</h3><h4 id="training">training</h4><ul><li>datset<br>Image 2012 trainign set（1.2million iamges，C=$1000$ classes)。</li><li>data argumented<br>对每张图片进行下采样，所以每个图片最小的dimension需要是$256$。<br>提取$5$个random crops以及horizaontal flips，总共$10$个$221\times 221$的图片</li><li>batchsize<br>$128$</li><li>初始权重<br>$(\mu, \sigma)= (0, 1\times 10^{-2})$</li><li>momentum<br>0.6</li><li>l2 weigth decay<br>$1\times 10^{-5}$</li><li>lr<br>初始是$5\times 10^{-2}$，在$(30,50,60,70,80)$个epoches后，乘以$0.5$</li><li>non-spatial<br>这个说的是什么呢，在test的时候，会输出多个output maps，对他们的结果做平均，而在training的时候，output maps是$1\times 1$。</li></ul><h4 id="model架构">model架构</h4><p>下图展示的是fast model，spatial input size在train和test时候是不同的，这里展示的是train时的spatial seize。layer 5是最上层的CNN，receptive filed最大。后续是FC layers，在test时候使用了sliding window。在spatial设置中，FC-layers替换成了$1\times 1$的卷积。<br><img src="/2019/03/13/cnn/overfeat_fast.png" alt="overfeat_fast"><br>下图给出了accuracy model的结构，<br><img src="/2019/03/13/cnn/overfeat_accuracy.png" alt="overfeat_accuracy"><br>总的来说，这两个模型都在alexnet上做了一些修改，但是整体架构没有大的创新。</p><h4 id="多scale-classification">多scale classification</h4><p>alexnet中，对一张照片的$10$个views（中间，四个角和horizontal flip)的结果做了平均，这种方式可能会忽略很多趋于，同时如果不同的views有重叠的话，计算很redundant。此外，alexnet中只使用了一个scale。<br>作者对每个iamge的每一个location和多个scale都进行计算。<br>如下图，对应了不同大小的输入图片，layer 5 post pool中$(m\times n)\time(3\times 3)$，前面$m\times n$是fcn得到的不同位置的feature map，后面$3\times 3$是$kernel_size=3$的offset max pooling得到的featrue map。乘起来是所有的预测结果。<br><img src="/2019/03/13/cnn/multi_scale.png" alt="multi_scale"></p><h3 id="localization">localization</h3><h3 id="detection">Detection</h3><h2 id="zfnet-2014">ZFNet(2014)</h2><p>论文名称：Visualizing and Understanding Convolutional Networks<br>论文地址：<a href="https://cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf" target="_blank" rel="noopener">https://cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf</a><br>为什么叫ZFNet，两个作者名字首字母的拼写。</p><p>首先我有一个问题？就是什么是一个activation。在原文的$2.1$节，有这样一个介绍：</p><blockquote><p>We present a novel way to map these activities back to the input pixel space, showing what input pattern originally caused a given activation in the feature maps.<br>我的理解是一个activation就是feature map中的一个unit。事实上，feature map也叫activation map，因为它是image中不同parts的acttivation，而叫feature map是因为它是image中找到特定的feature。</p></blockquote><h3 id="概述-v3">概述</h3><p>这篇文章从可视化的角度给出中间特征的和classifier的特点，分析如何改进alexnet来提高imagenet classification的accuracy。<br>为什么CNN结果这么好？</p><ol><li>training set越来越大</li><li>GPU的性能越来越好</li><li>Dropout等正则化技术</li></ol><p>但是CNN还是一个黑盒子，我们不知道它为什么表现这么好？这篇文章给出了一个可视化方法可视化任意层的feature。</p><p>那么本文的contribution是什么呢？使用deconvnet进行可视化，通过分析特征行为，对alexnet进行fine tune提升模型性能。</p><h3 id="使用deconvnet可视化">使用deconvnet可视化</h3><p>什么是deconvnet？可以看成和convnet拥有同样组成部分（pooling, filter)等，但是是反过来进行的。如下图所示，convnet是把pixels映射到feature，或者到底层features映射到高层features，而deconvnet是把高层features映射到底层features，或者把features映射到pixels。在测试convnet中给定feature maps的一个activation时，设置所有其他的activation为0，将这个feature map传入deconvnet网络中。<br><img src="/2019/03/13/cnn/fig1.png" alt="fig1"><br>图片左上为deconv，右上为conv。conv的流程为filter-&gt;rectify-&gt;pooling；deconv的流程为unpool-&gt;rectify-&gt;filter。</p><h4 id="unpooling">Unpooling</h4><p>convnet中的max pooling是不可逆的，这里作者使用switch variables记录下max pooling后的元素在没有pooling时的位置，进行近似的恢复。</p><h4 id="rectification">Rectification</h4><p>convnet使用relu non-linearities。deconvnet还是使用relu，这里我有些不理解，为什么？为什么deconve还是使用relu</p><h4 id="filtering">Filtering</h4><p>deconvnet使用convnet中filters的transposed版本。</p><h3 id="training-v2">Training</h3><h4 id="整体架构-v2">整体架构</h4><p><img src="/2019/03/13/cnn/fig3.png" alt="fig3.png"></p><ul><li>training set<br>1.3百万张图片，1000类</li><li>processed<br>每个RGB图像resized成最小边维度为$256$，cropping中间的$256 \times 256$，减去所有像素的平均值。crops$10$个$224\times 224$（四个角落和中心以及horizontal flips)</li><li>优化方法<br>带momentumSGD</li><li>batch size<br>128</li><li>lr<br>初始是$10^{-2}$,然后手动anneal</li><li>momentum<br>0;9</li><li>Dropout<br>layer 6和layer 7,0.5</li><li>weights和biases初始化<br>weights设置为$10^{-2}$，biases设置为$0$</li><li>normalizaiton<br>对第一层的filter，如果RMS超过了$10^{-1}$就设置为$10^{-1}$</li><li>训练次数<br>70epochs</li></ul><h3 id="visualizaiton">Visualizaiton</h3><h4 id="feature-visualization">Feature visualization</h4><p>如下图所示，使用deconvnet可视化一些feacutre activation。给定一个feature map，选择其中最大的$9$个activations对应的样本，一个feature map是通过一个filter得到的，而一个filter提取的是一个特征，所以这$9$个activations都是一个filter提取的不同图片中的同一个特征。然后将它们输入deconvnet，得到pixel spaces，可以查看哪些不同的结构（哪些原始）产生了这个feature，展现这个filter对于输入deformation的invariance。在黑白图像的旁边有对应的图像原图，他们要比feature的variation更多，因为feature关注的是图像的invariance。比如layer 5的第一行第二列的九个图，这几个patch看起来差异很大，但是却在同一个feature map中，因为这个feature map关注的是背景中的草，并不是其他objects。更多的我们可以看出来，第二层对应corner和edge等，第三次对应更复杂的invariances，比如textures和text等。第四层更class-specific，第五层是object variation。<br><img src="/2019/03/13/cnn/fig2.png" alt="fig2"></p><h4 id="feature-evolution-durign-training">Feature evolution durign training</h4><p>下图随机选择了几个不同的feature，然后展示了他们在不同layer不同epochs（1, 2, 5, 10, 20, 30, 40, 64）的可视化结果。<br><img src="/2019/03/13/cnn/fig4.png" alt="fig4"></p><h4 id="架构选择">架构选择</h4><p>通过可视化alexnet的first layer和second layer，有了各种各样的问题。First layer中主要是high和low frequency的信息，而2nd layer有很多重复的，因为使用stride为$4$而不是$2$。作者做了两个改进：</p><ol><li>将first layer的filter size从$11\times 11$改成了$7\times 7$</li><li>卷积的步长从$4$改成了$2$</li></ol><p>如下图所示：<br><img src="/2019/03/13/cnn/fig5.png" alt="fig5"></p><h4 id="occlusion-sensitivity">Occlusion Sensitivity</h4><p>model是否真的识别了object在image中的位置，还是仅仅使用了上下文信息？下图中的例子证明了model真的locate了object,当遮挡住物体的部分增大时，给出正确分类的概率就减小了。移动遮挡方块的位置，给出一个和方块位置相关的分类概率函数，我们可以看出来，model really works。<br><img src="/2019/03/13/cnn/fig6.png" alt="fig6"></p><h3 id="实验-v3">实验</h3><p>第一个实验通过使用，证明了前面的特征提取层和fc layers都是有用的。<br>第二个实验保留前面的特征提取层和fc layers，将最后的softmax替换。</p><h2 id="vggnet-2014">VGGNet(2014)</h2><p>论文名称：VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION<br>论文地址：<a href="https://arxiv.org/pdf/1409.1556.pdf%20http://arxiv.org/abs/1409.1556.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1409.1556.pdf http://arxiv.org/abs/1409.1556.pdf</a><br>VGG是Visual Geometry Group的缩写</p><h3 id="概述-v4">概述</h3><p>这篇文章主要研究了CNN深度对大规模图像识别问题精度的影响。本文的主要contribution就是使用多层的$3\times 3$ filters替换大的filter，增加网络深度，提高识别精度。</p><h3 id="方案">方案</h3><h4 id="架构">架构</h4><p><strong>训练</strong>，输入$224\times 224$大小的RGB图片。对每张图片减去训练集上所有图片RGB 像素的均值。预处理后的图片被输入多层CNN中，CNN的filter是$3\times 3$的，作何也试了$1\times 1$的filter，相当于对输入做了一个线性变换，紧跟着一个non-linear 激活函数，这里的$1\times 1$的filter没有用于dimention reduction。stride设为$1$，添加padding使得卷积后的输出大小不变。同时使用了$5$个max-pooling层（并不是每一层cnn后面都有max-pooling)，max-pooling的window是$2\times 2$，stride是$2$。<br>在训练的时候CNN后面接的是三个FC layers，前两个是$4096$单元，最后一层是$1000$个单元的softmax。所有隐藏层都使用ReLu非线性激活函数。<br>在测试的时候使用fcn而不是直接flatten。</p><h4 id="配置">配置</h4><p>这篇文章给出了五个网络架构，用$A-E$表示，它们只有在深度上有所不同：从$11$层($8$个conv layers和$3$个FC layers)到$19$层（$16$个conv layers和$3$个FC layers）。Conv layers的channels很小，从第一层的$64$，每过一个max pooling layers，变成原理啊的两倍，直到$512$。具体如下表所示。<br><img src="/2019/03/13/cnn/vgg_conf.png" alt="vgg_conf"><br>网络的参数个数如下表所示。<br><img src="/2019/03/13/cnn/vgg_weights_num.png" alt="vgg_weights_num"><br>网络$A$的参数计算：<br>\begin{align*}<br>64\times 3\times 3\times 3 + \\<br>128\times 3\times 3\times 64 + \\<br>256\times 3\times 3\times 128 + \\<br>256\times 3\times 3\times 256 + \\<br>512\times 3\times 3\times 256 + \\<br>512\times 3\times 3\times 512 + \\<br>2\times 512\times 3\times 3\times 512 + \\<br>7\times 7\times 512\times 4096 + \\<br>4096\times 4096 + \\<br>4096\times 1000 = \\<br>132851392<br>\end{align*}<br>网络$B$的参数计算：<br>\begin{align*}<br>64\times 3\times 3\times 3 + \\<br>128\times 3\times 3\times 64 + \\<br>128\times 3\times 3\times 128 + \\<br>256\times 3\times 3\times 128 + \\<br>256\times 3\times 3\times 256 + \\<br>256\times 3\times 3\times 256 + \\<br>512\times 3\times 3\times 256 + \\<br>512\times 3\times 3\times 512 + \\<br>2\times 512\times 3\times 3\times 512 + \\<br>7\times 7\times 512\times 4096 + \\<br>4096\times 4096 + \\<br>4096\times 1000 = \\<br>133588672<br>\end{align*}<br>其实主要的网络参数还是在全连接层，$7\times 7\times 512\times 4096=102760448<br>$。</p><h4 id="卷积核作用">卷积核作用</h4><ol><li>为什么要用三个$3\times 3$的conv layers替换$7\times 7$个conv layers？</li></ol><ul><li>使用三个激活函数而不是一个，让整个决策更discriminative。</li><li>减少了网络参数，三个有$C$个通道的$3\times 3$conv layers,总的参数是$3\tims(3<sup>2C</sup>2)=27C^2$，而一个$C$通道的$7\times 7$ conv layers，总参数是$49C^2$。可以看成是一种正则化。</li></ul><ol start="2"><li>$1\times 1$ conv layers用来增加非线性程度，本文中使用的$1\times 1$的conv layers可以看成加了非线性激活函数的投影。</li></ol><h3 id="分类框架">分类框架</h3><h4 id="training-v3">training</h4><ul><li>目标函数<br>多峰logistic regression</li><li>训练方法<br>mini-batch gradient descent with momentum</li><li>batch size<br>256</li><li>momentum<br>0.9</li><li>正则化<br>$L_2$参数正则化(5\codt 10^{-4})<br>0.5 dorpout 用于前两个FC layers</li><li>lr<br>初始值为$10^{-2}$，当验证集的accuracy不再提升时，除以$10$。学习率总共降了$3$次，$370K$次迭代后停止。</li><li>图像预处理<br>从rescaled中随机cropped $224\times 224$的RGB图像。<br>使用alexnet中的随机horizontal flipping和随机RGB colour shift。</li><li>iamge rescale<br>用$S$表示training image的小边的大小，$S$也叫作train sacle。网络的输入是从training image中cropped得到的$224\times 224$的图像。所以只要$S$取任何不小于$224$的值即可，如果$S=224$，那么crop在统计上会captuer整个图片，完全包含training image最小的那边；$S&gt;&gt;224$的时候，crop会产生很小一部分的图像。<br>作者尝试了固定$S$和不固定的$S$。对于固定$S$，设置$S=256$和$S=384$，首先在$S=256$上训练，然后用$S=256$训练的参数初始化$S=384$的参数，使用更小的初始学习率$10^{-3}$。不固定$S$时，$S$从$[S_{min}, S_{max}](S_{max}=512,S_{min}=256)$任意采样，然后crop。</li><li>VGG vs alexnet<br>VGG参数多，深度深，但是收敛快，原因：</li></ul><ol><li>更小的filter带来的implicit regularisation</li><li>某些层的预先初始化。<br>这个解决的是网络深度过深，某些初值使得网络不稳定的问题。解决方法：先随机初始化不是很深的网络A，进行训练。在训练更深网络的时候，使用A网络的值初始化前$4$个卷基层和最后三个FC layers。随机初始化的网络参数，从均值为$0$，方差为$10^{-2}$的高斯分布中采样得到。</li></ol><h4 id="testing">testing</h4><ol><li>测试的时候先把input image的窄边缩放到$Q$，$Q$也叫test scale，$Q$和$S$不一定需要相等。</li><li>这里和overfeat模型一样，在卷积网络之后采用了fcn，而不是fc layers。</li></ol><h3 id="classfication">classfication</h3><p>ILSVRC-2012，training($1.3M$张图片)，validation($50K张$)，testing($100K$张)<br>两个metrics：top-1和top-5 error。top-1 error是multi-class classification error，不正确分类图像占的比例；top-5 error是预测的top-5都不是ground-truth。</p><h4 id="single-scale-evaluation">single scale evaluation</h4><p>$S$固定时，设置test image size $Q=S=256$；<br>$S$抖动时，设置test image size $Q=0.5(S_{min}+S_{max})=0.5(256+512)=384$，$S\in [S_{min},S_{max}]$。</p><h4 id="multi-scale-evaluation">multi scale evaluation</h4><p>用同一个模型对不同rescaled大小的图片多次test，即对于不同的$Q$。<br>固定$S$时，在三个不同大小的test image size $Q={S-32,S,S+32}$评估。<br>$S$抖动时，模型是在$S\in [S_{min},S_{max}]$上训练的，在$Q={S_{min}, 0.5(S_{min}+S_{max}), S_{max}}$上进行test。</p><h4 id="多个crop-evaluation">多个crop evaluation</h4><p>这个是为了和alexnet做对比，alexnet网络在testing时，对每一张图片都进行多次cropped，对testing的结果做平均。</p><h4 id="convnet-funsion">convnet funsion</h4><p>之前作者的evaluation都是在单个的网络上进行的，作者还试了将不同网络的softmax输出做了平均。</p><h2 id="inception-v1-googlelenet">Inception V1(GoogleLeNet)</h2><h3 id="摘要-v2">摘要</h3><p>提出一种方法能够在不增加太多计算代价的同时增加网络的深度和宽度。</p><h3 id="motivation">motivation</h3><p>直接增加网络的深度和宽度有两个缺点：</p><ol><li>参数更多，容易过拟合，尤其是训练集太小的情况下，高质量的训练集很难生成。</li><li>需要更多的计算资源。比如两层CNN，即使每一层中线性增加filters的个数也会造成计算代价指数级增加。如果增加的权重接近$0$的话，计算代价就浪费了。而现实中的计算资源是有限的。</li></ol><p>如何解决这个问题呢？使用sparsity layers取代fully connetcted layers。但是现在的计算资源在处理non-uniform 的sparse data时是非常低效的，即使数值操作减小$100$倍，查找的时间也是很多的。而针对CPU和GPU的dense matrix计算能够加快fc layer的学习。现在绝大部分的机器学习视觉模型在sparsity spatial domain都仅仅利用了CNN，而convolution是和前一层patches的dense connection。1998年的convnet为了打破网络对称性，改善学习结果，使用的是random和sparse连接，而在alexnet中为了并行优化计算，使用了全连接。当前cv的state-of-the-art架构使用的都是unifrom structure，为了高效的进行dense计算，filters和batch size的数量都是很大的。<br>稀疏性可以解决过拟合和资源消耗过多的问题，而稠密连接可以提高计算效率。所以接下来要做的是一个折中，利用filter维度的稀疏结构，同时利用硬件在dense matrices上的计算进行加速。<br>Inception架构就是使用一个dense组件去逼近sparse结构的例子。</p><h3 id="算法">算法</h3><p>Inception的idea是使用dense组件近似卷积的局部稀疏结构。本文的旋转不变型是利用convolutional building blocks完成的，找到optimal local construction，然后不断堆叠。文章[11]中建议layer-by-layer的构建，分析上一层之间的关系，并将具有高相关性的units进行分组。这些相关的units cluster构建成了下一层的units，并且和上一层的units相连接。假设之前层中的每一个unit都对应输入图片中的一些region，这些units分组构成filter banks。这就意味着在靠近输入的层中我们会得到很多关于local regions相关的units。通过在下一层中使用$1\times 1$的卷积，可以找到关注于同一个region的很多个clusters。（这里加一些我自己的理解，$1\times 1$的卷积层可以找到那些重复的feature map？？）当然，也有可能有更大的cluster可以通过在更大的patches上进行卷积得到，所以这里同时在一层中同时使用$1\times 1, 3\times 3, 5\times 5$的filters，使用这些大小的filter仅仅是因为方便，然后将他们的输出进行组合当做下一层的输入。当然可以加上pooling，如下图所示。<br><img src="/2019/03/13/cnn/naive_inception.png" alt="naive inception"><br>但是，这样子计算量还是很大，大量$3 \times 3, 5\times 5$在卷积时的计算量，如果再加上输入shape和输出shape相等的max pooling操作，下一层的输入维度相当大，计算开销j就爆炸了。这就使用了本文的第二个idea：使用$1\times 1$的filter降维减少计算量。在$3\times 3, 5\times 5$大小filter之前添加$1\times 1$的卷积进行降维。<br><img src="/2019/03/13/cnn/dr_inception.png" alt="dimension reduction inception"></p><p>这个架构的好处：</p><ol><li>在每一层都可以增加units的数量而不用担心计算量暴增。首先将上一层大量filters的输出进行进行降维，然后输入到下一层。</li><li>visual信息用不同的scales进行处理，然后拼接起来，这样子在下一层可以同时从不同scales中提出features。</li></ol><h3 id="googlenet">GoogLeNet</h3><p>作者给出了Inception的一个示例，叫GoogLeNet。网络具体配置如下：<br><img src="/2019/03/13/cnn/GoogLeNet.png" alt="GoogLeNet"><br>其中，&quot;#$3 \times 3$ reduce&quot;和&quot;#$5 \times 5$ reduce&quot;表示在$3\times 3, 5\times 5$卷积之前使用$1\times 1$的filters个数，pool proj这一列表示在max pooling之后的$1\times 1$的filters个数。<br>作者在GoogLeNet中还使用了两个额外的分类层辅助训练。通过观察得知相对shallower的网络有很好的性能，那么在反向传播时，深层网络的中间特征应该是很有判别力的。<br>通过在网络中间添加辅助的classfiers，作者想要让网络底层也有判别力。在训练的时候，在$4a$和$4d$模块后添加分类器，然后将所有的loss乘上一个权重加到总的loss上，在test时，这些辅助网络被扔掉。</p><h2 id="batch-normalization">Batch Normalization</h2><p>论文名称：Batch Normalization: Accelerating Deep Network Training b<br>y Reducing Internal Covariate Shift<br>论文地址：<a href="https://arxiv.org/pdf/1502.03167.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1502.03167.pdf</a></p><h3 id="概述-v5">概述</h3><p>在训练深度神经网络的时候，随着训练的不断进行，网络权重在不停的变，除了第一层之外的每层输入也在不停的变，所以就使得权重每次都要去适应新的输入distributions。这就导致训练速度很慢，学习率的要很小，很难使用saturaing nonlinearities激活函数训练。作者把这个问题叫做internal covariate shift，提出了batch normalization解决该问题，bn对于参数初始化的要求没那么高，允许使用更高的学习率。<br>BN可以看成一种正则化手段。</p><h3 id="简介">简介</h3><p>SGD相对于单个样本的GD来说，使用mini-batch的梯度作为整个训练集的估计值，效果更好；同时并行计算提高了效率。之前的工作使用ReLU，更好的初始化以及小的学习率来解决梯度消失问题，而本文作者的想法是让非线性输入的分布尽可能稳定，从而解决梯度饱和等问题，加快训练。本文提出的batch normalization通过固定每一层输入的均值和方差减少internal covariate shift，同时减少了gradients对于初始参数的依赖性。在使用了BN的网络中，也可以使用如sigmod和tanh的saturating nonlirearities激活函数，并不是一定要用relu激活函数。</p><h3 id="mini-batch-normalization">Mini-Batch Normalization</h3><p>Whitening每一层的所有inputs需要很大的代价，而且并不是每个地方都是可导的。作者进行了两个简化。第一个是并不是对所有输入的features进行whiten，而是对每一个feautre单独的normalization，将他们转化成均值为0，方差为1的数据。对于一个d维的输入$x=(x^1,\cdots, x^d)，对每一维进行normalize：<br>$$\hat{x}^k= \frac{x^k - \mathbb{E}\left[x<sup>k\right]}{\sqrt{Var\left[x</sup>k\right]}}$$<br>其中的期望和方差是整个training set 的期望和方差。但是仅仅normalize每一层的输入可能改变这一层的表示。比如normalize sigmod的输入会将它们的输出限制在非线性的线性区域。为了解决这个问题，在网络中添加的这个transformation应该能够表示identity transform，作者对每个activation $x<sup>k$引入了一对参数，$\gamma</sup>k, \beta^k$，它们对normalized value进行scale和shift：<br>$$y^k = \gamma^k \hat{x}^k + \beta^k$$<br>这些参数和模型参数一块，都是学习出来的，如果学习到$\gamma<sup>k=\sqrt{Var\left[x</sup>k\right]},\beta^k = \mathbb{E}\left[x^k\right]$，就可以表示恒等变换了。。<br>上面说的是使用整个training set的方差和期望进行normaliza，事实上，在sgd中这是不切合实际的。因此，就引入了第二个简化，使用每个mini-batch的方差和期望进行normalize，并且方差和期望是针对于每一个维度计算的。给出一个大小为$m$的batch $B$，normalization独立的应用于每一个维度。用$\hat{x}_{1,\cdots, m}$表示normalized values，以及它们的linear transformation：$y_{1,\cdots,m}$。这个transform表示为：$BN_{\gamma, \beta}:x_{1,\cdots, m} \rightarrow y_{1,\cdots,m}$，称为Batch Normalization Transform，完整的算法如下：<br>算法1 Batch Normalizing Transform<br>输入：　mini-batch：$B={x_{1,\cdots, m}}，要学习的参数$\gamma,\beta$<br>输出：${y_i=BN_{\gamma,\beta}(x_i)}$<br>$\mu\leftarrow \frac{1}{m}\sum_{i=1}^mx_i$  计算batch的mean<br>$\sigma^2_B\leftarrow \sum_{i=1}<sup>m(x_i-\mu_B)</sup>2$  计算batch的variance<br>$\hat{x}_i\leftarrow \frac{x_i-\mu_B}{\sqrt{\simga^2_B+\epsilon}}$ normalize<br>$y_i\leftarrow \gamma \hat{x}_i+ \beta \equiv BN_{\gamma, \beta}(x_i)$ scale以及shift。<br>整个过程的loss还可以通过backpropagate进行传播，即它是可导的。</p><h3 id="none"></h3><h3 id="batch-normalized-cnn">Batch-Normalized CNN</h3><p>原来的CNN是<br>$$ z= g(Wu+b)$$<br>现在在nonlinearity前加上BN transform。<br>$$ z= g(BN(Wu+b))$$<br>但是事实上，Wu+b和Wu的效果是一样的，因为normalized的时候会减去均值，所以最后就是：<br>$$ z= g(BN(Wu))$$<br>BN在Wu的每一个维度上单独使用BN，每一个维度有一对$\gamma<sup>k,\beta</sup>k$。</p><h3 id="bn能使用更大的学习率">BN能使用更大的学习率</h3><h3 id="bn正则化模型">BN正则化模型</h3><h2 id="residual-network-2015">Residual Network(2015)</h2><p>论文名称：Deep Residual Learning for Image Recognition<br>论文地址：<a href="https://arxiv.org/pdf/1512.03385.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1512.03385.pdf</a></p><h3 id="概述-v6">概述</h3><p>作者提出了参差网络，容易优化，仅仅增加深度就能得到更高的accuracy。在Imagenet上使用比VGG深八倍的152层的residual网络，但是计算复杂度更低。<br>网络是不是越深越好？并不是！事实上，随着网络的加深，会出现退化问题－即增加网络的深度，accuracy反而会下降。导致这个问题的原因并不是过拟合，至于是什么原因？<br>在这篇文章中，作者提出了deep residual network。使用一些stacked non-linear layers你和一个residual mapping，而不是直接学习一个underlying mapping。用$H(x)$表示一个underlying mapping，我们的目标是学习一个residual mapping：$F(x) = H(x)-x$，underlying mapping可以写成$H(x) = F(x)+x$。在某种情况下，如果identity mapping是optimal，那么让$F(x)$接近于$0$可能比让stacked non linear layers拟合一个identity mapping要简单。。如下图所示，$H(x)$可以用下图表示，由一个feedforward nn加上shortcut connections（skip one or more layers的connection）组成：<br><img src="/2019/03/13/cnn/residual_block.png" alt="residual block"><br>shortcut connection在这里就是一个identity mapping，不需要额外的参数和计算量，shorcut的输出和$F(x)$的输出再一块经过relu激活函数。</p><p>本文的contribution是什么？<br>加了一个恒等映射让深度网络的训练变得更容易。具体原理是什么？可以从这样一个角度看，在每一层都可以把不同维度的feature进行重组。residual connection是skip的一种方式？？</p><h3 id="residual-learning">Residual Learning</h3><p>用$H(x)$表示stacked non linear layers拟合的一个underlying mapping，$x$为stacked layers的输入。原来我们用这些layers逼近一个复杂的函数，现在我们用它逼近residual function，即$F(x) = H(x) -x$（假设输入和输出的维度是一样的），原来想要拟合的函数变成了$F(x)+x$，它们的意义是一样的，但是对于learning的帮助却有很大差别。<br>如网络degradation问题中，如果更深的网络中添加的新layers是identity mapping，那么这个更深的网络的training error至少也要和浅一些的网络一样，然而事实上并不是这样的。在degradation问题中，说明multip nonlinear layers在近似identity mappings时效果并不是很好。而在residual learnign中，如果identity mapping是optimal，那么可以让non linear layers的权重接近于0，最后得到一个indetity mappings。虽然在real cases中，identity mapping几乎不可能是optimal的，但是如果optimal function更接近identity mapping而不是zero ampping，residual learning的效果就要更好。<br><img src="/2019/03/13/cnn/residual_block.png" alt="residual block"></p><h3 id="identity-mapping-by-shortcuts">Identity Mapping by Shortcuts</h3><p>本文中采用的residual block如上上图所示，用公式表示为：<br>$$y = F(x, {W_i}) + x$$<br>其中$x,y$是输入和输出向量，函数$F(x, {W_i})$表示要学习的residual mapping，residual block块中有两层，$F=W_2\sigma(W_1x)$表示第一层和第二层，然后$F+x$表示shortcut connection以及element-wise addition。如果$x$和$F(x)$的维度不一样的话，可以进行一个linear projection：<br>$$y=F(x,{W_i}) + W_sx$$<br>$W_s$表示线性变换的矩阵。如果必要的话，$W_s$可以走一样线性变换，事实上，实验表明如果维度一样的话，identity mapping足够解决degradation问题，$W_s$就是用来进行dimension matting。<br>$F$的形式是很灵活的，可以像本文一样使用linear layers，当然也可以使用更多layers，无所谓。</p><h3 id="网络架构">网络架构</h3><p>作者给出了三个网络架构，一个是VGG，一个是VGG修改得到的网络，另一个是这个修改的网络加上shortcut connection，如图所示。基于VGG的修改有以下两个原则：</p><ol><li>feature map的大小不变的话，filters的数量不变</li><li>feature map的大小减半的话，filters的数量变为原来的$2$倍，保证每一层的计算复杂度不变。</li></ol><p>网络最后接一个global average pooling layer和一个1000way的fc layer和softmax。</p><h3 id="其他细节">其他细节</h3><ol><li>image的短边被resize到$[256, 480]$之间。然后从中裁剪一个$224 \times 224$的样本或者它的horizontal filp。</li><li>使用标准的颜色增强。</li><li>使用BN</li><li>从头开始训练网络</li><li>使用batch size为$256$的SGD</li><li>学习率从$0.1$开始，每到error不再改变时，除以$10$，总共进行$60\times 10^4$次迭代。</li><li>权重decay为$0.0001$，mementum为$0.9$。</li><li>测试时，对十个crop取平均，使用fcn，对多个scales上的scores进行平均。</li></ol><h3 id="结论">结论</h3><p>14.<a href="https://www.quora.com/How-does-deep-residual-learning-work" target="_blank" rel="noopener">https://www.quora.com/How-does-deep-residual-learning-work</a><br>15.<a href="https://kharshit.github.io/blog/2018/09/07/skip-connections-and-residual-blocks" target="_blank" rel="noopener">https://kharshit.github.io/blog/2018/09/07/skip-connections-and-residual-blocks</a><br>16.<a href="https://stats.stackexchange.com/questions/56950/neural-network-with-skip-layer-connections" target="_blank" rel="noopener">https://stats.stackexchange.com/questions/56950/neural-network-with-skip-layer-connections</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;cnn&quot;&gt;CNN&lt;/h2&gt;
&lt;h3 id=&quot;图片的表示&quot;&gt;图片的表示&lt;/h3&gt;
&lt;p&gt;图像在计算机中是一堆按顺序排列的顺子，数值为0到255。0表示最暗，255表示最亮。我们可以把这堆数字用一个长长的一维数组来表示，但是这样会失去平面结构的信息，为保留该结构信息，
      
    
    </summary>
    
      <category term="机器学习" scheme="http://mxxhcm.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="CNN" scheme="http://mxxhcm.github.io/tags/CNN/"/>
    
      <category term="卷积神经网络" scheme="http://mxxhcm.github.io/tags/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="alexnet" scheme="http://mxxhcm.github.io/tags/alexnet/"/>
    
  </entry>
  
  <entry>
    <title>python 常见问题（不定期更新）</title>
    <link href="http://mxxhcm.github.io/2019/03/13/python-%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/"/>
    <id>http://mxxhcm.github.io/2019/03/13/python-常见问题/</id>
    <published>2019-03-13T02:40:03.000Z</published>
    <updated>2019-07-13T02:30:01.224Z</updated>
    
    <content type="html"><![CDATA[<h2 id="问题1-dict-values-object-does-not-support-indexing">问题1-‘dict_values’ object does not support indexing’</h2><p>参考文献[1,2,3]</p><h3 id="报错">报错</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&apos;dict_values&apos; object does not support indexing&apos;</span><br></pre></td></tr></table></figure><h3 id="原因">原因</h3><p>The objects returned by dict.keys(), dict.values() and dict.items() are view objects. They provide a dynamic view on the dictionary’s entries, which means that when the dictionary changes, the view reflects these changes.<br>python3 中调用字典对象的一些函数，返回值是view objects。如果要转换为list的话，需要使用list()强制转换。<br>而python2的返回值直接就是list。</p><h3 id="代码示例">代码示例</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">m_dict = &#123;<span class="string">'a'</span>: <span class="number">10</span>, <span class="string">'b'</span>: <span class="number">20</span>&#125;</span><br><span class="line">values = m_dict.values()</span><br><span class="line">print(type(values))</span><br><span class="line">print(values)</span><br><span class="line">print(<span class="string">"\n"</span>)</span><br><span class="line"></span><br><span class="line">items = m_dict.items()</span><br><span class="line">print(type(items))</span><br><span class="line">print(items)</span><br><span class="line">print(<span class="string">"\n"</span>)</span><br><span class="line"></span><br><span class="line">keys = m_dict.keys()</span><br><span class="line">print(type(keys))</span><br><span class="line">print(keys)</span><br><span class="line">print(<span class="string">"\n"</span>)</span><br></pre></td></tr></table></figure><p>如果使用python3执行以上代码，输出结果如下所示：</p><blockquote><p>class 'dict_values’<br>dict_values([10, 20])<br>class 'dict_items’<br>dict_items([(‘a’, 10), (‘b’, 20)])<br>class 'dict_keys’<br>dict_keys([‘a’, ‘b’])</p></blockquote><p>如果使用python2执行以上代码，输出结果如下所示：</p><blockquote><p>type ‘list’<br>[10, 20]<br>type ‘list’<br>[(‘a’, 10), (‘b’, 20)]<br>type ‘list’<br>[‘a’, ‘b’]</p></blockquote><h2 id="问题2-timelimit-object-has-no-attribute-ale">问题2-‘TimeLimit’ object has no attribute ‘ale’</h2><p>参考文献[4,5,6]</p><h3 id="问题描述">问题描述</h3><p>运行github clone 下来的<a href="https://github.com/devsisters/DQN-tensorflow" target="_blank" rel="noopener">DQN-tensorflow</a>，报错:</p><blockquote><p>AttributeError: ‘TimeLimit’ object has no attribute ‘ale’.</p></blockquote><h3 id="原因-v2">原因</h3><p>是因为gym版本原因，在gym 0.7版本中，可以使用env.ale.lives()访问ale属性，但是0.8版本以及以上，就没有了该属性，可以在系列函数中添加如下修改：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, config)</span>:</span></span><br><span class="line">    self.step_info = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_step</span><span class="params">(self, action)</span>:</span></span><br><span class="line">    self._screen, self.reward, self.terminal, self.step_info = self.env.step(action)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lives</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> self.step_info <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> self.step_info[<span class="string">'ale.lives'</span>]</span><br></pre></td></tr></table></figure><h3 id="ale属性是什么">ale属性是什么</h3><p>我看官方文档也没有看清楚，但是我觉得就是生命值是否没有了</p><blockquote><p>info (dict): diagnostic information useful for debugging. It can sometimes be useful for learning (for example, it might contain the raw probabilities behind the environment’s last state change). However, official evaluations of your agent are not allowed to use this for learning.</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> gym</span><br><span class="line">env = gym.make(<span class="string">'CartPole-v0'</span>)</span><br><span class="line"><span class="keyword">for</span> i_episode <span class="keyword">in</span> range(<span class="number">20</span>):</span><br><span class="line">    observation = env.reset()</span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">        env.render()</span><br><span class="line">        print(observation)</span><br><span class="line">        action = env.action_space.sample()</span><br><span class="line">        observation, reward, done, info = env.step(action)</span><br><span class="line">        <span class="keyword">if</span> done:</span><br><span class="line">            print(<span class="string">"Episode finished after &#123;&#125; timesteps"</span>.format(t+<span class="number">1</span>))</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">env.close()</span><br></pre></td></tr></table></figure><h2 id="问题3-cannot-import-name">问题3-cannot import name ***</h2><p>参考文献[7]</p><h3 id="报错-v2">报错</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cannot import name tqdm</span><br></pre></td></tr></table></figure><h3 id="问题原因">问题原因</h3><p>谷歌了半天，没有发现原因，然后百度了一下，发现了原因，看来还是自己太菜了。。<br>因为自己起的文件名就叫tqdm，然后就和库中的tqdm冲突了，这也太蠢了吧。。。</p><h2 id="问题4-linux下python执行shell脚本输出重定向">问题4-linux下python执行shell脚本输出重定向</h2><p><a href="https://mxxhcm.github.io/2019/06/03/linux-python%E8%B0%83%E7%94%A8shell%E8%84%9A%E6%9C%AC%E5%B9%B6%E5%B0%86%E8%BE%93%E5%87%BA%E9%87%8D%E5%AE%9A%E5%90%91%E5%88%B0%E6%96%87%E4%BB%B6/">详细介绍</a></p><h2 id="问题4-importerror-no-module-named-conda-cli">问题4-ImportError: No module named conda.cli’</h2><h3 id="问题描述-v2">问题描述</h3><p>anaconda的python版本是3.7，执行了conda install python=3.6之后，运行conda命令出错。报错如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from conda.cli import main </span><br><span class="line">ModuleNotFoundError: No module named &apos;conda&apos;</span><br></pre></td></tr></table></figure><h2 id="解决方案">解决方案</h2><p>找到anaconda安装包，加一个-u参数，如下所示。重新安装anaconda自带的package，自己安装的包不会丢失。<br>~$:sh <a href="http://xxx.sh" target="_blank" rel="noopener">xxx.sh</a> -u</p><h2 id="问题5-python-pip使用国内源">问题5-python-pip使用国内源</h2><h3 id="暂时使用国内pip源">暂时使用国内pip源</h3><p>使用清华源<br>~$:pip install -i <a href="https://pypi.tuna.tsinghua.edu.cn/simple" target="_blank" rel="noopener">https://pypi.tuna.tsinghua.edu.cn/simple</a> package-name<br>使用阿里源<br>~$:pip install -i <a href="https://mirrors.aliyun.com/pypi/simple" target="_blank" rel="noopener">https://mirrors.aliyun.com/pypi/simple</a> package-name</p><h3 id="将国内pip源设为默认">将国内pip源设为默认</h3><p>~$:pip install pip -U<br>~$:pip config set global.index-url <a href="https://pypi.tuna.tsinghua.edu.cn/simple" target="_blank" rel="noopener">https://pypi.tuna.tsinghua.edu.cn/simple</a><br>~$:pip config set global.timeout 60</p><blockquote><p>Writing to /home/username/.config/pip/pip.conf</p></blockquote><h4 id="查看pip配置文件">查看pip配置文件</h4><p>~$:find / -name pip.conf<br>我的是在/home/username/.config/pip/pip.conf</p><h2 id="参考文献">参考文献</h2><p>1.<a href="https://www.cnblogs.com/timxgb/p/8905290.html" target="_blank" rel="noopener">https://www.cnblogs.com/timxgb/p/8905290.html</a><br>2.<a href="https://docs.python.org/3/library/stdtypes.html#dictionary-view-objects" target="_blank" rel="noopener">https://docs.python.org/3/library/stdtypes.html#dictionary-view-objects</a><br>3.<a href="https://stackoverflow.com/questions/43663206/typeerror-unsupported-operand-types-for-dict-values-and-int" target="_blank" rel="noopener">https://stackoverflow.com/questions/43663206/typeerror-unsupported-operand-types-for-dict-values-and-int</a><br>4.<a href="https://github.com/devsisters/DQN-tensorflow/issues/29" target="_blank" rel="noopener">https://github.com/devsisters/DQN-tensorflow/issues/29</a><br>5.<a href="https://gym.openai.com/docs" target="_blank" rel="noopener">https://gym.openai.com/docs</a><br>6.<a href="https://github.com/openai/baselines/issues/42" target="_blank" rel="noopener">https://github.com/openai/baselines/issues/42</a><br>7.<a href="https://blog.csdn.net/m0_37561765/article/details/78714603" target="_blank" rel="noopener">https://blog.csdn.net/m0_37561765/article/details/78714603</a><br>8.<a href="https://blog.csdn.net/u014432608/article/details/79066813" target="_blank" rel="noopener">https://blog.csdn.net/u014432608/article/details/79066813</a><br>9.<a href="https://mirrors.tuna.tsinghua.edu.cn/help/pypi/" target="_blank" rel="noopener">https://mirrors.tuna.tsinghua.edu.cn/help/pypi/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;问题1-dict-values-object-does-not-support-indexing&quot;&gt;问题1-‘dict_values’ object does not support indexing’&lt;/h2&gt;
&lt;p&gt;参考文献[1,2,3]&lt;/p&gt;
&lt;h3 id
      
    
    </summary>
    
      <category term="python" scheme="http://mxxhcm.github.io/categories/python/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="gym" scheme="http://mxxhcm.github.io/tags/gym/"/>
    
      <category term="常见问题" scheme="http://mxxhcm.github.io/tags/%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/"/>
    
      <category term="pip源" scheme="http://mxxhcm.github.io/tags/pip%E6%BA%90/"/>
    
  </entry>
  
  <entry>
    <title>markdown帮助</title>
    <link href="http://mxxhcm.github.io/2019/03/09/markdown%E5%B8%AE%E5%8A%A9/"/>
    <id>http://mxxhcm.github.io/2019/03/09/markdown帮助/</id>
    <published>2019-03-09T11:53:32.000Z</published>
    <updated>2019-06-06T11:24:18.121Z</updated>
    
    <content type="html"><![CDATA[<h2 id="引用">引用</h2><h3 id="代码引用">代码引用</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br></pre></td></tr></table></figure><h3 id="文字引用">文字引用</h3><blockquote><p>实际是人类进步的阶梯。　－－高尔基</p></blockquote><h2 id="表格">表格</h2><table><thead><tr><th style="text-align:center">name</th><th style="text-align:center">age</th><th style="text-align:center">gender</th></tr></thead><tbody><tr><td style="text-align:center">Alice</td><td style="text-align:center">11</td><td style="text-align:center">female</td></tr><tr><td style="text-align:center">Bob</td><td style="text-align:center">82</td><td style="text-align:center">male</td></tr></tbody></table><h2 id="表情">表情</h2><h3 id="安装过程">安装过程</h3><p>第一步，卸载hexo默认的hexo-renderer-marked markdown渲染器<br>~$:npm un hexo-renderer-marked --save<br>第二步，安装支持emoji的markdown渲染器<br>~$:npm i hexo-renderer-markdown-it --save<br>第三步，修改博客根目录下的_config.yml文件，添加下列内容：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"># Markdown-it config</span><br><span class="line">## Docs: https://github.com/celsomiranda/hexo-renderer-markdown-it/wiki</span><br><span class="line">markdown:</span><br><span class="line">  render:</span><br><span class="line">    html: true</span><br><span class="line">    xhtmlOut: false</span><br><span class="line">    breaks: true</span><br><span class="line">    linkify: true</span><br><span class="line">    typographer: true</span><br><span class="line">    quotes: &apos;“”‘’&apos;</span><br><span class="line">  plugins:</span><br><span class="line">    - markdown-it-abbr</span><br><span class="line">    - markdown-it-footnote</span><br><span class="line">    - markdown-it-ins</span><br><span class="line">    - markdown-it-sub</span><br><span class="line">    - markdown-it-sup</span><br><span class="line">    - markdown-it-emoji  ## add emoji</span><br><span class="line">  anchors:</span><br><span class="line">    level: 2</span><br><span class="line">    collisionSuffix: &apos;v&apos;</span><br><span class="line">    # If `true`, creates an anchor tag with a permalink besides the heading.</span><br><span class="line">    permalink: false  </span><br><span class="line">    permalinkClass: header-anchor</span><br><span class="line">    # The symbol used to make the permalink</span><br><span class="line">    permalinkSymbol: ¶</span><br></pre></td></tr></table></figure><p>然后重新生成部署即可。<br>测试：<br>😄<br>😆<br>👃</p><h2 id="测试">测试</h2><table>   <tr>      <td></td>   </tr>   <tr>      <td>Coding</td>   </tr>   <tr>      <td>Content</td>   </tr>   <tr>      <td>描述性提炼性质的研究</td>   </tr>   <tr>      <td>第一部分：</td>   </tr>   <tr>      <td>文献综述</td>   </tr>   <tr>      <td>（对话）</td>   </tr>   <tr>      <td>SPL</td>   </tr>   <tr>      <td>本文的文献综述贯穿在行文的过程中</td>   </tr>   <tr>      <td>（1）关于分家的原因：</td>   </tr>   <tr>      <td>敌军：①弗里德曼兄弟之间的利害冲突②许烺光夫妻纽带强于父子之间的纽带→概括为家庭内摩擦</td>   </tr>   <tr>      <td>作者（部分认同敌军基础上提出自己的观点）：分家逐渐演化成一种“文化现象”</td>   </tr>   <tr>      <td>（2）分家中的“继”与“合”</td>   </tr>   <tr>      <td>敌军：①孔迈隆以家产正式分才算分家的定义②分灶</td>   </tr>   <tr>      <td>评论：①经济上的考虑多于社会上的考虑②认为分家是家庭的破裂以及兄弟没有继承一个完整的家庭</td>   </tr>   <tr>      <td>作者：分家中也有垂直关系的“继承”、横纵一体的“合”</td>   </tr>   <tr>      <td></td>   </tr>   <tr>      <td>CPL</td>   </tr>   <tr>      <td>①对现有文献的理解和看法②作者在哪个细分领域展开研究</td>   </tr>   <tr>      <td>理论基础</td>   </tr>   <tr>      <td>RAT</td>   </tr>   <tr>      <td>上面两个部分的完善是为这个部分做准备</td>   </tr>   <tr>      <td>第二部分：</td>   </tr>   <tr>      <td>机制和结构</td>   </tr>   <tr>      <td>F（x）</td>   </tr>   <tr>      <td>结构： </td>   </tr>   <tr>      <td>（1）概念界定：分家的基本内容</td>   </tr>   <tr>      <td>什么是分家？分家时财产按照“股”分割；分家的原因</td>   </tr>   <tr>      <td>（2）分家带来的影响（案例分析）：分家带来了社会流动</td>   </tr>   <tr>      <td>  借用说“分家三年显高低”、“富不过三代”、“父子一条心，黄土变成金”三句俚语来说明分家对社会变化的影响</td>   </tr>   <tr>      <td>（3）分家的中“继”与“合”</td>   </tr>   <tr>      <td>  继：赡养老人、继宗祧（tiao 1声），对应儒的孝、父子一体观念</td>   </tr>   <tr>      <td>  合：生产生活上的合作</td>   </tr>   <tr>      <td>（4）结语</td>   </tr>   <tr>      <td>机制：对应儒的孝、父子一体观念；生产生活上的合作</td>   </tr>   <tr>      <td>Argument</td>   </tr>   <tr>      <td>CA</td>   </tr>   <tr>      <td>分中有继也有合</td>   </tr>   <tr>      <td>第三部分：</td>   </tr>   <tr>      <td>问题和发展</td>   </tr>   <tr>      <td>    （1） 文献综述找敌军可以借鉴，以及文献评述</td>   </tr>   <tr>      <td>    （2） 文献综述可以加一些友军</td>   </tr>   <tr>      <td>    （3） 文章可能写的太早了，不太符合现在的文章写作规范。不太理解第二部分“分家对社会发展的影响”对整篇文章有什么关系？？是不是没有必要占这么大篇幅</td>   </tr>   <tr>      <td>    （4） 内容方面：</td>   </tr>   <tr>      <td>随着时间的演变，他们的合越来越局限于小，男女双方的直系亲属，直系的兄弟关系和姻亲关系，大家族的联系越来越少；大家族即使祖坟放在一起，也难以通过祭祀的手段联系起来，慢慢农村也形成原子化的家庭单位，对于同村的人来说，地缘关系、邻里关系是比血缘关系更重要的存在</td>   </tr>   <tr>      <td></td>   </tr></table><h2 id="参考文献">参考文献</h2><p>1.<a href="https://daringfireball.net/projects/markdown/syntax" target="_blank" rel="noopener">https://daringfireball.net/projects/markdown/syntax</a><br>2.<a href="https://www.webfx.com/tools/emoji-cheat-sheet/" target="_blank" rel="noopener">https://www.webfx.com/tools/emoji-cheat-sheet/</a><br>3.<a href="https://guides.github.com/features/mastering-markdown/" target="_blank" rel="noopener">https://guides.github.com/features/mastering-markdown/</a><br>4.<a href="https://github.com/mxxhcm/use_vim_as_ide#8.4" target="_blank" rel="noopener">https://github.com/mxxhcm/use_vim_as_ide#8.4</a><br>5.<a href="https://chaxiaoniu.oschina.io/2017/07/10/HexoAddEmoji/" target="_blank" rel="noopener">https://chaxiaoniu.oschina.io/2017/07/10/HexoAddEmoji/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;引用&quot;&gt;引用&lt;/h2&gt;
&lt;h3 id=&quot;代码引用&quot;&gt;代码引用&lt;/h3&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;
      
    
    </summary>
    
      <category term="工具" scheme="http://mxxhcm.github.io/categories/%E5%B7%A5%E5%85%B7/"/>
    
    
      <category term="markdown" scheme="http://mxxhcm.github.io/tags/markdown/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow 常见问题（不定期更新）</title>
    <link href="http://mxxhcm.github.io/2019/03/07/tensorflow-problems/"/>
    <id>http://mxxhcm.github.io/2019/03/07/tensorflow-problems/</id>
    <published>2019-03-07T06:51:01.000Z</published>
    <updated>2019-07-18T12:27:16.349Z</updated>
    
    <content type="html"><![CDATA[<h2 id="问题1-the-value-of-a-feed-cannot-be-a-tf-tensor-object">问题1-The value of a feed cannot be a tf.Tensor object</h2><h3 id="报错">报错</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">TypeError: The value of a feed cannot be a tf.Tensor object</span><br></pre></td></tr></table></figure><h3 id="问题原因">问题原因</h3><p>sess.run(op, feed_dict={})中的feed value不能是tf.Tensor类型。</p><h3 id="解决方法">解决方法</h3><p>sess.run(train, feed_dict={x:images, y:labels}的输入不能是tensor，可以使用sess.run(tensor)得到numpy.array形式的数据再喂给feed_dict。</p><blockquote><p>Once you have launched a sess, you can use your_tensor.eval(session=sess) or sess.run(your_tensor) to get you feed tensor into the format of numpy.array and then feed it to your placeholder.</p></blockquote><h2 id="问题2-could-not-create-cudnn-handle-cudnn-status-internal-error">问题2-Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR</h2><h3 id="配置">配置</h3><p>环境配置如下：</p><ul><li>Ubuntu 18.04</li><li>CUDA 10.0</li><li>CuDNN 7.4.2</li><li>Python3.7.3</li><li>Tensorflow 1.13.1</li><li>Nvidia Drivers 430.09</li><li>RTX2070</li></ul><h3 id="报错-v2">报错</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">2019-05-12 14:45:59.355405: E tensorflow/stream_executor/cuda/cuda_dnn.cc:334] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR</span><br><span class="line">2019-05-12 14:45:59.357698: E tensorflow/stream_executor/cuda/cuda_dnn.cc:334] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR</span><br><span class="line">...</span><br></pre></td></tr></table></figure><h3 id="问题原因-v2">问题原因</h3><p>GPU不够用了。</p><h3 id="解决方法-v2">解决方法</h3><p>在代码中添加下面几句：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">config = tf.ConfigProto()</span><br><span class="line">config.gpu_options.allow_growth = <span class="literal">True</span></span><br><span class="line">session = InteractiveSession(config=config)</span><br></pre></td></tr></table></figure><h2 id="问题3-libcublas-so-10-0-cannot-open-shared-object-file-no-such-file-or-directory">问题3-libcublas.so.10.0: cannot open shared object file: No such file or directory</h2><p>在命令行或者pycharm中import tensorflow报错</p><h3 id="报错-v3">报错</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ImportError: libcublas.so.10.0: cannot open shared object file: No such file or directory</span><br><span class="line">Failed to load the native TensorFlow runtime.</span><br></pre></td></tr></table></figure><h3 id="问题原因-v3">问题原因</h3><p>没有配置CUDA环境变量</p><h3 id="解决方法-v3">解决方法</h3><h4 id="命令行中">命令行中</h4><p>在.bashrc文件中加入下列语句：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export PATH=/usr/local/cuda/bin$&#123;PATH:+:$&#123;PATH&#125;&#125;</span><br><span class="line">export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$&#123;LD_LIBRARY_PATH:+:$&#123;LD_LIBRARY_PATH&#125;&#125;</span><br><span class="line">export CUDA_HOME=/usr/local/cuda</span><br></pre></td></tr></table></figure><h4 id="pycharm中">pycharm中</h4><h5 id="方法1-这种方法我没有实验成功-不知道为什么">方法1（这种方法我没有实验成功，不知道为什么）</h5><p>在左上角选中<br>File&gt;&gt;Settings&gt;&gt;Build.Execution,Deployment&gt;&gt;Console&gt;&gt;Python Console<br>在Environment下的Environment variables中添加<br>LD_LIBRARY_PATH=/usr/local/cuda/lib64:${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}即可。</p><h5 id="方法2">方法2</h5><p>修改完.bashrc文件后从终端中运行pycharm。</p><h2 id="问题4-dlerror-libcupti-so-10-0-cannot-open-shared-object-file-no-such-file-or-directory">问题4-dlerror: libcupti.so.10.0: cannot open shared object file: No such file or directory</h2><p>执行mnist_with_summary代码时报错</p><h3 id="报错-v4">报错</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">I tensorflow/stream_executor/dso_loader.cc:142] Couldn&apos;t open CUDA library libcupti.so.10.0. LD_LIBRARY_PATH: /usr/local/cuda/lib64:</span><br><span class="line">2019-05-13 23:04:10.620149: F tensorflow/stream_executor/lib/statusor.cc:34] Attempting to fetch value instead of handling error Failed precondition: could not dlopen DSO: libcupti.so.10.0; dlerror: libcupti.so.10.0: cannot open shared object file: No such file or directory</span><br><span class="line">Aborted (core dumped)</span><br></pre></td></tr></table></figure><h3 id="问题问题问题问题问题问题问题问题问题原因">问题问题问题问题问题问题问题问题问题原因</h3><p>libcupti.so.10.0包没找到</p><h3 id="解决方法-v4">解决方法</h3><p>执行以下命令，找到相关的依赖包：<br>~$:find /usr/local/cuda/ -name libcupti.so.10.0<br>输出如下：</p><blockquote><p>/usr/local/cuda/extras/CUPTI/lib64/libcupti.so.10.0</p></blockquote><p>然后修改~/.bashrc文件中相应的环境变量:<br>export LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64/😒{LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}<br>重新运行即可。</p><h2 id="问题5-unhashable-type-list">问题5-unhashable type: ‘list’</h2><p>sess.run(op, feed_dict={})中feed的数据中包含有list的时候会报错。</p><h3 id="报错-v5">报错</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">TypeError: unhashable type: &apos;list&apos;</span><br></pre></td></tr></table></figure><h3 id="问题原因-v4">问题原因</h3><p>feed_dict中不能的value不能是list。</p><h3 id="解决方法-v5">解决方法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">feed_dict = &#123;</span><br><span class="line">               placeholder : value </span><br><span class="line">                  <span class="keyword">for</span> placeholder, value <span class="keyword">in</span> zip(placeholder_list, inputs_list))</span><br><span class="line">            &#125;</span><br></pre></td></tr></table></figure><h3 id="代码示例">代码示例</h3><p><a href="https://github.com/mxxhcm/code/blob/master/tf/ops/tf_placeholder_list.py" target="_blank" rel="noopener">代码地址</a></p><h2 id="问题6-attempting-to-use-uninitialized-value">问题6-Attempting to use uninitialized value</h2><p>tf.Session()和tf.InteractiveSession()混用问题。</p><h3 id="报错-v6">报错</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensorflow.python.framework.errors_impl.FailedPreconditionError: Attempting to use uninitialized value prediction/l1/w</span><br><span class="line"> [[&#123;&#123;node prediction/l1/w/read&#125;&#125;]]</span><br><span class="line"> [[&#123;&#123;node prediction/LogSoftmax&#125;&#125;]]</span><br></pre></td></tr></table></figure><h3 id="问题原因-v5">问题原因</h3><p>声明了如下session:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sess = tf.Session()</span><br><span class="line">sess.run(tf.global_variables_initializer())</span><br></pre></td></tr></table></figure><p>在接下来的代码中，因为我声明的是tf.Session()，使用了op.eval()函数，这种用法是tf.InteractiveSession的用法，所以就相当于没有初始化。<br>result = op.eval(feed_dict={})<br>然后就报了未初始化的错误。<br>把代码改成：<br>result = sess.run([op], feeed_dct={})<br>即可，即上下文使用的session应该一致。</p><h3 id="解决方案">解决方案</h3><p>使用统一的session类型</p><h2 id="问题7-setting-an-array-element-with-a-sequence">问题7-setting an array element with a sequence</h2><p>feed_dict键值对中中值必须是numpy.ndarray，不能是其他类型。</p><h3 id="报错-v7">报错</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">value error setting an array element with a sequence,</span><br></pre></td></tr></table></figure><h3 id="问题原因-v6">问题原因</h3><p>feed_dict中key-value的value必须是numpy.ndarray，不能是其他类型，尤其不能是tf.Variable。</p><h3 id="解决方法-v6">解决方法</h3><p>检查sess.run(op, feed_dict={})中的feed_dict，确保他们的类型，不能是tf.Variable()类型的对象，需要是numpy.ndarray。</p><h2 id="问题8-访问tf-variable-的值">问题8-访问tf.Variable()的值</h2><p>如何获得tf.Variable()对象的值</p><h3 id="解决方法-v7">解决方法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">x = tf.Varialbe([<span class="number">1.0</span>, <span class="number">2.0</span>])</span><br><span class="line">sess = tf.Session()</span><br><span class="line">sess.run(tf.global_variables_initializer())</span><br><span class="line">value = sess.run(x)</span><br></pre></td></tr></table></figure><p>或者</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">x = tf.Varialbe([1.0, 2.0])</span><br><span class="line">sess = tf.InteractiveSession()</span><br><span class="line">sess.run(tf.global_variables_initializer())</span><br><span class="line">x.eval()</span><br></pre></td></tr></table></figure><h2 id="问题9-can-not-convert-a-ndarray-into-a-tensor-or-operation">问题9-Can not convert a ndarray into a Tensor or Operation</h2><h3 id="报错-v8">报错</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Can not convert a ndarray into a Tensor or Operation.</span><br></pre></td></tr></table></figure><h3 id="问题原因-v7">问题原因</h3><p>原因是sess.run()前后参数名重了，比如outputs = sess.run(outputs)，outputs本来是自己定义的一个op，但是sess.run(outputs)之后outputs就成了一个变量，就把定义的outputs op覆盖了。</p><h3 id="解决方法-v8">解决方法</h3><p>换个变量名字就行</p><h2 id="问题10-本地使用gpu-server的tensorboard">问题10-本地使用gpu server的tensorboard</h2><h3 id="问题描述">问题描述</h3><p>在gpu server跑的实验结果，然后summary的记录也在server上，但是又没办法可视化，只好在本地可视化。</p><h3 id="解决方法-v9">解决方法</h3><p>使用ssh进行映射好了。</p><h4 id="本机设置">本机设置</h4><p>~$:ssh -L 12345:10.1.114.50:6006 <a href="mailto:mxxmhh@127.0.0.1" target="_blank" rel="noopener">mxxmhh@127.0.0.1</a><br>将本机的12345端口映射到10.1.114.50的6006端口，中间服务器使用的是本机。<br>或者可以使用10.1.114.50作为中间服务器。<br>~$:ssh -L 12345:10.1.114.50:6006 <a href="mailto:liuchi@10.1.114.50" target="_blank" rel="noopener">liuchi@10.1.114.50</a><br>或者可以使用如下方法：<br>~$:ssh -L 12345:127.0.0.1:6006 <a href="mailto:liuchi@10.1.114.50" target="_blank" rel="noopener">liuchi@10.1.114.50</a><br>从这个方法中，可以看出127.0.0.1这个ip是中间服务器可以访问的ip。<br>以上三种方法中，-L后的端口号12345可以随意设置，只要不冲突即可。</p><h4 id="服务端设置">服务端设置</h4><p>然后在服务端运行以下命令：<br>~$:tensorboard --logdir logdir -port 6006<br>这个端口号也是可以任意设置的，不冲突即可。</p><h4 id="运行">运行</h4><p>然后在本机访问<br><a href="https://127.0.0.1:12345" target="_blank" rel="noopener">https://127.0.0.1:12345</a>即可。</p><h2 id="问题11-每一步summary一个list的每一个元素">问题11-每一步summary一个list的每一个元素</h2><h3 id="问题原因-v8">问题原因</h3><p>有一个tf list的placeholder，但是每一步只能生成其中的一个元素，所以怎么样summary中其中的某一个？</p><h3 id="解决方法-v10">解决方法</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">number = 3</span><br><span class="line">x_ph_list = []</span><br><span class="line">for i in range(number):</span><br><span class="line">    x_ph_list.append(tf.placeholder(tf.float32, shape=None))</span><br><span class="line"></span><br><span class="line">x_summary_list = []</span><br><span class="line">for i in range(number):</span><br><span class="line">    x_summary_list.append(tf.summary.scalar("x%s" % i, x_ph_list[i]))</span><br><span class="line"></span><br><span class="line">writer = tf.summary.FileWriter("./tf_summary/scalar_list_summary/sep")</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    scope = 10</span><br><span class="line">    inputs = np.arange(scope*number)</span><br><span class="line">    inputs = inputs.reshape(scope, number)</span><br><span class="line">    # inputs = np.random.randn(scope, number)</span><br><span class="line">    for i in range(scope):</span><br><span class="line">        for j in range(number):</span><br><span class="line">            out, xj_s = sess.run([x_ph_list[j], x_summary_list[j]], feed_dict=&#123;x_ph_list[j]: inputs[i][j]&#125;)</span><br><span class="line">            writer.add_summary(xj_s, global_step=i)</span><br></pre></td></tr></table></figure><h2 id="问题12-for-value-in-summary-value-attributeerror-list-object-has-no-attribute-value">问题12- for value in summary.value: AttributeError: ‘list’ object has no attribute ‘value’</h2><h3 id="问题描述-v2">问题描述</h3><p>writer.add_summary时报错</p><h3 id="报错-v9">报错</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">File &quot;/home/mxxmhh/anaconda3/lib/python3.7/site-packages/tensorflow/python/summary/writer/writer.py&quot;, line 127, in add_summary</span><br><span class="line">    for value in summary.value:</span><br><span class="line">AttributeError: &apos;list&apos; object has no attribute &apos;value&apos;</span><br></pre></td></tr></table></figure><h3 id="问题原因-v9">问题原因</h3><p>执行以下代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">s_ = sess.run([loss_summary], feed_dict=&#123;p_losses_ph: inputs1, q_losses_ph: inputs2&#125;)</span><br><span class="line">writer.add_summary(s_, global_step=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>因为[loss_summary]加了方括号，就把它当成了一个list。。返回值也是list，就报错了</p><h3 id="解决方法-v11">解决方法</h3><ul><li>方法1，在等号左边加一个逗号，取出list中的值</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">s_, = sess.run([loss_summary], feed_dict=&#123;p_losses_ph: inputs1, q_losses_ph: inputs2&#125;)</span><br></pre></td></tr></table></figure><ul><li>方法2，去掉loss_summary外面的中括号。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">s_ = sess.run(loss_summary, feed_dict=&#123;p_losses_ph: inputs1, q_losses_ph: inputs2&#125;)</span><br></pre></td></tr></table></figure><h2 id="问题13-tf-get-default-session-always-returns-none-type">问题13- tf.get_default_session() always returns None type:</h2><h3 id="问题描述-v3">问题描述</h3><p>调用tf.get_default_session()时，返回的是None</p><h3 id="报错-v10">报错</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">    tf.get_default_session().run(y)</span><br><span class="line">AttributeError: &apos;NoneType&apos; object has no attribute &apos;run&apos;</span><br></pre></td></tr></table></figure><h3 id="问题原因-v10">问题原因</h3><p>只有在设定default session之后，才能使用tf.get_default_session()获得当前的默认session，在我们写代码的时候，一般会按照下面的方式写：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    some operations</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure><p>这种情况下已经把tf.Session()生成的session当做了默认session，但是如果仅仅使用以下代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">sess =  tf.Session()</span><br><span class="line">sess.run(some operations)</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>是没有把tf.Session()当成默认session的，即只有在with block内，才会将这个session当做默认session。</p><h3 id="解决方案-v2">解决方案</h3><h2 id="参考文献">参考文献</h2><p>1.<a href="https://github.com/tensorflow/tensorflow/issues/4842" target="_blank" rel="noopener">https://github.com/tensorflow/tensorflow/issues/4842</a><br>2.<a href="https://github.com/tensorflow/tensorflow/issues/24496" target="_blank" rel="noopener">https://github.com/tensorflow/tensorflow/issues/24496</a><br>3.<a href="https://github.com/tensorflow/tensorflow/issues/9530" target="_blank" rel="noopener">https://github.com/tensorflow/tensorflow/issues/9530</a><br>4.<a href="https://stackoverflow.com/questions/51128427/how-to-feed-list-of-values-to-a-placeholder-list-in-tensorflow" target="_blank" rel="noopener">https://stackoverflow.com/questions/51128427/how-to-feed-list-of-values-to-a-placeholder-list-in-tensorflow</a><br>5.<a href="https://github.com/tensorflow/tensorflow/issues/11897" target="_blank" rel="noopener">https://github.com/tensorflow/tensorflow/issues/11897</a><br>6.<a href="https://stackoverflow.com/questions/34156639/tensorflow-python-valueerror-setting-an-array-element-with-a-sequence-in-t" target="_blank" rel="noopener">https://stackoverflow.com/questions/34156639/tensorflow-python-valueerror-setting-an-array-element-with-a-sequence-in-t</a><br>7.<a href="https://stackoverflow.com/questions/33679382/how-do-i-get-the-current-value-of-a-variable" target="_blank" rel="noopener">https://stackoverflow.com/questions/33679382/how-do-i-get-the-current-value-of-a-variable</a><br>8.<a href="https://blog.csdn.net/michael__corleone/article/details/79007425" target="_blank" rel="noopener">https://blog.csdn.net/michael__corleone/article/details/79007425</a><br>9.<a href="https://stackoverflow.com/questions/47721792/tensorflow-tf-get-default-session-after-sess-tf-session-is-none" target="_blank" rel="noopener">https://stackoverflow.com/questions/47721792/tensorflow-tf-get-default-session-after-sess-tf-session-is-none</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;问题1-the-value-of-a-feed-cannot-be-a-tf-tensor-object&quot;&gt;问题1-The value of a feed cannot be a tf.Tensor object&lt;/h2&gt;
&lt;h3 id=&quot;报错&quot;&gt;报错&lt;/h3&gt;

      
    
    </summary>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/categories/tensorflow/"/>
    
    
      <category term="机器学习" scheme="http://mxxhcm.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="http://mxxhcm.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/tags/tensorflow/"/>
    
      <category term="常见问题" scheme="http://mxxhcm.github.io/tags/%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>lychee图床搭建</title>
    <link href="http://mxxhcm.github.io/2019/03/04/lychee%E5%9B%BE%E5%BA%8A%E6%90%AD%E5%BB%BA/"/>
    <id>http://mxxhcm.github.io/2019/03/04/lychee图床搭建/</id>
    <published>2019-03-04T13:03:55.000Z</published>
    <updated>2019-05-06T16:22:27.708Z</updated>
    
    <content type="html"><![CDATA[<h2 id="安装">安装</h2><h3 id="基本要求">基本要求</h3><ol><li>web server (Apache, nginx, etc)</li><li>A MySQL database (MariaDB also works)</li><li>PHP 7.1 or later with the following extensions: session, exif, mbstring, gd, mysqli, json, zip, and optionally, imagick<br>~#:apt install nginx<br>~#:apt install mysql-server<br>~#:apt install php</li></ol><h3 id="配置nginx">配置nginx</h3><p>重新安装nginx出现问题，见参考文献2。</p><h3 id="配置mysql">配置mysql</h3><p>~#:mysql -u root -p<br>默认密码是回车？？？<br>修改密码<br>~#:</p><h3 id="配置php">配置php</h3><h2 id="参考文献">参考文献</h2><p>1.<a href="https://juejin.im/post/5c1b869b6fb9a049ad770424" target="_blank" rel="noopener">https://juejin.im/post/5c1b869b6fb9a049ad770424</a><br>2.<a href="https://segmentfault.com/a/1190000014027697?utm_source=tag-newest" target="_blank" rel="noopener">https://segmentfault.com/a/1190000014027697?utm_source=tag-newest</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;安装&quot;&gt;安装&lt;/h2&gt;
&lt;h3 id=&quot;基本要求&quot;&gt;基本要求&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;web server (Apache, nginx, etc)&lt;/li&gt;
&lt;li&gt;A MySQL database (MariaDB also works)&lt;/li&gt;
&lt;li
      
    
    </summary>
    
      <category term="工具" scheme="http://mxxhcm.github.io/categories/%E5%B7%A5%E5%85%B7/"/>
    
    
      <category term="图床" scheme="http://mxxhcm.github.io/tags/%E5%9B%BE%E5%BA%8A/"/>
    
  </entry>
  
  <entry>
    <title>linux-查看python package的安装位置</title>
    <link href="http://mxxhcm.github.io/2019/03/04/linux-%E6%9F%A5%E7%9C%8Bpython-package%E7%9A%84%E5%AE%89%E8%A3%85%E4%BD%8D%E7%BD%AE/"/>
    <id>http://mxxhcm.github.io/2019/03/04/linux-查看python-package的安装位置/</id>
    <published>2019-03-04T06:52:16.000Z</published>
    <updated>2019-05-12T04:03:26.986Z</updated>
    
    <content type="html"><![CDATA[<p>使用pip install package-name之后，不知道该包存在了哪个路径下。<br>可以再次使用pip install package-name，这时候就会给出该包存放在哪个路径下。</p><h2 id="参考文献">参考文献</h2><ol><li><a href="https://blog.csdn.net/weixin_41712059/article/details/82940516" target="_blank" rel="noopener">https://blog.csdn.net/weixin_41712059/article/details/82940516</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;使用pip install package-name之后，不知道该包存在了哪个路径下。&lt;br&gt;
可以再次使用pip install package-name，这时候就会给出该包存放在哪个路径下。&lt;/p&gt;
&lt;h2 id=&quot;参考文献&quot;&gt;参考文献&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a
      
    
    </summary>
    
      <category term="linux" scheme="http://mxxhcm.github.io/categories/linux/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="技巧" scheme="http://mxxhcm.github.io/tags/%E6%8A%80%E5%B7%A7/"/>
    
  </entry>
  
  <entry>
    <title>shadowsocks服务端以及客户端配置</title>
    <link href="http://mxxhcm.github.io/2019/03/04/linux-shadowsocks%E6%9C%8D%E5%8A%A1%E7%AB%AF%E4%BB%A5%E5%8F%8A%E5%AE%A2%E6%88%B7%E7%AB%AF%E9%85%8D%E7%BD%AE/"/>
    <id>http://mxxhcm.github.io/2019/03/04/linux-shadowsocks服务端以及客户端配置/</id>
    <published>2019-03-04T05:03:57.000Z</published>
    <updated>2019-06-19T03:39:56.289Z</updated>
    
    <content type="html"><![CDATA[<h2 id="服务器端配置">服务器端配置</h2><p>首先需要有一个VPS账号，vultr,digitalocean,搬瓦工等等都行。<br>首先到下面两个网站检测22端口是否开启，如果关闭的话，vps换个ip把。。<br><a href="http://tool.chinaz.com/port" target="_blank" rel="noopener">http://tool.chinaz.com/port</a><br><a href="https://www.yougetsignal.com/tools/open-ports/" target="_blank" rel="noopener">https://www.yougetsignal.com/tools/open-ports/</a></p><h3 id="启用bbr加速">启用BBR加速</h3><p>~#:apt update<br>~#:apt upgrade<br>~#:echo “net.core.default_qdisc=fq” &gt;&gt; /etc/sysctl.conf<br>~#:echo “net.ipv4.tcp_congestion_control=bbr” &gt;&gt; /etc/sysctl.conf<br>~#:sysctl -p<br>上述命令就完成了BBR加速，执行以下命令验证：<br>~#:lsmod |grep bbr<br>看到输出包含tcp_bbr就说明已经成功了。</p><h3 id="搭建shadowsocks-server">搭建shadowsocks server</h3><h4 id="安装shadowsocks-server">安装shadowsocks server</h4><p>~#:apt install python-pip<br>~#:pip install shadowsocks<br>需要说一下的是，shadowsocks目前还不支持python3.5及以上版本，上次我把/usr/bin/python指向了python3.6，就是系统默认的python指向了python3.6，然后就gg了。一定要使用Python 2.6,2.7,3.3,3.4中的一个版本才能使用。。</p><h4 id="创建shadowsocks配置文件">创建shadowsocks配置文件</h4><p>如果你的VPS支持ipv6的话，那么可以开多进程分别运行ipv4和ipv6的shadowsocks server。本地只有ipv4的话，可以用本地ipv4访问ipv6，从而访问byr等网站，但是六维空间对此做了屏蔽。如果本地有ipv6的话，还可以用本地的ipv6访问ipv6实现校园网不走ipv4流量。</p><h5 id="ipv4配置">ipv4配置</h5><p>~#:vim /etc/shadowsocks_v4.json<br>配置文件如下</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line"><span class="attr">"server"</span>:<span class="string">"0.0.0.0"</span>,</span><br><span class="line"><span class="attr">"server_port"</span>:<span class="string">"你的端口号"</span>,</span><br><span class="line"><span class="attr">"local_address"</span>:<span class="string">"127.0.0.1"</span>,</span><br><span class="line"><span class="attr">"local_port"</span>:<span class="number">1080</span>,</span><br><span class="line"><span class="attr">"password"</span>:<span class="string">"你的密码"</span>,</span><br><span class="line"><span class="attr">"timeout"</span>:<span class="number">600</span>,</span><br><span class="line"><span class="attr">"method"</span>:<span class="string">"aes-256-cfb"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="ipv6配置">ipv6配置</h5><p>~#:vim /etc/shadowsocks_v6.json<br>配置文件如下</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line"><span class="attr">"server"</span>:<span class="string">"::"</span>,</span><br><span class="line"><span class="attr">"server_port"</span>:<span class="string">"你的端口号"</span>,</span><br><span class="line"><span class="attr">"local_address"</span>:<span class="string">"127.0.0.1"</span>,</span><br><span class="line"><span class="attr">"local_port"</span>:<span class="number">1080</span>,</span><br><span class="line"><span class="attr">"password"</span>:<span class="string">"你的密码"</span>,</span><br><span class="line"><span class="attr">"timeout"</span>:<span class="number">600</span>,</span><br><span class="line"><span class="attr">"method"</span>:<span class="string">"aes-256-cfb"</span></span><br><span class="line">&#125;</span><br><span class="line">``` </span><br><span class="line">注意这两个文件的server_port一定要不同，以及双引号必须是英文引号。</span><br><span class="line">##### 1.2.2.3.手动运行shadowsocks server</span><br><span class="line">~#:ssserver -c /etc/shadowsock_v4.json -d start --pid-file ss1.pid</span><br><span class="line">~#:ssserver -c /etc/shadowsock_v6.json -d start --pid-file ss2.pid</span><br><span class="line">注意这里要给两条命令分配不同的进程号。</span><br><span class="line"></span><br><span class="line">### 设置shadowsocks server开机自启</span><br><span class="line">如果重启服务器的话，就需要重新手动执行上述命令，这里我们可以把它写成开机自启脚本。</span><br><span class="line">~#:vim /etc/init.d/shadowsocks_v4</span><br><span class="line">内容如下：</span><br><span class="line">``` shell</span><br><span class="line">#!/bin/sh</span><br><span class="line">### BEGIN INIT INFO</span><br><span class="line"># Provides:          apache2</span><br><span class="line"># Required-Start:    $local_fs $remote_fs $network $syslog</span><br><span class="line"># Required-Stop:     $local_fs $remote_fs $network $syslog</span><br><span class="line"># Default-Start:     2 3 4 5</span><br><span class="line"># Default-Stop:      0 1 6</span><br><span class="line"># Short-Description: apache2 service</span><br><span class="line"># Description:       apache2 service daemon</span><br><span class="line">### END INIT INFO</span><br><span class="line">start()&#123;</span><br><span class="line">  ssserver -c /etc/shadowsocks_v4.json -d start --pid-file ss2.pid</span><br><span class="line">&#125;</span><br><span class="line">stop()&#123;</span><br><span class="line">  ssserver -c /etc/shadowsocks_v4.json -d stop --pid-file ss2.pid</span><br><span class="line">&#125;</span><br><span class="line">case "$1" in</span><br><span class="line">start)</span><br><span class="line">  start</span><br><span class="line">  ;;</span><br><span class="line">stop)</span><br><span class="line">  stop</span><br><span class="line">  ;;</span><br><span class="line">restart)</span><br><span class="line">  stop</span><br><span class="line">  start</span><br><span class="line">  ;;</span><br><span class="line">*)</span><br><span class="line">  echo "Uasage: $0 &#123;start|reload|stop&#125;$"</span><br><span class="line">  exit 1</span><br><span class="line">  ;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure><p>~#:vim /etc/init.d/shadowsocks_v6<br>内容如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span>!/bin/sh</span><br><span class="line"><span class="meta">#</span>## BEGIN INIT INFO</span><br><span class="line"><span class="meta">#</span> Provides:          apache2</span><br><span class="line"><span class="meta">#</span> Required-Start:    $local_fs $remote_fs $network $syslog</span><br><span class="line"><span class="meta">#</span> Required-Stop:     $local_fs $remote_fs $network $syslog</span><br><span class="line"><span class="meta">#</span> Default-Start:     2 3 4 5</span><br><span class="line"><span class="meta">#</span> Default-Stop:      0 1 6</span><br><span class="line"><span class="meta">#</span> Short-Description: apache2 service</span><br><span class="line"><span class="meta">#</span> Description:       apache2 service daemon</span><br><span class="line"><span class="meta">#</span>## END INIT INFO</span><br><span class="line">start()&#123;</span><br><span class="line">  ssserver -c /etc/shadowsocks_v6.json -d start --pid-file ss1.pid</span><br><span class="line">&#125;</span><br><span class="line">stop()&#123;</span><br><span class="line">  ssserver -c /etc/shadowsocks_v6.json -d stop --pid-file ss1.pid</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line">case "$1" in</span><br><span class="line">start)</span><br><span class="line">  start</span><br><span class="line">  ;;</span><br><span class="line">stop)</span><br><span class="line">  stop</span><br><span class="line">  ;;</span><br><span class="line">restart)</span><br><span class="line">  stop</span><br><span class="line">  start</span><br><span class="line">  ;;</span><br><span class="line">*)</span><br><span class="line">  echo "Uasage: $0 &#123;start|reload|stop&#125;$"</span><br><span class="line">  exit 1</span><br><span class="line">  ;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure><p>然后执行下列命令即可：<br>~#:chmod a+x /etc/init.d/shadowsocks_v4<br>~#:chmod a+x /etc/init.d/shadowsocks_v6<br>~#:update-rc.d shadowsocks_v4 defaults<br>~#:update-rc.d shadowsocks_v6 defaults</p><p>至此，服务器端配置完成。</p><h2 id="服务端自动配置脚本">服务端自动配置脚本</h2><p><a href="https://github.com/mxxhcm/code/tree/master/shell/ss" target="_blank" rel="noopener">地址</a><br>首先将该文件中所有文件复制到vps上，然后执行<br>~#:sh install_ss_server.sh<br>即可</p><h3 id="补充说明">补充说明</h3><p>该文件夹共包含五个文件<br>shadowsocks_v4.json为ipv4 ss配置文件，可根据自己的需要修改端口号和密码<br>shadowsocks_v6.json为ipv6 ss配置文件，可根据自己的需要修改端口号和密码<br>shadowsocks_v4为ipv4 ss自启动文件，无需修改<br>shadowsocks_v6为ipv6 ss自启动文件，无需修改<br>install_ss_server.sh为安装脚本，该脚本同时配置ipv4和ipv6 ss server。可根据自己需要自行选择。</p><h2 id="客户端配置">客户端配置</h2><h3 id="windows客户端配置">Windows客户端配置</h3><h4 id="安装shadowsock客户端">安装shadowsock客户端</h4><p>到该网址 <a href="https://github.com/shadowsocks/shadowsocks-windows/releases" target="_blank" rel="noopener">https://github.com/shadowsocks/shadowsocks-windows/releases</a> 下载相应的windows客户端程序。<br>然后配置服务器即可～</p><h3 id="linux客户端配置">Linux客户端配置</h3><h4 id="安装shadowsocks程序">安装shadowsocks程序</h4><p>~$:sudo pip install shadowsocks</p><h4 id="运行shadowsocks客户端程序">运行shadowsocks客户端程序</h4><p>~$:sudo vim /etc/shadowsocks.json<br>填入以下配置文件<br>{<br>“server”:“填上自己的shadowsocks server ip地址”,<br>“server_port”:“8888”,//填上自己的shadowsocks server 端口&quot;<br>“local_port”:1080,<br>“password”:“mxxhcm150929”,<br>“timeout”:600,<br>“method”:“aes-256-cfb”<br>}</p><p>接下来可以执行以下命令运行shadowsocks客户端：<br>~$:sudo sslocal -c /etc/shadowsocks.json<br>然后报错：</p><blockquote><p>INFO: loading config from /etc/shadowsocks.json<br>2019-03-04 14:37:49 INFO     loading libcrypto from libcrypto.so.1.1<br>Traceback (most recent call last):<br>File “/usr/local/bin/sslocal”, line 11, in <module><br>load_entry_point(‘shadowsocks==2.8.2’, ‘console_scripts’, ‘sslocal’)()<br>File “/usr/local/lib/python2.7/dist-packages/shadowsocks/local.py”, line 39, in main<br>config = shell.get_config(True)<br>File “/usr/local/lib/python2.7/dist-packages/shadowsocks/shell.py”, line 262, in get_config<br>check_config(config, is_local)<br>File “/usr/local/lib/python2.7/dist-packages/shadowsocks/shell.py”, line 124, in check_config<br>encrypt.try_cipher(config[‘password’], config[‘method’])<br>File “/usr/local/lib/python2.7/dist-packages/shadowsocks/encrypt.py”, line 44, in try_cipher<br>Encryptor(key, method)<br>File “/usr/local/lib/python2.7/dist-packages/shadowsocks/encrypt.py”, line 83, in <strong>init</strong><br>random_string(self._method_info[1]))<br>File “/usr/local/lib/python2.7/dist-packages/shadowsocks/encrypt.py”, line 109, in get_cipher<br>return m[2](method, key, iv, op)<br>File “/usr/local/lib/python2.7/dist-packages/shadowsocks/crypto/openssl.py”, line 76, in <strong>init</strong><br>load_openssl()<br>File “/usr/local/lib/python2.7/dist-packages/shadowsocks/crypto/openssl.py”, line 52, in load_openssl<br>libcrypto.EVP_CIPHER_CTX_cleanup.argtypes = (c_void_p,)<br>File “/usr/lib/python2.7/ctypes/<strong>init</strong>.py”, line 379, in <strong>getattr</strong><br>func = self.<strong>getitem</strong>(name)<br>File “/usr/lib/python2.7/ctypes/<strong>init</strong>.py”, line 384, in <strong>getitem</strong><br>func = self._FuncPtr((name_or_ordinal, self))<br>AttributeError: /usr/lib/x86_64-linux-gnu/libcrypto.so.1.1: undefined symbol: EVP_CIPHER_CTX_cleanup</module></p></blockquote><p>按照参考文献4的做法，是在openssl 1.1.0版本中放弃了EVP_CIPHER_CTX_cleanup函数</p><blockquote><p>EVP_CIPHER_CTX was made opaque in OpenSSL 1.1.0. As a result, EVP_CIPHER_CTX_reset() appeared and EVP_CIPHER_CTX_cleanup() disappeared.<br>EVP_CIPHER_CTX_init() remains as an alias for EVP_CIPHER_CTX_reset().</p></blockquote><p>将openssl库中的EVP_CIPHER_CTX_cleanup改为EVP_CIPHER_CTX_reset即可。<br>再次执行以下命令，查看shadowsocks安装位置<br>~#:pip install shadowsocks<br>Requirement already satisfied: shadowsocks in /usr/local/lib/python2.7/dist-packages<br>~#:cd /usr/local/lib/python2.7/dist-packages/shadowsocks<br>~#:vim crypto/openssl.py<br>搜索cleanup，将其替换为reset<br>具体位置在第52行libcrypto.EVP_CIPHER_CTX_cleanup.argtypes = (c_void_p,)和第111行libcrypto.EVP_CIPHER_CTX_cleanup(self._ctx)</p><h4 id="手动运行后台挂起">手动运行后台挂起</h4><p>将所有的log重定向到~/.log/sslocal.log文件中<br>~$:mkdir ~/.log<br>~$:touch ~/.log/ss-local.log<br>~$:nohup sslocal -c /etc/shadowsocks_v6.json &lt;/dev/null &amp;&gt;&gt;~/.log/ss-local.log &amp;</p><h4 id="开机自启shadowsocks-client">开机自启shadowsocks client</h4><p>但是这样子的话，每次开机都要重新运行上述命令，太麻烦了。可以写个开机自启脚本。执行以下命令：<br>~$:sudo vim /etc/init.d/shadowsocks<br>内容为以下shell脚本</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span>!/bin/sh</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span>## BEGIN INIT INFO</span><br><span class="line"><span class="meta">#</span> Provides:          shadowsocks local</span><br><span class="line"><span class="meta">#</span> Required-Start:    $local_fs $remote_fs $network $syslog</span><br><span class="line"><span class="meta">#</span> Required-Stop:     $local_fs $remote_fs $network $syslog</span><br><span class="line"><span class="meta">#</span> Default-Start:     2 3 4 5</span><br><span class="line"><span class="meta">#</span> Default-Stop:      0 1 6</span><br><span class="line"><span class="meta">#</span> Short-Description: shadowsocks service</span><br><span class="line"><span class="meta">#</span> Description:       shadowsocks service daemon</span><br><span class="line"><span class="meta">#</span>## END INIT INFO</span><br><span class="line">start()&#123;</span><br><span class="line">　　  sslocal -c /etc/shadowsocks.json -d start</span><br><span class="line">&#125;</span><br><span class="line">stop()&#123;</span><br><span class="line">　　  sslocal -c /etc/shadowsocks.json -d stop</span><br><span class="line">&#125;</span><br><span class="line">case “$1” in</span><br><span class="line">start)</span><br><span class="line">　　　start</span><br><span class="line">　　　;;</span><br><span class="line">stop)</span><br><span class="line">　　　stop</span><br><span class="line">　　　;;</span><br><span class="line">reload)</span><br><span class="line">　　　stop</span><br><span class="line">　　　start</span><br><span class="line">　　　;;</span><br><span class="line">\*)</span><br><span class="line">　　　echo “Usage: $0 &#123;start|reload|stop&#125;”</span><br><span class="line">　　　exit 1</span><br><span class="line">　　　;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure><p>然后执行以下命令即可：<br>~$:sudo chomod a+x /etc/init.d/shadowsocks<br>~$:sudo update_rc.d shadowsocks defaults<br>上述命令执行完成以后，进行测试<br>~$:sudo service shadosowcks start</p><h4 id="配置代理">配置代理</h4><p>上一步的目的是建立了shadowsocks服务的本地客户端，socks5流量会走该通道，但是浏览器的网页的流量是https的，我们需要配置相应的代理，将https流量转换为socks5流量，走ss客户端到达ss服务端。当然，也可以把其他各种流量，如tcp,udp等各种流量都转换为socks5流量，这个可以通过全局代理实现，也可以通过添加特定的代理规则实现。</p><h5 id="配置全局代理">配置全局代理</h5><p>如下图所示，添加ubuntu socks5系统代理：</p><p>然后就可以成功上网了。</p><h5 id="使用switchyomega配置chrome代理">使用SwitchyOmega配置chrome代理</h5><p>首先到 <a href="https://github.com/FelisCatus/SwitchyOmega/releases" target="_blank" rel="noopener">https://github.com/FelisCatus/SwitchyOmega/releases</a> 下载SyitchyOmega.crx。然后在chrome的地址栏输入chrome://extensions，将刚才下载的插件拖进去。<br>然后在浏览器右上角就有了这个插件，接下来配置插件。如下图：<br><img src="https:" alt="mxx"><br>直接配置proxy，添加如图所示的规则，这样chrome打开的所有网站都是走代理的。</p><h4 id="使用privoxy让terminal走socks5">使用privoxy让terminal走socks5</h4><p>~$:sudo apt install privoxy<br>~$:sudo vim /etc/privoxy/config<br>取消下列行的注释，或者添加相应条目<br>forward-socks5 / 127.0.0.1:1080 . # SOCKS5代理地址<br>listen-address 127.0.0.1:8118     # HTTP代理地址<br>forward 10.*.*.*/ .               # 内网地址不走代理<br>forward .abc.com/ .             # 指定域名不走代理<br>重启privoxy服务<br>~$:sudo service privoxy restart<br>在bashrc中添加如下环境变量<br>export http_proxy=&quot;<a href="http://127.0.0.1:8118" target="_blank" rel="noopener">http://127.0.0.1:8118</a>&quot;<br>export https_proxy=“<a href="http://127.0.0.1:8118" target="_blank" rel="noopener">http://127.0.0.1:8118</a>”</p><p>~$:source ~/.bashrc<br>~$:curl.gs</p><h2 id="参考文献">参考文献</h2><ol><li><a href="http://godjose.com/2017/06/14/new-article/" target="_blank" rel="noopener">http://godjose.com/2017/06/14/new-article/</a></li><li><a href="https://www.polarxiong.com/archives/%E6%90%AD%E5%BB%BAipv6-VPN-%E8%AE%A9ipv4%E4%B8%8Aipv6-%E4%B8%8B%E8%BD%BD%E9%80%9F%E5%BA%A6%E6%8F%90%E5%8D%87%E5%88%B0100M.html" target="_blank" rel="noopener">https://www.polarxiong.com/archives/搭建ipv6-VPN-让ipv4上ipv6-下载速度提升到100M.html</a></li><li><a href="https://blog.csdn.net/li1914309758/article/details/86510127" target="_blank" rel="noopener">https://blog.csdn.net/li1914309758/article/details/86510127</a></li><li><a href="https://blog.csdn.net/blackfrog_unique/article/details/60320737" target="_blank" rel="noopener">https://blog.csdn.net/blackfrog_unique/article/details/60320737</a></li><li><a href="https://blog.csdn.net/qq_31851531/article/details/78410146" target="_blank" rel="noopener">https://blog.csdn.net/qq_31851531/article/details/78410146</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;服务器端配置&quot;&gt;服务器端配置&lt;/h2&gt;
&lt;p&gt;首先需要有一个VPS账号，vultr,digitalocean,搬瓦工等等都行。&lt;br&gt;
首先到下面两个网站检测22端口是否开启，如果关闭的话，vps换个ip把。。&lt;br&gt;
&lt;a href=&quot;http://tool.c
      
    
    </summary>
    
      <category term="linux" scheme="http://mxxhcm.github.io/categories/linux/"/>
    
    
      <category term="工具" scheme="http://mxxhcm.github.io/tags/%E5%B7%A5%E5%85%B7/"/>
    
      <category term="linux" scheme="http://mxxhcm.github.io/tags/linux/"/>
    
      <category term="shadowsocks" scheme="http://mxxhcm.github.io/tags/shadowsocks/"/>
    
  </entry>
  
  <entry>
    <title>DQN</title>
    <link href="http://mxxhcm.github.io/2019/03/02/dqn/"/>
    <id>http://mxxhcm.github.io/2019/03/02/dqn/</id>
    <published>2019-03-02T11:29:35.000Z</published>
    <updated>2019-06-01T02:17:51.892Z</updated>
    
    <content type="html"><![CDATA[<h2 id="dqn">DQN</h2><p>论文名称<a href="https://arxiv.org/pdf/1312.5602.pdf" target="_blank" rel="noopener">Playing Atari with Deep Reinforcement Learning</a></p><h3 id="概述">概述</h3><p>用一个CNN表示值函数，直接从高维的输入中学习控制策略。用Q-learning的变种来训练这个CNN。网络的输入是原始的图片，输出是图片对应的state可能采取的action的$Q$值。<br>Atari 2600是一个RL的benchmark，有2600个游戏，每个agent会得到一个图像输入(210 x 160 RGB视频60Hz)。本文的目标是设计一个NN架构尽可能学会更多游戏，网络的输入只有视频信息，reward和terminal信号以及可能采取的action，和人类玩家得到的信息是一模一样的，当然是计算机能看得懂的信号。</p><h3 id="需要解决的问题">需要解决的问题</h3><ol><li>大量有标记的训练数据。</li><li>强化学习需要从一个稀疏的，有噪音的，通常是time-delayed(延迟的)标量信号中学习。这个delay存在于action和reward之间，而且可以达到几千个timesteps那么远，和supervised learnign中输入和输入之间直接的关系相比要复杂的多。</li><li>大多数深度学习算法假设样本之间都是独立的，然而强化学习的一个sequence(序列)通常是高度相关的。</li><li>强化学习算法学习到的policy变化时，数据服从的分布通常会改变，然而深度学习通常假设数据服从一个固定的分布。</li></ol><h3 id="解决方案">解决方案</h3><h4 id="背景">背景</h4><ol><li>agent与Atari模拟器不断交互，agent不能观测到模拟器的内部状态，只能得到当前屏幕信息的一个图片。这个task可以认为是部分可观测的，因为仅仅从当前的屏幕图像$x_t$上是不能完全理解整个游戏状况的。所有的序列都认为在有限步骤内是会结束的。</li><li>注意agent当前的得分取决于整个sequence的action和observation。一个action的feedback可能等到好几千个timesteps之后才能得到。</li><li>agent的目标是通过采取action和env交互最大化累计reward。定义$t$时刻的回报return为$R_t = \sum^T_{t’=t} \gamma^{t’-t}r_{t’}$，其中$\gamma$是折扣因子，$T$是游戏终止的时间步。</li><li>定义最优的动作值函数$Q^{*}(s,a)$是遵循最优策略在状态$s$处采取动作$a$能获得的最大的期望回报，$Q^{*}(s,a) = max_{\pi}E[R_t|s_t=s,a_t=a,\pi]$。</li><li>最优的动作值函数遵循Bellman optimal equation。如果在下个时间步的状态$s’$处，对于所有可能的$a’$，$Q^{*}(s’,a’)$的最优值是已知的（这里就是对于每一个$a’$，都会有一个最优的$Q(s’,a’)$，最优的策略就是选择最大化$r+Q^{*}(s’,a’)$的动作$a’$：<br>$$Q^{*}(s,a) = E_{s\sim E}[r+ \gamma max_{a’} Q^{*}(s’,a’)|s,a], \tag{1}$$<br>强化学习的一个思路就是使用Bellman optimal equation更新动作值函数，$Q_{i+1}(s,a) = E[r + \gamma Q_i(s’,a’)|s,a]$，当$i\rightarrow \infty$时，$Q_i \rightarrow Q^{*}$。</li><li>上述例子是state-action pair很少的情况，当有无穷多个的时候，是无法精确计算的。这时候可以采用函数来估计动作值函数，$Q(s,a;\theta) \approx Q^{*}(s,a)$。一般来说，通常采用线性函数进行估计，当然可以采用非线性的函数，如神经网络等等。这里采用的是神经网络，用$\theta$表示网络的参数，这个网络叫做Q网络，Q网络通过最小化下列loss进行训练：<br>$$L_i(\theta_i) = E_{s,a\sim \rho(\cdot)}\left[(y_i - Q(s,a;\theta_i))^2\right]\tag{2}$$<br>其中$y_i = E_{s’\sim E}[r+\gamma max_{a’}Q(s’,a’;\theta_{i-1})]$是第$i$次迭代的target值，其中$\rho(s,a)$是$(s,a)$服从的概率分布。</li><li>注意在优化$L_i(\theta_i)$时，上一次迭代的$\theta_{i-1}$是不变的，target取决于网络参数，和监督学习作对比，监督学习的target和网络参数无关。</li><li>对Loss函数进行求导，得到下列的gradient信息：<br>$$\nabla_{\theta_i}L_i(\theta_i) = E_{s,a\sim \rho(\cdot),s’\sim E}\left[(r+\gamma max_{a’}Q(s’,a’;\theta_{i-1})-Q(s,a;\theta_i))\nabla_{\theta_i}Q(s,a;\theta_i)\right]\tag{3}$$<br>通过SGD优化loss函数。如果权重是每隔几个timestep进行更新，并且用从分布$\rho$和环境$E$中采样得到的样本取代期望，就可以得到熟悉的Q-learning算法[2]。(这个具体为什么是这样，我也不清楚，可以看参考文献2)</li><li>dqn是Model-Free的，它直接从环境$E$中采样，并没有显式的对环境进行建模。</li><li>dqn是一个online的方法，即训练数据不断增加。offline是训练数据固定。</li><li>dqn是一个off-policy算法，target policy 是greedy policy，behaviour policy是$\varepsilon$ greedy policy，target policy和greedy policy策略不同。</li></ol><blockquote><p>On-policy methods attempt to evaluate or improve the policy that is used to make decisions, whereas  off-policy methods evaluate or improve a policy different from that used to generate the data.</p></blockquote><p>Sarsa和Q-learning的区别在于更新Q值时的target policy和behaviour policy是否相同。其实就是policy evaluation和value iteration的区别，policy evaluation使用动态规划算法更新$V(s)$，但是并没有改变行为策略，更新迭代用的数据都是利用之前的行为策略生成的。而值迭代是policy evaluation+policy improvement，每一步都用贪心策略选择出最大的$a$更新$V(s)$，target policy（greedy）和behaviour policy（$\varepsilon$-greedy）是不同的。</p><h4 id="创新和技巧">创新和技巧</h4><ol><li>DQN使用了experience replay，将多个episodes中的经验存储到一个大小为$N$的replay buffer中。在更新$Q$值的时候，从replay buffer中进行采样更新。behaviour policy是$\varepsilon$-greedy策略，保持探索。target policy是$\varepsilon$ greedy 算法，因为replay buffer中存放的都是behaviour policy生成的experience，所以是off-policy算法。<br>采用experience replay的online算法[5]和标准的online算法相比有三个好处[4]，第一个是每一个experience可以多次用来更新参数，提高了数据训练效率；第二个是直接从连续的样本中进行学习是低效的，因为样本之间存在强关联性。第三个是on-policy的学习中，当前的参数决定下一次采样的样本，就可能使学习出来的结果发生偏移。</li><li>replay buffer中只存储最近N个experience。</li><li>原始图像是$210\times 160$的RGB图像，预处理首先将它变为灰度图，并进行下采样得到一个$110\times 84$的图像，然后从这个图像中截取一个$84\times 84$的图像。</li><li>作者使用预处理函数$\phi$处理连续四张的图像而不是一张，然后将这个预处理后的结果输入$Q$函数。</li><li>预处理函数$\phi$是一个卷积神经网络，输入是$84\times 84\times 4$的图像矩阵，经过$16$个stride为$4$的$8\times 8$filter，经过relu激活函数，再经过$32$个stride为$2$的$4\times 4$filter，经过relu激活函数，最后接一个256个单元的全连接层。输出层的大小根据不同游戏的动作个数决定。</li><li>$Q$网络的输入是预处理后的图像state，输出是所有当前state可能采取的action的$Q$值。</li><li>DQN是不收敛的。</li></ol><h3 id="伪代码">伪代码</h3><p>算法 1 Deep Q-learning with Experience Replay<br>Initialize replay memory D to capacity N<br>Initialize action-value function Q with random weights<br>for episode = $1, M$ do<br>$\ \ \ \ \ \ \ \ $Initialize sequence $s_1 = {x_1}$ and preprocessed sequenced $\phi_1 = \phi(s_1)$<br>$\ \ \ \ \ \ \ \ $for $t = 1,T$ do<br>$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $With probability $\varepsilon$ select a random action $a_t$<br>$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $otherwise select $a_t = max_a Q^{∗}(\phi(s_t), a; θ)$<br>$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $Execute action $a_t$ in emulator and observe reward $r_t$ and image $x_{t+1}$<br>$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $Set $s_{t+1} = s_t, a_t, x_{t+1}$ and preprocess $\phi_{t+1} = \phi(s_{t+1})$<br>$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $Store transition $(\phi_t, a_t, r_t, \phi_{t+1})$ in D<br>$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $Sample random minibatch of transitions $(\phi_j, a_j, r_j, \phi_{j+1})$ from D<br>$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $Set $y_j = \begin{cases}r_j&amp;\ \ \ \ for\ terminal\ \phi_{j+1}\\r_j+\gamma max_{a’}Q(\phi_{j+1},a’|\theta)&amp;\ \ \ \ for\ non-terminal\ \phi_{j+1}\end{cases}$<br>$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $Perform a gradient descent step on $(y_j − Q(\phi_j, a_j|θ))^2$<br>$\ \ \ \ \ \ \ \ $end for<br>end for</p><h3 id="实验">实验</h3><h4 id="datasets">Datasets</h4><p>七个Atari 2600 games: B.Rider, Breakout, Enduro, Pong, Q bert, Seaquest, S.Invaders。<br>在六个游戏上DQN的表现超过了之前所有的方法，在三个游戏上DQN的表现超过了人类。</p><h4 id="settings">Settings</h4><ol><li>不同游戏的reward变化很大，这里把正的reward全部设置为$1$，把负的reward全部设置为$-1$，reward为$0$的保持不变。这样子在不同游戏中也可以统一学习率。</li><li>采用RMSProp优化算法，batchsize为$32$，behaviour policy采用的是$\varepsilon$-greedy，在前$100$万步内，$\varepsilon$从$1$变到$0.1$，接下来保持不变。</li><li>使用了跳帧技术，每隔$k$步，agent才选择一个action，在中间的$k-1$步中，保持原来的action不变。这里选择了$k=4$，有的游戏设置的为$k=3$。</li></ol><h4 id="metrics">Metrics</h4><h5 id="average-reward">average reward</h5><p>第一个metric是在一个episode或者一次游戏内total reward的平均值。这个metric带有很大噪音，因为policy权值一个很小的改变可能就会对policy访问states的分布造成很大的影响。</p><h5 id="action-value-function">action value function</h5><p>第二个metric是估计的action-value function，这里作者的做法是在训练开始前使用random policy收集一个固定的states set，然后track这个set中states最大预测$Q$值的平均。</p><h4 id="baselines">Baselines</h4><ol><li>Sarsa</li><li>Contingency</li><li>DQN</li><li>Human</li></ol><h3 id="代码">代码</h3><p><a href="https://github.com/devsisters/DQN-tensorflow" target="_blank" rel="noopener">https://github.com/devsisters/DQN-tensorflow</a></p><h2 id="nature-dqn">Nature DQN</h2><h3 id="非线性拟合函数不收敛的原因">非线性拟合函数不收敛的原因</h3><ol><li>序列中状态的高度相关性。</li><li>$Q$值的一点更新就会对policy改变造成很大的影响，从而改变数据的分布。</li><li>待优化的$Q$值和target value(目标Q值)之间的关系，每次优化时的目标Q值都是固定上次的参数得来的，优化目标随着优化过程一直在变。<br>前两个问题是通过DQN中提出的replay buffer解决的，第三个问题是Natura DQN中解决的，在一定时间步内，固定target network参数，更新待network的参数，然后每隔固定步数将network的参数拷贝给target network。</li></ol><blockquote><p>This instability has several causes: the correlations present in the sequence of observations, the fact that small updates to Q may significantly change the policy and therefore change the data distribution, and the correlations between the action-values (Q) and the target values $r+\gamma max_{a’}Q(s’,a’)$.<br>We address these instabilities with a novel variant of Q-learning, which uses two key ideas. First, we used a biologically inspired mechanism termed experience replay that randomizes over the data, thereby removing correlations in the observation sequence and smoothing over changes in the data distribution. Second, we used an iterative update that adjusts the action-values (Q) towards target values that are only periodically updated, thereby reducing correlations with the target.</p></blockquote><h3 id="nature-dqn改进">Nature DQN改进</h3><ol><li>预处理的结构变了,CNN的层数增加了一层，</li><li>加了target network，</li><li>将error限制在$[-1,1]$之间。</li></ol><blockquote><p>clip the error term from the update $r + \gamma max_{a’} Q(s’,a’;\theta_i^{-} - Q(s,a;\theta_i)$ to be between $-1$ and $1$. Because the absolute value loss function $|x|$ has a derivative of $-1$ for all negative values of $x$ and a derivative of $1$ for all positive values of $x$, clipping the squared error to be between $-1$ and $1$ corresponds to using an absolute value loss function for errors outside of the $(-1,1)$ interval.</p></blockquote><h3 id="框架">框架</h3><p>DNQ的框架如下所示<br><img src="/2019/03/02/dqn/nature-dqn.png" alt="ndqn"></p><h3 id="伪代码-v2">伪代码</h3><p>算法 2 deep Q-learning with experience replay, target network<br>Initialize replay memory D to capacity N<br>Initialize action-value function Q with random weights $\theta$<br>Initialize target action-value function $\hat{Q}$ with weights $\theta^{-}=\theta$<br>for episode = $1, M$ do<br>$\ \ \ \ \ \ \ \ $Initialize sequence $s_1 = {x_1}$ and preprocessed sequenced $\phi_1 = \phi(s_1)$<br>$\ \ \ \ \ \ \ \ $for $t = 1,T$ do<br>$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $With probability $\varepsilon$ select a random action $a_t$<br>$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $otherwise select $a_t = max_a Q^{∗}(\phi(s_t), a; θ)$<br>$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $Execute action $a_t$ in emulator and observe reward $r_t$ and image $x_{t+1}$<br>$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $Set $s_{t+1} = s_t, a_t, x_{t+1}$ and preprocess $\phi_{t+1} = \phi(s_{t+1})$<br>$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $Store transition $(\phi_t, a_t, r_t, \phi_{t+1})$ in D<br>$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $Sample random minibatch of transitions $(\phi_j, a_j, r_j, \phi_{j+1})$ from D<br>$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $Set $y_j = \begin{cases}r_j&amp;\ \ \ \ for\ terminal\ \phi_{j+1}\\r_j+\gamma max_{a’}Q(\phi_{j+1},a’|\theta^{-})&amp;\ \ \ \ for\ non-terminal\ \phi_{j+1}\end{cases}$<br>$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $Perform a gradient descent step on $(y_j − Q(\phi_j, a_j|θ))^2$ with respect to the network parameters $\theta$<br>$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $Every $C$ steps reset $\hat{Q} = Q$<br>$\ \ \ \ \ \ \ \ $end for<br>end for</p><h2 id="double-dqn">Double DQN</h2><h3 id="目的">目的</h3><p>解决overestimate问题，Q-learning中在estimated values上进行了max操作，可能会导致某些更偏爱overestimated value而不是underestimated values。<br>本文将Double Q-learning的想法推广到了dqn上形成了double-dqn。实验结果表明了overestimated value对于policy有影响，double 会产生好的action value，同时在一些游戏上会得到更高的scores。</p><h3 id="contributions">Contributions</h3><ol><li>解释了在large scale 问题上，Q-learning被overoptimistic的原因是学习固有的estimation errors。</li><li>overestimation在实践中是很常见，也很严重的。</li><li>Double Q-learning可以减少overoptimism</li><li>提出了double-dqn。</li><li>double-dqn在某些游戏上可以找到更好的policy。</li></ol><h3 id="double-q-learning">Double Q-learning</h3><p>Q-learning算法计算target value $y$的公式如下：<br>$$y = r + \gamma max_a’ Q(s’, a’|\theta_t)\tag{4}$$<br>在计算target value的时候，使用同一个网络选择和评估action $a’$，这可能会让网络选择一个overestimated value，最后得到一个overoptimistic value estimate。所有就有了double Q-learning，计算公式如下：<br>$$y = r + \gamma Q(s’, argmax_a’ Q(s’,a;\theta_t);\theta’_t)\tag{5}$$<br>target policy还是greedy policy，通过使用$\theta$对应的网络选择action，然后在计算target value的时候使用$\theta’$对应的网络。<br>原有的公式可以写成下式，<br>$$y = r + \gamma Q(s’, argmax_a’ Q(s’,a;\theta_t);\theta_t)\tag{6}$$<br>即选择action和计算target value都是使用的同一个网络。</p><h3 id="double-dqn-v2">Double dqn</h3><p><img src="/2019/03/02/dqn/double-dqn.png" alt="double-dqn"><br>Double Q-learnign的做法是分解target action中的max opearation为选择和evaluation。而在Nature-dqn中，提出了target network，所以分别使用network和target network去选择和evaluation action是一个很好的做法，这样子公式就变成了<br>$$y = r + \gamma Q(s’, argmax_a’ Q(s’,a;\theta_t);\theta^{-}_t)\tag{7}$$<br>和Q-learnign相比，将$\theta’$换成了$\theta^{-}$ evaluate action，target network的更新和nature-dqn一样，过一段时间复制network的参数。</p><h3 id="double-q-learning-vs-q-learning">Double Q learning vs Q-learning</h3><p>可以在数学上证明，Q-learning是overestimation的，但是double q leraing是无偏的。。。证明留待以后再说。<br><todo><br><to do></to></todo></p><h3 id="伪代码-v3">伪代码</h3><p>算法 3: Double DQN Algorithm.<br>输入: replay buffer $D$, 初始network参数$\theta$,target network参数$\theta^{-}$<br>输入 : replay buffer的大小$N_r$, batch size $N_b$, target network更新频率$N^{-}$<br><strong>for</strong> episode $e \in {1, 2,\cdots, M}$ do<br>$\qquad$初始化frame sequence $\mathbf{x} \leftarrow ()$<br>$\qquad$<strong>for</strong> $t \in {0, 1, \cdots}$ do<br>$\qquad\qquad$设置state $s \leftarrow \mathbf{x}$, 采样 action $a \sim\pi_B$<br>$\qquad\qquad$给定$(s, a)$，从环境$E$中采样接下来的frame $x_t$,接收reward $r$,在序列$\mathbf{x}$上拼接$x$<br>$\qquad\qquad$<strong>if</strong> $|\mathbf{x}| \gt N_f$<br>$\qquad\qquad$<strong>then</strong><br>$\qquad\qquad\qquad$从$\mathbf{x}$中删除最老的frame $x_{t_min}$<br>$\qquad\qquad$设置$s’ \leftarrow \mathbf{x}$,添加transition tuple (s, a, r, s 0 ) 到buffer D中，如果$|D| \ge N_r$替换最老的tuple<br>$\qquad\qquad$采样$N_b$个tuples $(s, a, r, s’) \sim Unif(D)$<br>$\qquad\qquad$计算target values, one for each of $N_b$ tuples:<br>$\qquad\qquad$定义$a^{max}(s’; \theta) = arg max_{a’} Q(s’, a’;\theta)$<br>$\qquad\qquad y_j = \begin{cases}r&amp;\qquad if\ \ s’\ \ is\ \ terminal\\ r+\gamma Q(s’, a^{max}(s’;\theta);\theta^{-}, &amp;\qquad otherwise\end{cases}$<br>$\qquad\qquad$利用loss $||y_j − Q(s, a; \theta)||^2$的梯度更新<br>$\qquad\qquad$每隔$N^{-}$个步骤更新一下target network 参数$\theta^{-}$<br>$\qquad$<strong>end</strong><br><strong>end</strong></p><h3 id="实验-v2">实验</h3><p>提出了一个指标，normalized score，计算公式如下：<br>$$score_{normalized} = \frac{score_{agent}- score_{random}}{score_{human}-score_{random}}\tag{8}$$<br>分母是human和random之差，对应$100%$。</p><h2 id="prioritized-dqn-per">Prioritized DQN(PER)</h2><h3 id="contributions-v2">contributions</h3><p>本文提出一种了proritizing experience的框架，在训练过程中多次使用重要的transtions replay进行更新，让训练变得的更有效率。<br>使用TD-errors作为prioritization mechanism，给出了两种protitization计算方式，提出了一种stochastic prioritization以及importance sampling方法。</p><h3 id="prioritized-replay">Prioritized replay</h3><p>可以从两个维度上考虑replay memeory的改进，一个是存哪些experiences，一个是使用哪些experiences进行回放。本文是从第二个方向上进行的考虑。</p><p>从buffer中随机抽样的方法中，update steps和memory size是线性关系，作者想找一个update steps和memory size是log关系的oracle，但是很遗憾，这是不现实的，所以作者想要找一种比uniform random replay好尽量接近oracle的方法。</p><h4 id="prioritizion-with-td-error">Prioritizion with TD-error</h4><p>prioritized replay最重要的部分是如何评价每一个transition的重要程度。一个理想的criterion是agent在当前的state可以从某个transition中学到多少。这个measure metric是不确定的，一个替代方案是使用TD error $\delta$，表示how ‘suprising’ 或者upexpected the transition：就是当前的value离next-step bootstrap得到的value相差多少，booststrap就是基于其他估计值进行计算。。这中方法对于incremental,online RL方法，例如SARSA以及Q-learning来说都是很合适的，因为他们会计算TD-error，然后给TD-error一个比例系数用来更新参数。然后当reward是noisy的时候，TD-error效果可能很差。<br>作者在一个人工设计的环境中使用了greedy TD-error prioritization算法，算法在每次存transition到replay buffer的时候，同时还会存一下该transition最新的TD-error，然后在更新的时候从memory中选择TD-error最大的transition。最新的transition TD-error没有算出来，就给它一个最大的priority，保证所有的experience都至少被看到过一次。<br>采用二叉堆用实现优先队列，查找复杂度是$O(1)$，更新priorities的复杂度是$O(logN)$。</p><h4 id="stochastic-prioritization">Stochastic prioritization</h4><p>上述方法有很多问题。第一，每次都sweep整个replay memory的计算量很大，所以只有被replayed的experiences的TD-errors才会被更新。开始时一个TD error很小的transition可能很长一段事件不会被replayed，这就导致了replay buffer的sliding window不起作用了。第二，TD-error对于noise spike很敏感，还会被bootstrap加剧，估计误差可能会是另一个noise。第三，greedy prioritization集中在experiences的一个subset：errors减小的很慢，尤其是使用function appriximation时，这就意味着初始的高error的transitions会被replayed的很频繁，然后会over-fitting因为缺乏diversity。<br>为了解决这些问题，引入了一个介于pure greedy prioritizaiton以及uniform random sampling之间的stochastic采样方法，priority高的transition有更大的概率被采样，而lowest-priority的transition也有概率被选中，具体的定义transition $i$的概率如下：<br>$$P(i) = \frac{p_i^{\alpha}}{\sum_kp_k^{\alpha}}\tag{9}$$<br>$\alpha$确定prioritizaiton的比重，如果$\alpha=0$就是unifrom。</p><p>有两种$p_i$的计算方法，一种是直接的proportional prioritization，$p_i = |\delta_i| + \varepsilon$，其中$\varepsilon$是一个小的正整数，确定当$p_i=0$时，该transition仍能被replay；第二种是间接的，$p_i = \frac{1}{rank(i)}$，其中$rank(i)$是所有replay memory中的experiences根据$|\delta_i|$排序后的rank。第二种方法的鲁棒性更好。<br>在实现上，两种方法都有相应的trick，让复杂度不依赖于memory 大小$N$。Proportional prioritization采用了’sum-tree’数据结构，每一个节点都是它的子节点的children，priorities是leaf nodes。而rank-based方法，使用线性函数估计累计密度函数，具体怎么实现没有细看。</p><h4 id="annealing-the-bias">annealing the bias</h4><p>因为random sample方法，samples之间没有一点联系，选择每一个sample的概率都是相等的，但是如果加上了priority，就有一个bias toward高priority的samples。IS和prioritized replay的组合在non-learn FA中有一个用处，large steps可能会产生不好的影响，因为梯度信息可能是局部reliable，所以需要使用一个小点的step-size。<br>在本文中，high-error的样本可能会观测到很多次，使用IS减小gradient的大小，对应于高priority的samples的weight被微调了一下，而对应于低priority的样本基本不变。<br>weigth的计算公式如下：<br>$$w_i = (\frac{1}{N}\cdot \frac{1}{P(i)})^{\beta}\tag{10}$$<br>OK,这里IS的作用有些不明白。。。。<todo></todo></p><h3 id="伪代码-v4">伪代码</h3><p>算法 4<br>输入: minibatch $k$, 学习率（步长）$\eta$, replay period $K$ and size $N$ , exponents $\alpha$ and $\beta$, budget $T$.<br>初始化replay memory $H = \emptyset, \Delta = 0, p_1 = 1$<br>根据$S_0$选择 $A_0 \sim \pi_{\theta}|(S_0)$<br><strong>for</strong> $t = 1,\cdots, T$ do<br>$\qquad$观测$S_t, R_t, \gamma_t$<br>$\qquad$存储transition $(S_{t−1}, A_{t−1}, R_t , \gamma_t, S_t)$ 到replay memory，以及$p_t$的最大priority $p_t = max {i\lt t} p_i$<br>$\qquad$<strong>if</strong> $t ≡ 0$ mod $K$ then<br>$\qquad\qquad$<strong>for</strong> j = 1 to k do<br>$\qquad\qquad\qquad$Sample transition $j \sim P(j) = \frac{p_j^{\alpha}}{\sum_i p_i^{\alpha}}$<br>$\qquad\qquad\qquad$计算importance-sampling weight $w_j = \frac{(N \cdot P(j))^{\beta}}{max_i w_i}$<br>$\qquad\qquad\qquad$计算TD-error $\delta_j = R_j + \gamma_j Q_{target} (S_j$, $arg max_a Q(S_j, a)) − Q(S_{j−1} , A\ {j−1})$<br>$\qquad\qquad\qquad$更新transition的priority $p_j \leftarrow |\delta_j|$<br>$\qquad\qquad\qquad$累计weight-change $\Delta \leftarrow \Delta + w_j \cdot \delta_j \cdot \nabla_{\theta} Q(S_{j−1}, A_{j−1})$<br>$\qquad\qquad$<strong>end for</strong><br>$\qquad\qquad$更新weights $\theta\leftarrow \theta+ \eta\cdot\Delta$, 重置$\Delta = 0$<br>$\qquad\qquad$每隔一段时间更新target network $\theta_{target} \leftarrow \theta$<br>$\qquad$<strong>end if</strong><br>$\qquad$选择action $A_t \sim \pi_{\theta}(S_t)$<br><strong>end for</strong></p><h3 id="实验-v3">实验</h3><p>两组实验，<br>一组是DQN和proportional prioritization作比较。<br>一组是tuned Double DQN和rank-based以及proportional prioritizaiton。</p><h4 id="metrics-v2">Metrics</h4><p>用的是double dqn提出来的nomalized score，这里在分母上加了绝对值。<br>主要用的median scores和mean scores。</p><h2 id="dueling-dqn">Dueling DQN</h2><h3 id="介绍">介绍</h3><p>本文作者提出来将dueling网络框架应用在model-free算法上。The dueling architecture能用一个deep model同时表示$V(s)$和优势函数$A(s,a)$，网络的输出将$V$和$A$结合产生$Q(s,a)$。和advantage不一样的是，这种方式在构建时就将他们进行了解耦，因此，dueling architecture可以应用在各种各样的model free RL算法上。<br>本文的架构是对算法创新的补充，它可以对之前已有的各种DQN算法进行结合。</p><h3 id="dueling-network-architecture">dueling network architecture</h3><p>这个新的architecture的核心想法是，没有必要估计所有states的aaction value。在一些states，需要action value去确定执行哪个action，但是在许多其他states，action values并没有什么用。当然，对于bootstrap算法来首，每一个state的value estimation都很重要。<br><img src="/2019/03/02/dqn/deuling-dqn.png" alt="dueling-dqn"><br>作者给出了一个single Q-network的architecture，如图所示。<br>网络结构和nature-dqn一样，但是这里加了两个fully connected layers，一个用于输出$V$，一个用于输出$A$。然后$A$和$V$结合在一起，产生$Q$，网络的输出和nature dqn一样，对应于某个state的一系列action value。<br>从$Q$函数的定义$Q^{\pi}(s,a) = V^{\pi}(s)+A^{\pi}(s,a)$以及$Q$和$V$之间的关系$V^{\pi}(s) = \mathbb{E}_{a\sim\pi(s)}\left[Q^{\pi}(s,a)\right] = \pi(a|s)Q^{\pi}(s,a)$，所以有$\mathbb{E}_{a\sim\pi(s)}\left[A^{\pi}(s,a)\right]=0$。此外，对于deterministic policy，$a^{*} = arg max_{a’\in A}Q(s,a’)$，有$V(s) = Q(s,a^{*})$，即$A(s,a^{*}) = 0$。<br>如图所示的network中，一个网络输出scalar $V(s;\theta, \beta)$，一个网络输出一个$|A|$维的vector $A(s,a;\theta, \alpha)$，其中$\theta$是网络参数，$\alpha$和$\beta$是两个全连接层的参数。<br>根据advantage的定义，可以直接将他们加起来，即：<br>$$Q(s,a;\theta, \alpha, \beta) = V(s;\theta, \beta) + A(s,a;\theta, \alpha) \tag{11}$$<br>但是，我们需要知道的一点是，$Q(s, a;\theta, \alpha, \beta)$仅仅是$Q$的一个参数化估计。它由两部分组成，一部分是$V$，一部分是$A$，但是需要注意的是，这里的$V$和$Q$只是我们叫它$V$和$A$，它的实际意义并不是$V$和$A$。给了$Q$，我们可以得到任意的$Q(s, a) = V(s) + A(s,a)$，而$V$和$Q$并不代表value function和advantage functino。<br>为了解决这个问题，作者提出了选择让advantage为$0$的action，即：<br>$$Q(s, a; \theta,\alpha, \beta) = V(s; \theta, \beta) + \left(A(s,a;\theta,\alpha)-max_{a’\in |A|}A(s, a’; \theta, \alpha)\right)\tag{12}$$<br>选择$a^{*} = arg max_{a’\in A} Q(s, a’; \theta, \alpha, \beta) = arg max_{a’\in A}A(s, a’;\theta, \alpha)$，我们得到$Q(s,a^{*}; \theta, \alpha,\beta) = V(s;\theta, \beta)$。这个时候，输出$V$的网络给出的真的是state value的估计$V(s;\theta, \beta)$，另一个网络真的给出的是advantage的估计。<br>另一种方法是用mean取代max操作：<br>$$Q(s, a; \theta,\alpha, \beta) = V(s; \theta, \beta) + \left(A(s,a;\theta,\alpha)- \frac{1}{|A|}\sum_{a’}A(s, a’; \theta, \alpha)\right)\tag{13}$$<br>一方面这种方法失去了$V$和$A$的原始语义，因为它们有一个常数的off-target，但是另一方面它增加了优化的稳定性，因为上式中advantage的改变只需要和mean保持一致即可，不需要optimal action’s advantange一有变化就要改变。</p><h3 id="伪代码-v5">伪代码</h3><h2 id="distributed-dqn">Distributed DQN</h2><h2 id="noisy-dqn">Noisy DQN</h2><h3 id="介绍-v2">介绍</h3><p>已有方法的exploration都是通过agent policy的random perturbations，比如常见的$\varepsilon$-greedy等方法。这些方法不能找出环境中efficient exploration的behavioural patterns。常见的方法有以下几种:<br>第一种方法是optimism in the face of uncertainty，理论上证明可行，但是通常应用在state-action spaces很小的情况下或者linear FA，很难处理non-linearn FA，而且non-linear情况下收敛性没有保证。<br>另一种方法是添加额外的intrinsic motivation term，该方法的问题是将算法的generalisation mechanism和exploration分割开，即有instrinsic reward和environment reward，它们的比例如何去设置，需要认为指定。如果不仔细调整，optimal policy可能会受intrinsic reward影响很大。此外为了增加exploration的鲁邦性，扰动项仍然是需要的。这些算法很具体也能应用在参数化policy上，但是很低效，而且需要很多次policy evaluation。<br>本文提出NoisyNet学习网络参数的perturbations，主要想法是参数的一点改变可能会导致policy在很多个timsteps上的consistent，complex, state-dependent的变化，而如$\varepsilon$-greedy的dithering算法中，每一步添加到policy上的noise都是不相关的。pertubations从一个noise分布中进行采样，它的variance可以看成noise的energy，variance的参数和网络参数都是通过loss的梯度进行更新。网络参数中仅仅加入了噪音，没有distribution，可以自动学习。<br>在高维度上，本文的算法是一个randomised value function，这个函数是neural network，网络的参数并没有加倍，linear 的参数加倍，而参数是noise的一个简单变换。<br>还有人添加constant Gaussian niose到网络参数，而文本的算法添加的noise并不是限制在Gaussion noise distributions。添加noise辅助训练在监督学习等任务中一直都有，但是这些噪音都是不能训练的，而NoisyNet中的噪音是可以梯度下降更新的。</p><h3 id="noisynets">NoisyNets</h3><p><img src="/2019/03/02/dqn/noisy_linear_layer.png" alt="noisy_linear_layer"><br>用$\theta$表示noisy net的参数，输入是$x$，输出是$y$，即$y=f_{\theta}(x)$。$\theta$定义为$\theta=\mu+\Sigma\odot\varepsilon$，其中$\zeta=(\mu,\Sigma)$表示可以学习的参数，$\varepsilon$表示服从固定分布的均值为$0$的噪音,$\varepsilon$是random variable。$\odot$表示element-wise乘法。最后的loss函数是关于$\varepsilon$的期望：$\bar{L}(\zeta)=\mathbb{E}\left[L(\theta)\right]$，然后优化相应的$\zeta$，$\varepsilon$不能被优化，因为它是random variable。<br>一个有$p$个输入单元，$q$个输出单元的fully-connected layer表示如下：<br>$$y=wx+b \tag{14}$$<br>其中$w\in \mathbb{R}^{q\times p}$，$x\in \mathbb{R}^{p}$,$b\in \mathbb{R}^{q}$，对应的noisy linear layer定义如下：<br>$$y=(\mu^w+\sigma^w\odot\varepsilon^w)x + \mu^b+\sigma^b\odot\varepsilon^b \tag{15}$$<br>就是用$\mu^w+\sigma^w\odot\varepsilon^w$取代$w$，用$\mu^b+\sigma^b\odot\varepsilon^b$取代$b$。其中$\mu^w,\sigma^w\in \mathbb{R}^{q\times p} $，而$\mu^b,\sigma^b\in\mathbb{R}^{q}$是可以学习的参数，而$\varepsilon^w\in \mathbb{R}^{p\times q},\varepsilon^b \in \mathbb{R}^{q}$是random variable。<br>作者提出了两种添加noise的方式，一种是Independent Gaussian noise，一种是Factorised Gaussion noise。使用Factorised的原因是减少随机变量的计算时间，这些时间对于单线程的任务来说还是很多的。</p><h4 id="independent-gaussian-noise">Independent Gaussian noise</h4><p>应用到每一个weight和bias的noise都是independent的，对于$\varepsilon^w$的每一项$\varepsilon_{i,j}^w$来说，它们的值都是从一个unit Gaussion distribution中采样得到的；$varepsilon^b$同理。所以对于一个$p$个输入,$q$个输出的noisy linear layer总共有$pq+q$个noise 变量。</p><h4 id="factorised-gaussian-noise">Factorised Gaussian noise</h4><p>通过对$\varepsilon_{i,j}^w$来说，可以将其分解成$p$个$\varepsilon_i$用于$p$个输入和$q$个$\varepsilon_j$用于$q$个输出，总共有$p+q$个noiss变量。每一个$\varepsilon_{i,j}^w$和$\varepsilon_{j}^b$可以写成：<br>$$\varepsilon_{i,j}^w = f(\varepsilon_i)f(\varepsilon_j) \tag{16}$$<br>$$\varepsilon_{j}^b = f(\varepsilon_j)\tag{17}$$<br>其中$f$是一个实函数，在第一个式子中$f(x) = sng(x)\sqrt{|x|}$，在第二个式子中可以取$f(x)=x$，这里选择了和第一个式子中一致。<br>因为noisy network的loss函数是$\bar{L}(\zeta)=\mathbb{E}\left[L(\theta)\right]$，是关于noise的一个期望，梯度如下：<br>$$\nabla\bar{L}(\zeta)=\nabla\mathbb{E}\left[L(\theta)\right]=\mathbb{E}\left[\nabla_{\mu,\Sigma}L(\mu+\Sigma\odot\varepsilon)\right] \tag{18}$$<br>使用Monte Carlo估计上述梯度，在每一个step采样一个sample进行optimization:<br>$$\nabla\bar{L}(\zeta)\approx\nabla_{\mu,\Sigma}L(\mu+\Sigma\odot\varepsilon) \tag{19}$$</p><h3 id="noisy-dqn-and-dueling">Noisy DQN and dueling</h3><p>相对于DQN和dueling DQN来说，noisy DQN and dueling主要做了两方面的改进：</p><ol><li>不再使用$\varepsilon$-greedy behaviour policy了，而是使用greedy behaviour policy采样优化randomised action-value function。</li><li>网络中的fully connected layers全都换成了参数化的noisy network，noisy network的参数在每一次replay之后从noise服从的distribution中进行采样。这里使用的nose是factorised Gaussian noise。</li></ol><p>在replay 整个batch的过程中，noisy network parameter sample保持不变。因为DQN和Dueling每执行一个action step之后都会执行一次optimization，每次采样action之前都要重新采样noisy network parameters。</p><h4 id="loss">Loss</h4><p>$Q(s,a,\epsilon;\zeta)$可以看成$\zeta$的一个random variable，NoisyNet-DQN loss如下：<br>$$\bar{L}(\zeta) = \mathbb{E}\left[\mathbb{E}_{(x,a,r,y)}\sim D\left[r + \gamma max_{b\in A}Q(y, b, \varepsilon’;\zeta^{-}) - Q(x,a,\varepsilon;\zeta)\right]^2\right]\tag{20}$$<br>其中外层的期望是$\varepsilon$相对于noisy value function $Q(x,a, \varepsilon;\zeta)$和$\varepsilon’$相对于noisy target value function $Q(x,a, \varepsilon’;\zeta^{-}$。对于buffer中的每一个transition，计算loss的无偏估计，只需要计算target value和true value即可，为了让target value和true之间没有关联，target network和online network采用independent noises。<br>就double dqn中的action选择来说，采样一个新的independent sample $\varepsilon^{’’}$计算action value，然后使用greedy操作，NoisyNet-Dueling的loss如下：<br>$$\bar{L}(\zeta) = \mathbb{E}\left[\mathbb{E}_{(x,a,r,y)}\sim D\left[r + \gamma Q(y, b^{*}(y), \varepsilon’;\zeta^{-} - Q(x,a,\varepsilon;\zeta)\right]^2\right]\tag{21}$$<br>$$b^{*}(y) = arg max_{b\in A} Q(y, b(y), \varepsilon^{’’};\zeta)\tag{22}$$</p><h3 id="noisy-a3c">Noisy-A3C</h3><p>Noisy-A3C相对于A3C有以下的改进：</p><ol><li>entropy项被去掉了;</li><li>fully-connected layer被替换成了noisy network。</li></ol><p>A3C算法中没有像$\epsilon$-greedy这样进行action exploration，选中的action通常是从current policy中选的，加入entropy是为了鼓励exploration，而不是选择一个deterministic policy。当添加了noisy weights时，对参数进行采样就表示选择不同的current policy，就已经代表了exploration。NoisyNet相当于直接在policy space中进行exploration，而entropy项就可以去掉了。</p><h3 id="noisy-networks的初始化">Noisy Networks的初始化</h3><p>在unfactorised noisy networks中，每个$\mu_{i,j}$从独立的均匀分布$U\left[-\sqrt{\frac{3}{p}}, \sqrt{\frac{3}{p}}\right]$中采样初始化，其中$p$是对应linear layer的输入个数，$\sigma_{i,j}$设置为一个常数$0.0017$，这是从监督学习的任务中借鉴的。<br>在factorised noisy netowrks中，每个$\mu_{i,j}$从独立的均匀分布$U\left[-\sqrt{\frac{1}{p}}, \sqrt{\frac{1}{p}}\right]$中进行采样，$\sigma_{i,j}$设置为$\frac{\sigma_0}{p}$，超参数$\sigma_0$设置为$0.5$。</p><h3 id="伪代码-v6">伪代码</h3><p>算法5 NoisyNet-DQN / NoisyNet-Dueling<br>输入: Env Environment; $\varepsilon$ random variables of the network的集合<br>输入: DUELING Boolean; &quot;true&quot;代表NoisyNet-Dueling and &quot;false&quot;代表 NoisyNet-DQN<br>输入: $B$空replay buffer; $\zeta$初始的network parameters; $\zeta^{-}$初始的target network parameters<br>输入: replay buffer大小$N_B$; batch size $N_T$; target network更新频率$N^{-}$<br>输出: $Q(\cdot, \varepsilon; \zeta)$ action-value function<br><strong>for</strong> episode $e\in  {1,\cdots , M}$ do<br>$\qquad$初始化state sequence $x_0 \sim Env$<br>$\qquad$<strong>for</strong> $t \in {1,\cdots }$ do<br>$\qquad\qquad$设置$x \leftarrow x_0$<br>$\qquad\qquad$采样 a noisy network  $\xi\sim \varepsilon$<br>$\qquad\qquad$选择an action $a \leftarrow arg max_{b\in A} Q(x, b, \xi; \zeta)$<br>$\qquad\qquad$采样 next state $y \sim  P (\cdot|x, a)$, 接收 reward $r \leftarrow R(x, a) $以及$x_0 \leftarrow y$<br>$\qquad\qquad$将transition (x, a, r, y)添加到replay buffer<br>$\qquad\qquad$<strong>if</strong> $|B| \gt N_B$ then<br>$\qquad\qquad\qquad$删掉最老的transition<br>$\qquad\qquad$<strong>end if</strong><br>$\qquad$采样一个大小为$N_T$的batch, transitions $((x_j, a_j, r_j, y_j) \sim D)_{j=1}^{N_T}$<br>$\qquad\qquad$采样noisy variables用于online network $\xi \sim\varepsilon$<br>$\qquad\qquad$采样noisy variables用于target network $\xi’\sim\varepsilon$<br>$\qquad\qquad\qquad$<strong>if</strong> DUELING then<br>$\qquad\qquad\qquad$采样noisy variables用于选择action的network $\xi\sim\varepsilon$<br>$\qquad\qquad$<strong>end if</strong><br>$\qquad\qquad$<strong>for</strong> $j \in {1,\cdots, N_T}$ do<br>$\qquad\qquad\qquad$<strong>if</strong> $y_j$ is a terminal state then<br>$\qquad\qquad\qquad\qquad$$\hat{Q}\leftarrow r_j$<br>$\qquad\qquad\qquad$<strong>end if</strong><br>$\qquad\qquad\qquad$<strong>if</strong> DUELING then<br>$\qquad\qquad\qquad\qquad b^{*}(y_j) = arg max_{b\in A} Q(y_j, b, \xi^{’’}; \zeta)$<br>$\qquad\qquad\qquad\qquad\qquad \hat{Q}\leftarrow r_j + \gamma Q(y_j, b^{*}(y_j), \xi’;\zeta^{-})$<br>$\qquad\qquad\qquad$<strong>else</strong><br>$\qquad\qquad\qquad\qquad$$\hat{Q}\leftarrow r_j + \gamma max_{b\in A} Q(y_j, b, \xi’;\zeta^{-})$<br>$\qquad\qquad$<strong>end if</strong><br>$\qquad\qquad\qquad$利用loss $(\hat{Q}-Q(x_j,a_j, \xi;\zeta))^2$的梯度更新$\zeta$<br>$\qquad\qquad$<strong>end for</strong><br>$\qquad\qquad$每隔$N^{-}$步更新target network:$ \zeta^{−}\leftarrow \zeta$<br>$\qquad$<strong>end for</strong><br><strong>end for</strong></p><p>算法6 NoisyNet-A3C for each actor-learner thread<br>输入: Environment Env, 全局共享参数$(\zeta_{\pi},\zeta_{V})$ , 全局共享counter $T$和maximal time $T_{max}$<br>输入: 每个线程的参数 $(\zeta’_{\pi},\zeta’_{V})$, random variables $\varepsilon$的集合, 每个线程的counter $t$和TD-$\gamma$的长度$t_{max}$<br>输出: policy $\pi(\cdot; \zeta_{\pi}, \varepsilon)$和value $V(\cdot; \zeta_{V}, \varepsilon)$<br>初始化线程counter $t \leftarrow 1$<br><strong>repeat</strong><br>$\qquad$重置acumulative gradients: $d\zeta_{\pi}\leftarrow 0$和$d\zeta_V \leftarrow 0$<br>$\qquad$Synchronise每个线程的parameters: $\zeta’_{\pi}\leftarrow \zeta_{\pi}$和$\zeta_V\leftarrow \zeta_V$<br>$\qquad$counter $\leftarrow 0$<br>$\qquad$从Env中得到state $x_t$<br>$\qquad$采样noise: $\xi\sim\varepsilon$<br>$\qquad r \leftarrow []$<br>$\qquad a \leftarrow []$<br>$\qquad x \leftarrow []$和$x[0] \leftarrow x_t$<br>$\qquad$<strong>repeat</strong><br>$\qquad\qquad$采样action: $a_t \sim\pi(\cdot|x_t;\zeta’_{\pi};\xi)$<br>$\qquad\qquad$$a[−1]\leftarrow a_t$<br>$\qquad\qquad$接收reward $r_t$和next state $x_{t+1}$<br>$\qquad\qquad$$r[−1]\leftarrow r_t$和$x[−1]\leftarrow x_t+1$<br>$\qquad\qquad$$t\leftarrow t + 1$和 $T\leftarrow T + 1$<br>$\qquad\qquad$$counter = counter + 1$<br>$\qquad\qquad$<strong>until</strong> $x_t\ \ terminal\ \ or\ \ counter == t_{max} + 1$<br>$\qquad$<strong>if</strong> $x_t$ is a terminal state then<br>$\qquad\qquad$$Q = 0$<br>$\qquad$<strong>else</strong><br>$\qquad\qquad$$Q = V(x_t; \zeta’_{V}, \xi)$<br>$\qquad$<strong>end if</strong><br>$\qquad$<strong>for</strong> $i \in {counter − 1, \cdots, 0}$ do<br>$\qquad\qquad$更新Q: $Q\leftarrow r[i] + \gamma Q$<br>$\qquad\qquad$累积policy-gradient: $d\zeta_{\pi} \leftarrow d\zeta_{\pi} + \nabla \zeta’_{\pi}log(\pi(a[i]|x[i]; \zeta’_{\pi}, \xi))[Q − V(x[i]; \zeta’_{\pi}V, \xi)]$<br>$\qquad\qquad$累积 value-gradient: $d\zeta_V \leftarrow ← d\zeta_V+ \nabla \zeta’_{V}[Q − V(x[i]; \zeta’_{V}, \xi)]^2$<br>$\qquad$<strong>end for</strong><br>$\qquad$执行$\zeta_{\pi}$的asynchronous update: $\zeta_{\pi}\leftarrow \zeta_{\pi} + \alpha_{\pi}d\zeta_{\pi}$<br>$\qquad$执行$\zeta_{V}$的asynchronous update: $\zeta_{V}\leftarrow \zeta_{V} − \alpha_VdV\zeta_{V}$<br><strong>until</strong> $T \gt T_{max}$</p><h2 id="rainbow">Rainbow</h2><h2 id="参考文献">参考文献</h2><p>1.<a href="https://blog.csdn.net/yangshaokangrushi/article/details/79774031" target="_blank" rel="noopener">https://blog.csdn.net/yangshaokangrushi/article/details/79774031</a><br>2.<a href="https://link.springer.com/article/10.1007%2FBF00992698" target="_blank" rel="noopener">https://link.springer.com/article/10.1007%2FBF00992698</a><br>3.<a href="https://www.jianshu.com/p/b92dac7a4225" target="_blank" rel="noopener">https://www.jianshu.com/p/b92dac7a4225</a><br>4.<a href="https://datascience.stackexchange.com/questions/20535/what-is-experience-replay-and-what-are-its-benefits/20542#20542" target="_blank" rel="noopener">https://datascience.stackexchange.com/questions/20535/what-is-experience-replay-and-what-are-its-benefits/20542#20542</a><br>5.<a href="https://stats.stackexchange.com/questions/897/online-vs-offline-learning" target="_blank" rel="noopener">https://stats.stackexchange.com/questions/897/online-vs-offline-learning</a><br>6.<a href="https://www.freecodecamp.org/news/improvements-in-deep-q-learning-dueling-double-dqn-prioritized-experience-replay-and-fixed-58b130cc5682/" target="_blank" rel="noopener">https://www.freecodecamp.org/news/improvements-in-deep-q-learning-dueling-double-dqn-prioritized-experience-replay-and-fixed-58b130cc5682/</a><br>7.<a href="https://jaromiru.com/2016/11/07/lets-make-a-dqn-double-learning-and-prioritized-experience-replay/" target="_blank" rel="noopener">https://jaromiru.com/2016/11/07/lets-make-a-dqn-double-learning-and-prioritized-experience-replay/</a><br>8.<a href="https://datascience.stackexchange.com/questions/32873/prioritized-replay-what-does-importance-sampling-really-do" target="_blank" rel="noopener">https://datascience.stackexchange.com/questions/32873/prioritized-replay-what-does-importance-sampling-really-do</a><br>9.<a href="https://papers.nips.cc/paper/5249-weighted-importance-sampling-for-off-policy-learning-with-linear-function-approximation.pdf" target="_blank" rel="noopener">https://papers.nips.cc/paper/5249-weighted-importance-sampling-for-off-policy-learning-with-linear-function-approximation.pdf</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;dqn&quot;&gt;DQN&lt;/h2&gt;
&lt;p&gt;论文名称&lt;a href=&quot;https://arxiv.org/pdf/1312.5602.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Playing Atari with Deep Reinforcem
      
    
    </summary>
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://mxxhcm.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="http://mxxhcm.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="值迭代" scheme="http://mxxhcm.github.io/tags/%E5%80%BC%E8%BF%AD%E4%BB%A3/"/>
    
  </entry>
  
  <entry>
    <title>Modeling Others using Oneself in Multi-Agent Reinforcement Learning</title>
    <link href="http://mxxhcm.github.io/2019/01/29/Modeling-Others-using-Oneself-in-Multi-Agent-Reinforcement-Learning/"/>
    <id>http://mxxhcm.github.io/2019/01/29/Modeling-Others-using-Oneself-in-Multi-Agent-Reinforcement-Learning/</id>
    <published>2019-01-29T05:19:33.000Z</published>
    <updated>2019-05-06T16:22:27.704Z</updated>
    
    <content type="html"><![CDATA[<h2 id="摘要">摘要</h2><p>我们考虑使用不完全信息的多智能体强化学习问题，每个智能体的目标是最大化自身的效用。奖励函数取决于两个智能体的隐藏状态（或者目标），每一个智能体必须从它观察到的行为中推断出其他玩家的隐藏目标从而完成任务。我们提出了一种新的方法在这些领域中进行学习：自我其他建模（SOM），智能体使用自己的策略来预测其他智能体的动作并实时更新其他智能体隐藏状态的置信度。我们在三个不同的任务上对该方法进行了评估，结果表明智能体无论在合作还是对抗环境中都能使用他们对其他玩家隐藏状态的估计来学习到更好的策略。</p><h2 id="引言">引言</h2><p>在多智能体系统中推理其他智能体的意图并预测它们的行为是很重要的，这些智能体可能有不同的甚至是竞争的目标集。由于多智能体系统的不稳定性，这仍然是一个非常具有挑战性的问题。<br>在本文中，我们介绍了一种从其他智能体的行为中估计对应的未知的目标和并利用这些估计的目标选择动作的新方法。我们证明了在本文提到的任务中，在游戏中显式的对其他玩家进行建模比将其他智能体看做环境的一部分会有更好的性能。我们将问题定义为双人随机游戏，也叫双人马尔可夫游戏，其中环境对于智能体是完全可见的，但是没有关于其他智能体目标的明确知识而且没有沟通信道。每个智能体在回合结束时收到的奖励取决于两个智能体的目标，因此是每个智能体最优的策略都必须考虑到所有智能体的目标。<br>认知科学研究表明，人类维持与他们联系的其他人的模型，这些模型用来捕捉那些人的目标，信仰或偏好。在某些情况下，人类利用自己的心理过程来模拟他人的行为。这使他们能够理解其他人的意图或动机，并能在社交场合采取相应的行动。受这些研究的启发，关键想法是要理解游戏中其他玩家正在做什么，智能体应该问自己“如果我扮演另一个玩家的角色，我的目标是什么？”。我们通过使用一个多层循环神经网络参数化智能体的动作和值函数来实现这个想法，该神经网络将状态和目标作为输入。当智能体玩游戏时，它通过直接使用自己的动作函数优化目标来最大化对方行动的可能性，从而推断出其他智能体的未知目标。</p><h2 id="方法">方法</h2><p><strong>背景</strong> 两个智能体的马尔可夫游戏由描述所有智能体的可能配置的一组状态集合$S$，两组动作集合$A_1$，$A_2$和两个智能体的观察$O_1$，$O_2$以及转换函数$\Tau$：$S\times A_1 \times A_2 \rightarrow S$作为当前状态和动作的函数给出下一个状态的概率分布。每个智能体$i$通过从随机策略$\pi_{\theta_i}:S\times A_i\rightarrow [0,1]$中采样选择动作。每个智能体都有一个奖励函数，它取决于智能体的状态和动作：$r_i：S\times A_i\rightarrow R$。每个智能体$i$试图最大化自己的总预期收益$R_i = \sum_{t =0}<sup>T\gamma</sup>tr_i^t$，其中$\gamma$是折扣因子，$T$是时间范围。在本文中，我们考虑了合作以及竞争环境。<br>接下来介绍自我其他模型（SOM），这是一种在一个回合内以实时方式推断其他智能体的目标并使用这些估计来选择动作的新方法。为了决定一个动作并估计一个状态的值，我们使用一个神经网络$f$将它自己的目标$z_{self}$，另一个玩家的估计目标$\hat{z}<em>{self}$，并且他自己的角度的观察状态$s</em>{self}$作为输入，输出动作$\pi$的一个概率分布和值估计$V$，即对于每个玩游戏的智能体，有：<br>$$\begin{bmatrix}\pi<sup>i\V</sup>i\end{bmatrix}=f<sup>i(s_{self}</sup>i,z_{self}<sup>i,\hat{z}_{other}</sup>i;\theta^i)$$<br>其中$\theta_i$是智能体$i$的神经网络$f$的参数，包括一个softmax层输出策略，一个线性层输出值函数，所有非输出层是共享的。动作是从策略$\pi$中采样得到的。观察状态$s_{self}<sup>i$包含$f</sup>i$智能体的位置，以及其他智能体的位置。每个智能体都有两个网络（为了简洁，省略了智能体上标$i$），一个计算它自己的动作和值函数，一个计算其他智能体的估计值，如下：<br>\begin{equation}<br>f_{self}(s_{self},z_{self},\hat{z}<em>{other};\theta</em>{self})<br>\end{equation}<br>\begin{equation}<br>f_{other}(s_{other},\hat{z}<em>{other},z</em>{self};\theta_{self})<br>\end{equation}<br>这两个网络使用的方式不同：$f_{self}$用于计算智能体自己的行为和价值，并以前馈方式运行。给出其他智能体观察到的动作，智能体使用$f_{other}$通过优化$\hat{z}<em>{other}$推断其他智能体的目标。<br>我们建议每个智能体使用自己的策略模拟其他玩家的行为，这样$f</em>{other}$的参数与$f_{self}$的参数是相同的。但请注意，两个网络的输入$z_{self}$和$\hat{z}<em>{other}$的相对位置不同。另外，由于环境是完全可观测的，两个智能体的观察状态的不同仅通过地图上智能体的身份体现出来（即，每个智能体将能够区分其自己的位置和另一个智能体的位置）。因此，在acting模式下，$f</em>{self}$网络将$s_{self}$作为输入；在推理模式下，$f_{other}$网络将$s_{other}$作为输入。在游戏的每一步，智能体需要推理$\hat{z}<em>{other}$将其作为(1)的输入并选择其动作。为了实现这个目的，在每一步中，智能体观察另一个智能体采取的行动，并且在下一步中，智能体使用先前观察到的另一个智能体的动作作为监督信号，使用式子(2)反向传播并优化其$\hat{z}</em>{other}$，如图1所示。<br>推理过程优化器中采取的步数是一个可根据游戏的不同而变化的超参数。因此，在游戏的每一步中其他智能体的目标估计$\hat{z}<em>{other}$会被更新多次。参数$\theta</em>{self}$在每个回合结束时使用和带有智能体获得的奖励信号的Asynchronous Advantage Actor-Critic（A3C）进行更新。<br>算法1给出了一个回合内训练SOM智能体的伪代码。这里考虑的所有任务的目标都是离散的，智能体的目标$\hat{z}<em>{self}$被<br>表示独热向量，维度是智能体目标所有可能的情况数。另一个玩家的目标嵌入$\hat{z}</em>{other}$有相同的维度。为了估计经过离散而不可微的变量$\hat{z}<em>{other}$的梯度，我们用Gumbel-Softmax分布上的一个可微样本$\hat{z}</em>{other}^G$代替它。这种重新参数化技巧被证明可以有效地产生低方差偏置的梯度。使用该方法在每一步优化过$\hat{z}<em>{other}$之后，$\hat{z}</em>{other}$通常偏离独热向量。在下一步中，$f_{self}$将对应于先前更新的$z_{other}$ argmax的一个独热向量量$\hat{z}<em>{other}^OH$作为输入。<br>智能体的策略由长短期记忆（LSTM）单元参数化，以及两个全连接的线性层和指数线性单元（ELU）激活函数。神经网络的权重用半正交矩阵初始化。<br>由于$f</em>{other}$的循环性，当推理步数$\gt 1$时必须特别小心。在这种情况下，在游戏的每一步中，我们在推理模式中的第一次前向传播之前保存$f_{other}$的循环状态，并且在每个推理步骤将循环状态初始化为此值。这个过程可以确保在动作和推理模式下$f_{other}$可以展开相同数量的步骤。</p><h2 id="相关工作">相关工作</h2><p>不完全信息的游戏中对手建模一直在被广泛研究。但是，大多数以前的方法都侧重于研究特定领域内的概率先验或参数化策略的模型。相比之下，本文的工作为对手建模提出了一个更通用的框架。给定比赛历史，Davidson使用MLP预测对手的动作，但是智能体无法实时适应对手的行为。Lockett等人设计了一种神经网络结构，通过在给定的一组主要对手上学习权重的值来识别对手类型。然而，游戏并没有在强化学习框架内展开。<br>大量多智能体深度强化学习的研究中侧重于部分可见的，完全合作和紧急通信等环境。本文不允许智能体之间进行任何沟通，因此玩家必须利用他们观察到的行为间接推理他们对手的意图。作为对比，Leibo等考虑半合作多智能体环境，智能体根据任务类型和奖励结构制定合作和竞争策略。类似地，Lowe等人提出了一种集中AC框架，用于在具有混合策略的环境中进行高效的训练。 Lerer和Peysakhovich通过将针锋相对的著名游戏理论策略推广到多智能体马尔可夫游戏，设计了能够在复杂社会困境中保持合作的强化学习智能体。最近认知科学方面的工作试图通过使用分层的社会智能体模型来理解人类的决策，它能推断出其他人类智能体的意图，从而决定是否采取合作或竞争策略。然而，这些论文都没有设计出能够显式模拟环境中其他人工智能体或者估计他们意图的算法来改善智能体的决策。<br>逆强化学习领域也与本文考虑的问题有关。逆强化学习的目的是通过观察智能体的行为来推断智能体的奖励函数。相反，我们的方法使用观察到其他玩家的行为以在线方式直接推断他们的目标，然后在环境的acting模式中由智能体使用。这避免为了估计奖励函数收集其他智能体状态-动作对离线样本的需要，然后使用它来学习最大化该效用的单独策略。最近Hadfield-Menell等的论文也关注推理他人意图的问题，但他们关注的是人机交互和价值调整。在类似目标的推动下，Chandrasekaran等人考虑建立人工智能理论的问题，以改善人工智能交互和人工智能系统的可解释性。为了这个目标，他们展示了可以使用少量示例训练人们预测视觉问答模型的响应。<br>Foerster等人和He等人的工作与我们的工作最接近。Foerster等人设计强化学习智能体在更新自己的策略时同时考虑到环境中其他智能体的学习。这使得智能体能够发现自私而又协作的策略，例如在迭代囚徒困境中的针锋相对策略。虽然我们的工作没有明确地试图塑造其他智能体的学习，但它的优点是智能体可以在一个回合中更新他们的信念并以在线方式更新策略以获得更多奖励。我们的设置也有所不同，它认为每个智能体都有一些其他玩家所需的隐藏信息，以便最大化其回报。<br>我们的工作非常符合He等人的工作，作者构建了一个用于在强化学习环境中构建其他智能体的一般框架。He等人提出了一个模型，通过将对手的观察使用DQN进行编码，共同学习一个策略和对手的行为对手。他们的混合专家架构能够在两个纯对抗性任务中发现不同对手的策略模式。我们的工作与He等人的工作之间的一个区别在于，我们的目标不是推断其他智能体的策略，而是专注于显式估计他们在环境中的目标。此外，在这项工作中，智能体不是使用其他智能体动作的人工设计特征，而是根据自己的模型端到端的学习其他智能体模型。另一个区别是，在这项工作中，智能体使用优化推断其他智能体的隐藏状态，而不是通过前馈网络推断其他智能体的隐藏状态。在下面的实验中，我们表明SOM优于He等人的方法。</p><h2 id="实验">实验</h2><p>在本节中，我们在三个任务上评估SOM模型：</p><ul><li>硬币游戏，这是一个完全合作的任务，智能体的角色是对称的。</li><li>配方游戏，它是对抗的，但具有对称角色。</li><li>门禁游戏，它是完全合作的，但是两个玩家拥有不对称的角色。</li></ul><p>我们将SOM与其他三个baselines以及一个可以访问其他智能体目标的ground truth的模型进行比较。所有任务都是在Mazebase gridworld环境中创建的。</p><h3 id="baselines">Baselines</h3><p>TRUE-OTHER-GOAL（TOG）：我们提供了一个给出的模型性能上限的策略网络，该网络将其他智能体的真正目标$z_{other}$，以及状态特征$s_{self}$和自己的目标$z_{self}$作为输入。因为这个模型可以直接访问其他智能体真正的目标，因此不需要单独的网络来模拟其他智能体的行为。 TOG的结构与SOM的一个策略网络$f_{self}$相同。 NO-OTHER-MODEL（NOM）：我们使用的第一个baseline仅使用观察状态$s_{self}$和自身目标$z_{self}$作为输入。NOM与SOM的一个策略网络$f_{self}$有相同的架构。该baseline没有对其他智能体的显式建模或估计它们的目标。<br>集成-策略-预测器（IPP）：从NOM的体系结构和输入开始，我们构建了一个更强的baseline IPP，它有一个额外的最终线性层输出另一个智能体下一个动作的概率分布。除了用于训练该网络策略的A3C损失函数，我们还添加交叉熵损失项训练其他智能体的行为的预测。<br>分离-策略-预测器（SPP）：He等人提出了一个基于DQN的对手建模框架。在他们的方法中，给定对手特有的人工提取的状态信息，训练一个神经网络预测对手的动作。该网络的中间隐藏表示用作Q网络的输入。<br>我们修改了He等人的模型应用到本文的场景中。特别的，我们使用A3C而不是DQN，我们不使用特定领域的特征表示对手的隐藏状态。<br>最后产生的SPP模型由两个独立的网络组成，一个策略网络用于决定智能体的动作，一个对手网络用于预测其他智能体的动作。对手网络将世界状态$s$和自己的目标$z_{self}$作为输入，并输出其他智能体在下一步采取动作的概率分布，以及其隐藏状态（由网络的循环给出）。与IPP一样，我们使用其他智能体的真实动作训练对手策略预测器的交叉熵损失。在每一步中，该网络输出的隐藏状态以及智能体观察状态和智能体自身的目标被作为智能体的策略网络的输入。策略网络和对手策略预测器都是与SOM结构相同的LSTM网络。<br>与SOM作对比，SPP没有显式推断出其他智能体的目标。相反，它通过预测智能体在每个时间步的动作来隐式的构建对手模型。在SOM中，一个参考的目标作为策略网络的附加输入。而在SPP，类似的参考目标是从对手策略预测器得到的隐藏表示，把它作为策略网络的附加输入。<br><strong>训练细节</strong>。在我们的所有实验中，我们使用系数为$0.01$的熵，价值损失系数为$0.5$，折扣系数为$0.99$的A3C训练智能体的策略。使用Adam优化智能体商策略的参数，其中$\beta_1= 0.9,\beta_2= 0.999,\epsilonn =1\times 10^{-8}$，权重衰减为$0$。学习率为$0.1$的SGD用于推断另一个智能体的目标，$\hat{z}_{other}$。<br>硬币和食谱游戏中策略网络的隐藏层维度为$64$，门游戏中为$128$。所有游戏和模型的学习率都是$1\times 10^{-4}$。<br>观测状态$s$用一些独热向量表示，包括环境中所有物体的位置，以及智能体和另一个智能体的位置。这个输入状态的维度是$1\times n$特征，其中Coin，Recipe和Door游戏的特征数分别为$384$,$192$和$900$。对于每个实验，我们使用5个不同的随机种子训练模型。除非特殊说明，否则论文中展示的所有游戏结果都是每步进行的10次优化更新的结果。</p><!--### 硬币游戏。首先，我们在一个完全合作的任务上评估模型，在这个任务中，当智能体使用他们两个的目标而不仅仅是他们自己的目标时，他们可以获得更多的奖励。因此，估计其他玩家的目标并在采取行动时使用该信息符合每个智能体人的最佳利益。如图4的左图所示，游戏在8×8网格上进行，该网格包含12个3种不同颜色的硬币（每种颜色4个硬币）。在每集开始时，智能体被随机分配三种颜色中的一种。动作空间包括：上，下，左，右或通过。一旦智能体人踩到硬币，那个硬币就会从网格中消失。游戏在20个步骤后结束（即每个智能体需要10个步骤）。两名特工在比赛结束时收到的奖励由下面的公式给出：2），其他n其他Cself是自我目标颜色的硬币数量，由其他智能体人收集，而n self Cneither是与自己收集的智能体人目标相对应的硬币数量。对于图4中的示例，智能体1具有Cself =橙色和Cother =青色，而智能体2的Cself是青色而Cother是橙色。对于两种药剂，两者都是红色的。收集不符合任何智能体人目标的硬币的惩罚的作用是避免收敛到暴力政策，在这种政策中，智能体人可以通过收集其附近的所有硬币而获得不可忽视的奖励金额，而不是关于他们的颜色。为了最大化其回报，每个智能体人需要收集自己的硬币或其合作者的颜色，而不是剩余颜色的硬币。因此，当两个智能体人能够在游戏中尽可能早地高精度地推断其合作者的目标时.--><h2 id="讨论">讨论</h2><p>在本文中，我们介绍了一种新方法，用于从其他智能体的行为中推断他们的隐藏状态，并使用这些估计来选择动作。我们证明了智能体能够在合作和竞争环境中估计其他参与者的隐藏目标，这使他们能够收敛到更好的政策并获得更高的回报。在本文提出的任务中，对其他智能体的显式建模比仅仅考虑其他代理成为环境的一部分更好的性能。 SOM的一个限制是它比其他baseline需要更长的训练时间，因为我们在每一步都进行了反向传播。但是，它的online更新方式对于适应环境中其他智能体的动作变化至关重要。SOM的一些主要优点是简单性和灵活性，它不需要任何额外参数来模拟环境中的其他代理，可以使用任何强化学习算法进行训练，并且可以轻松地与任何策略参数化或网络结构集成。SOM可以适应具有两个以上智能体的环境，因为智能体可以使用自己的策略来模拟任意数量的智能体的动作并推断其目标。而且，它可以很容易地推广到许多不同的环境和任务。<br>我们计划通过评估更复杂环境中的模型来扩展这项工作，包括两个以上的参与者，混合策略，更多样化的智能体类型（例如具有不同动作空间的智能体，奖励函数，角色或策略），以及假设其他玩家和自己一样的模型偏差。<br>未来研究的其他重要途径是设计能够适应环境中其他智能体非平稳策略的模型，处理具有分层目标的任务，并在测试时遇到新智能体时表现良好。<br>最后，许多研究领域可以从拥有其他智能体的模型中受益，这些智能体能够推理其他智能体的意图并预测他们的动作。这些模型可能对人机或师生互动，以及价值对齐问题有恒大帮助。此外，这些方法可用于多智能体任务中基于模型的强化学习，因为前向模型的准确性很大程度上取决于预测其他智能体动作的能力。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;摘要&quot;&gt;摘要&lt;/h2&gt;
&lt;p&gt;我们考虑使用不完全信息的多智能体强化学习问题，每个智能体的目标是最大化自身的效用。奖励函数取决于两个智能体的隐藏状态（或者目标），每一个智能体必须从它观察到的行为中推断出其他玩家的隐藏目标从而完成任务。我们提出了一种新的方法在这些领域
      
    
    </summary>
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="http://mxxhcm.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Policy Gradient With Value Function Approximation For Collective Multiagent Planning</title>
    <link href="http://mxxhcm.github.io/2019/01/26/Policy-Gradient-With-Value-Function-Approximation-For-Collective-Multiagent-Planning/"/>
    <id>http://mxxhcm.github.io/2019/01/26/Policy-Gradient-With-Value-Function-Approximation-For-Collective-Multiagent-Planning/</id>
    <published>2019-01-26T11:33:50.000Z</published>
    <updated>2019-05-06T16:22:27.704Z</updated>
    
    <content type="html"><![CDATA[<h2 id="摘要">摘要</h2><p>分布式的部分可观测马尔科夫决策过程(Dec POMDP)为解决多智能体系统中的序列决策问题提供了一个框架。考虑到POMDP的计算复杂度，最近的研究主要集中在Dec-POMDP中一些易于处理但是比较实用的子问题。本文解决的就是其中的一个子问题叫做CDec-POMDP其中一系列智能体的共同行为影响了它们公共的reward和环境变化。本文的主要贡献是提出了一个actor-critic(AC)强化学习算法优化CDec-POMDP问题的policy。普通的AC算法对于大型问题收敛的很慢，为了解决这个问题，本文展示了如何将智能体的估计动作值函数进行分解从而产生有效的更新以及推导出一个基于局部奖励信号的新的critic训练方式。通过在一个合成的benchmark以及真实的出租车车队优化问题上和其他方法进行对比，结果表明本文的AC方法提供了比之前最好的方法还要高质量的方法。</p><h2 id="引言">引言</h2><p>近些年来，分布式的部分可观测马尔科夫决策过程已经发展成了解决多智能体协作的序列决策问题的一个很有前景的(promising)方法。Dec-POMDP对智能体基于环境和其他智能体的不同部分观测最大化一个全局的目标进行建模。Dec-POMDP的具体应用包括协调行星探测，多机器人协调控制以及无线网络的吞吐量优化。然而，解决分布式的部分马尔科夫决策过程是相当困难的，即使对于只有$2$个智能体的问题呢是NP难的。<br>为了增大规模和提高真实问题中的应用，过去的研究已经探索了智能体之间严格的交互，如状态转换和观测独立，事件驱动的的交互以及智能体之间的弱耦合性。最近，一系列工作开始关注于智能体的身份不影响它们之间的交互上，环境的变化主要受到智能体的共同影响，和著名的阻塞游戏很像。一些城市交通中的问题如出租车调度可以用这样的协同规划模型进行建模。<br>在本文中，作者着重于集中的Dec-POMDP框架将一类不确定情况下的集中多智能体序列决策问题形式化。Nguyen等人提出了一个采样方法优化CDec-POMDP模型中的policy。之前方法的一个主要缺点是policy是用表格形式展现的，随着智能体的observation spaces改变时，表格形式的policy不能很好的进行扩展。受到最近一些强化学习工作的启发，本文的贡献是一个AC框架的强化学习算法用来优化CDec-POMDP的policy。Policy用函数如神经网络来表示可以避免表格形式的policy的扩展性问题。我们推导出了策略梯度并且基于CDec-POMDP中智能体的交互提出了一个估计的因子动作值函数。普通的AC算法因为学习全局reward的原因，在解决大型多智能体系统问题时收敛的很慢。为了解决这个问题，本文提出了一种新的方式去训练critic，高效利用智能体的局部值函数的估计动作值函数。<br>我们在一个合成的多机器人导航领域和现实世界中一个亚洲城市的出租车调度问题上测试了本文的方法，结果展示了本文的方法可以扩展到大型多智能体系统上。根据经验，我们因式AC方法比以前最好的方法给出的解决方案都要好。因式AC方法收敛的也比普通的AC方法快很多，验证了我们提出的critic训练方法的有效性。<br><strong>相关工作</strong> 我们的工作基于具有近似值函数的策略梯度框架。然而，根据以往的经验显示，直接应用原始的策略梯度到多智能体任务中，尤其是CDec-POMDP模型中会产生较高方差。在本文中，我们展示了一个和CDec-POMDP兼容的估计值函数，它能产生高效且低方差的策略梯度更新。Peshkin很早之前就研究过了应用于分布式policy的强化学习，Guestrin还提出使用REINFORCE从协调图中训练一个因子值函数的softmax策略。然而，这些以前的工作中，策略梯度都是从全局的经验回报而不是分解后的critic中估计的。我们在第四章中展示了一个分解后ciritc和基于训练这个critic得到的一个单个值函数对于高效的采样学习是很重要的。我们的实验结果表明了我们提出的critic训练方式比用全局经验回报训练收敛的还要快。</p><h2 id="集中分布式pomdp模型">集中分布式POMDP模型</h2><p>我们首先介绍一下Nguyen提出的CDec-POMDP模型。一个对应于这个模型的$T$步的动态贝叶斯网络如图所示。它由以下几个部分组成：</p><ul><li>一个有限的计划范围$H$</li><li>智能的数量$M$，一个智能体m可能处在state space $S$中的任意一个状态，联合state space是$\times_{m=1}^MS$，我们用$i\in S$表示一个state。</li><li>每一个智能体m都有一个action spaceA，我们用$j\in A$表示一个action。</li><li>用$(s_{1:H},a_{1:H})<sup>m=(s_1</sup>m,a_1<sup>m,\cdots,s_H</sup>m,a_H<sup>m)$表示一个智能体m完整的state-action轨迹。用随机变量$s_t</sup>m,a_t^m$表示智能体$m$在$t$时刻的state和action。不同的指示函数$I_t(\cdot)$如表$1$所示。给定每一个智能体$m\in M$的轨迹，定义以下的计数方式：<br>$$n_t(i,j,i’) = \sum_{m=1}^M I_t^m(i,j,i’),\forall i,i’\in S,j\in A.$$<br>如表$1$所示，计数器$n_t(i,j,i’)$表示在$t$时刻处于state $i$，采取action $j$，转换到state $i’$的智能体数量。其他计数器$n_t(i)$和$n_t(i,j)$的定义类似。使用这些计数器，我们可以定义$t$时刻的计数表$\bf{n}<em>{s_t}$和$\bf{n}</em>{s_ta_t}$如表$1$所示。</li><li>我们假设一个普遍的部分观测环境，其中智能体基于其他智能体的总体影响可以有不同的ovservation。一个智能体观测到它的局部state $s_t<sup>m$。此外在$t$时刻基于它的局部状态$s_t</sup>m$和计数表$\bf{n}_{s_t}$观测到$o_t^m$。例如，一个智能体m在$t$时刻处于state $i$，可以观测到其他也处在state $i(=n_t(i))$的智能体或者其他处在state $i$临近状态$j$的智能体，即$n_t(j),\forall j\in Nb(i)$。</li><li>状态转换函数是$\Phi_t(s_{t+1}<sup>m=i’|s_t</sup>m=i,a_t^m=j,\bf{N}<em>{s_t})$。所有智能体的状态转换函数是一样的，注意它会受到$\bf{n}</em>{s_t}$的影响，而$\bf{n}_{s_t}$依赖于智能体的共同行为。</li><li>每一个智能体m有一个不平稳的policy $\pi_t<sup>m(j|i,o_t</sup>m(i,\bf{n}<em>{s_t}))$，表示在$t$时刻给定智能体m的observation $(i,o_t^m(i,\bf{n}</em>{s_t})$之后，智能体采取action $j$的概率。我们用$\pi^m=(\pi_1,\cdots,\pi_H)$表示智能体m水平范围的policy。</li><li>一个智能体接收到的reward $r_t^m=r_t(i,j,\bf{n}<em>{s_t}$取决于它的局部state和action，以及计数表$\bf{n}</em>{s_t}$。</li><li>初始的state分布，$b_o=(P(i)\forall i \in S)$，对于所有的智能体都是相同的。</li></ul><p>我们在这里展示了最简单的版本，所有的智能体的类型都相同，并且有相似的state transition，observation和reward模型。模型也可以处理多种类型的智能体，不同类型的智能体有不同的变化。我们还可以引入一个不受智能体action影响的external state，如交通领域的出租车需求。我们的结果也可以扩展到解决类似的问题。<br>像CDec-POMDP之类的模型对于解决智能体数量很大或者智能体的身份不影响reward或者transition function之类的问题是很有用的。其中一个应用是出租车车队优化问题，这个问题是计算出出租车调度的policy使得车队的利润最大化。一个出租车的决策过程如下。在时刻$t$时，每个出租车观测到它当前的城市空间$z$，不同的空间构成了state space $S$，以及当前空间和它的相邻空间的其他出租车的计数和当前局部请求的一个估计。这构成了出租车基于计数的observation $o(\cdot)$。基于这个observation，出租车必须决定待在当前空间$z$寻找乘客还是移动到下一个空间。这些决策选择取决于不同的因子，如请求比率和当前空间其他出租车的计数。类似的，环境是随机的，在不同时间出租车请求是变化的。使用出租车车队的的GPS记录可以得到这些历史的请求数据。<br><strong>基于计数的统计数据用于规划</strong> CDec-POMDP模型的一个关键属性是模型的变换取决于智能体的集中交互而不是智能体的身份。在出租车车队优化问题中，智能体数量可以相当大（大约有$8000$个智能体在现实世界的实验中）。给出这么大数量的智能体个数，为每一个智能体计算出独一无二的policy是不可能的。因此，和之前的工作类似，我们的目标是对所有智能体计算出一个相同的policy $\pi$。因为policy $\pi$取决于计数，它代表了一种富有表现力的policy。<br>对于一个固定的数量M来说，用${(s_{1:T},a_{1:T})^m\forall m}$表示从图$1$的DBN网络中采样得到的不同智能体的state-action轨迹。用$\mathbf{n}<em>{1:T}={(\mathbf{n}</em>{s_t},\mathbf{n}<em>{s_ta_t},\mathbf{n}</em>{s_ta_ts_{t+1}})\forall t=1:T}$表示每一个时间步$t$的结果计数表的组合向量。Nguyen等人展示了计数器$\mathbf{n}$中拥有足够的统计数据用来规划。也就是说，一个policy $\pi$在水平范围H内的联合值函数可以通过计数器的期望进行计算：<br>$$V(\pi) = \sum_{m=1}<sup>M\sum_{T=1}</sup>H E[r_T^m] = \sum_{\mathbf{n}\in \Omega_{1:H}}P(\mathbf{n};\pi) \left[\sum_{T=1}^H\sum_{i\in S,j\in A} n_T(i,j)r_T(i,j,\mathbf{n}<em>T)\right]$$<br>集合$\Omega</em>{1:H}$是所有允许的一致计数表的集合，如下所示：<br>$$\sum_{i\in S}n_T(i) = M \forall T;$$<br>$$\sum_{j\in A}n_T(i,j) = n_T(i) = \forall j \forall T;$$<br>$$\sum_{i’\in S}n_T(i,j,i’) = n_T(i,j)\forall i\in S,\forall j \in A, \forall T;$$<br>$P(\mathbf{n},\pi)$是计数器的分布。这个结果的一个关键好处是我们可以直接从分布$P(\mathbf{n})$中对计数器$\mathbf{n}$采样而不是对单个不同智能体的轨迹$(s_{1:H},a_{1:H})进行采样来$评估policy $\pi$，这显著节省了计算开销。我们的目标是计算最优的policy $\pi$来最大化$V(\pi)$。我们假设一个集中式学习，分布式执行的强化学习设置。我们假设有一个模拟器可以从$P(\mathbf{n};\pi)$中提供计数器样本。</p><h2 id="cdec-pomdp的策略梯度">CDec-POMDP的策略梯度</h2><p>之前的工作提出了一个基于采样的EM算法来优化policy $\pi$。这个policy被表示成计数器$\mathbf{n}$空间中的一个线性分段表policy，其中每一个线性片段指定了下一个action的分布。然而，这种表格形式的表示限制了它的表达能力，因为片段的数量是固定的先验，并且每个范围都必须手动定义，这可能会对性能产生不利影响。此外，当observation o是多维的时候，即，一个智能体观测到它位置相邻区域的计数器时，需要指数多个片段。为了解决这个问题，我们的目标是优化函数形式（如神经网络）的policy。<br>我们首先扩展策略梯度理论到CDec-POMDP上，用$\theta$表示policy参数的向量。我们接下来展示如何计算$\Delta_\theta V(\pi)$。用$\mathbf{s}_t,\mathbf{a}<em>t$表示$t$时刻所有智能体的联合state和联合action。给定一个policy $\pi$，值函数表示形式如下：<br>$$V_t(\pi)=\sum</em>{\mathbf{s}_t,\mathbf{a}<em>t}P<sup>{\pi}(\mathbf{s}_t,\mathbf{a}_t|b_o,\pi)Q_t</sup>{\pi}(\mathbf{s}<em>t,\mathbf{a}<em>T)$$<br>其中$P<sup>{\pi}(\mathbf{s}_t,\mathbf{a}_t|b_o)=\sum_{\mathbf{s}_{1:t-1},\mathbf{a}_{1:t-1}}P</sup>{\pi}(\mathbf{s}</em>{1:t},\mathbf{a}</em>{1:t}|b_o)$是policy $\pi$下联合state $\mathbf{s}<em>t$，和联合action $\mathbf{a}<em>t$的分布。值函数$Q_t^{\pi}(\mathbf{s}<em>t,\mathbf{a}<em>t)$的计算过程如下：<br>$$Q_t^{\pi}(\mathbf{s}<em>t,\mathbf{a}<em>t) = r_t(\mathbf{s}<em>t,\mathbf{a}<em>t)+\sum</em>{\mathbf{s}</em>{t+1},\mathbf{a}</em>{t+1})}P<sup>{\pi}(\mathbf{s}_{t+1},\mathbf{a}_{t+1}|\mathbf{s}_t,\mathbf{a}_t))Q_{t+1}</sup>{\pi}(\mathbf{s}</em>{t+1},\mathbf{a}</em>{t+1})$$<br>接下来介绍以下CDec-POMDP的策略梯度理论：<br><strong>定理1.</strong> 对于任何CDec-POMDP，策略梯度计算公式如下：<br>$$\Delta</em>{\theta}V_1(\pi)=\sum</em>{t=1}<sup>HE_{\mathbf{s}_t,\mathbf{a}_t)|b_o,\pi}\left[Q_t</sup>{\pi}(\mathbf{s}<em>t,\mathbf{a}<em>t)\sum</em>{i\in S,j\in A}n_t(i,j)\Delta</em>{\theta}log\pi</em>{t}(j|i,o(i,\mathbf{n}</em>{s_t}))\right]$$<br>这个定理的证明和其他后续结果在附录中。<br>注意由于许多原因利用上述结果计算策略梯度是不切实际的。联合state-action $\mathbf{a}_t,\mathbf{s}_t$空间是组合的。考虑到智能体的个数可能有很多个，对每一个智能体的轨迹进行采样是计算上不可行的。为了补救，我们接下来会展示类似policy评估直接对计数器$\mathbf{n}~P(\mathbf{n};\pi)$进行采样计算梯度。类似的，也可以使用经验回报作为动作值函数$Q_t<sup>{\pi}(\mathbf{s}_t,\mathbf{a}_t)$的一个近似估计。这是标准的REINFORCE算法在CDec-POMDP上的应用。众所周知，REINFORCE可能比其他使用学习的动作值函数的方法学习的慢。因此，我们提出了一个$Q_t</sup>{\pi}$的近似函数，展示了直接采样计数器$\mathbf{n}$来计算策略梯度。</p><h3 id="使用估计动作值函数的策略梯度">使用估计动作值函数的策略梯度</h3><p>估计动作值函数$Q_t^{\pi}(\mathbf{s}<em>t,\mathbf{a}<em>t)$有几种不同的方式。我们考虑下列特征形式的近似值函数$f_w$：<br>$$Q_t^{\pi}(\mathbf{s}<em>t,\mathbf{a}<em>t)\approx f_w(\mathbf{s}<em>t,\mathbf{a}<em>t)=\sum</em>{m=1}<sup>Mf_w</sup>m(s_t<sup>m,o(s_t</sup>m,\mathbf{n</em>{s_t}}),s_t^m)$$<br>每一个智能体m都定义了一个$f_w<sup>m$，它的输入是智能体的局部state，action和observation。注意不同的$f_w</sup>m$是相关的，因为它们依赖于公共的计数器表$\mathbf{n}</em>{s_t}$。这样的一种分解方式是很有用的，因为它产生了有效的策略梯度计算方式。此外，CDec-POMDP中一类很重要的这种形式的估计值函数是兼容值函数最后会产生一个无偏的策略梯度。<br><strong>命题1</strong> CDec-POMDP中的兼容值函数可以分解成：<br>$$f_w(\mathbf{s}<em>t\mathbf{a}<em>t) = \sum_mf_w<sup>m(s_t</sup>m,o(s_t<sup>m,\mathbf{n}_{s_t}),a</sup>m)$$<br>我们可以直接用估计值函数$f_w$取代$Q^{\pi}(\cdot)$。经验上来说，我们发现使用这个估计的方差很大。我们利用$f_w$的结构进一步分解策略梯度会有更好的效果。<br><strong>定理2</strong> 对于任何具有如下的分解的值函数：<br>$$f_w(\mathbf{s}<em>t\mathbf{a}<em>t) = \sum_mf_w<sup>m(s_t</sup>m,o(s_t<sup>m,\mathbf{n}_{s_t}),a</sup>m)$$<br>策略梯度可以写成：<br>$$\Delta</em>{\theta}V_1(\pi)=\sum</em>{t=1}<sup>HE_{\mathbf{s}_t,\mathbf{a}_t)|b_o,\pi}\left[\sum_m\Delta_{\theta}log\pi(a_t</sup>m|s_t<sup>m,o(s_t</sup>m,\mathbf{n}</em>{s_t}))f_w<sup>m(s_t</sup>m,o(s_t<sup>m,\mathbf{n}_{s_t}),a_t</sup>m)\right]$$<br>上述结果展示了如果估计值函数被分解了，那么得到的策略梯度也是分解的。上述结果也可以应用到多种类型的智能体上，只要我们假设不同的智能体有不同的函数$f_t^m$。最简单的情况下，所有的智能体都是相同类型的，每一个智能体都有相同的函数$f_w$，推断出下式：<br>$$f_w(\mathbf{s}<em>t,\mathbf{a}<em>t) = \sum</em>{i,j}n_t(i,j)f_w(i,j,o(i,\mathbf{n}</em>{s_t}))$$<br>使用上式，我们可以将策略梯度简化成：<br>$$\Delta</em>{\theta}V_1(\pi) = \sum_tE</em>{\mathbf{s}<em>t,\mathbf{a}<em>t}\left[\sum</em>{i,j}n_t(i,j)\Delta</em>{\theta}log\pi (j|i,o(i,\mathbf{n}</em>{s_t}))f_w(i,j,o(i,\mathbf{n}</em>{s_t}))\right]$$</p><h3 id="基于计数器的策略梯度计算">基于计数器的策略梯度计算</h3><p>注意在上式中，期望仍然和联合state，action，$(\mathbf{s}<em>t,\mathbf{a}<em>t)$相关，当智能体的个数很大时效率很低。为了解决这个问题是<br><strong>定理3</strong> 对于任何拥有形式$f_w(\mathbf{s}<em>t,\mathbf{a}<em>t) = \sum</em>{i,j}n_t(i,j)f_w(i,j,o(i,\mathbf{n}</em>{s_t}))$的值函数，策略梯度都可以用下式计算：<br>\begin{equation}<br>E</em>{\mathbf{n}</em>{1:H}\in \Omega_{1:H}} \left[\sum_{t=1}^H\sum_{i\in S,j\in A}n_t(i,j) \Delta_{\theta}log\pi (j|i,o(i,\mathbf{n}_t)) f_w(i,j,o(i,\mathbf{n}<em>t))\right]<br>\end{equation}<br>上述结果展示了策略梯度可以类似于计算policy的值函数一样通过从底层分布$P(\cdot)$中采样计数表向量$\mathbf{n}</em>{1:H}$来计算策略梯度，在智能体数量很大的情况下也是可行的。</p><h2 id="训练动作值函数">训练动作值函数</h2><p>在我们的方法中，在计数器样本$\mathbf{n}<em>{1:H}$生成用来计算策略梯度后，我们还需要调整critic $f_w$的参数。注意对于每一个动作值函数$f_w(\mathbf{s}<em>t,\mathbf{a}<em>t)$只取决于联合state，action $(\mathbf{s}<em>t,\mathbf{a}<em>t)$生成的计数器。训练$f_w$可以通过一个梯度步最下化下列loss函数实现：<br>\begin{equation}<br>min_w\sum</em>{\xi=1}<sup>K\sum_{t=1}</sup>H\left(f_w(\mathbf{n}<em>t<sup>{\xi})-R_t</sup>{\xi}\right)^2<br>\end{equation}<br>其中$\mathbf{n}</em>{1:H}<sup>{\xi}$是从分布$P(\mathbf{n};\pi)$中生成的一个计数器样本；$f_w(\mathbf{n}_t</sup>{\xi})$是动作值函数，$R_t^{\xi}$是用式子$(1)$计算的$t$时刻的所有经验回报：<br>\begin{equation}<br>f_w(\mathbf{n}<em>t^{\xi}) = \sum</em>{i,j}n_t<sup>{\xi}(i,j)f_w(i,j,o(i,\mathbf{n}_t</sup>{\xi});R_t<sup>{\xi}=\sum_{T=t}</sup>H]\sum</em>{i\in S,j\in A}n_T{\xi}(i,j)r_T(i,j,\mathbf{n}<em>T^{\xi})<br>\end{equation}<br>然而，我们发现公式$(11)$中的loss函数在训练较大问题的critic时表现并不好。需要一定数量的计数器样本可靠的训练$f_w$，这对于拥有较多数量智能体的大问题的扩展有不利影响。已知在多智能体强化学习中单独利用全局reward信号的算法要比利用局部reward信号的方法多用一些样本。受到这些现象的启发，接下来我们提出了一个基于策略的局部reward信号去训练critic $f_w$。<br><strong>单个值函数</strong> 用$\mathbf{n}</em>{1:H}<sup>{\xi}$表示一个计数器样本。给定计数器样本$\mathbf{n}_{1:H}</sup>{\xi}$，用$V_t<sup>{\xi}(i,j)=E\left[\sum_{t’=t}</sup>Hr</em>{t’}<sup>m|s_t</sup>m=i,a_m<sup>t=j,n_{1:H}</sup>{\xi}\right]$表示一个智能体在时刻$t$处于state $i$，采取action $j$，所能得到的所有期望reward。这个单个的值函数可以用动态规划算法来计算。基于这个值函数，我们接下来展示了式子$(12)$中全局经验reward的重新参数化：<br><strong>引理(Lemma)1</strong> 给定计数器样本$\mathbf{1:H}<sup>{\xi}$，$t$时刻的经验回报$R_t</sup>{\xi}$可以被重新参数化为：<br>$$R_t^{\xi} = \sum</em>{i\in S,j\in A}n_t<sup>{\xi}(i,j)V_t</sup>{\xi}(i,j).$$<br><strong>基于单个值函数的loss</strong> 给出引理$1$，我们推导出式子$11$中真实loss的上界，它有效利用了单个值函数：<br>\begin{align*}<br>&amp;\sum</em>{\xi}\sum_t\left(f_w(\mathbf{n}<sup>{\xi})-R_t</sup>{\xi}\right)^2 \<br>= &amp;\sum_{\xi}\sum_t\left(\sum_{i,j}n_t<sup>{\xi}(i,j)f_w(i,j,o(i,\mathbf{n}_t</sup>{\xi}))-\sum_{i,j}n_t<sup>{\xi}(i,j)V_t</sup>{\xi}(i,h)\right)^2\<br>= &amp;\sum_{\xi}\sum_t\left( \sum_{i,j}n_t<sup>{\xi}(i,j)(f_w(i,j,o(i,\mathbf{n}_t</sup>{\xi}))-V_t<sup>{\xi}(i,h))\right)</sup>2\<br>\le &amp;M\sum_{\xi}\sum_{t,i,j}n_t(i,j)\left(f_w(i,j,o(i,\mathbf{n}_t<sup>{\xi}))-V_t</sup>{\xi}(i,j)\right)^2<br>\end{align*}<br>其中最后一部用了柯西施瓦茨不等式。我们用式子(14)中修改过的loss训练critic。按照经验来说，对于较大的问题，式子(14)中的新loss比式子(13)中的原始loss要收敛的快很多。直观上来说，这是因为式子(14)中的新loss尝试调整每一个critic组件$f_w(i,j,o(i,\mathbf{n}_t<sup>{\xi}))$更接近它的经验回报$V_t</sup>{\xi}(i,j)$。然而，原始的式子(13)中的loss着重于最小化全局loss，而不是调整每一个单个的critic因子$f_w(\cdot)$到相对应的每一个经验回报。<br>算法$1$展示了CDec-POMDP中AC算法的大纲。第$7$行和第$8$行展示了两种不同的方式训练critic。第$7$行代表基于局部值函数的critic更新，也可以称为factored cirtic更新(fC)。第$8$行展示了基于全局reward或者全局critic的更新©。第$10$行展示了使用定理$2$(fA)计算的策略梯度。第$11$行展示了直接使用$f_w$计算的梯度。</p><h2 id="实验">实验</h2><p>这一节中比较了我们的AC算法和另外两个解决CDec-POMDP问题的算法，Soft-Max based flow update(SMFU)，和期望最大化方法。SMFU只能优化智能体的action依赖于局部state的policy，$\pi(a_t<sup>m|s_t</sup>m)$，因为它通过计算在规划阶段单个最有可能的计数器向量来估计计数器$\mathbf{n}$的作用。EM方法优化基于计数器的分段线性policy，其中$\pi(a_t<sup>m|s_t</sup>m,\cdot)$是所有可能的计数器observation $o_t$空间上的一个分段函数。<br>算法$1$展示了更新critic的两种方式（第$7$行和第$8$行）和更新actor的两种方式（第$10$行和第$11$行），所以就有四种可能的AC方法－fAfC,AC,FfC,fAC。我们也研究了不同actor-critic方法的属性。在附录中有神经网络的结构和其他一些实验设置。<br>为了和之前方法公平的进行比较，我们使用了三种不同的模型用于基于计数的observation $o_t$。在$o0$设置中，policy只取决于智能体的局部state $s_t^m$并不需要计数器。在$o1$设置中，policy取决于局部state $s_t^m$和单个计数器observation $n_t(s_t<sup>m)$。也就是说，智能体只能观测到其他也在当前状态$s_t</sup>m$的智能体的计数器。在$oN$设置中，智能体能观测到它的局部state $s_t<sup>m$和当前状态$s_t</sup>m$的局部相邻状态内其他智能体的计数器。$oN$ observation模型提供给智能体最多的信息。然而，它也是最难优化的因为policy有更多的参数。SMFU方法只能在$o0$设置中起作用，EM方法和本文中的AC方法在所有设置中都能起作用。</p><!--**出租车调度** 我们在第二节中介绍的现实世界中的域测试了本文的方法。在这个问题中，目标是计算出租车policy优化整个车队的收入。数据包含亚洲一个大城市超过一年的出租车轨迹数据。我们使用了从数据集中提取到的车辆请求信息。平均来说，每天大概有$8000$辆出租车。整个城市被划分为$81$个空间，时间范围是$24$个小时划分为$48$个半小时的区间。图$2(a)$中展示了不同方法在不同的观测模型（$'o0','o1','oN'$)上的量化比较。我们测试了$4000$和$8000$辆出租车来验证是否出租车的数量会影响不同方法的性能。$y$轴展示了整个车队每天的利润。在$'o0'$设置下，所有的方法（fAfC-$o0$,SMFU,EM-$o0$）给出质量差不多的解，在$8000$个出租车上fAfC-$o0$和EM-$o0$表现的比SMFU稍微好一些。在$'o1'$设置下，-->]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;摘要&quot;&gt;摘要&lt;/h2&gt;
&lt;p&gt;分布式的部分可观测马尔科夫决策过程(Dec POMDP)为解决多智能体系统中的序列决策问题提供了一个框架。考虑到POMDP的计算复杂度，最近的研究主要集中在Dec-POMDP中一些易于处理但是比较实用的子问题。本文解决的就是其中的一个
      
    
    </summary>
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="http://mxxhcm.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="actor-critic" scheme="http://mxxhcm.github.io/tags/actor-critic/"/>
    
      <category term="论文" scheme="http://mxxhcm.github.io/tags/%E8%AE%BA%E6%96%87/"/>
    
  </entry>
  
  <entry>
    <title>EM(Expectation Maximization)算法</title>
    <link href="http://mxxhcm.github.io/2019/01/21/expectatin_maximization/"/>
    <id>http://mxxhcm.github.io/2019/01/21/expectatin_maximization/</id>
    <published>2019-01-21T02:22:45.000Z</published>
    <updated>2019-09-13T01:48:32.275Z</updated>
    
    <content type="html"><![CDATA[<h2 id="引言-introduction">引言(Introduction)</h2><h3 id="什么是期望最大化算法">什么是期望最大化算法</h3><p>期望最大化算法(Expectation Maximization,EM)，是利用参数估计的迭代法求解最大似然估计的一种方法。</p><h3 id="em和mle关系">EM和MLE关系</h3><p>MLE的目标是求解已知分布类型的单个分布的参数。<br>EM的目标是求解已知分布类型的多个混合分布的参数。<br>一般我们用到的极大似然估计都是求某种已知分布类型的单个分布的参数，如求高斯分布的均值和方差；而EM算法是用来求解已知分布类型，多个该已知类型分布的混合分布的参数，这句话听起来可能有些拗口，举个最常见的例子，高斯混合分布参数的求解，这个混合分布都是高斯分布，只是每个分布的参数不同而已。如果一个高斯分布，一个卡方分布是没有办法求解的。</p><h3 id="为什么叫它em算法">为什么叫它EM算法</h3><p>因为这个算法总共有两个迭代步骤，E步和M步。第一步是对多个分布求期望，固定每一个分布的参数，计算出混合分布的参数，即E步，第二步是对这个混合分布利用最大似然估计方法进行参数估计，即M步。</p><h2 id="推理过程">推理过程</h2><p>假设我们要求一个混合分布p的参数$\theta$，比如校园内男生和女生的身高参数，显然，男生和女生的身高服从的分布类型是相同的，但是参数是不一样的。这里通过引入一个隐变量$z$，求解出对应不同$z$取值的参数$\theta$的值。<br>\begin{align*}<br>p(x|\theta) &amp;= \sum_zp(x,z|\theta)\\<br>&amp;=\sum_zp(z|\theta)p(x|\theta, z) \tag{0}<br>\end{align*}<br>如果我们假设男女生的身高分布是一个高斯混合模型，现在要求它的参数$\theta$。混合模型的表达式可以写为：<br>\begin{align*}<br>p(x|\theta) &amp;= \sum_zw(z)N(x|\mu_z,\sigma_z)\\<br>&amp;=\sum_zp(z|\theta)p(x|\theta,z)<br>\end{align*}<br>其中$\sum_zw(z) = 1,\theta={w, \mu, \sigma}$，如果用最大似然估计来解该问题的话，log函数内有和式，不好优化，所以就要换种方法。<br>观测数据：$x=(x_1,\cdots, x_N)$<br>对应的隐变量：$z=(z_1,\cdots, z_N)$，$z_i$有$c$种取值。</p><p>\begin{align*}<br>l(\theta;x) &amp;= log p(x|\theta) \tag{1}\\<br>&amp;= log\prod_{i=1}^N\ p(x_i|\theta) \tag{2}\\<br>&amp;= \sum_{i=1}^Nlog\ p(x_i|\theta) \tag{3}\\<br>&amp;= \sum_{i=1}^Nlog\sum_zp(x_i,z|\theta) \tag{4}\\<br>\end{align*}<br>这里式子(4)中$\sum_zp(x,z|\theta)$该怎么变形，因为现在解不出来了。<br>最开始我想的是使用条件概率进行展开，即：<br>$$\sum_zp(x_i, z|\theta) = \sum_zp(x_i|z, \theta)p(z|\theta)$$<br>但是如果展开成这样子，就变成了文章开头给出的式子(0)，并没有什么用，不能继续化简了。<br>所以就对式子(4)做个变形<br>\begin{align*}<br>&amp;\ \ \ \ \sum_{i=1}^Nlog\sum_zp(x_i,z|\theta) \tag{4}\\<br>&amp;= \sum_{i=1}^Nlog\sum_zq(z|x_i)\frac{p(x_i,z|\theta)}{q(z|x_i)}, \ \ s.t.\sum_zq(z|x_i)=1 \tag{5}\\<br>&amp;\ge \sum_{i=1}^N \underbrace{\sum_zq(z|x_i)log\frac{p(x_i,z|\theta)}{q(z|x_i)}}_{L(q,\theta)},\ \ s.t. \sum_zq(z|x_i)=1 \tag{6}\\<br>\end{align*}<br>第(4)步到第(5)步引入了一个分布$q(z|x)$，就是给定一个观测数据$x$，隐变量$z$取值的概率分布。注意，$q(z)$是一个函数，但是给定$x$之后，$q(z|x)$是一个变量。然后因为变形之后还是没有求解，就利用杰森不等式做了缩放，将$log(sum())$变成了$sum(log())$，就变成了(6)式。<br>这里使用Jensen不等式的目的是使得缩放后的值还能取得和原式相等的值，重要的是等号能够取到。</p><h3 id="jensen不等式">Jensen不等式</h3><p>对于随机变量的Jensen不等式，当函数$f(x)$是凸函数的时候可以用下式表示：<br>$$f(E(x)) \le E(f(x))$$<br>当$f(x)$是凹函数的时候，有<br>$$f(E(x)) \ge E(f(x))$$</p><p>接下来我们就要求解使得式子(6)中杰森不等式等号成立的$q$分布的取值。这里有两种方法可以求解。</p><h3 id="拉格朗日乘子法">拉格朗日乘子法</h3><p>令<br>$$L(q,\theta) = \sum_z q(z|x_i)log{\frac{p(x_i,z|\theta)}{q(z|x_i)}}, s.t.\sum q(z|x_i) = 1 \tag{7}$$<br>构建拉格朗日目标函数：<br>\begin{align*}<br>L &amp;= L(q, \theta) + \lambda(\sum_zq(z|x)- 1) \tag{8}\\<br>&amp;= \sum_z q(z|x_i)log{\frac{p(x_i,z|\theta)}{q(z|x_i)}} + \lambda(\sum_z q(z|x_i) - 1)  \tag{9}<br>\end{align*}</p><p>对$L$求导，得到：<br>$$\frac{\partial L}{\partial q(z|x_i)} = log\frac{p(x_i, z|\theta)}{q(z|x_i)} + q(z|x_i)(-\frac{1}{q(z|x_i)}) + \lambda \tag{10}$$<br>令$\frac{\partial L}{\partial q(z|x_i)}$等于$0$，得到：$$log\frac{p(x_i, z|\theta)}{q(z|x_i)} = 1 - \lambda$$<br>两边同取$e$的对数：<br>$$\frac{p(x_i, z|\theta)}{q(z|x_i)} = e^{1-\lambda} \tag{11}$$<br>$$q(z|x_i) = e^{\lambda - 1}p(x_i, z|\theta) \tag{12}$$<br>两边同时求和得：<br>$$1 = e^{\lambda - 1}\sum_z p(x_i, z|\theta) \tag{13}$$<br>用$p$表示$e^{\lambda-1}$得到：<br>$$e^{\lambda-1} = \frac{1}{\sum_z p(x_i, z|\theta)}$$<br>将其代入式子(12)得：<br>\begin{align*}<br>q(z|x_i) &amp;= \frac{p(x_i, z|\theta)}{\sum_z p(x_i, z|\theta)}\\<br>&amp;= \frac{p(z, x_i|\theta)}{p(x_i|\theta)}\\<br>&amp;= p(z|x_i, \theta)  \tag{14}<br>\end{align*}</p><p>最后求出来$q(z|x_i) = p(z|x_i, \theta)$。</p><h3 id="杰森不等式成立条件">杰森不等式成立条件</h3><p>杰森不等式成立条件是常数，即：<br>$$\frac{p(x_i, z|\theta)}{q(z|x_i)} = c,  s.t. \sum q(z|x_i)=1 \tag{15}$$<br>则有:<br>$$p(x, z_i|\theta) = cq(z_i|x) \tag{16}$$<br>同时对式子左右两边求和，得到：<br>$$\sum p(x_i, z|\theta) = \sum cq(z|x_i) = c \tag{17}$$<br>将$c = \sum p(x_i, z|\theta)$代入式子(14)得：<br>\begin{align*}<br>q(z|x_i) &amp;= \frac{p(x_i, z|\theta)}{\sum p(x_i,z|\theta)}\\<br>&amp;= \frac{p(x_i, z)|\theta}{p(x_i|\theta)}\\<br>&amp;= p(z|x_i, \theta) \tag{18}<br>\end{align*}</p><h3 id="等号成立证明">等号成立证明</h3><p>上面两个方法都算出来在$q(z|x_i) = p(z|x_i, \theta)$时$L$能取得最大值。接下来证明这个这个$L$的最大值和$l$相等。<br>将$q = p(z|x_i, \theta)$代入$L(q, \theta)$得：<br>\begin{align*}<br>L(q, \theta) &amp;= L(p(z|x_i, \theta^t), \theta^t)\\<br>&amp;= \sum_z p(z|x_i, \theta^t) log\frac{p(z, x_i|\theta^t)}{p(z|x_i, \theta^t)} \\<br>&amp;= \sum_z p(z|x_i, \theta^t) log p(x_i|\theta^t)\\<br>&amp;= 1\cdot log p(x_i|\theta^t)\\<br>&amp;= log p(x_i|\theta^t)\\<br>&amp;= l(\theta^t; x_i)<br>\end{align*}</p><h3 id="另一种等号成立推导">另一种等号成立推导</h3><p>\begin{align*}<br>l(\theta; x) - L(q, \theta) &amp;= l(\theta; x_i) - \sum_z q(z|x_i) log{\frac{p(z, x_i|\theta)}{q(z|x_i)}}\\<br>&amp;= \sum_z q(z|x_i) log p(x_i|\theta) - \sum_z q(z|x_i) log{\frac{p(z, x_i|\theta)}{q(z|x_i)}}\\<br>&amp;= \sum_z q(z|x_i)log {\frac{p(x_i|\theta)q(z|x_i)}{p(z, x_i|\theta)}}\\<br>&amp;= \sum_z q(z|x_i)log {\frac{q(z|x_i)}{p(z|x_i, \theta)}}\\<br>&amp;= KL(q(z|x_i)||p(z|x_i,\theta))<br>\end{align*}<br>最后算出来两个函数之差是一个KL散度，是从$p$到$q$的KL散度。当前仅当$p=q$时取等，否则就非负。</p><h3 id="m步">M步</h3><p>\begin{align*}<br>L(q, \theta) &amp; = \sum_z q(z|x_i) log\frac{p(z, x_i|\theta)}{q(z|x_i)} \\<br>&amp; = \underbrace{\sum_z q(z|x_i)log{p(z, x_i|\theta)}}_{Expected\ complete\ log-likelyhood} - \underbrace{\sum_z q(z|x_i)l{q(z|x_i)}}_{Entropy}<br>\end{align*}</p><h2 id="em流程">EM流程</h2><h3 id="计算流程">计算流程</h3><p>（１）首先随机初始化模型的不同隐变量对应的参数，<br>（２）对于每一个观测，首先判断它对应的隐变量的分布。<br>（３）求期望<br>（４）最大似然估计求参数<br>用公式来表示如下：<br>E步：$q^{t+1} = arg\ max_q L(q, \theta^t)$<br>M步：$\theta^{t+1} = arg max_{\theta}L(q^{t+1}, \theta)$<br>E步就是根据$t$时刻的$\theta^t$利用概率$q$求出$L$的期望，然后M步使用最大似然估计计算出新的$\theta$，就这样迭代下去。</p><h2 id="em收敛性分析">EM收敛性分析</h2><p>EM算法的收敛性就是要证明$L(q=p(z|x_i, \theta^t) , \theta)$的值一直在增大。<br>\begin{align*}<br>L(p(z|x_i, \theta^{t+1}) , \theta^{t+1}) - L(p(z|x_i, \theta^{t}) , \theta^{t}) &amp;= log p(x_i|\theta^{t+1}) - log p(x_i|\theta^t)\\<br>&amp; \ge 0<br>\end{align*}</p><h2 id="例子">例子</h2><p>假如有两个硬币A和B，假设随机从A,B中选一个硬币，掷$10$次，重复$5$次实验，分别求出两个硬币正面向上的概率。假设硬币服从二项分布<br>$5$次实验结果如下：<br>5H5T<br>9H1T<br>8H2T<br>4H6T<br>7H3T</p><p>这个时候有两种情况</p><h3 id="知道每次选的是a还是b">知道每次选的是A还是B</h3><p>这个时候就变成了极大似然估计。</p><h3 id="不知道每次选的是a还是b">不知道每次选的是A还是B</h3><p>这个时候就用EM算法了。<br>首先随机初始化$\theta_A = 0.5, \theta_B = 0.5$，<br>对于每一个观测，首先判断它对应的隐变量的分布。<br>$i={1,2,3,4,5}$，分别代表$5$个实验。<br>首先求出$\theta_A$的参数。<br>$$P(z = A|x_i, \theta_A, \theta_B) = \frac{P(z = A|x_i, \theta_A)}{P(z = A|x_i, \theta_A) + P(z = B|x_i, \theta_B)}$$<br>$$P(z = B|x_i, \theta_A, \theta_B) = 1 - P(z = A|x_i,\theta_A,\theta_B)$$<br>然后计算下式：<br>\begin{align*}<br>L(q,\theta_A) &amp;= \sum_{i=1}^5 \sum_zp(z|x_i, \theta_A, \theta_B)log p(x_i|\theta)\\<br>&amp;= \sum_{i=1}^5 (p(z=A|x_i, \theta_A)log p(x_i|\theta_A) + p(z=B|x_i, \theta_B)log p(x_i|\theta_B))<br>\end{align*}<br>然后利用极大既然估计计算$\theta_A$和$\theta_B$的值。</p><h2 id="参考文献">参考文献</h2><p>1.<a href="https://www.zhihu.com/question/27976634/answer/153567695" target="_blank" rel="noopener">https://www.zhihu.com/question/27976634/answer/153567695</a><br>2.<a href="https://en.wikipedia.org/wiki/Jensen%27s_inequality" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Jensen's_inequality</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;引言-introduction&quot;&gt;引言(Introduction)&lt;/h2&gt;
&lt;h3 id=&quot;什么是期望最大化算法&quot;&gt;什么是期望最大化算法&lt;/h3&gt;
&lt;p&gt;期望最大化算法(Expectation Maximization,EM)，是利用参数估计的迭代法求解最大似然
      
    
    </summary>
    
      <category term="机器学习" scheme="http://mxxhcm.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://mxxhcm.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="非监督学习" scheme="http://mxxhcm.github.io/tags/%E9%9D%9E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="最大似然估计" scheme="http://mxxhcm.github.io/tags/%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/"/>
    
      <category term="EM" scheme="http://mxxhcm.github.io/tags/EM/"/>
    
      <category term="期望最大化" scheme="http://mxxhcm.github.io/tags/%E6%9C%9F%E6%9C%9B%E6%9C%80%E5%A4%A7%E5%8C%96/"/>
    
      <category term="Jensen不等式" scheme="http://mxxhcm.github.io/tags/Jensen%E4%B8%8D%E7%AD%89%E5%BC%8F/"/>
    
  </entry>
  
  <entry>
    <title>maximum likelyhood estimation</title>
    <link href="http://mxxhcm.github.io/2019/01/20/maximum-likelyhood-estimation/"/>
    <id>http://mxxhcm.github.io/2019/01/20/maximum-likelyhood-estimation/</id>
    <published>2019-01-20T07:22:45.000Z</published>
    <updated>2019-09-16T07:33:45.612Z</updated>
    
    <content type="html"><![CDATA[<h2 id="最大似然估计">最大似然估计</h2><p>前提：数据的分布已知（如服从高斯分布或者指数分布），但是分布参数未知。例如某学校学生的身高服从高斯分布$N(\mu, \sigma^2 )$，$\mu$和$\sigma^2 $未知。现随机抽取$200$个学生的身高，估计该学校学生身高的均值$\mu$和方差$\sigma^2 $。<br>概率密度：$p(x|\theta)$<br>样本集：$X=(x_1, x_2,\cdots, x_N), N=200$，$x_i$为第$i$个人的身高。<br>假设分布：$N(\mu, \sigma^2 )$，$\mu, \sigma^2 $未知</p><h3 id="联合概率密度函数">联合概率密度函数</h3><p>样本集中的$N$个样本是独立同分布的，它们的联合概率可以表示为：<br>$$L(\theta; X) = L(x_1, \cdots, x_n;\theta) = \prod_{i=1}^{N} p(x_i|\theta), \theta \in \Theta$$</p><h3 id="取对数">取对数</h3><p>因为$L$中包含乘法，不方便求导，可以对其取log，不改变函数的单调性，并且方便计算：<br>$$ln\ L(\theta; X) = ln\ L(\theta; x_1, \cdots, x_n) = \sum_{i=1}^N ln\ p(x_i|\theta), \theta \in \Theta$$</p><h3 id="求极值">求极值</h3><p>计算偏导数，令其等于$0$，取函数的极值点，因为只有一个极值点，所以一定是最大值点，即<br>$$\hat{\theta} = arg\ max_{\theta} ln\ L(\theta; x)$$<br>求偏导等于$0$即：<br>$$\frac{\partial ln\ L(\theta; X)}{\partial \theta} = \sum_{i=1}^N \frac{\partial ln\ p(x_i;\theta)}{\partial \theta}, \theta={\mu, \sigma^2}$$</p><h2 id="最大似然估计求解高斯分布">最大似然估计求解高斯分布</h2><p>若$p$为高斯分布，即$p(x; \theta) = \frac{1}{\sqrt{2\pi} \sigma} e^{-\frac{(x-\mu)^2 }{2 \sigma^2 } } $，$ln\ p(x;\theta) = -ln\ \sqrt{2\pi } - ln\ \sigma - \frac{(x_i-\mu)^2 }{ 2\sigma^2 }$<br>则：<br>$$ln\ L(\theta; X) = ln\ L(\theta; x_1, \cdots, x_n) = \sum_{i=1}^N ln\ p(x_i;\theta) = \sum_{i=1}^N \left(-ln\ \sqrt{2\pi} - ln\ \sigma - \frac{(x_i-\mu)^2 }{ 2\sigma^2 }\right), \theta \in \Theta$$</p><h3 id="求解-mu">求解$\mu$</h3><p>对$ \mu $ 求偏导得：<br>$$\frac{\partial ln\ L(\mu; X)}{\partial \mu} = \sum_{i=1}^N \frac{\partial ln\ p(x_i;\mu)}{\partial \mu} = \sum_{i=1}^N -\frac{2(x_i-\mu) }{2\sigma^2 } $$<br>令其等于$0$，即<br>$$\sum_{i=1}^N(x_i-\mu) = 0$$<br>解得$$ \mu = \frac{\sum_{i=1}^N x_i }{N}$$</p><h3 id="求解-sigma-2">求解$\sigma^2 $</h3><p>对$ \sigma^2 $求偏导，令$t=\sigma^2 $，则$ln\ p(x;\sigma^2 ) = ln\ p(x;t) = -ln\ \sqrt{2\pi} - ln\ \sqrt{t} - \frac{(x_i-\mu)^2 }{2t}$，有：<br>$$ \frac{\partial ln\ L(t; X)}{\partial t} = \sum_{i=1}^N \frac{\partial ln\ p(x_i;t)}{\partial t} = \sum_{i=1}^N\left( -\frac{1}{2t} + \frac{(x-\mu)^2 }{2t^2 }\right) $$<br>令其等于$0$，即<br>$$ \sum_{i=1}^N\left( -\frac{1}{2t} + \frac{(x-\mu)^2 }{2t^2 }\right) = 0 $$<br>解得：<br>$$ \sigma^2 = t = \frac{\sum_{i=1}^N (x_i) -\mu)^2 }{N} $$</p><h2 id="参考文献">参考文献</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;最大似然估计&quot;&gt;最大似然估计&lt;/h2&gt;
&lt;p&gt;前提：数据的分布已知（如服从高斯分布或者指数分布），但是分布参数未知。例如某学校学生的身高服从高斯分布$N(\mu, \sigma^2 )$，$\mu$和$\sigma^2 $未知。现随机抽取$200$个学生的身高，估
      
    
    </summary>
    
      <category term="概率论" scheme="http://mxxhcm.github.io/categories/%E6%A6%82%E7%8E%87%E8%AE%BA/"/>
    
    
      <category term="机器学习" scheme="http://mxxhcm.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="概率论" scheme="http://mxxhcm.github.io/tags/%E6%A6%82%E7%8E%87%E8%AE%BA/"/>
    
      <category term="最大似然估计" scheme="http://mxxhcm.github.io/tags/%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/"/>
    
      <category term="参数估计" scheme="http://mxxhcm.github.io/tags/%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1/"/>
    
      <category term="点估计" scheme="http://mxxhcm.github.io/tags/%E7%82%B9%E4%BC%B0%E8%AE%A1/"/>
    
  </entry>
  
  <entry>
    <title>Bayesian Networks</title>
    <link href="http://mxxhcm.github.io/2019/01/06/bayesian-networks/"/>
    <id>http://mxxhcm.github.io/2019/01/06/bayesian-networks/</id>
    <published>2019-01-06T06:32:55.000Z</published>
    <updated>2019-08-30T06:31:44.078Z</updated>
    
    <content type="html"><![CDATA[<h2 id="介绍">介绍</h2><p>贝叶斯网络是一个有向无环图(directed acyclic graphs)，它用节点代表随机变量，用边代表变量之间的依赖关系。</p><h2 id="意义">意义</h2><p>贝叶斯网络可以用来表示任意的联合分布。</p><h2 id="推理">推理</h2><p>贝叶斯网络的一个基本任务就是求后验概率。<br>在AI这本书中，贝叶斯网络中的变量被分为了证据变量(evidence variable)，隐变量(hidden variable)和查询变量(query variable)。<br>而在PRML这本书中，贝叶斯网络中的变量被分为了观测变量(observed variable)和隐变量(latent variable,hidden variable)。</p><p>具体的可以看另外两篇笔记有详细的记录。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;介绍&quot;&gt;介绍&lt;/h2&gt;
&lt;p&gt;贝叶斯网络是一个有向无环图(directed acyclic graphs)，它用节点代表随机变量，用边代表变量之间的依赖关系。&lt;/p&gt;
&lt;h2 id=&quot;意义&quot;&gt;意义&lt;/h2&gt;
&lt;p&gt;贝叶斯网络可以用来表示任意的联合分布。&lt;/p&gt;
&lt;
      
    
    </summary>
    
      <category term="机器学习" scheme="http://mxxhcm.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://mxxhcm.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="人工智能" scheme="http://mxxhcm.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
      <category term="概率图模型" scheme="http://mxxhcm.github.io/tags/%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/"/>
    
      <category term="贝叶斯网络" scheme="http://mxxhcm.github.io/tags/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BD%91%E7%BB%9C/"/>
    
      <category term="监督学习" scheme="http://mxxhcm.github.io/tags/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="模式识别" scheme="http://mxxhcm.github.io/tags/%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB/"/>
    
      <category term="有向图" scheme="http://mxxhcm.github.io/tags/%E6%9C%89%E5%90%91%E5%9B%BE/"/>
    
  </entry>
  
  <entry>
    <title>AI chapter 14 Probabilistic reasoning</title>
    <link href="http://mxxhcm.github.io/2019/01/06/AI-chapter-14-Probabilistic-reasoning/"/>
    <id>http://mxxhcm.github.io/2019/01/06/AI-chapter-14-Probabilistic-reasoning/</id>
    <published>2019-01-06T06:32:16.000Z</published>
    <updated>2019-08-30T05:42:18.607Z</updated>
    
    <content type="html"><![CDATA[<p>在这里加一些自己的总结，这一章主要讲的是贝叶斯网络，首先介绍了贝叶斯网络的定义，是一个有向无环图，节点代表随机变量，边代表因果关系。这里给出了贝叶斯公式的两个意义，一个是数值意义，用贝叶斯网络表示全概率分布，另一个是拓扑意义，给定某个节点的父节点，这个节点条件独立于所有它的非后裔节点，或者给定某个节点的马尔科夫毯，这个节点条件独立于所有其他节点。接下来讲了条件独立的高效表示，噪音或模型表示离散型父节点和离散型子节点之间的关系，用参数化模型表示连续型父节点和连续型子节点之间的关系，用probit模型或者logit模型表示连续型父节点和离散型子节点之间的关系。接下来就介绍了贝叶斯精确推理计算后验分布的集中方法，一种是枚举推理，一种是消元法。因为精确推理的复杂度太高了，没有实际应用价值，所以就给出了一些估计推理的方法，直接采样，拒绝采样，以及可能性加权，还有另一类采样方法，蒙特卡洛算法，主要介绍了吉布森采样，大概就是这些。后面的两个小节没有看。</p><p>第$13$章讲的是概率论的基础知识并且强调了在概率表示中独立(independence)和条件独立(conditional independence)之间的关系。本章引入了一个系统的方式–贝叶斯网络去表现独立和条件独立之间的关系。概括的来说，本章的内容可以分为以下五部分：</p><ol><li>首先定义了贝叶斯网络的语法(syntax)和语义(semantics)，并且展示了如何用贝叶斯网络表示不确定知识。</li><li>接下来介绍了概率推理在最坏的情况下是很难计算的(computionally intratable)，但是在很多情况下可以高效的完成。</li><li>介绍了一系列在精确推理(exact inference)不可行时可以采用的估计推理算法(approximate inference algorithms)。</li><li>介绍了一些在概率论中可以被应用到带对象和关系的世界的方法，即与命题，表示相对的一阶模型。</li><li>最后，介绍了一些其它不确定性推理的方法。</li></ol><h2 id="不确定域的知识表示-representing-knowledge-in-an-uncertain-domain">不确定域的知识表示(Representing knowledge in an uncertain domain)</h2><p>我们可以根据联合概率分布(full joint probability distribution)算出任何想要的概率值，但是随着随机变量个数的增加，联合概率分布可能会变得特别大。此外，一个一个的指定可能世界中的概率是不可行的。</p><h3 id="贝叶斯网络的定义">贝叶斯网络的定义</h3><p>如果在联合概率中引入独立和条件独立，将会显著的减少定义联合概率分布所需要的概率。所以这节就介绍了贝叶斯网络来表示变量之间的依赖关系。本质上贝叶斯网络可以表示任何联合概率分布，而且在很多情况下是非常精确地表示。一个贝叶斯网络是一个有向图，图中的节点包含量化后的概率信息。具体的说明如下：</p><ol><li>每一个节点对应一个随机变量，这个随机变量可以是离散的也可以是连续的。</li><li>有向边或者箭头连接一对节点。如果箭头是从节点$X$到节点$Y$，那么节点$X$称为节点$Y$的父节点。图中不能有环，因此贝叶斯网络是一个有向无环图(directed acyclic graph,DAG)。</li><li>每一个节点$X_i$有一个条件概率分布$P(x_i|Parents(X_i))$量化(quantifiy)父节点对其影响。</li></ol><p>网络的拓扑，即节点和边的集合，指定了条件概率分布之间的关系。箭头的直观意义是节点$X$对节点$Y$有直接的影响，$Y$发生的原因是其父节点的影响。通常对于一个领域(domain)的专家来说，指出该域受哪些因素的直接影响要比直接给出它的概率值简单的多。一旦贝叶斯网络的拓扑结构定了，给出一个变量的父节点，我们仅仅需要给出每个节点的条件概率分布。我们能看出，拓扑和条件概率的组合能计算出所有变量的联合概率分布。</p><h3 id="贝叶斯网络的示例">贝叶斯网络的示例</h3><h4 id="牙疼和天气">牙疼和天气</h4><p>给定一组随机变量牙疼(Toothache)，蛀牙(Cavity)，拔牙(Catch)和天气(Weather)。Weather是独立于另外三个随机变量的，此外，给定Cavity，Catch和Toothache是条件独立的，即给定Cavity，Catch和Toothache是相互不受影响的，如下图所示。正式的：给定Cavity，Toochache和Catch是条件独立的，图中Toothache和Catch之间缺失的边体现出了条件独立。直观上，网络表现出Cavity是Toothache和Catch发生的直接原因，然而在Toothache和Catch之间没有直接的因果关系。<br><img src="/2019/01/06/AI-chapter-14-Probabilistic-reasoning/" alt="figure 14.1"></p><h4 id="警报和打电话">警报和打电话</h4><p>我家里有一个新安装的防盗警报(burglar alarm)，这个警报对于小偷的检测是相当可靠的，但是也会对偶然发生的微小的地震响应。我有两个邻居(Mary和John)，他们听到警报后会打电话给我。John有时会把电话铃和警报弄混了，也会打电话。Mary听音乐很大声，经常会错过警报。现在给出John或者Mary谁是否打电话，估计警报响了的概率。<br><img src="/2019/01/06/AI-chapter-14-Probabilistic-reasoning/" alt="figure 14.2"><br>该例子的贝叶斯网络如上图所示。该网络体现了小偷和地震两个因素会直接影响警报响的概率，但是John和Mary会不会打电话只取决于警报有没有响。贝叶斯网络展示出了我们的假设，即John和Mary不直接观察小偷有没有来，也不直接观察小的地震是否，也不受之前是否打过电话的影响。上图中的条件概率分布以一个条件概率分布表(conditional probability table,CPT)的形式展现了出来。这个表适合离散型的随机变量，但是不适合连续性随机变量。没有父节点的节点只有一行，用来表示随机变量的可能取值的先验概率(prior probabilities)。<br>注意到这个网络中没有节点对应Mary听音乐很大时，也没有节点对应John把电话铃声当成了警报。事实上这些因素都被包含在和边Alarm到JohnCalls和MaryCall相关的不确定性中了，概率包含了无数种情况可能让警报失灵（停电，老鼠咬坏了，等等）或者John和Mary没有打电话的原因（吃饭去了，午睡了，休假了等等），这些不确定性都包含在了概率中了。</p><h2 id="贝叶斯网络的意义-the-semantics-of-bayesian-networks">贝叶斯网络的意义(the semantics of bayesian networks)</h2><p>上一节主要讲的是什么是贝叶斯网络，但是没有讲它的意义。本节主要给出两种方式可以理解贝叶斯网络的意义。第一个是一种数值化的意义，即&quot;numerical semantics&quot;，把它当成联合概率分布的一种表示形式。第二个是一种拓扑的意义，即&quot;topological semantics&quot;，将它看成条件独立的一种编码方式。事实上，这两种方式是等价的，但是第一种方式更有助于理解如何构建贝叶斯网络，第二种方式更有助于设计推理过程。</p><h3 id="贝叶斯网络表示联合分布-representing-the-full-joint-distribution">贝叶斯网络表示联合分布(Representing the full joint distribution)</h3><h4 id="定义">定义</h4><p>一个贝叶斯网络是一个有向无环图，并且每个节点都有一个数值参数。数值方式给出这个网络的意义是，它代表了所有变量的联合概率分布。之前说过节点上的值代表的是条件概率分布$P(X_i|Parents(X_i)$，这是对的，但是当赋予整个网络意义以后，这里我们认为它们只是一些数字$\theta(X_i|Parents(X_i)$。<br>联合概率中的一个具体项(entry)表示的是每一个随机变量取某个值的联合概率，如$P(X_1=x_1 \wedge \cdots\wedge X_n = x_n)$，缩写为$P(x_1,\cdots,x_n)$。这个项的值可以通过以下公式进行计算：<br>$$P(x_1,\cdots,x_n) = \prod_{i=1}^n \theta(x_i|parents(X_i)),$$<br>其中$parents(X_i)$表示节点$X_i$在$x_1,\cdots,x_n$中的父节点。因此，联合概率分布中的每一项都可以用贝叶斯网络中某些条件概率的乘积表示。从定义中可以看出，很容易证明$\theta(x_i|parents(X_i))$就是条件概率$P(x_i|parents(X_i))$，因此，我们可以把上式写成：<br>$$P(x_1,\cdots,x_n) = \prod_{i=1}^n P(x_i|parents(X_i)),$$<br>换句话说：根据上上个式子定义的贝叶斯网络的意义，我们之前叫的条件概率表真的是条件概率表。（这句话。。。）</p><h4 id="示例">示例</h4><p>我们可以计算出警报响了，但是没有小偷或者地震发生，John和Mary都打电话了的概率。即计算联合分布$P(j,m,a,\neg b, \neg e)$（使用小写字母表示变量的值）：<br>\begin{align*}<br>P(j,m,a,\neg b, \neg e) &amp;=P(j|a)P(m|a)P(a|\neg b \wedge \neg e)P(\neg b)P(\neg e)\<br>&amp;=0.90\times 0.70\times 0.001 \times 0.999 \times 0.998\<br>&amp;=0.000628<br>\end{align*}</p><h4 id="构建贝叶斯网络-constructing-bayesian-networks">构建贝叶斯网络(Constructing Bayesian networks)</h4><p>上面给出了贝叶斯网络的一种意义，接下来给出如何根据这种意义去构建一个贝叶斯网络。确定的条件独立可以用来指导网络拓扑的构建。首先，我们把联合概率的项用乘法公式写成条件概率表示：<br>$$P(x_1,\cdots,x_n) = P(x_n|x_{n-1},\cdots,x_1)P(x_{n-1},\cdots,x_1)$$<br>接下来重复这个过程，将联合概率(conjunctive probability)分解成一个条件概率和一个更小的联合概率。最后得到下式：<br>\begin{align*}<br>P(x_1,\cdots,x_n) &amp;= P(x_n|x_{n-1},\cdots,x_1)P(x_{n-1}|,x_{n-2}\cdots,x_1)\cdots P(x_2|x_1)P(x_1)\<br>&amp;= \prod_{i=1}^nP(x_i|x_{i-1},\cdots,x_1)<br>\end{align*}<br>这个公式被称为链式法则，它对于任意的随机变量集都成立。对于贝叶斯网络中的每一个变量$X_i$，如果给定$Parents(X_i) \subset {X_{i-1},\cdots,X_1}$（每一个节点的序号应该和图结构的偏序结构一致），那么有：<br>$$P(x_1,\cdots,x_n) = \prod_{i=1}^n P(x_i|parents(X_i)),$$<br>将它和上式对比，得出：<br>$$P(X_i|X_{i-1},\cdots,X_1) = P(X_i|Parents(X_i).$$<br>这个公式成立的条件是给定每个节点的父节点，它条件独立于所有它的非父前置节点。这里给出一个生成贝叶斯网络的方式：</p><ol><li>节点：首先，确定需要对领域建模所需要的随机变量集合。对它们进行排序：${X_1,\cdots,X_n}$，任意顺序都行，但是如果随机变量的因(causes)在果(effects)之前，最终的结果会更加紧凑。</li><li>边：从$i = 1$到$n$，</li></ol><ul><li>从$X_1,\cdots,X_{i-1}$中选出$X_i$的最小父节点集合。</li><li>对于每一个父节点，插入一条从父节点到$X_i$的边。</li><li>写下条件概率表，$P(X_i| Parents(X_i))$。</li></ul><p>直观上，$X_i$的父节点应该包含$X_1,\cdots,X_{i-1}$中所有直接影响$X_i$的节点。因为每一个节点都只和它前面的节点相连，这就保证了每个网络都是无环的(acyclic)。此外，贝叶斯网络还不包含冗余的概率值，如果有冗余值，就会产生不一致：不可能生成一个违反概率论公理的贝叶斯网络。</p><h4 id="紧凑性和节点顺序-compactness-and-node-ordering">紧凑性和节点顺序(Compactness and node ordering)</h4><h5 id="紧凑性-compactness">紧凑性(compactness)</h5><p>因为不包含冗余信息，贝叶斯网络会比联合概率分布更加紧凑，这让它能够处理拥有很多变量的任务。贝叶斯网络的紧凑性是稀疏(sparse)系统或者局部结构化(local structured)系统普遍拥有的稀疏性的一个例子。在一个局部结构化系统中，每一个子部件仅仅和有限数量的其他部件进行交互，而不用管整个系统。局部结构化的复杂度通常是线性增加的而不是指数增加的。在贝叶斯网络中，一个随机变量往往最多受$k$个其他随机变量直接影响，这里的$k$是一个常数。为了简化问题，我们假设有$n$个布尔变量，指定一个条件概率表所需要的数字最多是$2<sup>k$个，整个网络则需要$n2</sup>k$个值；作为对比，联合概率分布需要$2^n$个值。举个例子，如果我们有$n=30$个节点，每一个节点至多有五个父节点(k=5)，那么贝叶斯网络只需要$960$个值，而联合概率分布需要超过十亿个值。<br>但是在某些领域，可能每一个节点都会被所有其他节点直接影响，这时候网络就成了全连接的网络(fully connected)，它和联合概率分布需要同样多的信息。有时候，增加一条边，也就是一个依赖关系，可能会对结果产生影响，但是如果这个依赖很弱(tenuous)，添加这条边的花费比获得的收益还要大，那么就没有必要加这条边了。比如，警报的那个例子，如果John和Mary感受到了地震，他们认为警报是地震引起的，所以就不打电话了。是否添加Earthquake到JohnCalls和MaryCalls这两条边取决于额外的花费和得到更高的警报率之间的关系。</p><h5 id="节点顺序-node-ordering">节点顺序(node ordering)</h5><p>即使在一个局部结构化的领域，只有当我们选择好的节点顺序的时候，我们才能得到一个紧凑的贝叶斯网络。考虑警报的例子，我们给出下图：<br><img src="/2019/01/06/AI-chapter-14-Probabilistic-reasoning/" alt="figure 14.3"><br>Figure 14.2和Figure 14.3两张图中的三个贝叶斯网络表达的都是同一个联合分布，但是Figure 14.3中的两张图没有表现出来条件独立，尤其是Figure 14.3(b)中的贝叶斯网络，它需要用和联合分布差不多相同个数的值才能表现出来。可以看出来，节点的顺序会影响紧凑性。</p><h3 id="贝叶斯网络中的条件独立-conditional-independence-relations-in-bayesion-networks">贝叶斯网络中的条件独立(Conditional independence relations in Bayesion networks)</h3><p>贝叶斯网络的一个数值意义(“numerical” semantics)是用来表示联合概率分布。根据这个意义，给定每个节点的父节点，使得每一个节点条件独立于它的父节点之外的节点，我们能构建一个贝叶斯网络。此外我们也可以从用图结构编码整个条件独立关系的拓扑意义出发，然后推导出贝叶斯网络的数值意义。拓扑语义说的是给定每个节点的父节点，则该节点条件独立于所有它的非后裔(non-descendants)节点。举例来说，Figure 14.2的警报例子中，给定alarm后，JohnCalls独立于Burglary,Eqrthquake和MaryCalls。如图Figuree 14.4(a)中所示。从条件独立断言(assertions)和网络参数$\theta(x_i|parents(X_i))$就是条件概率$P(x_i|parents(X_i))$的解释中，联合概率可以计算出来。在这种情况下，数值意义和拓扑语义是相同的。<br>另一个拓扑意义的重要属性是：给定某个节点的马尔科夫毯(Markov blanket)，即节点的父节点，子节点，子节点的父节点，这个节点条件独立于所有其他的节点。如图Figure 14.4(b)所示。</p><h2 id="条件分布的高效表示-efficient-representation-of-conditional-distributions">条件分布的高效表示(Efficient representation of conditional distributions)</h2><p>即使每个节点有$k$个父节点，一个节点的CPT还需要$O(2^k)$，最坏的情况下父节点和子节点是任意连接的。一般情况下，这种关系可以用符合一些标准模式(standard pattern)的规范分布(canonical distribution)表示，这样子就可以仅仅提供分布的一些参数就能生成整个CPT。<br>最简单的例子是确定性节点(deterministic node)。一个确定性节点的值被它的父节点的值精确确定。这个确定性关系可以是逻辑关系：父节点是加拿大，美国和墨西哥，子节点是北美洲，它们之间的关系是子节点是父节点所在的洲。这个关系也可以是数值型的，一条河的流量是流入它的流量减去流出它的流量。<br>不确定关系通常称为噪音逻辑关系(noisy logical relationships)。一个例子是噪音或(noisy-OR)，它是逻辑或的推广。在命题逻辑中，当且仅当感冒(Cold)，流感(Flu)或者疟疾(Malaria)是真的时候，发烧(Fever)才是真的。噪音或模型允许不确定性，即每一个父节点都有可能让子节点为真，可能父节点和子节点之间的关系被抑制了(inhibited)，可能一个人感冒了，但是没有表现出发烧。这个模型做了两个假设。第一个，它假设所有的原因都被列了出来，有时候会加一个节点(leak node)包含所有的其他原因(miscellaneous causes)。第二个，抑制每一个父节点和子节点之间的原因是独立的，比如抑制疟疾产生发烧和抑制感冒产生发烧的原因是独立的。所以，当且仅当所有的父节点都是假的时候，发烧才一定不会发生。给出以下的假设：<br>$q_{cold} = P(\neg fever| cold,\neg flu, \neg malaria) = 0.6$<br>$q_{flu} = P(\neg fever|\neg cold, flu, \neg malaria) = 0.2$<br>$q_{malaria} = P(\neg fever|\neg cold,\neg flu, malaria) = 0.1$<br>根据这些信息，以及噪音或的假设，整个CPT可以被创建。一般的规则是：<br>$P(x_i|parents(X_i)) = 1 - \prod_{j:X_j=ture} q_j.$<br>最后生成如下的表：</p><table><thead><tr><th style="text-align:center">Cold</th><th style="text-align:center">Flu</th><th style="text-align:center">Malaria</th><th style="text-align:center">P(Fever)</th><th style="text-align:center">P($\neg$Fever)</th></tr></thead><tbody><tr><td style="text-align:center">F</td><td style="text-align:center">F</td><td style="text-align:center">F</td><td style="text-align:center">$0.0$</td><td style="text-align:center">$1.0$</td></tr><tr><td style="text-align:center">F</td><td style="text-align:center">F</td><td style="text-align:center">T</td><td style="text-align:center">$0.9$</td><td style="text-align:center">$0.1$</td></tr><tr><td style="text-align:center">F</td><td style="text-align:center">T</td><td style="text-align:center">F</td><td style="text-align:center">$0.8$</td><td style="text-align:center">$0.2$</td></tr><tr><td style="text-align:center">F</td><td style="text-align:center">T</td><td style="text-align:center">T</td><td style="text-align:center">$0.98$</td><td style="text-align:center">$0.1\times 0.2=0.02$</td></tr><tr><td style="text-align:center">T</td><td style="text-align:center">F</td><td style="text-align:center">F</td><td style="text-align:center">$0.4$</td><td style="text-align:center">$0.6$</td></tr><tr><td style="text-align:center">T</td><td style="text-align:center">F</td><td style="text-align:center">T</td><td style="text-align:center">$0.94$</td><td style="text-align:center">$0.6\times 0.1 = 0.06 $</td></tr><tr><td style="text-align:center">T</td><td style="text-align:center">T</td><td style="text-align:center">F</td><td style="text-align:center">$0.88$</td><td style="text-align:center">$0.5\times 0.2 = 0.12 $</td></tr><tr><td style="text-align:center">T</td><td style="text-align:center">T</td><td style="text-align:center">T</td><td style="text-align:center">$0.988$</td><td style="text-align:center">$0.6\times 0.2\times 0.1 = 0.012$</td></tr></tbody></table><p>对于这个表，感觉自己一直有点转不过来圈。就是有症状不一定发烧，也可能不发烧，没有症状一定不发烧。什么时候不发烧呢，只有某个症状表现出来不发烧，如果多个症状的话，直接把有症状表现但不发烧的概率相乘。<br>一般情况下，噪声逻辑模型中，有$k$个父节点的变量可以用$O(k)$个参数表示而不是$O(2^k)$去表示整个CPT。这让访问(assessment)和学习(learning)更容易了。</p><h3 id="连续性随机变量的贝叶斯网络-bayesian-nets-with-continuous-variables">连续性随机变量的贝叶斯网络(Bayesian nets with continuous variables)</h3><h4 id="常用方法">常用方法</h4><p>现实中很多问题都是连续型的随机变量，它们有无数可能的取值，所以显式的指定每一个条件概率行不通。常用的总共有三种方法，第一个可能的方法是离散(discretization)连续型随机变量，将随机变量的可能取值划分成固定的区间。比如，温度可以分成，小于$0$度的，$0$度到$100$度之间的，大于$100$度的。离散有时候是可行的，但是通常会造成精度的缺失和非常大的CPT。第二个方法也是最常用的方法是通过指定标准概率密度函数的参数，比如指定高斯分布的均值和方差。第三种方法是非参数化(nonparametric)表示，用隐式的距离去定义条件分布。</p><h4 id="示例-v2">示例</h4><p>一个同时拥有离散型和随机性变量的网络被称为混合贝叶斯网络(hybrid Bayesian network)。为了创建这样一个网络，我们需要两种新的分布。一种是给定离散或者连续的父节点，子节点是连续型随机变量的条件概率，另一种是给定连续的父节点，子节点是离散型随机变量的条件概率。</p><h5 id="连续型子节点">连续型子节点</h5><p><img src="/2019/01/06/AI-chapter-14-Probabilistic-reasoning/" alt="figure 14.5"><br>考虑Figure 14.5的例子，一个顾客买了一些水果，买水果的量取决取水果的价格(Cost)，水果的价格取决于收成(Harvest)和政府是否有补助(Subsidy)。其中，Cost是连续型随机变量，他有连续的父节点Harvest和离散的父节点Subsidy，Buys是离散的，有一个连续型的父节点Cost。<br>对于变量Cost，我们需要指定条件概率$P(Cost|Subsidy,Harvest)$。离散的父节点通过枚举(enumeration)来表示，指定$P(Cost|subsidy,Harvest)$和$P(Cost|\neg subsidy,Harvest)$。为了表示Harvest，可以指定一个分布来表示变量Cost的值$c$取决于连续性随机变量Harvest的值$h$。换句话说，将$c$看做一个$h$的函数，然后给出这个函数的参数即可，最常用的是线性高斯分布。比如这里，我们可以用两个不同参数的高斯分布来表示有补贴和没补贴时Harvest对Cost的影响：<br>$$P(c|h, subsidy) = N(a_th+b_t,\sigma_t^2)© = \frac{1}{\sigma_t \sqrt{2\pi} }e^{- \frac{1}{2}(\frac{c-(a_th+b_t)}{\sigma_t})^2}$$<br>$$P(c|h,\neg subsidy) = N(a_fh+b_t,\sigma_f^2)© = \frac{1}{\sigma_f \sqrt{2\pi} }e^{- \frac{1}{2}(\frac{c-(a_fh+b_f)}{\sigma_f})^2}$$<br>所以，只需要给出$a_t,b_t,\sigma_t,a_f,b_f,\sigma_f$这几个参数就行了，Figure 14.6(a)和(b)就是一个示例图。注意到坡度(slope)是负的，因为随着供应的增加，cost在下降，当然，这个线性模型只有在harvest在很小的一个区间内才成立，而且cost有可能为负。假设有补贴和没补贴的两种可能性相等，是$0.5$，那么就有了Figure 14.6©的图$P(c|h)$。</p><h5 id="连续型父节点">连续型父节点</h5><p>当离散型随机变量有连续型父节点时，如Figure 14.5中的Buys节点。我们有一个合理的假设是：当cost高的时候，不买，cost底的时候，买，在中间区域买不买是一个变化很平滑的概率。我们可以把条件分布当成一个软阈值函数(soft-threshold)，一种方式是用标准正态分布的积分(intergral)。<br>$$\Phi(x) = \int_{-\infty}^{x} N(0,1)(x)dx$$<br>给定Cost买的概率可能是:<br>$$P(buys|Cost = c) = \Phi((-x+\nu)/ \sigma))$$<br>其中cost的阈值在$\nu$附近，阈值的区域和正比于$\sigma$，当价格升高的时候，买的概率会下降。这个probit distribution模型如Figure 14.7(a)所示。<br>另一个可选择的模型是logit distribution，使用logistic function $1/(1+e^{-x})$来生成一个软阈值：<br>$$P(buys|Cost = c) = \frac{1}{1+exp(-2\frac{-c+u}{\sigma})}.$$<br>如Figure 14.7(b)所示，这两个分布很像，但是logit有更长的尾巴。probit更符合实际情况，但是logit数学上更好算。它们都可以通过对父节点进行线性组合推广到多个连续性父节点的情况。</p><h2 id="贝叶斯网络的精确推理-exact-inference-in-bayesian-networks">贝叶斯网络的精确推理(Exact inference in bayesian networks)</h2><p>概率推理系统的基本任务就是给出一些观察到的事件，即给证据变量(evidence variable)赋值，然后计算一系列查询变量(query variable)的后验概率。我们用$X$表示查询变量，用$\mathbf{E}$表示证据变量$E_1,\cdots,E_m$的集合，$\mathbf{e}$是一个特定的观测事件，$\mathbf{Y}$表示既不是证据变量，也不是查询变量的变量$Y_1,\cdots,Y_l$的集合（隐变量,hidden variables)。变量的所有集合是$\mathbf{X}={X}\cup \mathbf{E}\cup \mathbf{Y}$。一个典型的查询是求后验概率$P(X|\mathbf{e})$。<br>在这一节中主要讨论的是计算后验概率的精确算法以及这些算法的复杂度。事实上，在一般情况下精确推理的复杂度都是很高的，为了降低复杂度，就只能进行估计推理(approximate inference)了，这个会在下一节中介绍到。</p><h3 id="枚举实现精确推理-inference-by-enumeration">枚举实现精确推理(Inference by enumeration)</h3><p>任何条件概率都可以用联合概率分布的项相加得到，即：<br>$$P(X|\mathbf{e}) = \alpha P(X,\mathbf{e}) = \alpha \sum_{\mathbf{y}}P(X,\mathbf{e},\mathbf{y})$$<br>贝叶斯网络给出了所有的联合概率分布，任何项$P(x,\mathbf{e},\mathbf{y})$都可以用贝叶斯网络中的条件概率的乘积表示出来。比如警报例子中的查询$P(Burglary|JohnCalls=true,MaryCalls=true)$。隐变量是Earthquake和Alarm，我们可以算出：<br>$$P(B|j,m) = \alpha P(B,j,m) = \alpha \sum_{e}\sum_{a}P(B,j,m,e,a).$$<br>贝叶斯网络已经给出了所有CPT项的表达式，比如当Burglary = true时：<br>$$P(b|j,m) = \alpha \sum_e\sum_aP(b,j,m,e,a) = \alpha \sum_e\sum_aP(b)P(e)P(a|b,e)P(j|a)P(m|a).$$<br>为了计算这个表达式，我们得计算一个四项的加法，分别是e为true和false,a为true和false对应的$P(b,j,m)$的值，每一项都是五个数的乘法。最坏的情况下，所有的变量都用到了，那么拥有$n$个布尔变量的贝叶斯网络的时间复杂度是$O(n2^n)$。我们可以做一些简化，将一些重复的计算保存下来，比如将上面的式子变成：<br>$$P(b|j,m) = \alpha \sum_e\sum_aP(b,j,m,e,a) = \alpha P(b) \sum_eP(e)\sum_aP(a|b,e)P(j|a)P(m|a).$$<br>这样子可以按照顺序进行计算，具体的计算过程如Figure 14.8所示。这种算法叫做ENUMERATION-ASK，它的空间复杂度是线性的，但是它的事件复杂度是$O(2<sup>n)$比$O(n2</sup>n)$要好，却仍然是实际上不可行的。（这里我理解的是$O(2<sup>n)$而不是$O(n2</sup>n)$的原因是，总共有$n$个布尔变量，所以总共有$2^n$个可能的取值，每次算一个，存一个，而原来的是算完之后不存。）<br>事实上，Figure 14.8中的计算过程还有很多重复计算，比如$P(j|a)P(m|a)$和$P(j|\neg a)P(m|\neg a)$这两项被计算了两次。我原来在想这里是不是和上面一段说的冲突了，事实上是没有的，这$2^n$个值，其中可能会有$P(b,j,m,e,a)$和$P(b,j,m,e,\neg a)$，这两个概率中都用到了$P(j|a)P(m|a)$，但是这里就会计算两次，事实上有很多值都会被重复计算很多次。下面就介绍一个避免这种运算的方法。</p><h3 id="消元法-the-variable-elimination-algorithm">消元法(The variable elimination algorithm)</h3><p>上面问题的解决思路就是保存已经计算过的值，实际上这是一种动态规划。还有很多其他方法可以解决这个问题，这里介绍了最简单的消元算法。消元法对表达式进行从右至左的计算，而枚举法是自底向上的。所有的中间值被报存起来，最对和每个变量有关的表达式进行求和。例如对于下列表达式：<br>$$P(B|j,m) = \alpha \underbrace{P(B)}<em>{f_1(B)} \sum_e\underbrace{P(e)}</em>{f_2(E)} \sum_a\underbrace{P(a|B,e)}<em>{f_3(A,B,E)} \underbrace{P(j|a)}</em>{f_4(A)} \underbrace{P(m|a)}_{f_5(A)}.$$<br>表达式的每一部分都是一个新的因子，每一个因子都是由它的参数变量(argument variables)决定的矩阵，参数变量指定的取值是没有固定的变量。比如因子$f_4(A)$和$f_5(A)$对应$P(j|a)$和$P(m|a)$的表达式只取决于$A$的值因为$J$和$M$在这个查询中都是固定的。它们都是两个元素的向量：<br>$$f_4(A) = \begin{pmatrix}P(j|a)\P(j|\neg a)\end{pmatrix} = \begin{pmatrix}0.90\0.05\end{pmatrix}$$<br>$$f_5(A) = \begin{pmatrix}P(m|a)\P(m|\neg a)\end{pmatrix} = \begin{pmatrix}0.70\0.01\end{pmatrix}$$<br>$f_3(A,B,E)$是一个$2\times 2\times 2$的矩阵。用因子表达的话，查询的表达式变成了：<br>$$P(B|j,m) = \alpha f_1(B)\times \sum_ef_2(E)\times \sum_af_3(A,B,E)\times f_4(A)\times f_5(A)$$<br>其中$\times$不是普通的矩阵乘法，而是对应元素相乘(pointwise product)。整个表达式的计算过程可以看成从右到左变量相加的过程，将现有的因子消去产生新的因子，最后只剩下一个因子的过程。具体的步骤如下：<br>首先先利用$f_3,f_4,f_5$把变量$A$消掉，产生一个新的$2\times 2$的只含有变量$B$和$E$的新因子$f_6(B,E)$：<br>\begin{align*}<br>f_6(B,E) &amp;= \sum_af_3(A,B,E)\times f_4(A) \times f_5(A)\<br>&amp;= (f_3(a,B,E)\times f_4(a) \times f_5(a)) + (f_3(\neg a,B,E)\times f_4(\neg a)\times f_5(\neg a)<br>\end{align*}<br>这样目标变成了：<br>$$P(B|j,m) = \alpha f_1(B)\times \sum_ef_2(E)\times \sum_af_6(B,E)$$<br>利用$f_2,f_6$消去$E$：<br>\begin{align*}<br>f_7(B) &amp;= \sum_ef_2(E)\times \sum_af_6(B,E)\<br>&amp; = f_2(e)\times f_6(B,e) + f_2(\neg e)\times f_6(B,\neg e)<br>\end{align*}<br>将表达式化成：<br>$$P(B|j,m) = \alpha f_1(B)\times f_7(B)$$<br>显然，根据这个表达式就可以计算出我们想要的结果了。上面的过程可以总结成两步，第一步是point-wise的因子乘法，第二步是利用因子的乘法进行消元。</p><h4 id="因子运算-operations-on-factors">因子运算(Operations on factors)</h4><p>两个因子$f_1$和$f_2$进行point-wise乘法运算产生新的因子(factor)$f$的变量是$f_1$和$f_2$变量的并，新的因子中的元素的值是$f_1$和$f_2$中对应项的积。假设两个因子有公共变量$Y_1,\cdots,Y_k$，那么就有：<br>$$f(X_1,\cdots,X_j,Y_1,\cdots,Y_k,Z_1,\cdots,Z_l)=f_1(X_1,\cdots,X_j,Y_1,\cdots,Y_k)f_2(Y_1,\cdots,Y_k,Z_1,\cdots,Z_l).$$<br>如果所有的变量都是二值化的，那么$f_1$和$f_2$各有$2<sup>{j+l}$和$2</sup>{l+k}$项，$f$有$2^{j+l+k}$项。比如，$f_1(A,B),f_2(B,C)$，那么point-wise乘法产生的$f_3(A,B,C)=f_1\times f_2$有$8$项，如Figure 14.10所示。<br><img src="/2019/01/06/AI-chapter-14-Probabilistic-reasoning/" alt="figure 14.10"><br>根据图中给出的值，消去$f_3(A,B,C)$中的$A$：<br>\begin{align*}<br>f(B,C) &amp;= \sum_af_3(A,B,C)\<br>&amp;= f_3(a,B,C) + f_3(\neg a,B,C)\<br>&amp;= \begin{pmatrix} 0.06&amp;0.24\0.42&amp;0.28\end{pmatrix} + \begin{pmatrix}0.18&amp;0.72\0.06&amp;0.04\end{pmatrix}\<br>&amp;= \begin{pmatrix}0.24&amp;0.96\048&amp;0.32\end{pmatrix}<br>\end{align*}<br>产生新的因子用的是pointwise乘法，消元用的是累乘。给定pointwise乘法和消元函数，消元算法就变得很简单，一个消元算法如Figure 14.11所示。<br><img src="/2019/01/06/AI-chapter-14-Probabilistic-reasoning/" alt="figure 14.11"></p><h4 id="变量顺序和变量相关性-variable-ordering-and-variable-relevance">变量顺序和变量相关性(Variable ordering and variable relevance)</h4><p>Figure 14.11中的算法包含一个没有给出具体实现的排序函数Order()对要消去的变量进行排序，每一种排序选择都会产生一组有效的算法，但是不同的消元顺序会产生不同的中间因子。一般情况下，消元法的时间和空间复杂度是由算法产生的最大因子决定的，这个最大因子是由消元的顺序和贝叶斯网络的结构决定的，选取最优的消元顺序是很困难的，但是有一些小的技巧：总是消去让新产生的因子最小的变量。<br>另一个属性是：每一个不是查询变量或者证据变量的祖先变量都和这次查询无关，在实现消元算法的时候可以把这些变量都去掉。（具体的示例可以看第十四章，在$528$页）。</p><h3 id="精确推理的复杂度-the-complexity-of-exact-inference">精确推理的复杂度(The complexity of exact inference)</h3><p>贝叶斯网络的精确推理跟网络的结构有很大的关系。<br>Figure 14.2中警报贝叶斯网络中的复杂度是线性的。该网络中任意两个节点只有一条路径，这种网络称为单连接的(singly-connected)或者多树(polytrees)，这种结构有一个很好的属性就是：多树结构中精确推理的时间，空间复杂度对于网络大小来说都是线性关系，这里网络大小指的是CPT项的个数。如果每一个节点的父节点都是一个有界的常数，那么复杂度和节点数之间也是线性关系。<br>对于多连接(multiply connected)的网络，如Figure 14.12(a)所示，最坏情况下，即使每一个节点的父节点个数都是有界常数，消元法的时间和空间复杂度也都是指数级别的。因为贝叶斯网络的推理也是NP难问题。<br><img src="/2019/01/06/AI-chapter-14-Probabilistic-reasoning/" alt="figure 14.12"></p><h3 id="聚类算法-clustering-algorithms">聚类算法(clustering algorithms)</h3><p>用消元法来计算单个的后验概率是简单而高效的，但是如果要计算网络中所有变量的后验概率是很低效的。例如：在单连接的网络中，每一个查询都是$O(n)$，总共有$O(n)$个查询，所以总共的代价是$O(n^2)$。使用聚类算法(clustering algorithms)，代价可以降到$O(n)$，因此贝叶斯网络中的聚类算法已经被广泛商用。（这里不明白为什么？）。<br>聚类算法的基本思想是将网络中的一些节点连接成聚点(cluster nodes)，最后形成一个多树(polytree)结构。例如Figure 14.12(a)中的多连接网络可以转换成Figure 14.12(b)所示的多树，Sprinkler和Rain节点形成了SPrinkler+Rain聚点，这两个布尔变量被一个大节点(meganode)取代，这个大节点有四个可能的取值：$tt,tf,ft,ff$。一旦一个多树形式的网络生成了以后，就需要特殊的推理算法进行推理了，因为普通的推理算法不能处理共享变量的大节点，有了这样一个特殊的算法，后验概率的时间复杂度就是线性于聚类网络的大小。但是，NP问题并没有消失，如果消元需要指数级别的时间和空间复杂度，聚类网络中的CPT也是指数级别大小。</p><h2 id="贝叶斯网络的估计推理-approximate-inference-in-bayesian-networks">贝叶斯网络的估计推理(Approximate inference in bayesian networks)</h2><p>因为多连接网络中的推理是不可行的，所以用估计推理取代精确推理是很有用的。这一节会介绍随机采样算法，也叫蒙特卡洛算法(Monte Carlo)，它的精确度取决于生成的样本数量。我们的目的是采样用于计算后验概率。这里给出了两类算法，直接采样(direct sampling)和马尔科夫链采样(Markov chain sampling)。变分法(variational methods)和循环传播(loopy propagation)将会在本章的最后进行介绍。</p><h3 id="直接采样-direct-sampling-methods">直接采样(Direct sampling methods)</h3><p>任何采样算法都是通过一个已知的先验概率分布生成样本。比如一个公平的硬币，服从一个先验分布$P(coin) = &lt;0.5,0.5 &gt; $，从这个分布中采样就像抛硬币。<br>一个最简单的从贝叶斯网络中进行随机采样的方法就是：从没有证据和它相关的网络中生成事件，即按照拓扑顺序对每一个变量进行采样。如Figure 14.13所示的算法，每一个变量的采样都取决于前之前已经采样过了的父节点变量的值。按照Figure 14.13中的算法对Figure 14.12(a)中的网络进行采样，假设一个采样顺序是[Cloudy,Sprinkler,Rain,WetGrass]：</p><ol><li>从$P(Cloudy)=&lt;0.5,0.5&gt;$中采样，采样值是true；</li><li>从$P(Sprinkler|Cloudy=true) = &lt;0.1,0.9&gt;$中采样，采样值是false；</li><li>从$P(Rain|Cloudy=true)=&lt;0.8,0.2&gt;$中采样，采样值是true；</li><li>从$P(WetGrass|Sprinkler=false,Rain=true)=&lt;0.9,0.1&gt;$中采样，采样值是true；</li></ol><p>这个例子中，PRIOR-SAMPLE算法返回事件[true,false,true,true]。可以看出来，PRIOR-SAMPLE算法根据贝叶斯网络指定的先验联合分布生成样本。假设$S_{PS}(x_1,\cdot,x_n)$是PRIOR-SAMPLE算法生成的一个样本事件，从采样过程中我们可以得出：<br>$$S_{PS}(x_1,\cdots,x_n) = \prod_{i=1}^nP(x_i|parents(X_i))$$<br>即每一步采样都只取决于父节点的值。这个式子和贝叶斯网络的联合概率分布是一样的，所以，我们可以得到：<br>$$S_{PS} = P(x_1,\cdots,x_n).$$<br>通过采样让这个联合分布的求解很简单。<br><img src="/2019/01/06/AI-chapter-14-Probabilistic-reasoning/" alt="figure 14.13"><br>事实上在任何采样算法中，结果都是通过对产生的样本进行计数得到的。假设生成了$N$个样本，$N_{PS}(x_1,\cdots,x_n)$是样本集中的一个具体事件$(x_1,\cdots,x_n)$发生的次数。我们希望这个值比上样本总数取极限和采样概率$S_{PS}$是一样的，即：<br>$$ lim_{N\rightarrow \infty}\frac{N_{PS}(x_1,\cdots,x_n)}{N} = S_{PS}(x_1,\cdots,x_n) = P(x_1,\cdots,x_n).$$<br>例如之前利用PRIOR-SAMPLE算法产生的事件[true,false,true,true]，这个事件的采样概率是：<br>$$S_{PS}(true,false,true,true) = 0.5 \times 0.9 \times 0.8 \times 0.9 = 0.324.$$<br>即当$N$取极限时，我们希望有$32.4%$的样本都是这个事件。(这里为什么要用采样进行计算呢，我的想法是因为实际情况中，采样概率$S_{PS}$是很难计算的，就通过不断的采样，计算出某个样本出现的概率。)<br>我们用$\approx$表示估计概率(estimated probability)在样本数量$N$取极限时和真实概率一样的估计，这叫一致(consistent)估计。比如，对于任意的含有隐变量的事件(partially spefified event)，$x_1,\cdots,x_m,m\le n$，会产生一个一致估计：<br>$$P(x_1,\cdots,x_m)\approx N_{PS}(x_1,\cdots,x_m)/N.$$<br>这个事件的概率可以看成所有满足观测变量条件的样本事件（隐变量所有值都可以取）比上所有样本事件的比值。比如在Spinkler网络中，生成$1000$个样本，其中有$511$个样本的Rain=true，那么rain的估计概率就是$\hat{P}(Rain=true) = 0.511.$</p><h4 id="贝叶斯网络的拒绝采样-rejection-sampling-in-bayesian-networks">贝叶斯网络的拒绝采样(Rejection sampling in Bayesian networks)</h4><h5 id="算法">算法</h5><p><img src="/2019/01/06/AI-chapter-14-Probabilistic-reasoning/" alt="figure 14.14"><br>拒绝采样(rejection sampling)利用容易采样的分布来生成难采样分布的样本，计算后验概率$P(X|\mathbf{e})$，算法流程如Figure 14.14所示，首先根据贝叶斯网络的先验分布生成样本，接下来拒绝(reject)那些和证据变量不匹配的结果，最后在剩下的样本中统计每个$X=x$出现的概率，估计$\hat{P}(X|\mathbf{e}).$<br>用$\hat{P}(X|\mathbf{e})$表示估计概率分布，利用拒绝采样算法的定义计算：<br>$$\hat{P}(X|\mathbf{e}) = \alpha N_{PS}(X,\mathbf{e}) = \frac{N_{PS}(X,\mathbf{e})}{N_{PS}(\mathbf{e})}.$$<br>而根据$P(x_1,\cdots,x_m)\approx N_{PS}(x_1,\cdots,x_m)/N$，就有：<br>$$\hat{P}(X|\mathbf{e}) = \alpha N_{PS}(X,\mathbf{e}) = \frac{N_{PS}(X,\mathbf{e})}{N_{PS}(\mathbf{e})} =  \frac {P(X,\mathbf{e})}{P(\mathbf{e})} = P(X|\mathbf{e}).$$<br>所以，拒绝采样产生了真实概率的一个一致估计(consistent estimate)，但是这个一致估计和无偏估计还不一样。</p><h5 id="示例-v3">示例</h5><p>举一个例子来说明，假设我们要估计概率$P(Rain|Sprinkler=true)$，生成了$100$个样本，其中$73$个是$Sprinkler=false$，$27$是$Sprinkler=true$，这$27$个中有$8$个$Rain=true$，有$19$个$Rain=false$，因此：<br>$$P(Rain|Sprinkler=true)\approx NORMALIZE \lt\lt 8,19&gt;&gt; = &lt;0.296,0.704&gt;.$$<br>正确答案是$&lt;0.3,0.7&gt;$，可以看出来，估计值和真实值差的不多。生成的样本越多，估计值就会和正确值越接近，概率的估计误差和$1/\sqrt{n}$成比例，$n$是用来估计概率的样本数量。</p><h5 id="不足">不足</h5><p>拒绝采样最大的问题是它拒绝了很多样本，随着证据变量的增加，和证据$\mathbf{e}$一致的样本指数速度减少，所以这个方法对于复杂的问题是不可行的。拒绝假设和现实生活中条件概率是很像的，比如估计观测到晚上天空是红的，第二天下雨的概率$P(Rain|RedSkyAtNight=ture)$，这个条件概率的估计就是根据日常生活的观察实现的。但是如果天空很少是红的，就需要很长时间才能估计它的值，这就是拒绝假设的缺点。</p><h4 id="可能性加权-likelihood-weighting">可能性加权(Likelihood weighting)</h4><h5 id="算法-v2">算法</h5><p>可能性加权(Likelihood weighting)只产生和证据$\mathbf{e}$一致的事件，因此避免了拒绝采样的低效。它是统计学中重要性采样的一个例子，专门为贝叶斯推理设计的。<br>如Figure 14.15所示，加权似然固定证据变量$\mathbf{E}$的值，只对非证据变量进行采样，这就保证了每一个事件都是和证据一致的。但是，不是所有的事件权重都是一样的。给定每一个证据变量的父节点，它的可能性(likelihood)是证据变量的条件概率的乘积，每一个事件都根据证据的可能性进行加权。</p><h5 id="示例-v4">示例</h5><p>对于Figure 14.12(a)中的例子，计算后验概率$P(Rain|Cloudy=true,WetGrass=true)$，采样顺序是Cloudy,Sprinkler,Rain,WetGrass。过程如下，首先，权重$w$设为$1$，一个事件生成过程如下：</p><ol><li>Cloudy是一个证据变量，它的值是true,因此，令：<br>$$w\leftarrow w\times P(cloudy=true) = 0.5.$$</li><li>Sprinkler是隐变量，所以从$P(Sprinkler|Cloudy=true)=&lt;0.1,0.9&gt;$中采样，假设采样结果是false；</li><li>Rain是隐变量，从$P(Rain|Cloudy=true)=&lt;0.8,0.2&gt;$中采样，假设采样结果是true；</li><li>WetGrass是证据变量，值是true,令：<br>$$w\leftarrow w\times P(WetGrass=true|Sprinkler=false,Rain=true) = 0.45.$$</li></ol><p>所以WEIGHTED-SAMPLE算法生成事件[true,false,true,true]，相应的权重是$0.45$。</p><h5 id="原理">原理</h5><p>用$S_{WS}$表示WEIGHTED-SAMPLE算法中事件的采样概率，证据变量$\mathbf{E}$的取值$\mathbf{e}$是固定的，用$\mathbf{Z}$表示非证据变量，包括隐变量$\mathbf{Y}$和查询变量$\mathbf{X}$。给定变量$\mathbf{Z}$的父节点，算法对变量$\mathbf{Z}$进行采样：<br>$$S_{WS}(\mathbf{z},\mathbf{e}) = \prod_{i=1}^lP(z_i|parents(Z_i)).$$<br>其中$Parents(Z_i)$可能同时包含证据变量和非证据变量。<br>和先验分布$P(\mathbf{z})$不同的是，每一个变量$Z_i$的取值会受到$Z_i$的祖先(ancestor)变量的影响。比如，对Sprinkler进行采样的时候，算法会受到它的父节点中的证据变量Cloudy=true的影响，而先验分布不会。另一方面，$S_{WS}$比后验分布$P(\mathbf{z}|\mathbf{e})$受证据的影响更小，因为对$Z_i$的采样忽略了$Z_i$的非祖先(non-ancestor)变量中的证据。比如，对Sprinkler和Rain进行采样的时候，算法忽略了子节点中的证据变量WetGrass=true，事实上这个证据已经排除了(rule out)Sprinkler=false和Rain=false的情况，但是WEIGHTED-SAMPLE还会产生很多这样的样本事件。<br>理想情况下，我们想要一个采样分布和真实的后验概率$P(\mathbf{z}|\mathbf{e})$相等，不幸的是不存在这样的多项式时间的算法。如果有这样的算法的话，我们可以用多项式数量的样本以任意精度逼近想要求的概率值。<br>可能性权重$w$弥补了实际的分布和我们想要的分布之间的差距。一个由$\mathbf{z}$和$\mathbf{e}$组成的样本$\mathbf{x}$的权重是给定了父节点的证据变量的可能性乘积：<br>$$w(\mathbf{z},\mathbf{e}) = \prod_{i=1}^mP(e_i|parents(E_i)).$$<br>将上面的两个式子乘起来，可以得到一个样本的加权概率(weighted probability)是：<br>$$S_{WS}(\mathbf{z},\mathbf{e})w(\mathbf{z},\mathbf{e}) = \prod_{i=1}<sup>lP(z_i|parents(Z_i))\prod_{i=1}</sup>mP(e_i|parents(E_i)) = P(\mathbf{z},\mathbf{e}).$$<br>可能性加权估计是一致估计。对于任意的$x$，估计的后验概率按下式计算：<br>\begin{align*}<br>\hat{P}(x|\mathbf{e}) &amp;= \alpha \sum_{\mathbf{y}} N_{WS}(x,\mathbf{y},\mathbf{e})w(x,\mathbf{y},\mathbf{e})\<br>&amp;\approx \alpha’\sum_{\mathbf{y}}S_{WS}(x,\mathbf{y},\mathbf{e})w(x,\mathbf{y},\mathbf{e})\<br>&amp;=\alpha’\sum_{\mathbf{y}}P(x,\mathbf{y},\mathbf{e})\<br>&amp;=\alpha’\sum_{\mathbf{y}}P(x,\mathbf{y},\mathbf{e})\<br>&amp;=P(x|\mathbf{e})<br>\end{align*}<br>算法中真实实现的是第一行，即统计出用WEIGHTED-SAMPLE产生的样本$(x,\mathbf{y},\mathbf{e})$数量$N_{WS}$，以及对应的权重$w(x,\mathbf{y},\mathbf{e})$，后面的都是理论推导，当$N$取极限的时候$lim_{N\rightarrow \infty}\frac{N_{WS}(x_1,\cdots,x_n)}{N} = S_{WS}(x_1,\cdots,x_n)$，后面的都是为了证明算法是一致估计。</p><h5 id="不足-v2">不足</h5><p>可能性加权算法使用了所有生成的样本，它比拒绝假设算法更高效。然而，随着证据变量的增加，算法性能会退化(degradation)，这是因为很多样本的权重都会很小，因此加权估计可能会受一小部分权重很大的样本的影响(dominated)。如果证据变量在非证据变量的后边，这个问题会加剧，因为它们的父节点或者祖先节点没有证据变量来指导样本的生成。这就意味着生成的样本和证据变量支撑的真实情况可能差距很大(bear little resemblance)。</p><h3 id="马尔科夫链仿真推理-inference-by-markov-chain-simulation">马尔科夫链仿真推理(Inference by Markov chain simulation)</h3><p><a href="https://mxxhcm.github.io/2019/08/01/Monte-Carlo-Markov-Chain/">马尔科夫链蒙特卡洛(Markov chain Monte Carlo,MCMC)</a>算法和拒绝采样以及可能性加权很不一样。那两个方法每次都从头开始生成样本，而MCMC算法在之前的样本上做一些随机的变化。可以将MCMC算法看成指定了每一个变量值的特殊当前状态(current state)，通过对当前状态(current state)做任意的改变生成下一个状态(next state)。这一节要介绍的一种MCMC算法是吉布森采样(Gibbs sampling)。</p><h4 id="贝叶斯网络中的吉布森采样-gibbs-sampling-in-bayesian-networks">贝叶斯网络中的吉布森采样(Gibbs sampling in Bayesian networks)</h4><h5 id="算法-v3">算法</h5><p>贝叶斯网络中的吉布森采样从任意一个状态开始，其中证据变量的取值固定为观测值，通过随机选取非证据变量$X_i$的值生成下一个状态。变量$X_i$的采样取决于变量$X_i$的马尔科夫毯的当前值。算法在状态空间（所有非证据变量的全部可能取值空间）中随机采样，每次采样都保持证据变量不变，一次改变一个非证据变量的值。完整的算法如Figure 14.16所示。<br><img src="/2019/01/06/AI-chapter-14-Probabilistic-reasoning/" alt="figure 14.16"></p><h5 id="示例-v5">示例</h5><p>Figure 14.12(a)中的查询(query)$P(Rain|Sprinkler=true, WetGrass=true)$，证据变量Spinkler和WetGrass取它们的观测值不变，非证据变量Cloudy和Rain随机初始化，假设取的是true和false。那么初始状态就是[true,true,false,true]，接下来对非证据变量进行重复的随机采样。<br>比如第一次对Cloudy采样（也可以对Rain采样），给定它的马尔科夫毯变量，然后从$P(Cloudy|Sprinkler=true,Rain=false)$中进行采样，假设采样结果是false，新的状态就是[false,true,false,true]。接下来随机可以对Rain采样（也可以对Cloudy采样），给定Rain的马尔科夫毯变量的取值，从$P(Rain|Cloudy=false,Sprinkler=true,WetGrass=true)$中进行采样，假设采样值是true,那么新的状态是[true,true,false,false]。接下来可以一直进行采样。。最终利用生成的样本计算出相应的概率。</p><h4 id="为什么吉布森采样有用-why-gibbs-sampling-works">为什么吉布森采样有用(Why Gibbs sampling works)</h4><p>接下来给出为什么吉布森采样计算后验概率是一致估计。基本的解释是直截了当的：采样过程建立了一个动态平衡，每个状态花费的时间长期来说和它的后验概率是成比例的。<br>具体的，不想看了。。。就随缘吧</p><h2 id="关系和一阶概率模型-relational-and-first-order-probability-models">关系和一阶概率模型(Relational and first-order probability models)</h2><h2 id="其他不确定性推理的方法-other-approaches-to-uncertain-reasoning">其他不确定性推理的方法(Other approaches to uncertain reasoning)</h2><h2 id="参考文献">参考文献</h2><p>1.<a href="http://aima.cs.berkeley.edu/" target="_blank" rel="noopener">Artificial Intelligence A Modern Approach Third Edition,Stuart Russell,Peter Norvig.</a><br>2.<a href="https://en.wikipedia.org/wiki/Chain_rule_(probability)" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Chain_rule_(probability)</a><br>3.<a href="https://en.wikipedia.org/wiki/Consistent_estimator" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Consistent_estimator</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在这里加一些自己的总结，这一章主要讲的是贝叶斯网络，首先介绍了贝叶斯网络的定义，是一个有向无环图，节点代表随机变量，边代表因果关系。这里给出了贝叶斯公式的两个意义，一个是数值意义，用贝叶斯网络表示全概率分布，另一个是拓扑意义，给定某个节点的父节点，这个节点条件独立于所有它的
      
    
    </summary>
    
      <category term="机器学习" scheme="http://mxxhcm.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="人工智能" scheme="http://mxxhcm.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
      <category term="概率图模型" scheme="http://mxxhcm.github.io/tags/%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/"/>
    
      <category term="推理" scheme="http://mxxhcm.github.io/tags/%E6%8E%A8%E7%90%86/"/>
    
      <category term="贝叶斯网络" scheme="http://mxxhcm.github.io/tags/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>PRML chapter 8 Graphical Models</title>
    <link href="http://mxxhcm.github.io/2019/01/06/PRML-chapter-8-Graphical-Models/"/>
    <id>http://mxxhcm.github.io/2019/01/06/PRML-chapter-8-Graphical-Models/</id>
    <published>2019-01-06T06:31:09.000Z</published>
    <updated>2019-05-06T16:22:27.704Z</updated>
    
    <content type="html"><![CDATA[<p>$\newcommand{\mmm}{\mathbf}$<br><strong>概率图模型</strong><br>概率论在现代模式识别中有很重要的地位。第一章中介绍了概率论可以被表示成两个简单的加法和乘法公式。事实上在这本书中讨论的所有概率推理和学习的计算（无论有多复杂）都可以看成这两个公式的重复应用。我们可以只用代数计算(algebraic manipulation)来形式化并解决复杂的概率问题。但是，使用概率分布(probability distributions)的图表示(diagrammatic representations)，即概率图模型(graphical models)会更有优势。概率图模型有以下几个有用的属性：</p><ol><li>概率图模型提供了一个简单的方式可视化(visualize)概率模型的结构，并且能够用来设计和产生新的模型。</li><li>通过观察概率图模型，可以看到模型的一些属性，包括条件独立性(conditional independence)等等。</li><li>在复杂模型上进行的需要推理和学习的复杂计算，可以被表示为图计算，底层的数据表达式隐式的被执行。</li></ol><p>一个图由节点(nodes)，有时也叫顶点(vertices)，连接顶点的连接(links)，也叫边(edges)。在一个概率图模型中，每一个节点代表一个随机变量，或者一组随机变量，边代表着变量之间的概率关系(probabilistic relationships)。所有随机变量的联合分布可以被分解成一系列部分随机变量的乘积。<br>本章从有向图(directed graphical models)中的贝叶斯网络(Beyesian networks)开始介绍，有向图中的边通过箭头表示方向。另一个主要的图模型是马尔科夫随机场(Markov random fields)，它是一个无向图模型(undirected graphical models)，没有明显的方向性。有向图用来描述随机变量之间的因果关系(causal relationships)，而无向图用来描述随机变量之间的一些软约束(soft constraints)。为了解决推理问题，将无向图和有向图转化成另一种因子图(factor graph)表示是很方便的。</p><h2 id="贝叶斯网络-bayesian-networks">贝叶斯网络(Bayesian Networks)</h2><p>图的一个很强大的特点就是一个具体的图可以用来解释一类概率分布。给定随机变量$a,b,c$的联合概率分布$p(a,b,c)$，通过利用乘法公式，我们可以把它写成以下形式：<br>$$p(a,b,c) = p(c|a,b)p(b|a)p(a).$$<br>这个公式对于任意的联合分布都成立，我们用节点$a,b,c$表示随机变量，按照上式的右边找出每个节点对应的条件分布，在图中添加一个有向箭头从依赖变量指向该变量。如Figure 8.1所示,$a$到$b$的边表示$a$是$b$的父节点。上式中左边是$a,b,c$是对称的，但是右边不是，事实上，在做分解的时候，一个隐式的顺序$a,b,c$已经被确定了，当然也可以选其他顺序，这样会得到一个新的分解和一个新的图。<br><img src="/2019/01/06/PRML-chapter-8-Graphical-Models/" alt="figure 8.1"><br>如果把三个变量可以扩展到$K$个变量，则对应的联合概率为$p(x_1,\cdots,x_k)$，写成如下形式：<br>$$p(x_1,\cdots,x_K) = p(x_K|x_{K-1},\cdots,x_1)\cdots p(x_2|x_1)p(x_1).$$<br>这个式子也叫链式法则，微积分中也有链式法则，这个是概率论中的链式法则。给定$K$值，我们也能生成一个含有$K$个节点的有向图，每一个节点都对应一个条件分布，每一个节点都和比它序号小的节点全部直接相连，所以这个图也叫全连接图，因为任意两个节点都直接相连，但是只有一条有向边由小号节点指向大号节点，所以没有环。<br><img src="/2019/01/06/PRML-chapter-8-Graphical-Models/" alt="figure 8.2"><br>目前为止，所有的操作都是在完全的联合概率分布，相应的分解以及全连接网络上进行的，它们可以应用到任何分布。但是图中也可能有缺失的边，如Figure 8.2所示，它不是一个全连接的图。我们可以直接根据这个图将联合分布表示为很多条件分布的乘积。每一个条件分布的取值只跟图中对应的父节点。比如，$x_5$只取决于$x_1$和$x_3$，$7$个变量的联合概率分布可以写成：<br>$$p(x_1)p(x_2)p(x_3)p(x_4|x_1,x_2,x_3)p(x_5|x_1,x_3)p(x_6|x_4)p(x_7|x_4,x_5)$$<br>从上面我们可以看出有向图和变量的条件概率之间的关系，图中定义的联合概率分布是图中所有节点给定其父节点的条件概率的乘积，即:<br>$$p(\mathbf{x}) = \prod_{k=1}^Kp(x_k|pa_k).$$<br>其中$pa_k$是$x_k$节点的父节点的集合，$\mathbf{x} = {x_1,\cdots,x_k}$，这个式子给出了一个有向图的联合概率具有因式分解属性。<br>贝叶斯网络中不能存在有向的圈，即不能存在闭路，所以这种图也叫有向无环图。另一种说法是如果图中的节点有顺序的话，不能存在大号节点到小号节点的有向边。</p><h3 id="示例：多项式回归-example-polynomial-regression">示例：多项式回归(Example: Polynomial regression)</h3><p><img src="/2019/01/06/PRML-chapter-8-Graphical-Models/" alt="figure 8.3"><br>这里给出了一个用有向图描述概率分布的例子，贝叶斯多项式回归模型。模型中的随机变量是多项式系数向量$\mathbf{w}$以及观测值$\mathbf{t}=(t_1,\cdots,t_N)<sup>T$，此外，还有一些模型中确定的参数，它们不是随机变量，如输入数据$\mathbf{x}=(x_1,\cdots,x_N)</sup>T$，噪音方差$\sigma^2$，还有$\mathbf{w}$上高斯分布精度的超参数$\alpha$。如果只关注随机变量，联合分布可以看成先验分布$p(\mathbf{w})$和$N$个条件分布$p(t_n|\mathbf{w}),n=1,\cdots,N$的乘积：<br>$$p(\mathbf{t},\mathbf{w}) = p(\mathbf{w})\prod_{n=1}^Np(t_n|\mathbf{w}).$$<br>这个模型可以用Figure 8.3表示。<br><img src="/2019/01/06/PRML-chapter-8-Graphical-Models/" alt="figure 8.4"><br>为了方便表示，我们把$t_1,\cdots,t_N$用一个单独的节点，外面用一个盒子包着，叫做盘子(plate)，盘子上写上$N$代表有$N$个这样的节点，得到Figure 8.4中的图。如果把模型确定的参数写出来，我们可以得到下式：<br>$$p(\mathbf{t},\mathbf{w}|\mathbf{x},\alpha,\sigma^2) = p(\mathbf{w}|\alpha)\prod_{n=1}<sup>Np(t_n|\mathbf{w},x_n,\sigma</sup>2).$$<br><img src="/2019/01/06/PRML-chapter-8-Graphical-Models/" alt="figure 8.5"><br>如果在图中把模型参数和随机变量都表示出来，用空心圆圈代表随机变量，用实心圆点代表确定性参数(deterministic parameters)，用图形表示如Figure 8.5。<br><img src="/2019/01/06/PRML-chapter-8-Graphical-Models/" alt="figure 8.6"><br>当用图模型去解决机器学习或者模型时，有时候会固定一些随机变量的值，比如在多项式拟合问题中训练集的变量${t_n}$，在图模型中，将对应节点加上阴影，表示观测变量(observed variables)。如Figure 8.6所示，变量${t_n}$是观测变量。$\mathbf{w}$没有被观测到，所以是一个隐变量(latent variable)或者是(hidden variable)。<br>利用观测到的${t_n}$的值，我们可以估计多项式系数$\mathbf{w}$，利用贝叶斯公式：<br>$$p(\mathbf{w}|\mathbf{T}) \propto p(\mathbf{w}) \prod_{n=1}^Np(t_n|\mathbf{w})$$<br>为了整洁(uncluttered)，模型的确定性参数被略去了。<br><img src="/2019/01/06/PRML-chapter-8-Graphical-Models/" alt="figure 8.7"><br>一般来说，我们对于如$\mathbf{w}$之类的模型参数不感兴趣，因为我们的目标是用模型对新的输入进行预测。即在给定观测数据之后，我们给出一个新的输入$\hat{x}$，要找到对应的$\hat{t}$的概率分布，如Figure 8.7所示。给定确定性参数之后，图中所有随机变量的联合分布如下所示：<br>$$p(\hat{t},\mathbf{t},\mathbf{w}|\hat{x},\mathbf{x},\alpha,\sigma^2) = \left[\prod_{n=1}<sup>Np(t_n|x_n,\mathbf{w},\sigma</sup>2)\right] p(\mathbf{w}|\alpha)p(\hat{t}|\hat{x},\mathbf{w},\sigma^2).$$<br>刚开始有一些不理解，但是实际上就是这样一个公式$p(a,b,c) = p(a)p(b|a)p(c|a)$，把$\hat{t}$和$\mathbf{t}$当成两个变量看就行了。<br>利用概率论的加法公式$p(X) = \sum\limits_Yp(X,Y)$，对模型的参数$\mathbf{w}$积分就得到了$\hat{t}$的预测分布：<br>\begin{align*}<br>p(\hat{t}|\hat{x},\mathbf{x},\mathbf{t},\alpha,\sigma^2) = \int p(\hat{t},\mathbf{w}|\hat{x},\mathbf{x},\mathbf{t},\alpha,\sigma^2)d\mathbf{w}<br>\propto \int p(\hat{t},\mathbf{t},\mathbf{w}|\hat{x},\mathbf{x},\alpha,\sigma^2)d\mathbf{w}<br>\end{align*}<br>其中随机变量$\mathbf{t}$被隐式的赋值为数据集中的观测值，即是一个$p(t)$是一个定值。这里刚开始有些不理解,实际上是当$p(b)$为定值的时候，$p(a|b) \propto p(ab)$。</p><h3 id="生成模型-generative-models">生成模型(Generative models)</h3><p>这里实际上介绍的是采样方法，叫祖先采样，实际上就是直接采样，AI的第十四章有讲很多采样，可以直接看那个。<br>很多时候我们需要从一个给定的分布中进行采样，十一章还会更详细的讲采样，这里要介绍一种采样分布叫祖先采样(ancestral sampling)，是一种和概率图模型相关的采样方法。给定$K$个变量的联合分布$p(x_1,\cdots,x_K)$对应的有向无环图，假设所有变量的父节点的序号都比它本身小。我们的目标是从联合分布中采样$\hat{x_1},\cdots,\hat{x_k}$。<br>首先从最小的序号根据$p(x_1)$开始采样，采样结果称为$\hat{x_1}$，接下来按顺序对第$n$个节点按照条件分布$p(x_n|pa_n)$进行采样，每个节点的父节点都取采样值，因为每个父节点都已经采完样了，所以这里不用担心。一直到第$K$个节点采样完成，就生成了一个样本。为了对某些边缘分布进行采样，对需要的节点进行采样，忽略其他节点即可，比如为了对边缘分布$p(x_2,x_4)$进行采样，从联合分布中进行采样，保留$\hat{x_2},\hat{x_4}$的值，其他的值不用管即可。<br>在概率图的实际应用中，通常小节点对应的是隐变量，大节点对应的图上的最终节点代表着一些观测变量。隐变量的目的是让观测变量的复杂概率分布可以表示成多个简单的条件概率分布的乘积。<br>我们可以把这样的模型解释为观测变量产生的过程，比如，一个模式识别任务中，每一个观测数据对应一张图片。隐变量解释为物体的位置和方向，给定一个观测图像，我们的目标是找到物体的一个后验分布，在后验分布中对所有可能的位置和方向进行积分，如Figure 8.8所示。<br>图模型通过观测数据的生成过程描述了一种因果关系过程，因为这个原因，这样的模型也叫做生成式模型(generative model)。相反，Figure 8.5中的模型不是生成式模型，因为多项式回归模型中的输入变量$x$没有概率分布，所以不能用来合成数据。通过引入一个合适的先验分布$p(x)$，我们可以把它变成一个生成式模型。<br>事实上，概率图模型中的隐变量不是必须要有显式的物理意义，它的引入只是为了方便从简单的条件概率生成复杂的联合分布。在任何一种情况下，应用到生成式模型的祖先采样模拟了观测数据的生成过程，因此产生了和观测数据分布相同（如果模型完美的表现了现实）的美好(fantasy)数据。实际应用中国，利用生成模型产生合成的观测数据，对于理解模型表达的概率分布很有帮助。</p><h3 id="离散型随机变量-discrete-variables">离散型随机变量(Discrete variables)</h3><p>指数分布是很重要的一类分布，它们虽然很简单，但是可以形成更复杂的概率分布，概率图的框架对于表达这些概率分布是如何连接的很有用。<br>如果我们有向图中亲本和子节点对之间的关系选择为conjugate，会发现这些模型有很好的属性。这里主要探讨两种情况，父节点和子节点都是离散的以及父节点和子节点都对应高斯变量，因为这两种关系可以分层扩展(extended hierarchically)构建任何复杂的有向无环图。首先从离散变量开始：<br>有$K$个可能状态的单个离散变量$\mathbf{x}$的概率分布是：<br>$$p(\mathbf{x}|\nu) = \prod_{k=1}<sup>k\nu_k</sup>{x_k}$$<br>由参数$\nu = (\nu_1,\cdots,\nu_K)^T$控制，由于有约束条件$\sum_k\nu_k=1$，为了定义分布有$K-1$个$\nu_k$的值需要指定。<br>假设有两个离散型随机变量$\mathbf{x}<em>1,\mathbf{x}<em>2$，每个变量都有$K$个可能的取值。用$\nu</em>{kl}$表示同时观测到$x</em>{1k}=1$和$x_{2l}=1$，其中$x_{1k}$表示$\mathbf{x}<em>1$的第$k$个分量，$x</em>{2l}$类似。联合分布可以写成：<br>$$p(\mathbf{x}<em>1,\mathbf{x}<em>2|\nu) = \prod</em>{k=1}<sup>K\prod_{l=1}</sup>K\nu</em>{kl}^{x_{1k}x_{2l}}.$$<br>$\nu_{kl}$满足约束条件$\sum_k\sum_l\nu_{kl} =1$，被$K<sup>2-1$个参数控制，任意$M$个具有$K$个取值的随机变量的联合分布需要$K</sup>M-1$个参数，随着随机变量$M$个数的增加，参数的个数以指数速度增加。<br>使用乘法公式，联合分布$p(\mathbf{x}_1,\mathbf{x}_2)$可以分解成$p(\mathbf{x}_2|\mathbf{x}_1)p(\mathbf{x}_1)$，对应的图如Figure 8.9(a)所示，边缘分布$p(\mathbf{x}_1)$的分布需要$K-1$个参数，$p(\mathbf{x}_2|\mathbf{x}_1)$对于$K$个可能的$\mathbf{x}_1$，每个都需要$K-1$个参数。所以，和联合分布一样，总共需要的参数为$K-1+K(K-1) = K^2-1$个。<br>假设$\mmm{x}_1$和$\mmm{x}_2$是独立的，如Figure 8.9(b)所示，每一个变量可以用分开的多峰分布(multinomial distribution)表示，所需的参数量为$2(K-1)$个。类似的，$M$个独立变量需要$M(K-1)$个参数，和变量个数之间是线性关系。从概率图的角度来看，通过在图中去掉边减少了参数的数量，同时代价是这个图只能代表有限类别的分布。<br>更普通的是，如果我们有$M$个离散型随机变量$\mmm{x}_1,\cdots,\mmm{x}_M$，我们可以用一个节点代表一个随机变量，建立一个有向图表示联合概率分布。每个节点处的条件概率由一组非负参数给出，并且需要满足归一化条件。如果图是全连接的，那么这个分布需要$K^M-1$个参数，如果图中没有连接，那么联合分布可以分解成边缘分布的乘积，需要的所有参数是$M(K-1)$个。拥有中间水平连接性的图比分解成单个边缘分布的乘积能解释更多的分布同时比普遍的联合概率分布需要更少的参数。如Figure 8.10中的节点链，边缘分布$p(\mmm{x}_1)$需要$K-1$个参数，其余的$M-1$个条件分布$p(\mmm{x}<em>i|\mmm{x}</em>{i-1}),i = 2,\cdots,M$，需要$K(K-1)$个参数，总共需要的参数是$K-1+(M-1)K(K-1)$个，是$K$的二次函数(quadratic)，随着链的长度$M$增加，参数个数线性增加。<br>另一个减少模型中独立参数个数的方法是共享参数。例如，Figure 8.10中的链，我们可以用共享的$K(K-1)$个参数去控制条件概率$p(\mmm{x}<em>i|\mmm{x</em>{i-1}),i=2,\cdots,M$，用$K-1$个变量去控制$\mmm{x}_1$的概率分布，总共需要$K^2-1$个参数需要被指定去定义联合概率分布。<br>通过引入每个参数对应的Dirichlet先验，我们可以把一个随机变量图转换成贝叶斯模型。从概率图的角度来看，每一个节点需要一个额外的父节点代表和这个离散节点相关的Dirichlet分布，如Figure 8.11所示。将控制条件分布$p(\mmm{x}<em>i|\mmm{x}</em>{i-1}),i=2,\cdots,M$的参数共享，得到如Figure 8.12所示的图。<br>另一种控制模型中离散变量参数指数速度增加的方法是用参数模型而不是条件概率表来表示条件分布。如Figure 8.13中，所有的节点都是一个二值变量，用参数$\nu_i$表示每一个父节点$\mmm{x}<em>i$取值为$1$的概率$p(x_i=1)$，总共有$M$个父节点，所以总共需要$2<sup>M$个参数表示条件概率$p(y|x_1,\cdots,x_M)$的$2</sup>M$可能取值，如$p(y=1)$。所以指定这个条件分布所需要的参数随着$M$指数级增长。我们可以通过使用logistic sigmoid函数作用于父节点的线性组合上，得到一个更简洁的条件概率分布：<br>$$p(y=1|x_1,\cdots,x_M) = \sigma\left(w_0+\sum</em>{i=1}<sup>Mw_ix_i\right)=\sigma(\mmm{w}</sup>T\mmm{x})$$<br>其中$\sigma(a) = \frac{1}{1+exp(-a)}$是logistic sigmoid，$\mmm{x}=(x_0,x_1,\cdots,x_M)<sup>T$是由$M$个父节点状态和一个$x_0=1$构成的$M+1$维向量，$\mmm{w}=(w_0,w_1,\cdots,w_M)</sup>T$是$M+1$维参数项。与一般情况相比，这是一个更加严格的条件概率分布形式，但是它的参数个数随着$M$的增加线性增加。在这种情况下，类似于选择多元高斯分布的协方差矩阵的限制形式（如对角矩阵等）。</p><h3 id="线性高斯模型-linear-gaussian-models">线性高斯模型(Linear-Gaussian models)</h3><h2 id="条件独立性-conditional-independence">条件独立性(Conditional Independence)</h2><h2 id="马尔科夫随机场-markov-random-fields">马尔科夫随机场(Markov Random Fields)</h2><h2 id="概率图模型中的推理-inference-in-graphical-models">概率图模型中的推理(Inference in Graphical Models)</h2><h2 id="参考文献-references">参考文献(references)</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;$\newcommand{\mmm}{\mathbf}$&lt;br&gt;
&lt;strong&gt;概率图模型&lt;/strong&gt;&lt;br&gt;
概率论在现代模式识别中有很重要的地位。第一章中介绍了概率论可以被表示成两个简单的加法和乘法公式。事实上在这本书中讨论的所有概率推理和学习的计算（无论有多复
      
    
    </summary>
    
      <category term="机器学习" scheme="http://mxxhcm.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://mxxhcm.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="概率图模型" scheme="http://mxxhcm.github.io/tags/%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/"/>
    
      <category term="贝叶斯网络" scheme="http://mxxhcm.github.io/tags/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BD%91%E7%BB%9C/"/>
    
      <category term="模式识别" scheme="http://mxxhcm.github.io/tags/%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB/"/>
    
  </entry>
  
  <entry>
    <title>ESL chapter 1 Introduction</title>
    <link href="http://mxxhcm.github.io/2019/01/05/ESL-chapter-1-Introduction/"/>
    <id>http://mxxhcm.github.io/2019/01/05/ESL-chapter-1-Introduction/</id>
    <published>2019-01-05T01:46:39.000Z</published>
    <updated>2019-05-06T16:22:27.704Z</updated>
    
    <content type="html"><![CDATA[<h2 id="引言">引言</h2><p>这本书主要介绍的是统计学习。一些典型的学习问题如下：</p><ul><li>基于一个心脏病患者的饮食，临床检测等等，去预测一个这个因为心脏病住院的人会不会第二次患心脏病。</li><li>基于一个公司的运行状况或一些经济数据，去预测未来六个月股票的价格。</li><li>从一个手写字母图像中识别出来其中的字母。</li><li>从一个糖尿病(diabetic)患者血液的红外吸收频谱去预测他的血糖(glucose)含量。</li><li>基于人口统计(demographic)和临床检测，分析前列腺癌的致病因素。</li></ul><p>在一个典型的学习场景下，我们通常有一些定量的结果(outcome measurement)，如上面例子中的股票价格或者分类问题中问题的类别，我们希望基于一系列的特征进行预测。<br>接下来给了几个真实的学习问题的示例。下面就简要介绍一下这几个例子。</p><h3 id="邮件分类">邮件分类</h3><p>给定一封邮件，邮件分类的目标就是根据邮件的特征去判断这封邮件是正常邮件还是垃圾邮件。这是监督学习中的二分类问题，因为该问题有ouputs，且只有两个类别。</p><h3 id="前列腺癌-prostate-cancer">前列腺癌(prostate cancer)</h3><p>该问题的目标是给定一系列临床检测，如记录癌症量(log cancer volume)，去预测前列腺特异性抗原(prstate specific antigen)的数量。该问题是监督学习中的回归问题，因为结果(outcome measurement)是定量的(quatitative)。</p><h3 id="手写数字识别">手写数字识别</h3><p>给定一个手写数字的图片，该问题的目标是识别出图片中的数字。</p><h3 id="dna-expression-microarrays">DNA Expression Microarrays</h3><p>这个问题是通过基因数组去学习基因和不同基因样本之间的关系，一些典型的问题是：</p><ol><li>哪些样本之间是相似的？在不同的基因之间都相似。</li><li>哪些基因是相似的？在不同的样本之间都相似。</li><li>一些特定的基因对于特定的癌症患者表达是达不是很明显？</li></ol><p>这个问题可以看成回归问题，或者更有可能是无监督问题。</p><h2 id="本书结构">本书结构</h2><p>第一章就是本章。第二章讲监督学习的介绍。第三章和第四章介绍线性回归和分类。第五章介绍仿样(splines)，小波(wavelets)，正则化(regularization)和惩罚(penalization)。第六章介绍核方法(kernel methods)和局部回归(local regression)。第七章将模型估计和选择(model assessment and selection)，涉及到偏置(bias)和方差(variance)，过拟合(overfitting)以及交叉验证(cross-validation)等等。第八章讲模型推理。第十章讲boosting。<br>第九到十三章讲监督学习的一系列结构化方法。十四章介绍非监督学习。十五和十六章分别介绍随机森林(random forests)和集成学习(ensemble learning)。第十七章介绍无向图(undirected graphical models)。第十八章介绍高维问题。<br>第一到四章是基础最好按顺序阅读，第七章也是。其他的可以不按顺序。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;引言&quot;&gt;引言&lt;/h2&gt;
&lt;p&gt;这本书主要介绍的是统计学习。一些典型的学习问题如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;基于一个心脏病患者的饮食，临床检测等等，去预测一个这个因为心脏病住院的人会不会第二次患心脏病。&lt;/li&gt;
&lt;li&gt;基于一个公司的运行状况或一些经济数据，去
      
    
    </summary>
    
      <category term="机器学习" scheme="http://mxxhcm.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://mxxhcm.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="统计" scheme="http://mxxhcm.github.io/tags/%E7%BB%9F%E8%AE%A1/"/>
    
  </entry>
  
  <entry>
    <title>ESL chapter 2 Overview of supervised learning</title>
    <link href="http://mxxhcm.github.io/2019/01/05/ESL-chapter-2-Overview-of-supervides-learning/"/>
    <id>http://mxxhcm.github.io/2019/01/05/ESL-chapter-2-Overview-of-supervides-learning/</id>
    <published>2019-01-05T01:30:55.000Z</published>
    <updated>2019-05-06T16:22:27.704Z</updated>
    
    <content type="html"><![CDATA[<h2 id="引言">引言</h2><p>在机器学习领域，监督学习(supervised learning)的每一个样本都由输入(inputs)和输出(outputs)组成。监督学习的目标就是根据inputs的值去预测outpus的值。<br>在统计学(statistical)中，inputs通常被称为预测器？？(predictors)，或者叫自变量(independent variables)。<br>在模式识别(pattern recognition)领域，inputs通常称为特征(features)，或者叫因变量(dependent variables)</p><h2 id="变量类型和一些术语-terminology">变量类型和一些术语(terminology)</h2><p>不同的问题中，输出也不一样。血糖预测问题中，输出是一个定量的(quantitative)测量。手写数字识别问题中，输出是十个不同的类，是定性的(qualitative)，定性的输出也通常被称为类别(catrgorical)，这里的类别是无序的。通常，预测定量的输出被称为回归问题(regression)，预测定性的输出被称为分类问题。这两个问题很相像，多可以看成函数拟合。第三种输出是有序类别，像小，中，大，没有合适的度量表示，因为中和小之间的差别和中和大之间的差别是不同的。<br>定性分析在代码实现中进行二值化数值表示。即如果只有两类的话，用一个二进制位$0$或者$1$表示，或者$1$和$-1$。当超过两类的时候，通常用虚拟变量(dummy variables)来表示，一个$K$级变量是一个长度为$K$的二进制位，每一个时刻只有一位被置一。<br>一些常用的表示，$X$表示inputs，$Y$表示定量outputs，$G$表示定性outputs。大写字母表示通用的表示，观测值用小写字母表示，inputs $X$的第$i$个观测值用$x_i$表示，其中$x_i$是一个标量或者向量。矩阵用粗体的大写字母表示，如具有$N$个$p$维向量$x_i, j= 1,\cdots,N$的$N\times p$矩阵$\mathbf{X}$。所有的向量都用的是列向量表示，$\mathbf{A}$的第$i$行是$x_i^T$，第$i$列的转置。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;引言&quot;&gt;引言&lt;/h2&gt;
&lt;p&gt;在机器学习领域，监督学习(supervised learning)的每一个样本都由输入(inputs)和输出(outputs)组成。监督学习的目标就是根据inputs的值去预测outpus的值。&lt;br&gt;
在统计学(statistica
      
    
    </summary>
    
      <category term="机器学习" scheme="http://mxxhcm.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://mxxhcm.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="统计" scheme="http://mxxhcm.github.io/tags/%E7%BB%9F%E8%AE%A1/"/>
    
      <category term="监督学习" scheme="http://mxxhcm.github.io/tags/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="非监督学习" scheme="http://mxxhcm.github.io/tags/%E9%9D%9E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>singular value decomposition（奇异值分解）</title>
    <link href="http://mxxhcm.github.io/2019/01/03/linear-algebra-singular-value-decomposition/"/>
    <id>http://mxxhcm.github.io/2019/01/03/linear-algebra-singular-value-decomposition/</id>
    <published>2019-01-03T07:19:54.000Z</published>
    <updated>2019-09-09T08:53:25.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="特征值分解-eigen-value-decomposition">特征值分解(eigen value decomposition)</h2><p>要谈奇异值分解，首先要从特征值分解(eigen value decomposition, EVD)谈起。<br>矩阵的作用有三个：一个是旋转，一个是拉伸，一个是平移，都是线性操作。如果一个$n\times n$方阵$A$对某个向量$x$只产生拉伸变换，而不产生旋转和平移变换，那么这个向量就称为方阵$A$的特征向量(eigenvector)，对应的伸缩比例叫做特征值(eigenvalue)，即满足等式$Ax = \lambda x$。其中$A$是方阵，$x$是方阵$A$的一个特征向量，$\lambda$是方阵$A$对应特征向量$x$的特征值。<br>假设$S$是由方阵$A$的$n$个线性无关的特征向量构成的方阵，$\Lambda$是方阵$A$的$n$个特征值构成的对角矩阵，则$A=S\Lambda S^{-1}$，这个过程叫做对角化过程。<br>证明：<br>因为$Ax_1 = \lambda_1 x_1,\cdots,Ax_n = \lambda_n x_n$,<br>所以<br>\begin{align*}AS &amp;= A\begin{bmatrix}x_1&amp; \cdots&amp;x_n\end{bmatrix}\\<br>&amp;=\begin{bmatrix} \lambda_1x_1&amp;\cdots&amp;\lambda x_n\end{bmatrix}\\<br>&amp;= \begin{bmatrix}x_1&amp; \cdots&amp;x_n\end{bmatrix} \begin{bmatrix}\lambda_1&amp; &amp; &amp;\\&amp;\lambda_2&amp;&amp;\\&amp;&amp;\cdots&amp;\\&amp;&amp;&amp;\lambda_n\end{bmatrix}\<br>&amp;= S\Lambda<br>\end{align*}<br>所以$AS=S\Lambda, A=S\Lambda S^{-1}, S^{-1}AS=\Lambda$。<br>若方阵$A$为对称矩阵，矩阵$A$的特征向量是正交的，将其单位化为$Q$，则$A=Q\Lambda Q^T$，这个过程就叫做特征值分解。</p><h2 id="奇异值分解-singular-value-decomposition">奇异值分解(singular value decomposition)</h2><p>特征值分解是一个非常好的分解，因为它能把一个方阵分解称两类非常好的矩阵，一个是正交阵，一个是对角阵，这些矩阵都便于进行各种计算，但是它对于原始矩阵的要求太严格了，必须要求矩阵是对称正定矩阵，这是一个很苛刻的条件。所以就产生了奇异值分解，奇异值分解可以看作特征值分解在$m\times n$维矩阵上的推广。对于对称正定矩阵来说，有特征值，对于其他一般矩阵，有奇异值。</p><p>奇异值分解可以看作将一组正交基映射到另一组正交基的变换。普通矩阵$A$不是对称正定矩阵，但是$AA^T $和$A^TA $一定是对称矩阵，且至少是半正定的。从对$A^TA $进行特征值分解开始，$A^T A=V\Sigma_1V^T $，$V$是一组正交的单位化特征向量${v_1,\cdots,v_n}$，则$Av_1,\cdots,Av_n$也是正交的。<br>证明：<br>\begin{align*}Av_1\cdot Av_2 &amp;=(Av_1)^T Av_2\\<br>&amp;=v_1^T A^T Av_2\\<br>&amp;=v_1^T \lambda v_2\\<br>&amp;=\lambda v_1^T v_2\\<br>&amp;=0<br>\end{align*}<br>所以$Av_1,Av_2$是正交的，同理可得$Av_1,\cdots,Av_n$都是正交的。<br>而：<br>\begin{align*}<br>Av_i\cdot Av_i &amp;= v_i^T A^T Av_i\\<br>&amp;=v_i \lambda v_i\\<br>&amp;=\lambda v_i^2\\<br>&amp;=\lambda<br>\end{align*}<br>将$Av_i$单位化为$u_i$，得$u_i = \frac{Av_i}{|Av_i|} = \frac{Av_i}{\sqrt{\lambda_i}}$，所以$Av_i = \sqrt{\lambda_i}u_i$。<br>将向量组${v_1,\cdots,v_r}$扩充到$R^n $中的标准正交基${v_1,\cdots,v_n}$，将向量组${u_1,\cdots,u_r}$扩充到$R^n $中的标准正交基${u_1,\cdots,u_n}$，则$AV = U\Sigma$，$A=U\sigma V^T $。</p><p>事实上，奇异值分解可以看作将行空间的一组正交基加上零空间的一组基映射到列空间的一组正交基加上左零空间的一组基的变换。对一矩阵$A,A\in \mathbb{R}^{m\times n} $，若$r(A)=r$，取行空间的一组特殊正交基${v_1,\cdots,v_r}$，当矩阵$A$作用到这组基上，会得到另一组正交基${u_1,\cdots,u_r}$，即$Av_i = \sigma_iu_i$。<br>矩阵表示是：<br>\begin{align*}<br>AV &amp;= A\begin{bmatrix}v_1&amp;\cdots&amp;v_r\end{bmatrix}\\<br>&amp;= \begin{bmatrix}\sigma_1u_1 &amp; \cdots &amp; \sigma_ru_r\end{bmatrix}\\<br>&amp;= \begin{bmatrix}u_1&amp;u_2&amp;\cdots&amp;u_r\end{bmatrix}\begin{bmatrix}\sigma_1&amp;&amp;&amp;\\&amp;\sigma_2&amp;&amp;\\&amp;&amp;\cdots&amp;\\&amp;&amp;&amp;\sigma_n\end{bmatrix}\\<br>&amp;=U\Sigma<br>\end{align*}<br>其中$A\in \mathbb{R}^{m\times n}, V\in \mathbb{R}^{n\times r},U\in \mathbb{R}^{m\times r}, \Sigma \in \mathbb{R}^{r\times n}$。<br>当有零空间的时候，行空间的一组基是$r$维，加上零空间的$n-r$维，构成$R^n $空间中的一组标准正交基。列空间的一组基也是$r$维的，加上左零空间的$m-r$维，构成$R^m $空间的一组标准正交基。零空间中的向量在对角矩阵$\Sigma$中体现为$0$，<br>则$A=U\Sigma V^{-1} $，$V$是正交的，所以$A=U\Sigma V^T $，其中$V\in \mathbb{R}^{n\times n}, U\in \mathbb{R}^{m\times m}, \Sigma \in \mathbb{R}^{m\times n}$。</p><p>$A=U\Sigma V^T $,<br>$A^T = V\Sigma^T U^T $,<br>$AA^T = U\Sigma V^T V\Sigma^T U^T $,<br>$A^T A = V\Sigma^T U^T U\Sigma V^T $<br>对$A A^T $和$A^T A$作特征值分解，则$A A^T = U\Sigma_1U^T $,$A^T A=V\Sigma_2V^T $，所以对$AA^T $作特征值分解求出来的$U$和对$A^T A$作特征值分解求出来的$V$就是对$A$作奇异值分解求出来的$U$和$V$，$AA^T $和$A^T A$作特征值分解求出来的$\Sigma$的非零值是相等的，都是对$A$作奇异值分解的$\Sigma$的平方。</p><h3 id="a-t-a-和-aa-t-的非零特征值是相等的">$A^T A$和$AA^T $的非零特征值是相等的</h3><p>证明：对于任意的$m\times n$矩阵$A$，$A^T A$和$AA^T $的非零特征值相同的。 设$A^T A$的特征值为$\lambda_i$，对应的特征向量为$v_i$，即$A^T Av_i = \lambda_i v_i$。<br>则$AA^T Av_i = A\lambda_iv_i = \lambda_i Av_i$。<br>所以$AA^T $的特征值为$\lambda_i$，对应的特征向量为$Av_i$。<br>因此$A^T A$和$AA^T $的非零特征值相等。</p><h3 id="几何意义">几何意义</h3><p>对于任意一个矩阵，找到其行空间(加上零空间)的一组正交向量，使得该矩阵作用在该向量序列上得到的新的向量序列保持两两正交。奇异值的几何意义就是这组变化后的新的向量序列的长度。</p><h3 id="物理意义">物理意义</h3><p>奇异值往往对应着矩阵隐含的重要信息，且重要性和奇异值大小正相关。每个矩阵都可以表示为一系列秩为$1$的“小矩阵”的和，而奇异值则衡量了这些秩一矩阵对$A$的权重。<br>奇异值分解的物理意义可以通过图像压缩表现出来。给定一张$m\times n$像素的照片$A$，用奇异值分解将矩阵分解为若干个秩一矩阵之和，即：<br>\begin{align*}<br>A&amp;=\sigma_1 u_1v_1^T +\sigma_2 u_2v_2^T +\cdots+\sigma_r u_rv_r^T\\<br>&amp;= \begin{bmatrix}u_1&amp;u_2&amp;\cdots&amp;u_r\end{bmatrix}\begin{bmatrix}\sigma_1&amp;&amp;&amp;\&amp;\sigma_2&amp;&amp;\&amp;&amp;\cdots&amp;\&amp;&amp;&amp;\sigma_n\end{bmatrix}\begin{bmatrix}v_1<sup>T\v_2</sup>T\ \vdots\v_r^T\end{bmatrix}\\<br>&amp;=U\Sigma V^T<br>\end{align*}</p><p>这个也叫部分奇异值分解。其中$V\in R^{r\times n}, U\in R^{m\times r}, \Sigma \in R^{r\times r}$。因为不含有零空间和左零空间的基，如果加上零空间的$n-r$维和左零空间的$m-r$维，就是奇异值分解。<br>较大的奇异值保存了图片的主要信息，特别小的奇异值有时可能是噪声，或者对于图片的整体信息不是特别重要。做图像压缩的时候，可以只取一部分较大的奇异值，比如取前八个奇异值作为压缩后的图片：<br>$$A = \sigma_1 u_1v_1^T +\sigma_2 u_2v_2^T + \cdots + \sigma_8 u_8v_8^T$$<br>现实中常用的做法有两个：</p><ol><li>保留矩阵中$90%$的信息：将奇异值平方和累加到总值的%90%为止。</li><li>当矩阵有上万个奇异值的时候，取前面的$2000$或者$3000$个奇异值。。</li></ol><h2 id="参考文献-references">参考文献(references)</h2><p>1.Gilbert Strang, MIT Open course：Linear Algebra<br>2.<a href="https://www.cnblogs.com/pinard/p/6251584.html" target="_blank" rel="noopener">https://www.cnblogs.com/pinard/p/6251584.html</a><br>3.<a href="http://www.ams.org/publicoutreach/feature-column/fcarc-svd" target="_blank" rel="noopener">http://www.ams.org/publicoutreach/feature-column/fcarc-svd</a><br>4.<a href="https://www.zhihu.com/question/22237507/answer/53804902" target="_blank" rel="noopener">https://www.zhihu.com/question/22237507/answer/53804902</a><br>5.<a href="http://charleshm.github.io/2016/03/Singularly-Valuable-Decomposition/" target="_blank" rel="noopener">http://charleshm.github.io/2016/03/Singularly-Valuable-Decomposition/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;特征值分解-eigen-value-decomposition&quot;&gt;特征值分解(eigen value decomposition)&lt;/h2&gt;
&lt;p&gt;要谈奇异值分解，首先要从特征值分解(eigen value decomposition, EVD)谈起。&lt;br&gt;
矩
      
    
    </summary>
    
      <category term="线性代数" scheme="http://mxxhcm.github.io/categories/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/"/>
    
    
      <category term="线性代数" scheme="http://mxxhcm.github.io/tags/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/"/>
    
      <category term="奇异值分解" scheme="http://mxxhcm.github.io/tags/%E5%A5%87%E5%BC%82%E5%80%BC%E5%88%86%E8%A7%A3/"/>
    
      <category term="特征值分解" scheme="http://mxxhcm.github.io/tags/%E7%89%B9%E5%BE%81%E5%80%BC%E5%88%86%E8%A7%A3/"/>
    
  </entry>
  
  <entry>
    <title>主成分分析(Principal Component Analysis)</title>
    <link href="http://mxxhcm.github.io/2019/01/02/pca/"/>
    <id>http://mxxhcm.github.io/2019/01/02/pca/</id>
    <published>2019-01-02T12:51:19.000Z</published>
    <updated>2019-08-30T09:54:46.506Z</updated>
    
    <content type="html"><![CDATA[<h2 id="降维">降维</h2><h3 id="降维的目标">降维的目标</h3><p>降维可以看做将$p$维的数据映射到$m$维，其中$p\gt m$。或者可以说将$n$个$p$维空间中的点映射成$n$个$m$维空间中的点。</p><h3 id="降维的目的">降维的目的</h3><ol><li>维度灾难(curse of dimensionity)</li><li>随着维度增加，精确度和效率的退化。</li><li>可视化数据</li><li>数据压缩</li><li>去噪声<br>…</li></ol><h3 id="降维的方法">降维的方法</h3><h4 id="无监督的降维">无监督的降维</h4><ol><li>线性的:PCA</li><li>非线性的: GPCA, Kernel PCA, ISOMAP, LLE</li></ol><h4 id="有监督的降维">有监督的降维</h4><ol><li>线性的: LDA</li><li>非线性的: Kernel LDA</li></ol><h2 id="主成分分析-pca">主成分分析(PCA)</h2><p>主成分分析(principal component analysis,PCA)是一个降维工具。PCA使用正交变换(orthogonal transformation)将可能相关的变量的一系列观测值(observation)转换成一系列不相关的变量，这些转换后不相关的变量叫做主成分(principal component)。第一个主成分有着最大的方差，后来的主成分必须和前面的主成分正交，然后最大化方差。或者PCA也可以看成根据数据拟合一个$m$维的椭球体(ellipsoid)，椭球体的每一个轴代表着一个主成分。<br>上课的时候，老师给出了五种角度来看待PCA，分别是信息保存，投影，拟合，嵌入(embedding)，mainfold learning。本文首先从保存信息的角度来给出PCA的推理过程，其他的几种方法就随缘了吧。。。</p><h3 id="信息保存-preserve-information">信息保存(preserve information)</h3><h4 id="目标">目标</h4><p>从信息保存的角度来看PCA的目标是用尽可能小的空间去存储尽可能多的信息。一般情况下，信息用信息熵$-\int p lnp$来表示，如果这里使用信息熵的话，不知道信息的概率表示，一般不知道概率分布的情况下就采用高斯分布，带入高斯分布之后得到$\frac{1}{2}log(2\pi e\sigma^2)$，其中$2\pi e$都是常量，只剩下方差。给出一堆数据，直接计算信息熵是行不通的，但是计算方差是可行的，而方差和信息熵是有联系的，所以可以考虑用方差来表示信息。考虑一下降维前的$p$维数据$x$和降维后的$m$维数据$z$方差之间的关系，$var(z)?var(x)$，这里$z$和$x$的方差维度是不同的，所以不能相等，这里我们的目标就是最大化$z$的方差。方差能解释变化，方差越大，数据的变化就越大，越能包含信息。PCA的目标就是让降维后的数据方差最大。</p><h4 id="线性pca过程">线性PCA过程</h4><h5 id="目标函数">目标函数</h5><p>给定$n$个观测数据$x_1,x_2,\cdots,x_n \in \mathbb{R}^p$，形成一个观测矩阵$X,X\in \mathbb{R}^{p\times n}$，即$X = \begin{bmatrix}x_{11}&amp;\cdots &amp;x_{1n}\\ &amp;\cdots&amp;\\ x_{p1}&amp;\cdots &amp;x_{pn}\end{bmatrix}$。我们的目标是将这样一组$p$维的数据转换成$m$维的数据。线性PCA是通过线性变换(matrix)来实现的，也就是我们要求一个$p\times m$的矩阵$V$，将原始的$X$矩阵转换成$Z$矩阵，使得<br>$$Z_{m\times n}= V_{p\times m}^{T}X_{p\times n},$$<br>其中$V\in \mathbb{R}^{p\times m}$, $v_i=\begin{bmatrix}v_{1i}\\v_{2i}\\ \vdots\\v_{pi}\end{bmatrix}$, $V = \begin{bmatrix}v_{11}&amp;v_{12}&amp;\cdots&amp;v_{1m}\\v_{21}&amp;v_{22}&amp;\cdots&amp;v_{2m}\\ \vdots&amp;\vdots&amp;\cdots&amp;\vdots\\v_{p1}&amp;v_{p2}&amp;\cdots&amp;v_{pm}\end{bmatrix}=\begin{bmatrix}v_1&amp;v_2&amp;\cdots&amp;v_m\end{bmatrix}$, $V^T = \begin{bmatrix}v_{11}&amp;v_{21}&amp;\cdots&amp;v_{p1}\\v_{12}&amp;v_{22}&amp;\cdots&amp;v_{p2}\\ \vdots&amp;\vdots&amp;\cdots&amp;\vdots\\v_{1m}&amp;v_{2m}&amp;\cdots&amp;v_{pm}\end{bmatrix}=\begin{bmatrix}v_1^T \\v_2^T \\ \vdots\\v_m^T \end{bmatrix}$。<br>所以就有：<br>\begin{align*}<br>z_1 &amp;= v_1^Tx_j\\<br>&amp;\cdots\\<br>z_k &amp;= v_k^Tx_j\\<br>&amp;\cdots\\<br>z_m &amp;= v_m^Tx_j<br>\end{align*}<br>其中$z_1,\cdots,z_m$是标量，$v_1^T,\cdots, v_m^T $是$1\times p$的向量，$x_j$是一个$p\times 1$维的观测向量，而我们有$n$个观测向量，所以随机变量$z_k$共有$n$个可能取值：<br>$$z_{k} = v_k^Tx_i= \sum_{i=1}^{p}v_{ik}x_{ij}, j = 1,2,\cdots,n$$<br>其中$x_i$是观测矩阵$X$的第$i$列，$X\in \mathbb{R}^{p\times n}$。</p><h5 id="协方差矩阵">协方差矩阵</h5><p>离散型随机变量$X$($X$的取值等可能性)方差的计算公式是：<br>$$var(X) = E[(X-\mu)^2] = \frac{1}{n}\sum_{i=1}^n (x_i-\mu)^2,$$<br>其中$\mu$是X的平均数，即$\mu = \frac{1}{n}\sum_{i=1}^nx_i$。</p><p>让$z_k$的方差最大即最大化：<br>\begin{align*}<br>var(z_1) &amp;=E(z_1,\bar{z_1})^2 \\<br>&amp;=\frac{1}{n}\sum_{i=1}^n (v_1^T x_i - v_1^T \bar{x_i})^2\\<br>&amp;=\frac{1}{n}\sum_{i=1}^n (v_1^T x_i - v_1^T \bar{x_i})(v_1^T x_i - v_1^T \bar{x_i})^T\\<br>&amp;=\frac{1}{n}\sum_{i=1}^n v_1^T (x_i - \bar{x_i})(x_i - \bar{x_i})^T v_1\\<br>\end{align*}<br>其中$x_i=\begin{bmatrix}x_{1i}\\x_{2i}\\ \vdots\\x_{pi}\end{bmatrix}$,$\bar{x_i}=\begin{bmatrix}\bar{x_{1i}}\\\bar{x_{2i}}\\ \vdots\\\bar{x_{pi}}\end{bmatrix}$,$x_i$是$p$维的，$x_i^p$也是$p$维的，$(x_i-\bar{x_i})$是$p\times 1$维的，$(x_i -\bar{x_i})^T$是$1\times p$维的。<br>令$S=\frac{1}{n}\sum_{i=1}^n(x_i -\bar{x_i})(x_i-\bar{x_i})^T$，$S$是一个$p\times p$的对称矩阵，其实$S$是一个协方差矩阵。这个协方差矩阵可以使用矩阵$X$直接求出来，也可以通过对$X$进行奇异值分解求出来。<br>如果使用奇异值分解的话，首先对矩阵$X$进行去中心化，即$\bar{x_i}=0$，则：<br>\begin{align*}<br>S &amp;= \frac{1}{n}\sum_{i=1}^T x_ix_i^T \\<br>&amp;=\frac{1}{n}X_{p\times n}X_{n\times p}^T<br>\end{align*}<br>$X=U\Sigma V^T $<br>$X X^T =U\Sigma V^T V\Sigma U^T = U\Sigma_1^2 U^T $<br>$X^T X =V\Sigma U^T U\Sigma V^T = V\Sigma_2^2 V^T $<br>$S=\frac{1}{n}XX^T =\frac{1}{n}U\Sigma^2 U^T $</p><h5 id="拉格朗日乘子法">拉格朗日乘子法</h5><p>将$S$代入得：<br>$$var(z_1) = v_1^TSv_1,$$<br>接下来的目标是最大化$var(z_1)$，这里要给出一个限制条件，就是$v_1^Tv_1 = 1$，否则的话$v_1$无限大，$var(z_1)$就没有最大值了。<br>使用拉格朗日乘子法，得到目标函数：<br>$$L=v_1^TSv_1 - \lambda (v_1^Tv_1 -1)$$<br>求偏导，令偏导数等于零得：<br>\begin{align*}<br>\frac{\partial{L}}{\partial{v_1}}&amp;=2Sv_1 - 2\lambda v_1\\<br>&amp;=2(S-\lambda) v_1\\<br>&amp;=0<br>\end{align*}<br>即$Sv_1 = \lambda v_1$，所以$v_1$是矩阵$S$的一个特征向量(eigenvector)。所以：<br>$$var(z_1) = v_1^TSv_1 = v_1^T\lambda v_1 = \lambda v_1^Tv_1 = \lambda,$$<br>第一个主成分$v_1$对应矩阵$S$的最大特征值。</p><h5 id="其他主成分">其他主成分</h5><p>对于$z_2$,同理可得：<br>$var(z_2) = v_2^TSv_2$，<br>但是这里要加一些限制条件$v_2<sup>Tv_2=1$，除此以外，第2个主成分还有和之前的主成分不相关，即$cov[z_1,z_2]=0$,或者说是$v_1</sup>Tv_2=0$，证明如下。<br>\begin{align*}<br>cov[z_1,z_2] &amp;=\mathbb{E}[(z_1-\bar{z_1})(z_2-\bar{z_2})]\\<br>&amp;=\frac{1}{n}(v_1^T x_i - v_1^T \bar{x_i})(v_2^T x_i-v_2^T \bar{x_i})\\<br>&amp;=\frac{1}{n}v_1^T (x_i-\bar{x_i})(x_i-\bar{x_i})v_2\\<br>&amp;=\frac{1}{n}v_1^T SV_2\\<br>&amp;=\frac{1}{n}\lambda v_1^T v_2\\<br>&amp;=0<br>\end{align*}</p><p>维基百科上是通过将数据减去第一个主成分之后再最大化方差，这两种理解方法都行。<br>所以拉格朗日目标函数就成了：<br>$$L=v_1^TSv_1 - \lambda (v_1^Tv_1 -1) -\beta v_2^Tv_1$$<br>求导，令导数等于零得：<br>$$\frac{\partial{L}}{\partial{v_1}}=2Sv_2 - 2\lambda v_2 - \beta v_1 = 0$$<br>而$v_1$和$v_2$不相关，所以$\beta=0$，所以$Sv_2 = \lambda v_2$，即$v_2$也是矩阵$S$的特征向量，但是最大的特征值对应的特征向量已经被$v_1$用了，所以$v_2$是第二大的特征值对应的特征向量。<br>同理可得第$k$个主成分是$S$的第$k$大特征值对应的特征向量。</p><p>但是这种理解方法没有办法推广到非线性PCA。接下来的集中理解方式可以由线性PCA开始，并且可以推广到非线性PCA。</p><h3 id="函数拟合">函数拟合</h3><h4 id="线性pca过程-v2">线性PCA过程</h4><h4 id="非线性pca过程">非线性PCA过程</h4><h5 id="广义主成分分析-generalized-pca-gpca">广义主成分分析(Generalized PCA,GPCA)</h5><p>刚才讲的PCA是线性PCA，是拟合一个超平面(hyperplane)的过程，但是如果数据不是线性的，比如说是一个曲面$x^2 +y^2 +z=0$，这样子线性PCA就不适用了，可以稍加变化让其依然是可以用的。比如$x+y+1=0$可以看成$\begin{bmatrix}a&amp;b&amp;c\end{bmatrix}\begin{bmatrix}x\\y\\1\end{bmatrix}$，而$x^2 +y^2 +z=0$可以看成$\begin{bmatrix}a&amp;b&amp;c\end{bmatrix}\begin{bmatrix}x^2 \\y^2 \\z\end{bmatrix}$。</p><p>如果原始数据是非线性的，我们可以通过多个特征映射函数$\Phi$从原始数据提取非线性特征（也可看成升维，变成高维空间中数据，在高维中可以看成是线性的），然后利用线性PCA对非线性特征进行降维。例如：<br>假设$x=[x_1,x_2,x_3]^T \in \mathbb{R}^3$，按照转换函数$v(x) = [x_1^2 , x_1x_2,x_1x_3,x_2^2 ,x_2x_3,x_3^2 $将其转换成$\mathbb{R}^6$中的特征，接下来使用线性PCA对这些非线性特征进行降维。</p><p>给定一个函数$\Phi$将$p$维数据映射到特征空间$F$中，即$\Phi:\mathbb{R}^p\rightarrow F,\mathbf{x}\rightarrow X$。我们可以通过计算协方差矩阵$C_F = \frac{\Phi\Phi^T }{n}$,即$C_F = \frac{1}{n}\sum_{i=1}<sup>{n}\phi(x_i)\phi(x_i)</sup>T $，然后对协方差矩阵$C_F$进行特征值分解$C_Fx=\lambda x$就可以求解，这里我们假设空间$F$中的数据均值为$0$，即$E[\Phi(x)] = 0$。</p><h3 id="嵌入-embedding-保距离">嵌入(embedding)，保距离</h3><h4 id="核函数技巧-kernel-trick">核函数技巧(Kernel trick)</h4><p>在GPCA中，如果不知道$\Phi$的话，或者$\Phi$将数据映射到了无限维空间中，就没有办法求解了。这里就给出了一个假设，假设低维空间中$x_i,x_j$的点积(dot product)可以通过一个函数计算，将$x_i,x_j$的点积记为$K_{ij}$，则：<br>$$K_{ij} = &lt; \phi(x_i),\phi(x_j) &gt; = k(x_i,x_j)$$<br>其中$k()$是一个函数，比如可以取高斯函数，$k(x,y) = e^{\frac{(\Vert x-y\Vert)^2 }{2\sigma^2 }}$，我们叫它核函数(kernel function)。<br>这样即使我们不知道$\Phi$，也可以计算点积，直接使用核函数计算。</p><h4 id="dot-pca">dot PCA</h4><p>给定原始数据$X_D = [x_1,\cdots,x_n],x_i\in \mathbb{R}^p$，假定$\hat{x}=0$，那么$X_D$的协方差矩阵：<br>\begin{align*}<br>S&amp;= \frac{\sum_{i=1}^n (x_i-\bar{x})(x_i-\bar{x})^T }{n}\\<br>&amp;= \frac{\sum_{i=1}^n (x_i-0)(x_i-0)^T }{n}\\<br>&amp;= \frac{\sum_{i=1}^n (x_i)(x_i)^T }{n}\\<br>&amp;= \frac{\begin{bmatrix}x_1&amp;\cdots&amp;x_n\end{bmatrix}\begin{bmatrix}x_1\\\vdots\\x_n\end{bmatrix}}{n}\\<br>&amp;= \frac{X_DX_D^T}{n}\\<br>&amp;= Cov(X_D, X_D)<br>\end{align*}<br>即$S=\frac{X_DX_D^T}{n}$，而对$X_D$做奇异值分解，有$X_D = V\Sigma U^T$，所以$S = \frac{V\Sigma^2 V^T}{n}$，其中$U$是$S$的特征值矩阵，则：$Z’ = V^T X’$，其中$V\in \mathbb{R}^{p\times m}$，$X’$是新的样本数据。</p><p>这里我们推导一下点积和PCA的关系，即假设我们有$K = Dot(X_D,X_D) = X_D^T X_D$，则$K=U^T \sigma^2U$，而我们根据奇异值分解$X_D = V\Sigma U^T $可以得到$U$和$V$的关系，即$V=X_DU\Sigma^{-1} $，对$K$进行特征值分解，可以求得$U$和$\Sigma$，所以来了一个新的样本$X’$，<br>$$Z’ = V^TX’ = D^{-1} U^T X_D^T X’ = D^{-1} U^T &lt; X_D,X’ &gt;.$$<br>事实上，这里$X’$是已知的，可以直接计算协方差，但是这里是为了给Kernel PCA做引子，所以，推导的过程中是没有用到$X$的，只用到了$X$的点积，在测试的时候会用到$X’$。</p><h5 id="kernel-pca">Kernel PCA</h5><p>Kernel PCA就是将Kernel trick应用到了dot PCA中，由Kernel trick得$K = \Phi^T \Phi$，$K=U\Sigma^2 U^T $，则<br>$$V = \Phi U\Sigma^{-1} = \Phi U diag(1/sqrt(\lambda_1),1/sqrt(\lambda_2),\cdots)$$<br>但是我们求不出来$V$，因为$\Phi$不知道，但是可以让$V$中的$\Phi$和样本$X’$中的$\Phi$在一起，就可以计算了，即<br>$$Z’ = V^T\phi(X’) = \Sigma^{-1}U\Phi\phi(X’) = \Sigma^{-1}UK(X,X’)$$</p><h3 id="流形学习-manifold">流形学习(manifold)</h3><h4 id="线性">线性</h4><h5 id="pca">PCA</h5><h5 id="mds">MDS</h5><h4 id="非线性">非线性</h4><h5 id="lle">LLE</h5><h5 id="isomap">ISOMAP</h5><h2 id="线性判断分析-fisher-linear-discrimiant-analysis-lda">线性判断分析(Fisher linear discrimiant analysis,LDA)</h2><h3 id="线性lda">线性LDA</h3><h4 id="两类">两类</h4><h5 id="示例">示例</h5><h4 id="c类-c-gt-2">C类(C$\gt 2$)</h4><p>两维的问题是通过将原始数据投影到一维空间进行分类，而$C$维的问题则是将原始数据投影到$C-1$空间进行分类，通过一个投影矩阵$W=\begin{bmatrix}w_1&amp;\cdots&amp;w_{C-1}\end{bmatrix}$将$C$维的$x$投影到$C-1$维，得到$y=\begin{bmatrix}y_1&amp;\cdots&amp;y_{C-1}\end{bmatrix}$，即$y_i = w_i^Tx\Rightarrow y = W^Tx$。</p><h5 id="示例-v2">示例</h5><h3 id="不足">不足</h3><ul><li>最多投影到$C-1$维特征空间。</li><li>LDA是参数化的方法，它假设数据服从单高斯分布，并且所有类的协方差都是等价的。对于多个高斯分布，线性的LDA是无法分开的。</li><li>当数据之间的差异主要通过方差而不是均值体现的话，LDA就会失败(fail)。如下图<br><img src="/2019/01/02/pca/" alt="figure"></li></ul><h3 id="kernel-lda">Kernel LDA</h3><h2 id="pca和lda区别和联系">PCA和LDA区别和联系</h2><p>PCA是一个无监督的降维方法，通过最大化降维后数据的方差实现；LDA是一个有监督的降维方法，通过最大化类可分性实现(class discrimnatory)。</p><h2 id="参考文献">参考文献</h2><p>1.<a href="https://en.wikipedia.org/wiki/Principal_component_analysis" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Principal_component_analysis</a><br>2.<a href="https://en.wikipedia.org/wiki/Variance" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Variance</a><br>3.<a href="https://sebastianraschka.com/faq/docs/lda-vs-pca.html" target="_blank" rel="noopener">https://sebastianraschka.com/faq/docs/lda-vs-pca.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;降维&quot;&gt;降维&lt;/h2&gt;
&lt;h3 id=&quot;降维的目标&quot;&gt;降维的目标&lt;/h3&gt;
&lt;p&gt;降维可以看做将$p$维的数据映射到$m$维，其中$p\gt m$。或者可以说将$n$个$p$维空间中的点映射成$n$个$m$维空间中的点。&lt;/p&gt;
&lt;h3 id=&quot;降维的目的&quot;&gt;降维
      
    
    </summary>
    
      <category term="机器学习" scheme="http://mxxhcm.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://mxxhcm.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="监督学习" scheme="http://mxxhcm.github.io/tags/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="主成分分析" scheme="http://mxxhcm.github.io/tags/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/"/>
    
      <category term="PCA" scheme="http://mxxhcm.github.io/tags/PCA/"/>
    
      <category term="降维" scheme="http://mxxhcm.github.io/tags/%E9%99%8D%E7%BB%B4/"/>
    
  </entry>
  
  <entry>
    <title>内积空间、赋范空间和希尔伯特空间</title>
    <link href="http://mxxhcm.github.io/2018/12/28/%E5%86%85%E7%A7%AF%E7%A9%BA%E9%97%B4%E3%80%81%E8%B5%8B%E8%8C%83%E7%A9%BA%E9%97%B4%E5%92%8C%E5%B8%8C%E5%B0%94%E4%BC%AF%E7%89%B9%E7%A9%BA%E9%97%B4/"/>
    <id>http://mxxhcm.github.io/2018/12/28/内积空间、赋范空间和希尔伯特空间/</id>
    <published>2018-12-28T07:15:37.000Z</published>
    <updated>2019-05-06T16:22:27.708Z</updated>
    
    <content type="html"><![CDATA[<h2 id="引言">引言</h2><p>数学的空间是研究工作的对象和遵循的规则。<br>线性空间：加法和数乘<br>拓扑空间：距离，范数和内积。</p><h2 id="距离">距离</h2><p>距离是用来衡量两个点有多“近”的。</p><h3 id="距离定义">距离定义</h3><p>$X$是一非空集合，任给一对这一集合中的元素$x,y$，都会给定一个实数$d(x,y)$与它们对应，并且这个实数满足以下条件：</p><ol><li>$d(x,y)\ge 0, d(x,y) = 0 \Leftrightarrow x=y$；</li><li>$d(x,y) = d(y,x)$；</li><li>$d(x,y) \le d(x,z) + d(z,y)$。</li></ol><p>则$d(x,y)$为这两点$x$和$y$之间的距离。</p><h3 id="示例">示例</h3><h4 id="向量的距离">向量的距离</h4><p>$d_1(x,y) = \sqrt{(x_1-y_1)<sup>2+\cdots,(x_n-y_n)</sup>n}$<br>$d_2(x,y) = max{|x_1-y_1|,\cdots,|x_n,y_n|}$<br>$d_3(x,y) = |x_1-y_1|+\cdots+|x_n,y_n|$</p><h4 id="曲线的距离">曲线的距离</h4><p>$d_1(f,g) = \int_a<sup>b(f(x)-g(x))</sup>2 dx$<br>$d_2(f,g) = max_{a\le x\le b}|f(x)-f(y)|$<br>$d_3(f,g) = \int_a<sup>b(f(x)-g(x))</sup>k dx$</p><h3 id="线性空间">线性空间</h3><p>向量的加法和数乘</p><h3 id="线性空间的八个性质">线性空间的八个性质</h3><p>加法的交换律和结合律，零元，负元，数乘的交换律，单位一，数乘与加法的结合律。</p><h2 id="范数-向量到零点的距离-定义">范数（向量到零点的距离）定义</h2><p>如果$\Vert x\Vert $是$R^n$上的范数（$x$是向量），那么它需要满足以下条件：</p><ol><li>$\Vert x\Vert \ge 0, \forall x\in R, \Vert x\Vert  = 0  \Leftrightarrow x = 0$；</li><li>$\Vert \alpha x\Vert  = |\alpha|\Vert x\Vert, \forall \alpha \in R, x\in R^n$</li><li>$\Vert x+y\Vert  \le \Vert x\Vert  + \Vert y\Vert , \forall x,y\in R$</li></ol><p>可以看成是点到零点距离多了条件2。</p><h3 id="示例-v2">示例</h3><p>$\Vert x\Vert_2  = \sqrt{x_1<sup>2+\cdots,x_n</sup>n}$<br>$\Vert x\Vert_{\infty}  = max{|x_1|,\cdots,|x_n|}$<br>$\Vert x\Vert_1  = |x_1|+\cdots+|x_n|$</p><h3 id="距离和范数的关系">距离和范数的关系</h3><p>由范数可以定义距离。$d(x,y) = \Vert  x-y\Vert$。<br>但是由距离不一定可以定义范数。如$\Vert x\Vert  = d(0,x)$,但是$\Vert \alpha x\Vert  = d(0, \alpha x) \ne |\alpha|\Vert x\Vert $。</p><h3 id="赋范空间和度量空间">赋范空间和度量空间</h3><p>赋予范数的集合称为赋范空间。<br>距离的集合称为度量空间。</p><h3 id="线性赋范空间和线性度量空间">线性赋范空间和线性度量空间</h3><p>赋予范数加上线性空间称为线性赋范空间。<br>距离加上线性空间称为线性度量空间。</p><h2 id="内积">内积</h2><h3 id="引言-v2">引言</h3><p>赋范空间有向量的模长，即范数。但是范数只有大小，没有夹角，所以就引入了内积。</p><h3 id="定义">定义</h3><p>给定$(x,y)\in R$,如果它满足：</p><ol><li>对称性；</li><li>对第一变元的线性性；</li><li>正定性。</li></ol><p>那么就称$(x,y)$为内积。</p><h3 id="示例-v3">示例</h3><ol><li>$(x,y) = \sum_{n=1}^nx_ny_n$。</li><li>$(f,g) = \int_{-\infty}^{\infty}f(x)g(x)dx$。</li></ol><h3 id="内积和范数的关系">内积和范数的关系</h3><ul><li>内积可以导出范数</li><li>范数不能导出距离</li></ul><h3 id="内积空间">内积空间</h3><p>在线性空间上定义内积，这个空间称为内积空间。<br>常见的欧几里得空间就是一个内积空间，内积空间是一个抽象的空间，而欧几里得空间是一个具象化了的内积空间。<br>希尔伯特引入了无穷维空间并定义了内积，其空间称为内积空间，再加上完备性，称为希尔伯特空间。完备性是取极限之后还在这个空间内。<br>完备的赋范空间称为巴拿赫空间。</p><h2 id="拓扑空间">拓扑空间</h2><p>欧几里得几何学需要定义内积，连续的概念不需要内积，甚至不需要距离。</p><h3 id="定义-v2">定义</h3><p>给定一个集合$X$,$\tau$是$X$的一系列子集，如果$\tau$满足以下条件：</p><ol><li>空集(empty set)和全集X都是$\tau$的元素;</li><li>$\tau$中任意元素的并集(union)仍然是$\tau$的元素;</li><li>$\tau$中任意有限多个元素的交集(intersection)仍然是$\tau$中的元素。<br>则称$\tau$是$X$上的一个拓扑。</li></ol><h2 id="距离-范数和内积之间的关系">距离，范数和内积之间的关系</h2><p>距离$\gt$范数$\gt$内积</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;引言&quot;&gt;引言&lt;/h2&gt;
&lt;p&gt;数学的空间是研究工作的对象和遵循的规则。&lt;br&gt;
线性空间：加法和数乘&lt;br&gt;
拓扑空间：距离，范数和内积。&lt;/p&gt;
&lt;h2 id=&quot;距离&quot;&gt;距离&lt;/h2&gt;
&lt;p&gt;距离是用来衡量两个点有多“近”的。&lt;/p&gt;
&lt;h3 id=&quot;距离定义&quot;
      
    
    </summary>
    
      <category term="线性代数" scheme="http://mxxhcm.github.io/categories/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/"/>
    
    
      <category term="内积空间" scheme="http://mxxhcm.github.io/tags/%E5%86%85%E7%A7%AF%E7%A9%BA%E9%97%B4/"/>
    
      <category term="赋范空间" scheme="http://mxxhcm.github.io/tags/%E8%B5%8B%E8%8C%83%E7%A9%BA%E9%97%B4/"/>
    
      <category term="希尔伯特空间" scheme="http://mxxhcm.github.io/tags/%E5%B8%8C%E5%B0%94%E4%BC%AF%E7%89%B9%E7%A9%BA%E9%97%B4/"/>
    
      <category term="距离" scheme="http://mxxhcm.github.io/tags/%E8%B7%9D%E7%A6%BB/"/>
    
      <category term="范数" scheme="http://mxxhcm.github.io/tags/%E8%8C%83%E6%95%B0/"/>
    
      <category term="内积" scheme="http://mxxhcm.github.io/tags/%E5%86%85%E7%A7%AF/"/>
    
  </entry>
  
  <entry>
    <title>convex optimization chapter 2 Convex sets</title>
    <link href="http://mxxhcm.github.io/2018/12/24/convex-optimization-chapter-2-Convex-sets/"/>
    <id>http://mxxhcm.github.io/2018/12/24/convex-optimization-chapter-2-Convex-sets/</id>
    <published>2018-12-24T08:28:45.000Z</published>
    <updated>2019-09-09T08:25:28.782Z</updated>
    
    <content type="html"><![CDATA[<h2 id="仿射集-affine-sets-和凸集-convex-sets">仿射集(affine sets)和凸集(convex sets)</h2><h3 id="直线-line-和线段-line-segmens">直线(line)和线段(line segmens)</h3><p>假设$x_1,x_2 \in \mathbb{R}^n $是n维空间中不重合$(x_1 \ne x_2)$的两点，给定：<br>$$y = \theta x_1 + (1 - \theta)x_2,$$<br>当$\theta\in R$时，$y$是经过点$x_1$和点$x_2$的直线。当$\theta=1$时，$y=x_1$,当$\theta=0$时，$y=x_2$。当$\theta\in[0,1]$时，$y$是$x_1$和$x_2$之间的线段(line segment)。 把$y$改写成如下形式： $$y = x_2 + \theta(x_1 - x_2)$$，可以给出另一种解释，$y$是点$x_2$和方向$x_1 - x_2$(从$x_2$到$x_1$的方向)乘上一个缩放因子$\theta$的和。<br>如下图所示，可以将y看成$\theta$的函数。<br><img src="https://ws1.sinaimg.cn/large/006wtfMEly1fyhy7m4llij30mz0alwep.jpg" alt="line_line-segment"></p><h3 id="仿射集-affine-sets">仿射集(affine sets)</h3><h4 id="仿射集的定义">仿射集的定义</h4><p>给定一个集合$C\subset \mathbb{R}^n $,如果经过$C$中任意两个不同点的直线仍然在$C$中，那么$C$就是一个仿射集。即，对于任意$x_1,x_2\in C$和$\theta\in R$，都有$\theta x_1 + (1 - \theta)x_2 \in C$。换句话说，给定线性组合的系数和为$1$，$C$中任意两点的线性组合仍然在$C$中，我们就称这样的集合是仿射的(affine)。</p><h4 id="仿射组合-affine-combination">仿射组合(affine combination)</h4><p>我们可以把两个点的线性组合推广到多个点的线性组合，这里称它为仿射组合。<br>仿射组合的定义：给定$\theta_1+\cdots+\theta_k = 1$,则$\theta_1 x_1 + \cdots + \theta_k x_k$是点$x_1,\cdots,x_k$的仿射组合(affine combination)。<br>根据仿射集的定义，一个仿射集(affine set)包含集合中任意两个点的仿射（线性）组合，那么可以推导出仿射集包含集合中任意点（大于等于两个）的仿射组合，即：如果$C$是一个仿射集，$x_1,\cdots,x_k\in C$,且$\theta_1 x_1 + \cdots + \theta_k x_k = 1$,那么点$\theta_1 x_1 + \cdots + \theta_k x_k$仍然属于$C$。</p><h4 id="仿射集的子空间-subspce">仿射集的子空间(subspce)</h4><p>如果$C$是一个仿射集，$x_0 \in C$,那么集合<br>$$V = C - x_0 = {x - x_0\big|x \in C}$$<br>是一个子空间(subspace),因为$V$是加法封闭和数乘封闭的。<br>证明：<br>假设$v_1, v_2 \in V$，并且$\alpha,\beta \in R$。<br>要证明V是一个子空间，那么只需要证明$\alpha v_1 + \beta v_2 \in V$即可。<br>因为$v_1, v_2 \in V$，则$v_1+x_0, v_2+x_0 \in C$。<br>而$x_0 \in C$，所以有<br>$$\alpha(v_1+x_0) + \beta(v_2+x_0) + (1 - \alpha - \beta)x_0 \in C$$<br>即：<br>\begin{align*}<br>\alpha v_1 + \beta v_2 + (\alpha + \beta + 1 - \alpha - \beta)x_0 &amp;\in C\\<br>\alpha v_1 + \beta v_2 + x_0 &amp;\in C<br>\end{align*}<br>所以$\alpha v_1 + \beta v_2 \in V$。<br>所以，仿射集$C$可以写成：<br>$$C = V + x_0 = { v + x_0\big| v \in V},$$<br>即，一个子空间加上一个偏移(offset)。而与仿射集$C$相关的子空间$V$与$x_0$的选择无关，即$x_0$可以为$C$中任意一点。</p><h4 id="示例">示例</h4><p>线性方程组的解。一个线性方程组的解可以表示为一个仿射集:$C={x\big|Ax = b}$,其中 $A\in \mathbb{R}^{m \times n}, b \in \mathbb{R}^m $。<br>证明：<br>设$x_1, x_2 \in C$,即$Ax_1 = b, Ax_2 = b$。对于任意$\theta \in R$,有:<br>\begin{align*}<br>A(\theta x_1 + (1-\theta x_2) &amp;= \theta Ax_1 + (1-\theta)Ax_2\\<br>&amp;= \theta b + (1 - \theta) b\\<br>&amp;= b \end{align*}<br>所以线性方程组的解是一个仿射组合：$\theta x_1 + (1 - \theta) x_2$，这个仿射组合在集合$C$中，所以线性方程组的解集$C$是一个仿射集。<br>和该仿射集$C$相关的子空间$V$是$A$的零空间(nullspace)。因为仿射集$C$中的任意点都是方程$Ax = b$的解，而$V = C - x_0 = {x - x_0\big|x \in C}$，有$Ax = b, Ax_0 = b$，则$Ax - Ax_0 = A(x - x_0) = b - b = 0$，所以$V$是$A$的零空间。</p><h4 id="仿射包-affine-hull">仿射包(affine hull)</h4><p>给定集合$C\subset \mathbb{R}^n $，集合中点的仿射组合称为集合$C$的仿射包(affine hull),表示为$aff C$:<br>$aff C = {\theta_1 x_1 + \cdots + \theta_k x_k\big| x_1,\cdots,x_k \in C, \theta_1 + \cdots + \theta_k = 1}$<br>集合$C$可以是任意集合。仿射包是包含集合$C$的最小仿射集（一个集合的仿射包只有一个，是不变的）。即如果$S$是任意仿射集，满足$C\subset S$，那么有$aff C \subset S$。或者说仿射包是所有包含集合$C$的仿射集的交集。</p><h3 id="仿射纬度-affine-dimension-和相对内部-relative-interior">仿射纬度(affine dimension)和相对内部(relative interior)</h3><h4 id="拓扑-topology">拓扑(topology)</h4><p>拓扑(topology)，开集(open sets),闭集(close sets),内部(interior),边界(boundary),闭包(closure),邻域(neighbood),相对内部(relative interior)<br>同一个集合可以有很多个不同的拓扑。</p><h5 id="定义">定义</h5><p>给定一个集合$X$,$\tau$是$X$的一系列子集，如果$\tau$满足以下条件：</p><ol><li>空集(empty set)和全集X都是$\tau$的元素;</li><li>$\tau$中任意元素的并集(union)仍然是$\tau$的元素;</li><li>$\tau$中任意有限多个元素的交集(intersection)仍然是$\tau$中的元素。</li></ol><p>则称$\tau$是集合$X$上的一个拓扑。<br>如果$\tau$是$X$上的一个拓扑，那么$(X,\tau)$对称为一个拓扑空间(topological space)。<br>如果$X$的一个子集在$\tau$中，这个子集被称为开集(open set)。<br>如果$X$的一个子集的补集是在$\tau$中，那么这个子集是闭集(closed set)。<br>$X$的子集可能是开集，闭集，或者都是，都不是。<br>空集和全集是开集，也是闭集（定义）。</p><h5 id="示例-v2">示例</h5><ol><li>给定集合$X={1,2,3,4}$, 集合$\tau = { {},{1,2,3,4} }$就是$X$上的一个拓扑。</li><li>给定集合$X={1,2,3,4}$, 集合$\tau = { {},{1}, {3,4},{1,3,4},{1,2,3,4} }$就是$X$上的另一个拓扑。</li><li>给定集合$X={1,2,3,4}$, $X$的幂集(power set)也是$X$上的另一个拓扑。</li></ol><p><strong>通常如果不说的话，默认是在欧式空间(1维，2维,…,n维欧式空间)的拓扑，即欧式拓扑。以下讲的一些概念是在欧式空间的拓扑（通常拓扑）上的定义和一般拓扑直观上可能不太一样，但实际上意义是相同的。</strong></p><h4 id="epsilon-disc-或-epsilon-邻域">$\epsilon-disc$或$\epsilon$邻域</h4><h5 id="定义-v2">定义</h5><p>给定$x\in \mathbb{R}^n $以及$\epsilon\gt 0$，集合<br>$$D(x,\epsilon) = {y\in \mathbb{R}^n \big|d(x,y) \lt \epsilon}$$<br>称为关于$x$的$\epsilon-disc$或者$\epsilon$邻域(neighbood)或者$\epsilon$球(ball)。即所有离点$x$距离小于$\epsilon$的点$y$的集合。</p><h4 id="开集-open-sets">开集(open sets)</h4><h5 id="定义-v3">定义</h5><p><strong>给定集合$A\subset \mathbb{R}^n $，对于$A$中的所有元素，即$\forall x\in A$，都存在$\epsilon \gt 0$使得$D(x,\epsilon)\subset A$，那么就称该集合是开的。</strong><br>即集合$A$中所有元素的$spsilon$邻域都还在集合$A$中（定理$1$）。<br><strong>注意：必须满足$\epsilon \gt 0$</strong></p><h5 id="定理">定理</h5><h6 id="定理-1-epsilon-邻域是开集">定理$1$ $epsilon$邻域是开集</h6><ul><li>在$\mathbb{R}^n $中，对于一个$\epsilon \gt 0, x\in \mathbb{R}^n $,那么集合$x$的$\epsilon$邻域$D(x,\epsilon)$是开的，给定一个$\epsilon$，能找到一个更小的$epsilon$邻域。</li></ul><h6 id="定理-2">定理$2$</h6><ul><li>$\mathbb{R}^n $中有限个开子集的交集是$\mathbb{R}^n $的开子集。</li><li>$\mathbb{R}^n $中任意个开子集的并集是$\mathbb{R}^n $的开子集。</li></ul><p><strong>注意：任意开集的交可能不是开集，一个点不是开集，但是它是所有包含它的开集的交。</strong></p><h5 id="示例-v3">示例</h5><p><img src="/2018/12/24/convex-optimization-chapter-2-Convex-sets/unit_circle.png" alt="unit_circle"></p><ol><li>$\mathbb{R}^2 $中的不包含边界的球是开的，如图。</li><li>考虑一个$\mathbb{R}^1 $中的开区间，如$(0,1)$，它是一个开集，但是如果把它放在二维欧式空间中(是x轴上的一个线段)，它不是开的，不满足定义，所以开集是必须针对于某一个给定的集合$X$。</li><li>$\mathbb{R}^2 $上的包含边界的单位圆$X = {x\in \mathbb{R}^2 \big||x|\le 1}$不是开的。因为边界上的点$x$不满足$\epsilon \gt 0, D(x,\epsilon) \subset X$。</li><li>集合$S={(x,y) \in \mathbb{R}^2 \big|0 \lt x \lt 1}$是开集。对于每个点$(x,y)\in S$,我们可以画出半径$r = min{x,1-x}$的邻域并且其全部含于$S$，所以$S$是开集。</li><li>集合$S={(x,y) \in \mathbb{R}^2 \big|0 \lt x \le 1}$不是开集。因为点$(1,0) \in S$的邻域包含点$(x,0)$,其中$x\gt 1$。</li></ol><h4 id="内部-interior">内部(interior)</h4><h5 id="定义-v4">定义</h5><p><strong>给定集合$A\subset \mathbb{R}^n $,点$x \in A$，如果有一个开集$U$使得$x \in U\subset A$,那么该点就称为$A$的一个内点。或者说对于$x\in A$，有一个$\epsilon \gt 0$使得$D(x,\epsilon)\subset A$。$A$的所有内点组成的集合叫做$A$的内部(interior)，记做$int(A)$。</strong></p><h5 id="属性">属性</h5><ol><li>集合内部可能是空的，单点的内部就是空的。</li><li>单位圆的内部是不包含边界的单位圆。</li><li>事实上$A$的内部是$A$所有开子集的并，由开集的定理得$A$的内部是开的，且$A$的内部是$A$的最大的开集。</li><li>当且仅当$A$的内部等于$A$的时候，$A$是开集（$A$可能是闭集）。</li><li>只需要寻找集合内$\epsilon$邻域还在这个集合内的点即可。</li></ol><h5 id="示例-v4">示例</h5><ol><li>给定集合$S={(x,y)\in \mathbb{R}^2 \big| 0 \lt x \le 1}$，$int(S) = {(x,y)\big|0 \lt x \lt 1}$。因为区间$(0,1)$中的点都满足它们的$\epsilon$邻域在$S$中。</li><li>$int(A) \cup int(B) \ne int(A\cup B)$。在实数轴上，$A=[0,1],B=[1,2]$，那么$int(A) = (0,1),int(B) = (1,2)$，所以$int(A) \cup int(B) = (0,1)\cup (1,2) = (0,2)\backslash {1}$，而$int(A\cup B) = int[0,2] = (0,2)$。</li></ol><h4 id="闭集-closed-set">闭集(closed set)</h4><h5 id="定义-v5">定义</h5><p><strong>对于$\mathbb{R}^n $中的集合$B$，如果它在$\mathbb{R}^n $的补（即集合$\mathbb{R}^n \backslash B$）是开集，那么它是闭集。</strong><br>单点是闭集。含有边界的单位圆组成的集合是闭集，因为它的补集不包含边界。一个集合可能既不是开集也不是闭集。例如，在一维欧几里得空间，半开半闭区间（如$(0,1]$）既不是开集也不是闭集。</p><h5 id="定理-v2">定理</h5><ol><li>$\mathbb{R}^n $中有限个闭子集的并是闭集。</li><li>$\mathbb{R}^n $中任意个闭子集的交是闭集。</li></ol><p>这个定理是从开集的定理中得出的，在对开集取补变成闭集时候，并与交相互变换即可。</p><h5 id="示例-v5">示例</h5><ol><li>给定集合$S={(x,y) \in \mathbb{R}^2 \big| 0 \lt x \le 1, 0 \lt y \lt 1}$，$S$不是闭集。因为目标区域的下边界不在S中。</li><li>给定集合$S={(x,y) \in \mathbb{R}^2 \big| x^2 +y^2 \le 1}$，$S$是闭集，因为它的闭集是$\mathbb{R}^2 $中的开集。</li><li>$\mathbb{R}^n $中任何有限集是闭集。因为单点是闭集，有限集可以看成很多个单点的并，由定理$1$可以得出。</li></ol><h4 id="聚点-accumulation-point">聚点(accumulation point)</h4><h5 id="定义-v6">定义</h5><p>对于点$x\in \mathbb{R}^n $，如果包含$x$的每个开集$U$包含不同于$x$但依然属于集合$A$中的点，那么就称$x$是$A$的一个聚点(accumulation points)，也叫聚类点(cluster points)。**注意这里是包含集合$A$中的点，而不是全部是集合$A$中的点，所以集合的聚点不一定必须在集合中。**如，在一维欧式空间中，单点集合没有聚点，开区间$(0,1)$的聚点是$[0,1]$，${0,1}$不在区间内，但是是聚点。<br>此外，$x$是聚类点等价于：对于每个$\epsilon \gt 0$，$D(x,\epsilon)$包含$A$中的某点$y$且$y\ne x$。</p><h5 id="定理-v3">定理</h5><p>当且仅当集合$S$的所有聚点属于$S$时，$S\subset \mathbb{R}^n $是闭集。</p><h5 id="示例-v6">示例</h5><ol><li>给定集合$S={x\in R\big|x\in [0,1]且x是有理数}$，$S$的聚点为$[0,1]$中所有点。任何不属于$[0,1]$的点都不是聚点，因为这类点有一个包含它的$\epsilon$邻域与$[0,1]$不相交。</li><li>给定集合$S={(x,y)\in \mathbb{R}^2 \big| 0 \le  x\le 1\ or\ \ x = 2}$, 它的聚点是它本身，因为它是闭集。</li><li>给定集合$S={(x,y)\in \mathbb{R}^2 \big|y \lt x^2 + 1}$，S的聚点为集合${(x,y)\in \mathbb{R}^2 \big|y \le x^2 + 1}$，</li></ol><h4 id="闭包-closure">闭包(closure)</h4><h5 id="定义-v7">定义</h5><p>给定集合$A\subset \mathbb{R}^n $,集合$A$的闭包$cl(A)$定义成所有包含$A$的闭集的交，所以$cl(A)$是一个闭集。定价的定义是给定集合$A$，包含$A$的最小闭集叫做这个集合$X$的闭包(closure)，用$cl(A)$或者${\overline{A}}$表示。</p><h5 id="定理-v4">定理</h5><p>给定$A\subset \mathbb{R}^n $，那么$cl(A)$由$A$和$A$的所有聚点组成。</p><h5 id="示例-v7">示例</h5><ol><li>$R$中$S=[0,1)\cup {2}$的闭包是$[0,1]$和${2}$。$S$的聚点是$[0,1]$，再并上$S$得到$S$的闭包是$[0,1]\cup{2}$。</li><li>对于任意$S\subset \mathbb{R}^n $，$\mathbb{R}^n \backslash cl(S)$是开集。因为$cl(S)$是闭集，所以它的补集是开集。</li><li>$cl(A\cap B) \ne cl(A)\cap cl(B)$。比如$A=(0,1),B(1,2),cl(A)=[0,1],cl(B)=[1,2]$,$A\cap B = \varnothing$,$cl(A\cap B) = \varnothing$,而$cl(A)\cap cl(B) = {1}$。</li></ol><h4 id="边界-boundary">边界(boundary)</h4><h5 id="定义-v8">定义</h5><p>对于$\mathbb{R}^n $中的集合$A$，边界定义为集合：<br>$bd(A) = cl(A)\cap cl(\mathbb{R}^n \backslash A)$<br>即集合$A$的补集的闭包和$A$的闭包的交集，所以$bd(A)$是闭集。$bd(A)$是$A$与$\mathbb{R}^n \backslash A$之间的边界。</p><h5 id="定理-v5">定理</h5><p>给定$A\subset \mathbb{R}^n $，当且仅当对于每个$\epsilon \gt 0$，$D(x,\epsilon)$包含$A$与$\mathbb{R}^n \backslash A$的点，$x\in bd(A)$。</p><h5 id="示例-v8">示例</h5><ol><li>给定集合$S={x\in R\big|x\in [0,1],x是有理数}$，$bd(S) = [0,1]$。因为对于任意$\epsilon \gt 0, x\in [0,1],D(x,\epsilon) = (x-\epsilon, x+\epsilon)$包含有理数和无理数，即x是有理数和无理数之间的边界。</li><li>给定$x\in bd(S)$，$x$不一定是聚点。给定集合$S = {0} \subset R$，$bd(S) = {0}$，但是单点没有聚点。</li><li>给定集合$S={(x,y)\in \mathbb{R}^2 \big| x^2 -y^2 \gt 1 }$，$bd(S)={(x,y)\big|x^2 - y^2 = 1}$。</li></ol><h4 id="仿射维度-affine-dimension">仿射维度(affine dimension)</h4><h5 id="定义-v9">定义</h5><p>给定一个仿射集$C$，仿射维度是它的仿射包的维度。<br>仿射维度和其他维度的定义不总是相同的，具体可以看以下的示例。</p><h5 id="示例-v9">示例</h5><p>给定一个二维欧几里得空间的单位圆，${x\in C\big|x_1^2 +x_2^2 =1}$。它的仿射包是整个$\mathbb{R}^2$，所以二维平面的单位圆仿射维度是$2$。但是在很多定义中，二维平面的单位圆的维度是$1$。</p><h4 id="相对内部-relative-interior">相对内部(relative interior)</h4><p>给定一个集合$C\subset \mathbb{R}^n $，它的仿射维度可能小于$n$，这个时候仿射集$aff\ C \ne \mathbb{R}^n $。</p><h5 id="定义-v10">定义</h5><p>给定集合$C$，相对内部的定义如下：<br>$relint\ C = {x\in C\big|(B(x,r)\cup aff\ C) \subset C, \exists \ r \gt 0}.$<br>就是集合$C$内所有$\epsilon$球在$C$的仿射集内的点的集合。<br>其中$B(x,r)={y \big|\Vert y- x\Vert \le r}$，是以$x$为中心，以$r$为半径的圆。这里的范数可以是任何范数，它们定义的相对内部是相同的。</p><h5 id="示例-v10">示例</h5><p>给定一个$\mathbb{R}^3 $空间中$(x_1,x_2)$平面上的正方形，$C={x\in \mathbb{R}^3 \big|-1 \le x_1 \le 1, -1\le x_2 \le 1, x_3 = 0}$。它的仿射包是$(x_1,x_2)$平面，$aff\ C = {x\in \mathbb{R}^3 \big|x_3=0}$。$C$的内部是空的，但是相对内部是：<br>$relint\ C = {x \in \mathbb{R}^3 \big|-1 \le x_1 \le 1, -1\le x_2 \le 1,x_3=0}$。</p><h4 id="相对边界-relative-boundary">相对边界(relative boundary)</h4><h5 id="定义-v11">定义</h5><p>给定集合$C$，相对边界(relative boundary)定义为$cl\ C \backslash relint\ C$，其中$cl\ C$是集合$C$的闭包(closure)。</p><h5 id="示例-v11">示例</h5><p>对于上例（相对内部的示例）来说，它的边界(boundary)是它本身。它的相对内部是边框，${x\in \mathbb{R}^3 \big|max{|x_1|,|x_2|}=1,x_3=0}$。</p><h3 id="凸集-convex-sets">凸集(convex sets)</h3><h4 id="凸集定义">凸集定义</h4><p>给定一个集合$C$，如果集合$C$中经过任意两点的线段仍然在$C$中，这个集合就是一个凸集。<br>给定$\forall x_1,x_2 \in C, 0 \le \theta \le 1$，那么我们有$\theta x_1 + (1-\theta)x_2 \in C$。<br>每一个仿射集都是凸的，因为它包含经过任意两个不同点的直线，所以肯定就包含过那两个点的线段。</p><h4 id="凸组合-convex-combination">凸组合(convex combination)</h4><p>给定$k$个点$x_1,x_2,\cdots,x_k$，如果具有$\theta_1 x_1 + \cdots, \theta_k x_k$形式且满足$\theta_1 + \cdots + \theta_k=1, \theta_i \ge 0,i=1,\cdots,k$,那么就称这是$x_1,\cdots,x_k$的一个凸组合。<br>当且仅当一个集合包含其中所有点的凸组合，这个集合是一个凸集。点的一个凸组合可以看成点的混合或者加权，$\theta_i$是第$i$个点$x_i$的权重。<br>凸组合可以推广到无限维求和，积分，概率分布等等。假设$\theta_1,\theta_2,\cdots$满足：<br>$$\theta_i \le 0, i = 1,2,\cdots, \sum_{i=1}^{\infty}\theta_i = 1$$<br>并且$x_1,x_2,\cdots \in C$，$C\subset \mathbb{R}^n $是凸的，如果(series)$\sum_{i=1}^{\infty} \theta_i x_i$收敛，那么$\sum_{i=1}^{\infty} \theta_i x_i \in C$。<br>更一般的，假设概率分布$p$，$\mathbb{R}^n \rightarrow R$满足$p(x)\le 0 for\ all\ x\in C, \int_{C}p(x)dx = 1$,其中$C\subset \mathbb{R}^n $是凸的，如果$\int_{C}p(x)xdx$存在的话，那么$\int_{C}p(x)xdx\in C$。</p><h4 id="凸包-convex-hull">凸包(convex hull)</h4><p>给定一个集合$C$，凸包的定义为包含集合$C$中所有点的凸组合的结合，记为$conv\ C$，公式如下：<br>$conv\ C = {\theta_1 x_1 + \cdots + \theta_k x_k\big|x_i \in C, \theta_i \ge 0, i = 1,\cdots,k,\theta_1 +\cdots + \theta_k = 1}$<br>任意集合都是有凸包的。一个集合的凸包总是凸的。集合$C$的凸包是包含集合$C$的最小凸集。如果集合$B$是任意包含$C$的凸集，那么$conv\ C \subset B$。</p><h4 id="示例-v12">示例</h4><p><img src="/2018/12/24/convex-optimization-chapter-2-Convex-sets/figure2_2.png" alt="figure 2.2"><br><img src="/2018/12/24/convex-optimization-chapter-2-Convex-sets/figure2_3.png" alt="figure 2.3"></p><h3 id="锥-cones">锥(cones)</h3><h4 id="锥-cones-和凸锥-convex-cones-的定义">锥(cones)和凸锥(convex cones)的定义</h4><p>给定集合$C$，如果$\forall x \in C, \theta \ge 0$，都有$\theta x\in C$，这样的集合就称为一个锥(cone)，或者非负同质(nonnegative homogeneour)。<br>一个集合$C$如果既是锥又是凸的，那这个集合是一个凸锥(convex cone)，即：$\forall x_1,x_2 \in C, \theta_1,\theta_2 \ge 0$,那么有$\theta_1 x_1+\theta_2 x_2 \in C$。几何上可以看成经过顶点为原点，两条边分别经过点$x_1$和$x_2$的$2$维扇形。</p><h4 id="锥组合-conic-combination">锥组合(conic combination)</h4><p>给定$k$个点$x_1,x_2,\cdots,x_k$，如果具有$\theta_1 x_1 + \cdots, \theta_k x_k$形式且满足$\theta_i \ge 0,i=1,\cdots,k$,那么就称这是$x_1,\cdots,x_k$的一个锥组合(conic combination)或者非负线性组合(nonnegative combination)。<br>给定集合$C$是凸锥，那么集合$C$中任意点$x_i$的锥组合仍然在集合$C$中。反过来，当且仅当集合$C$包含它的任意元素的凸组合时，这个集合是一个凸锥(convex cone)。</p><h4 id="锥包-conic-hull">锥包(conic hull)</h4><p>给定集合$C$，它的锥包(conic hull)是集合$C$中所有点的锥组合。即：<br>$conic\ C = {\theta_1 x_1 + \cdots + \theta_k x_k\big|x_i \in C, \theta_i \ge 0, i = 1,\cdots,k}$<br>集合$C$的锥包是包含集合$C$的最小凸锥。</p><h4 id="示例-v13">示例</h4><p><img src="/2018/12/24/convex-optimization-chapter-2-Convex-sets/figure2_4.png" alt="figure 2.4"><br><img src="/2018/12/24/convex-optimization-chapter-2-Convex-sets/figure2_5.png" alt="figure 2.5"></p><h3 id="一些想法">一些想法</h3><p>在我自己看来，在几何上<br>仿射集可以看成是集合中任意两个点的直线的集合。<br>凸集可以看成是集合中任意两个点的线段的集合，因为直线一定包含线段，所以仿射集一定是凸集。<br>锥集可以看成是集合中任意一个点和原点构成的射线的集合，锥集不一定是连续的（两条射线也是锥集），所以锥集不一定是凸集。<br>而凸锥既是凸集又是锥集。<br>我在stackexchange看到这样一句话觉得说的挺好的。</p><blockquote><p>What basically distinguishes the definitions of convex, affine and cone, is the domain of the coefficients and the constraints that relate them.</p></blockquote><p>区别凸集，仿射和锥的是系数的取值范围和一些其他限制。仿射集要求$\theta_1+\cdots+\theta_k = 1$，凸集要求$\theta_1 +\cdots +\theta_k = 1, 0\le \theta \le 1$，锥的要求是$\theta \ge 0$，凸锥的要求是$\theta_i \ge 0,i=1,\cdots,k$。<br>仿射集不是凸集的子集，凸集也不是仿射集的子集。所有仿射集的集合是所有凸集的集合的子集，一个仿射集是一个凸集。</p><h2 id="示例-v14">示例</h2><ul><li>$\emptyset$，单点(single point)${x_0}$，整个$\mathbb{R}^n $空间都是$\mathbb{R}^n $中的仿射子集，所以也是凸集，点不一定是凸锥（在原点熵是凸锥），空集是凸锥，$\mathbb{R}^n $维空间也是凸锥。<strong>根据定义证明。</strong></li><li>任意一条直线都是仿射的，所以是凸集。如果经过原点，它是一个子空间，也就是一个凸锥，否则不是。</li><li>任意一条线段都是凸集，不是仿射集，当它退化成一点的时候，它是仿射的，线段不是凸锥。</li><li>一条射线${x_0 + \theta v\big| \theta \ge 0}$是凸的，但是不是仿射的，当$x_0=0$时，它是凸锥。</li><li>任意子空间都是仿射的，也是凸锥，所以是凸的。</li></ul><p>补充最后一条，任意子空间都是仿射的，也是凸锥。<br>如果$V$是一个子空间，那么$V$中任意两个向量的线性组合还在$V$中。即如果$x_1,x_2\in V$，对于$\theta_1,\theta_2 \in R$，都有$\theta_1 x_1 + \theta_2 x_2 \in V$。正如前面说的，子空间是加法和数乘封闭的。<br>而根据仿射集的定义，如果$x_1,x_2$在一个仿射集$C$中，那么对于$\theta_1+\theta_2 = 1$，都有$\theta_1 x_1 + \theta_2 x_2 \in C$。我们可以看出来，如果取子空间中线性组合的系数和为$1$，那么就成了仿射集。如果取子空间中的系数$\theta_1,\theta_2 \in R_+$,那么就成了锥，如果同时满足$\theta_1+\theta_2 = 1$，那么就成凸锥。那么如果加上这些限制条件，即取子空间中线性组合的系数和为$1$，或者取子空间中的系数$\theta_1,\theta_2 \in R_+$,同时满足$\theta_1+\theta_2 = 1$。<br>事实上，子空间要求的条件比仿射集和凸锥的条件要更严格。仿射集和凸锥只要求在系数$\theta_i$满足相应的条件时,有$\theta_1 x_1 + \theta_2 x_2 \in \mathbb{R}^n $；而子空间要求的是在系数$\theta_i$取任意值的时候，都有$\theta_1 x_1 + \theta_2 x_2 \in \mathbb{R}^n $，所以子空间一定是仿射集，也一定是凸锥。（拿二维的举个例子，给定$x_1$和$x_2$，仿射集可以看成是$\theta_1$的函数，因为$\theta_2=1-\theta_1$，而子空间可以看成$\theta_1$和$\theta_2$的函数，一个是一元函数，一个是二元函数）</p><h3 id="超平面-hyperplane-和半空间-halfspace">超平面(hyperplane)和半空间(halfspace)</h3><p>超平面是一个仿射集，也是凸集，但不一定是锥集(过原点才是锥集，也是一个子空间)。<br>闭的半空间是一个凸集，不是仿射集。</p><h4 id="超平面-hyperplane">超平面(hyperplane)</h4><p>超平面通常具有以下形式：<br>$${x\big|a^T x=b},$$<br>其中$a\in \mathbb{R}^n ,a\ne 0,b\in R$，它其实是一个平凡(nontrivial)线性方程组的解，因此也是一个仿射集。几何上，超平面可以解释为和一个给定向量$a$具有相同内积(inner product)的点集，或者说是法向量为$a$的一个超平面。常数$b$是超平面和原点之间的距离(offset)。<br>几何意义可以被表示成如下形式：<br>$${x\big|a^T (x-x_0) = 0},$$<br>其中$x_0$是超平面上的一点，即满足$a^T x_0=0$。可以被表示成如下形式：<br>$${x\big|a^T (x-x_0)=0} = x_0+a^{\perp},$$<br>其中$a^{\perp} $是$a$的正交补，即所有与$a$正交的向量的集合，满足$a^{\perp} ={v\big|a^T v=0}$。所以，超平面的几何解释可以看做一个偏移(原点到这个超平面的距离)加上所有垂直于一个特定向量$a$(正交向量)的向量，即这些垂直于$a$的向量构成了一个过原点的超平面，再加上这个偏移量就是我们要的超平面。几何表示如下图所示。<br><img src="/2018/12/24/convex-optimization-chapter-2-Convex-sets/figure2_6.png" alt="figure 2.6"></p><h4 id="半空间-halfspace">半空间(halfspace)</h4><p>一个超平面将$\mathbb{R}^n $划分为两个半空间(halfspaces)，一个是闭(closed)半空间，一个是开半空间。闭的半空间可以表示成${x\big|a^T x\le b}$，其中$a\ne 0$，半空间是凸的，但不是仿射的。下图便是一个闭的半空间。<br><img src="/2018/12/24/convex-optimization-chapter-2-Convex-sets/figure2_7.png" alt="figure 2.7"><br>这个半空间也可以写成：<br>$${x\big|a^T (x-x_0)\le 0},$$<br>其中$x_0$是划分两个半空间的超平面上的一点，即满足$a^T x_0=b$。一个几何解释是：半空间由一个偏移$x_0$加上所有和一个特定向量$a$(超平面的外(outward)法向量)成钝角(obtuse)或者直角(right)的所有向量组成。<br>这个半空间的边界是超平面${x\big|a^T x=b}$。这个半空间${x\big|a^T x\le b}$的内部是${x\big|a^T x\lt b}$，也被称为一个开半平面。</p><h3 id="欧几里得球-euclidean-ball-和椭球-ellipsoid">欧几里得球(Euclidean ball)和椭球(ellipsoid)</h3><h4 id="欧几里得球">欧几里得球</h4><p>$\mathbb{R}^n $空间中的欧几里得球或者叫球，有如下的形式：<br>$$B(x_r,r = {x\big|\Vert x-x_c\Vert_2\le r}={x \big|(x-x_c)^T (x-x_c)\le \mathbb{R}^2 },$$<br>其中$r\gt 0$,$\Vert \cdot\Vert_2$是欧几里得范数(第二范数)，即$\Vert u\Vert_2=(u^T u)^{\frac{1}{2}} $。向量$x_c$是球心，标量$r$是半径。$B(x_c,r)$包含所有和圆心$x_c$距离小于$r$的球。<br>欧几里得球的另一种表示形式是：<br>$$B(x_c,r)={x_c + ru\big| \Vert u \Vert_2 \le 1},$$<br>一个欧几里得球是凸集，如果$\Vert x_1-x_c\Vert_2 \le r,\Vert x_2-x_c\Vert_2\le r, 0\le\theta\le1$，那么：<br>\begin{align*}<br>\Vert\theta x_1 + (1-\theta)x_2 - x_c\Vert_2 &amp;= \Vert\theta(x_1-x_c)+(1-\theta)(x_2-x_c)\Vert_2\\<br>&amp;\le\theta\Vert x_1-x_c\Vert_2 + (1-\theta)\Vert x_2 - x_c \Vert_2\\<br>&amp;\le r<br>\end{align*}<br>用其次性和三角不等式可证明</p><h4 id="椭球">椭球</h4><p>另一类凸集是椭球，它们有如下的形式：<br>$$\varepsilon ={x\big|(x-x_c)^T P^{-1} (x-x_c) \le 1},$$<br>其中$P=P^T \succ 0$即$P$是对称和正定的。向量$x_c\in \mathbb{R}^n $是椭球的中心。矩阵$P$决定了椭球从$x_c$向各个方向扩展的距离。椭球$\varepsilon$的半轴由矩阵$P$的特征值$\lambda_i$算出，$\sqrt{\lambda_i}$，球是$P=\mathbb{R}^2 I$的椭球。<br><strong>这里这种表示形式为什么要用$P^{-1} $？</strong><br>椭球的另一种表示是：<br>$$\varepsilon = {x_c + Au\big| \Vert u \Vert_2 \le 1},$$<br>其中$A$是一个非奇异方阵。假设$A$是对称正定的，取$A=P^{\frac{1}{2}} $，这种表示就和上面的表示是一样的。第一次看到这种表示的时候，我在想，椭球的边界上有无数个点，一个方阵$A$是怎么实现对这无数个操作的，后来和球做了对比，发现自己一直都想错了，这无数个点是通过范数实现的而不是通过矩阵$A$实现的，到球心距离为$\Vert u\Vert_2\le 1$的点有无数个，$A$对这无数个点的坐标都做了仿射变换，将一个球变换成了椭球，特殊情况下就是球。当矩阵$A$是对称半正定但是是奇异的时候，这个情况下称为退化椭球(degenerate ellipsoid)，它的仿射维度和矩阵$A$的秩(rank)是相同的。退化椭球也是凸的。</p><h3 id="范数球-norm-ball-和范数锥-norm-cone">范数球(norm ball)和范数锥(norm cone)</h3><h4 id="范数球-norm-ball">范数球(norm ball)</h4><h5 id="定义-v12">定义</h5><p>$\Vert \cdot\Vert$是$\mathbb{R}^n $上的范数。一个范数球(norm ball)可以看成一个以$x_c$为中心，以$r$为半径的集合，但是这个$r$可以是任何范数，即${x\big|\Vert x-x_c \Vert \le r}$，它是凸的。</p><h5 id="示例-v15">示例</h5><p>我们常见的球是二范数（欧几里得范数）对应的范数球。</p><h4 id="范数锥">范数锥</h4><h5 id="定义-v13">定义</h5><p>和范数相关的范数锥是集合：$C = {(x,t)\big|\Vert x\Vert \le t} \subset \mathbb{R}^{n+1} $，它也是凸锥。</p><h5 id="示例-v16">示例</h5><p><img src="/2018/12/24/convex-optimization-chapter-2-Convex-sets/figure2_10.png" alt="figure 2.10"><br>二阶锥(second-order cone)是欧几里得范数对应的范数锥，如图所示，其表达式为：<br>\begin{align*}<br>C &amp;={(x,t)\in \mathbb{R}^{n+1} \big| \Vert x\Vert_2 \le t}\\<br>&amp;= \left{ \begin{bmatrix}x\\t\end{bmatrix} \big| \begin{bmatrix}x\\t\end{bmatrix}^T \begin{bmatrix}I&amp;0\\ 0&amp;-1\end{bmatrix} \begin{bmatrix}x\\t\end{bmatrix}\le 0, t \gt 0 \right}<br>\end{align*}<br>这个二阶锥也被称为二次锥(quadratic cone)，因为它是通过一个二次不等式定义的，也被叫做Lorentz cone或者冰激凌锥(ice-cream cone)。</p><h4 id="范数锥和范数球的区别">范数锥和范数球的区别</h4><p>范数球是所有点到圆心$x_c$的范数小于一个距离$r$。<br>范数锥是很多直线组成的锥。</p><h3 id="多面体-polyhedra">多面体(polyhedra)</h3><h4 id="定义-v14">定义</h4><p>多面体(polyhedron)是有限个线性不等式或者线性方程组的解集的集合：<br>$P = {x\big|a_j^T x\le b_j, j=1,\cdots,m,c_j^T x=d_j,j=1,\cdots,p}$<br>多面体因此也是有限个半空间或者超平面的交集。仿射集(如，子空间，超平面，直线)，射线，线段，半空间等等都是多面体，多面体也是凸集。有界的polyhedron有时也被称为polytope，一些作者会把它们两个反过来叫。<br>上式的紧凑(compact)表示是：<br>$$P={x\big|Ax\preceq b, Cx=d}$$<br>其中$A=\begin{bmatrix}a_1^T \\ \vdots\\ a_m^T \end{bmatrix},C=\begin{bmatrix}c_1^T \\ \vdots\\c_p^T \end{bmatrix}$，$\preceq$表示$\mathbb{R}^m $空间中的向量不等式(vector ineuqalitied)或者分量大小的不等式，$u\preceq v$代表着$u_i\le v_i, i=1,\cdots,m$。</p><h5 id="simplexes">simplexes</h5><p>simplexes是另一类很重要的多面体。假设$\mathbb{R}^n $空间中的$k+1$个点是仿射独立(affinely independent)，意味着$v_1-v_0, \cdots,v_k-v_0$是线性独立的。由$k+1$个仿射独立的点确定的simplex是：<br>$$C = conv{v_0,\cdots,v_k} = {\theta_0v_0+\cdots+\theta_kv_k\big| \theta \succeq 0, \mathcal{1}\theta=1 },$$<br>其中$\mathcal{1}$是全为$1$的列向量。这个simplex的仿射维度是$k$，所以它也叫$\mathbb{R}^n $空间中的$k$维simplex。为什么仿射维度是$k$，我的理解是simplex是凸集，而凸集不是子空间，凸集去掉其中任意一个元素才是子空间，所以就是$k$维而不是$k+1$维。<br>为了将simplex表达成一个紧凑形式的多面体。定义$y=(\theta_1,\cdots,\theta_k)$和$B=[v_1-v_0\ \cdots\ v_k-v_0]\in \mathbb{R}^{n\times k} $，当且仅当存在$y\succeq 0, \mathcal{1}^T y\le 1$，$x=v_0+By$有$x\in C$，<strong>疑问，这里为什么变成了$\mathcal{1}^T y\le 1$，难道是因为少了个$v_0$吗</strong>。点$v_0,\cdots,v_k$表明矩阵$B$的秩为$k$。因此存在一个非奇异矩阵$A=(A_1,A_2)\in \mathbb{R}^{n\times n} $使得：<br>$$AB = \begin{bmatrix}A_1\\A_2\end{bmatrix}B= \begin{bmatrix}I\\0\end{bmatrix}.$$<br>对$x = v_0+By$同时左乘$A$，得到：<br>$$A_1x = A_1v_0+y, A_2x=A_xv_0.$$<br>从中我们可以看出如果$A_2x=A_2v_0$，且向量$y=A_1x-A_1v_0$满足$y\succeq 0, \mathcal{1}^T y\le1$时，$x\in C$。换句话说，当且仅当$x$满足以下等式和不等式时：<br>$$A_2x = A_2v_0,A_1x\succeq A_1v_0, \mathcal{1}A_1x\le1+\mathcal{1}^T A_1v_0,$$<br>有$x\in C$。</p><h5 id="多面体的凸包描述">多面体的凸包描述</h5><p>一个有限集合${v_1,\cdots,v_k}$的凸包是：<br>$$conv{v_1,\cdots,v_k} = {\theta_1 v_1 +\cdots +\theta_k v_k\big| \theta \succeq 0, \mathcal{1}^T \theta = 1}.$$<br>这个集合是一个多面体，并且有界。但是它（除了simplex）不容易化成多面体的紧凑表示，即不等式和等式的集合。<br>一个一般化的凸包描述是：<br>$${\theta_1 v_1 +\cdots +\theta_k v_k\big| \theta_1+\cdots + \theta_m = 1,\theta_i \ge 0,i=1,\cdots,k}.$$<br>其中$m\le k$，它可以看做是点$v_1,\cdots,v_m$的凸包加上点$v_{m+1},\cdots,v_{k}$的锥包。这个集合定义了一个多面体，反过来，任意一个多面体可以看做凸包加上锥包。<br>一个多面体如何表示是很有技巧的。比如一个$\mathbb{R}^n $空间上的无穷范数单位球$C$：<br>$$C={x\big|\ |x_i|\le 1,i = 1,\cdots,n}.$$<br>集合$C$可以被表示成$2n$个线性不等式$\pm e_i^T x\le 1$，其中$e_i$是第$i$个单位向量。然而用凸包形式描述这个集合需要用至少$2^n $个点：<br>$$C = conv{v_{1},\cdots,v_{2^n }},$$<br>其中$v_{1},\cdots,v_{2n}$是$2^n $个向量，每个向量的元素都是$1$或$-1$。因此凸包描述和不等式描述有很大差异，尤其是$n$很大的时候。<br>这里为什么是$2^n $个点呢？因为是无穷范数构成的单位圆，在数轴上是区间$[-1,1]$，在$\mathbb{R}^2 $是正方形${(x,)\big|-1 \le x\le 1,-1\le y\le 1}$，对应的四个点是${(1,1),(1,-1),(-1,1),(-1,-1)}$，而在$\mathbb{R}^3 $是立方体${(x,y,z)\big|-1 \le x\le 1,-1\le y\le 1, -1\le z\le 1}$，对应的是立方体的八个顶点${(1,1,1),(1,1,-1),(1,-1,1),(1,-1,-1),(-1,1,1),(-1,1,-1),(-1,-1,1),(-1,-1,-1)}$。</p><h4 id="示例-v17">示例</h4><ol><li>如图所示，是五个半平面的交集定义的多面体。<br><img src="/2018/12/24/convex-optimization-chapter-2-Convex-sets/figure2_11.png" alt="figure 2.11"></li><li>非负象限(nonnegative orthant)是非负点的集合，即：<br>$$R_{+}^n = {x\in \mathbb{R}^n \big| x_i\ge 0, i = 1,\cdots,n} = {x\in \mathbb{R}^n \big| x\succeq 0}.$$<br>非负象限是一个多面体，也是一个锥，所以也叫多面体锥(polyhedral cone)，也叫非负象限锥。</li><li>一个1维的simplex是一条线段。一个二维的simplex是一个三角形（包含它的内部）。一个三维的simple是一个四面体(tetrahedron)。</li><li>由$\mathbb{R}^n $中的零向量和单位向量确定的simplex是$n$维unit simplex。它是向量集合：<br>$$x\succeq 0, \mathcal{1}^T x \le 1.$$</li><li>由$\mathbb{R}^n $中的单位向量确定的simplex是$n-1$维probability simplex。它是向量集合：<br>$$x\succeq 0, \mathcal{1}^T x = 1.$$<br>Probability simplex是中的向量可以看成具有$n$个元素的集合的概率分布，$x_i$解释为第$i$个元素的概率。</li></ol><h3 id="半正定锥-positive-sefidefinite-cone">半正定锥(positive sefidefinite cone)</h3><h4 id="定义-v15">定义</h4><p>用$S^n $表示$n\times n$的对称矩阵：$S^n ={X\in \mathbb{R}^{n\times n} \big| X = X^T }$，$S^n $是一个$n(n+1)/2$维基的向量空间。比如，三维空间中对称矩阵的一组基是：<br>$$\begin{bmatrix}1&amp;0&amp;0\\0&amp;0&amp;0\\0&amp;0&amp;0\end{bmatrix}\begin{bmatrix}0&amp;0&amp;0\\1&amp;0&amp;0\\0&amp;0&amp;0\end{bmatrix}\begin{bmatrix}0&amp;0&amp;0\\0&amp;1&amp;0\\0&amp;0&amp;0\end{bmatrix}\begin{bmatrix}0&amp;0&amp;0\\0&amp;0&amp;0\\1&amp;0&amp;0\end{bmatrix}\begin{bmatrix}0&amp;0&amp;0\\0&amp;0&amp;0\\0&amp;1&amp;0\end{bmatrix}\begin{bmatrix}0&amp;0&amp;0\\0&amp;0&amp;0\\0&amp;0&amp;1\end{bmatrix}.$$<br>用$S_{+}^n $表示半正定的对称矩阵集合：<br>$$S_{+}^n = {X\in S^n \big| X\succeq 0},$$<br>用$S_{++}^n $表示正定的对称矩阵集合：<br>$$S_{+}^n = {X\in S^n \big| X\succ 0},$$<br>集合$S_{+}^n $是凸锥：如果$\theta_1,\theta_2 \ge 0$且$A,B\in S_{+}^n $，那么$\theta_1 A+\theta_{2} B\in S_{+}^n $。这个可以直接从依靠半正定的定义来证明，如果$A,B\in S_{+}^n ,\theta_1,\theta_2\ge 0$，(<strong>这里原书中用的是$A,B\succeq 0$,我觉得应该是写错了吧</strong>)，对任意$\forall x \in \mathbb{R}^n $，都有：<br>$$x^T (\theta_1A+\theta_2B)x = \theta_1x^T Ax + \theta_2x^T Bx.$$</p><h4 id="示例-v18">示例</h4><p>对于$S^2 $空间中的半正定锥，有<br>$$X=\begin{bmatrix}x&amp;y\\y&amp;z\end{bmatrix}\in S_{+}^2 \Leftrightarrow x\ge 0,z\ge 0, xz\ge y^2 $$<br>这个锥的边界如下图所示。<br><img src="/2018/12/24/convex-optimization-chapter-2-Convex-sets/figure2_12.png" alt="figure 2.12"></p><h3 id="常见的几种锥">常见的几种锥</h3><p>范数锥，非负象限锥，半正定锥，它们都过原点。<br>想想对应的图像是什么样的。<br>范数锥和非负象限锥图像还好理解一些，非负象限锥是$\mathbb{R}^n $空间所有非负半轴围成的锥，范数锥的边界像一个沙漏，但是是无限延伸的。半正定锥怎么理解，还没有太好的类比。</p><h2 id="保凸运算-operations-that-preserve-convexity">保凸运算(operations that preserve convexity)</h2><p>这一小节介绍的是一些保留集合凸性，或者从一些集合中构造凸集的运算。这些运算和simplex形成了凸集的积分去确定或者建立集合的凸性。</p><h3 id="集合交-intersection">集合交(intersection)</h3><p>凸集求交集可以保留凸性：如果$S_1$和$S_2$是凸集，那么$S_1\cup S_2$是凸集。扩展到无限个集合就是：如果$\forall \alpha \in A,S_{\alpha}$都是凸的，那么$\cup_{\alpha\in A S_{\alpha}}$是凸的</p><h3 id="仿射函数-affine-functions">仿射函数(affine functions)</h3><h3 id="线性分式-linear-fractional-和视角函数-perspective-functions">线性分式(linear-fractional)和视角函数(perspective functions)</h3><h4 id="线性分式-linear-fractional">线性分式(linear-fractional)</h4><h4 id="视角函数-perspective-functions">视角函数(perspective functions)</h4><h2 id="广义不等式-generalized-inequalities">广义不等式（Generalized inequalities)</h2><h3 id="真锥-proper-cones-和广义不等式-generalized-inequalities">真锥(Proper cones)和广义不等式（Generalized inequalities)</h3><h3 id="最小-minimum-和最小元素-minimal-elemetns">最小(Minimum)和最小元素(minimal elemetns)</h3><h2 id="separating和supporting-hyperplanes">Separating和supporting hyperplanes</h2><h3 id="separating-hyperplane-theorem">Separating hyperplane theorem</h3><h3 id="supporting-hyperplanes">Supporting Hyperplanes</h3><h2 id="对偶锥-dual-cones-和广义不等式-generalized-inequalities">对偶锥(dual cones)和广义不等式(generalized inequalities)</h2><h3 id="none"></h3><h2 id="符号定义">符号定义</h2><p>$\preceq$表示$\mathbb{R}^m $空间中的向量不等式(vector ineuqalitied)或者element-wise的不等式，$u\preceq v$代表着$u_i\le v_i, i=1,\cdots,m$。</p><h2 id="参考文献">参考文献</h2><p>1.stephen boyd. Convex optimization<br>2.<a href="https://en.wikipedia.org/wiki/Topology" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Topology</a><br>3.<a href="https://en.wikipedia.org/wiki/Topological_space" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Topological_space</a><br>4.<a href="https://en.wikipedia.org/wiki/Power_set" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Power_set</a><br>5.<a href="https://en.wikipedia.org/wiki/Open_set" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Open_set</a><br>6.<a href="https://en.wikipedia.org/wiki/Closed_set" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Closed_set</a><br>7.<a href="https://en.wikipedia.org/wiki/Clopen_set" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Clopen_set</a><br>8.<a href="https://en.wikipedia.org/wiki/Interior_(topology)" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Interior_(topology)</a><br>9.<a href="https://en.wikipedia.org/wiki/Closure_(topology)" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Closure_(topology)</a><br>10.<a href="https://en.wikipedia.org/wiki/Boundary_(topology)" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Boundary_(topology)</a><br>11.<a href="https://en.wikipedia.org/wiki/Ball_(mathematics)" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Ball_(mathematics)</a><br>12.<a href="https://blog.csdn.net/u010182633/article/details/53792588" target="_blank" rel="noopener">https://blog.csdn.net/u010182633/article/details/53792588</a><br>13.<a href="https://blog.csdn.net/u010182633/article/details/53819910" target="_blank" rel="noopener">https://blog.csdn.net/u010182633/article/details/53819910</a><br>14.<a href="https://blog.csdn.net/u010182633/article/details/53983642" target="_blank" rel="noopener">https://blog.csdn.net/u010182633/article/details/53983642</a><br>15.<a href="https://blog.csdn.net/u010182633/article/details/53997843" target="_blank" rel="noopener">https://blog.csdn.net/u010182633/article/details/53997843</a><br>16.<a href="https://blog.csdn.net/u010182633/article/details/54093987" target="_blank" rel="noopener">https://blog.csdn.net/u010182633/article/details/54093987</a><br>17.<a href="https://blog.csdn.net/u010182633/article/details/54139896" target="_blank" rel="noopener">https://blog.csdn.net/u010182633/article/details/54139896</a><br>18.<a href="https://math.stackexchange.com/questions/1168898/why-is-any-subspace-a-convex-cone" target="_blank" rel="noopener">https://math.stackexchange.com/questions/1168898/why-is-any-subspace-a-convex-cone</a><br>19.<a href="https://www.zhihu.com/question/22799760/answer/139753685" target="_blank" rel="noopener">https://www.zhihu.com/question/22799760/answer/139753685</a><br>20.<a href="https://www.zhihu.com/question/22799760/answer/34282205" target="_blank" rel="noopener">https://www.zhihu.com/question/22799760/answer/34282205</a><br>21.<a href="https://www.zhihu.com/question/22799760/answer/137768096" target="_blank" rel="noopener">https://www.zhihu.com/question/22799760/answer/137768096</a><br>22.<a href="https://en.wikipedia.org/wiki/Positive-definite_matrix" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Positive-definite_matrix</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;仿射集-affine-sets-和凸集-convex-sets&quot;&gt;仿射集(affine sets)和凸集(convex sets)&lt;/h2&gt;
&lt;h3 id=&quot;直线-line-和线段-line-segmens&quot;&gt;直线(line)和线段(line segmens)&lt;/
      
    
    </summary>
    
      <category term="凸优化" scheme="http://mxxhcm.github.io/categories/%E5%87%B8%E4%BC%98%E5%8C%96/"/>
    
    
      <category term="凸优化" scheme="http://mxxhcm.github.io/tags/%E5%87%B8%E4%BC%98%E5%8C%96/"/>
    
      <category term="convex sets" scheme="http://mxxhcm.github.io/tags/convex-sets/"/>
    
      <category term="affine sets" scheme="http://mxxhcm.github.io/tags/affine-sets/"/>
    
      <category term="cones" scheme="http://mxxhcm.github.io/tags/cones/"/>
    
      <category term="convex cones" scheme="http://mxxhcm.github.io/tags/convex-cones/"/>
    
      <category term="convex optimizaitons" scheme="http://mxxhcm.github.io/tags/convex-optimizaitons/"/>
    
      <category term="锥" scheme="http://mxxhcm.github.io/tags/%E9%94%A5/"/>
    
      <category term="凸锥" scheme="http://mxxhcm.github.io/tags/%E5%87%B8%E9%94%A5/"/>
    
      <category term="凸集" scheme="http://mxxhcm.github.io/tags/%E5%87%B8%E9%9B%86/"/>
    
      <category term="仿射集" scheme="http://mxxhcm.github.io/tags/%E4%BB%BF%E5%B0%84%E9%9B%86/"/>
    
  </entry>
  
  <entry>
    <title>熵，交叉熵，相对熵（KL散度），条件熵，互信息</title>
    <link href="http://mxxhcm.github.io/2018/12/23/%E7%86%B5%E3%80%81%E4%BA%A4%E5%8F%89%E7%86%B5%E5%92%8CK-L%E6%95%A3%E5%BA%A6/"/>
    <id>http://mxxhcm.github.io/2018/12/23/熵、交叉熵和K-L散度/</id>
    <published>2018-12-23T02:54:31.000Z</published>
    <updated>2019-09-07T10:10:20.980Z</updated>
    
    <content type="html"><![CDATA[<h2 id="乡农熵-shannon-entropy">乡农熵(Shannon entropy)</h2><p>乡农定义了一个事件的信息量是其发生概率的负对数($-log§$)，即乡农信息量，乡农熵是信息量的期望。</p><h3 id="介绍">介绍</h3><p>这里的熵都是指的信息论中的熵，也叫乡农熵(shannon entropy)。通常，熵是无序或不确定性的度量。<br>与每个变量可能的取值相关的信息熵是每个可能取值的概率质量函数的负对数：<br>$$S = - \sum_i P_i lnP_i$$<br>当事件发生的概率较低时，该事件比高概率事件携带更多“信息”。这种方式定义的每个事件所携带的信息量是一个随机变量，事实上乡农熵定义的一个事件的信息量就是这个事件发生的概率的负对数，这个随机变量（信息量）的期望值是信息熵。信息熵通常以比特(或者称为shannons),自然单位(nats)或十进制数字(dits，bans或hartleys)来测量。具体的单位取决于用于定义熵的对数的基。<br>采用概率分布的对数形式作为信息的度量的原因是因为它的可加性。例如，投掷公平硬币的熵是$1$比特，投掷$m$个硬币的熵是$m$比特。以比特为单位的时候，如果$n$是$2$的指数次方，则需要$log_2n$位来表示一个具有$n$个取值的变量。如果该变量的$n$个取值发生的可能性是相等的，则熵等于$log_2n$。<br>如果一个事件发生的可能性比其他事件发生的可能性更高，观察到该事件发生的信息量少于观测到一些罕见事件，即观测到更罕见的事件时能提供更多的信息。由于小概率事件发生的可能性更低，因此最终的结果是从非均匀分布的数据接收的熵总是小于或等于$log_2n$。当一个结果一定发生时，熵为零。<br>但是熵仅仅量化考虑事件发生的概率，它封装的信息是有关概率分布的信息，事件本身的意义在这种度量方式的定义中无关紧要。<br>熵的另一种解释是最短平均编码长度。</p><h3 id="定义">定义</h3><p>乡农定义了entropy, 定义离散型随机变量$X$，其可能取值为${x_1,\cdots,x_n}$，它对应的概率质量函数(probability mass function) P(X)，则熵$H$为：<br>$$H(X) = E[I(X)] = E[-log(P(X))]$$<br>其中$E$是求期望，$I$是随机变量$X$的信息量, $I(X)$本身是一个随机变量。<br>它可以显示写成：<br>$$H(X) = \sum_{i=1}^nP(x_i)I(x_i) = -\sum_{i=1}^nP(x_i)log_bP(x_i)$$<br>其中b是自然对数的底，$b$常取的值为$2,e,10$，对应的熵的单位是bits, nats，bans。<br>当$P(x_i)=0$的时候，对应的$PlogP$的值为$0log_b(0)$, 和极限(limit)是一致的：<br>$$lim_{p\rightarrow 0_+}plog§ = 0.$$</p><h4 id="连续型随机变量的熵">连续型随机变量的熵</h4><p>将概率质量函数替换为概率密度函数，即可得到连续性随机变量的熵：<br>$$h[f] = E[-ln(f(x))] = - \int_X f(x)ln(f(x))dx.$$</p><h3 id="示例">示例</h3><p>抛一枚硬币，已知其正反两面出现的概率是不相等的，求其正面朝上的概率，该问题可以看做一个伯努利分布问题。<br>如果硬币是公平的，此时得到结果的熵是最大的。这是因为此时抛一次抛硬币的结果具有最大的不确定性。每一个抛硬币的结果会占满一整个bit位。因为<br>\begin{align*}<br>H(X) &amp;= - \sum_{i=1}^n P(x_i)log_bP(x_i)\\<br>&amp;= - \sum_{i=1}^2\frac{1}{2}log_2\frac{1}{2}\\<br>&amp;= - \sum_{i=1}^2\frac{1}{2}\cdot(-1)\\<br>&amp;= 1<br>\end{align*}<br>如果硬币是不公平的，正面向上的概率是$p$，反面向上的概率是$q$，$p \ne q$, 则结果的不确定性更小。因为每次抛硬币，出现其中一面的可能性要比另一面要大，减小的不确定性就得到了更小的熵：每一次抛硬币得到的信息都会小于$1$bit，比如，$p=0.7$时：<br>\begin{align*}<br>H(X) &amp;= -plog_2p - qlog_2q\\<br>&amp;= -0.7log_20.7 - 0.3log_20.3\\<br>&amp;= -0.7\cdot(-0.515) - 0.3\cdot(-1.737)\\<br>&amp;= 0.8816\\<br>&amp;\le 1<br>\end{align*}<br>上面的例子证明不确定性跟变量取值的概率有关。<br>不确定性也跟变量的取值个数有关，上面例子的极端情况是正反面一样（即只有一种取值），那么熵就是0，没有不确定性。</p><h3 id="解释-rationale">解释(rationale)</h3><p>为什么乡农定义了信息量为$-log§$？$-\sum p_i log(p_i)$的意义是什么？<br>首先我们需要想一想信息量需要满足什么条件，然后定义一个信息函数I表示发生概率为$p_i$的事件$i$的信息量，那么这个信息函数需要满足以下条件。</p><ul><li>$I§$是单调下降的；</li><li>$I§ \ge 0$, 即信息是非负的；</li><li>$I(1) = 0$, 一定发生的事件不包含信息；</li><li>$I(p_1p_2) = I(p_1) + I(p_2)$, 独立事件包含的信息是可加的。<br>最后一个条件很关键，它指出了两个独立事件的联合分布和两个分开的独立事件所包含的信息是一样多的。例如，$A$事件有$m$个等可能性的结果，$B$事件有$n$个等可能性的结果，$AB$有$mn$个等可能性的结果。$A$事件需要$log_2(m)$bits去编码，$B$事件需要用$log_2(n)$bits去编码，$AB$需要$log_2(mn) = log_2(m) + log_2(n)$bits编码。乡农发现了$log$函数能够保留可加性，即：<br>$$I§ = log(\frac{1}{p}) = - log§$$<br>事实上，这个函数$I$是唯一的(可以证明),选择$I$当做信息函数。如果一个分布中事件$i$发生的概率是$p_i$,那么采样$N$次，事件$i$发生的次数为$n_i = N p_i$, 所有$n_i$次的信息为$$\sum_in_iI(p_i) = - \sum_iNp_ilog(p_i).$$<br>每个事件的平均信息就是：<br>$$-\sum_ip_ilog(p_i)$$<br>所以$-\sum_ip_ilog(p_i)$就是信息量的期望，即信息熵。<br>在信息论中，熵的另一种解释是最短平均编码长度。</li></ul><h2 id="交叉熵-cross-entropy">交叉熵(cross entropy)</h2><h3 id="介绍-v2">介绍</h3><p>交叉熵用于衡量在给定真实分布下，用非真实分布表示真实概率分布需要付出的花费。<br>交叉熵是信息熵的推广。假设有两个分布$p$和$q$，$p$是真实分布，$q$是非真实分布。信息熵是用真实分布$p$来衡量识别一个事件所需要的编码长度的期望。而交叉熵是用非真实分布$q$来估计真实分布$p$的期望编码长度，用$q$来编码的事件来自分布$p$，所以期望中使用的概率是$p(i)$，$H(p,q)$称为交叉熵。</p><h3 id="定义-v2">定义</h3><p>给定真实分布$q$，分布p和q在给定集合X的交叉熵定义为：<br>$$H(p,q) = E_p[log\frac{1}{q}] = H§ + D_{KL}(p||q)$$<br>其中$H§$是$p$的信息熵，$D_{KL}(p||q)$是从$q$到$p$的$K-L$散度，或者说$p$相对于$q$的相对熵。</p><h3 id="示例-v2">示例</h3><p>如含有4个字母$(A,B,C,D)$的数据集中，真实$p=(\frac{1}{2}, \frac{1}{2}, 0, 0)$，即$A$和$B$出现的概率均为$\frac{1}{2}$，$C$和$D$出现的概率都为$0$。使用完美的编码方案进行编码所需要的编码长度是$H§$为$1$，即只需要$1$位编码即可识别$A$和$B$。如果使用非完美编码方案$q=(\frac{1}{4},\frac{1}{4},\frac{1}{4},\frac{1}{4})$编码则得到$H(p,q)=2$，即需要$2$位编码来识别$A$和$B$。</p><h3 id="解释">解释</h3><p>在机器学习中，交叉熵用于衡量估计的概率分布与真实概率分布之间的差异。<br>在信息论中，Kraft-McMillan定理建立了任何可直接解码的编码方案，为了识别一个$X$的可能值$x_i$f可以看做服从一个在$X$上的隐式概率分布$q(x_i)=2^{-l_i}$,其中$l_i$是$x_i$的编码长度，单位是bits。因此，交叉熵可以被解释为当数据服从真实分布$p$时，在假设分布$q$下得到的每个信息编码长度的期望。</p><h3 id="性质">性质</h3><ul><li>$H(p,q) \ge H§$,由吉布森不等式可以知道，该式子恒成立，当$q$等于$p$时等号成立。</li></ul><h3 id="to-do-交叉熵损失函数和logistic-regression之间的关系">to do ?交叉熵损失函数和logistic regression之间的关系。</h3><h2 id="k-l-散度-kullback-leibler-divergence">$K-L$散度(Kullback-Leibler divergence)</h2><h3 id="介绍-v3">介绍</h3><p>$K-L$散度也叫相对熵(relative entropy)，是用来衡量估计分布和真实分布之间的差异性。<br>$K-L$散度也叫相对熵(relative entropy)，信息熵是用来度量信息量的，信息熵给出了最小熵是多少，但是信息熵并没有给出如何得到最小熵，$K-L$散度也没有给出来如何得到最小熵。但是$K-L$散度可以用来衡量用一个带参数的估计分布来近似真实数据分布时损失了多少信息，可以理解为根据非真实分布$q$得到的平均编码长度比由真实分布$p$得到的平均编码长度多出的长度叫做相对熵。</p><h3 id="定义-v3">定义</h3><h4 id="离散型随机变量">离散型随机变量</h4><p>给定概率分布$P$和$Q$在相同的空间中，它们的$K-L$散度定义为：<br>\begin{align*}<br>D_{KL}(P||Q) &amp;= -\sum_iP(i)(logQ(i)) - (-\sum_iP(i)logP(i))\\<br>D_{KL}(P||Q) &amp;= \sum_iP(i)(logP(i) - logQ(i))\\<br>D_{KL}(P||Q) &amp;= -\sum_iP(i)log(\frac{Q(i)}{Q(i)})\\<br>D_{KL}(P||Q) &amp;= \sum_iP(i)log(\frac{P(i)}{Q(i)})<br>\end{align*}<br>可以看出，$K-L$散度是概率分布$P$和$Q$对数差相对于概率分布$P$的期望。需要注意的是$D_{KL}(P||Q) \ne D_{KL}(Q||P),$因为$P$和$Q$的地位是不同的。相对熵的前半部分就是交叉熵，后半部分是相对熵。</p><h4 id="连续型随机变量">连续型随机变量</h4><p>对于连续性随机变量的分布$P$和$Q$，$K-L$散度被定义为积分：<br>$$D_{KL}(P||Q) = \int_{-\infty}^{\infty} p(x)log(\frac{p(x)}{q(x)})dx,$$<br>其中$p$和$q$代表分布$P$和分布$Q$的概率密度函数。<br>更一般的，$P$和$Q$表示是同一个集合$X$的概率分布，$P$相对于$Q$是绝对连续的，从$Q$到$P$的$K-L$散度定义为：<br>$$D_{KL}(P||Q) = \int_X log(\frac{dP}{dQ})dP$$<br>上式可以被写成：<br>$$D_{KL}(P||Q) = \int_X log(\frac{dP}{dQ})\frac{dP}{dQ}dP$$<br>可以看成$\frac{P}{Q}$的熵。</p><h3 id="示例-v3">示例</h3><p>$P$是一个二项分布，$P~(2,0.4)$，$Q$是一个离散型均匀分布，$x = 0,1,2$, 每一个取值的概率都是$p=\frac{1}{3}$。</p><table><thead><tr><th></th><th>0</th><th>1</th><th>2</th></tr></thead><tbody><tr><td>$P$分布</td><td>0.36</td><td>0.48</td><td>0.16</td></tr><tr><td>$Q$分布</td><td>0.333</td><td>0.333</td><td>0.333</td></tr></tbody></table><p>$K-L$散度的计算公式如下（使用自然对数）：<br>\begin{align*}<br>D_{KL}(Q||P) &amp;= \sum_iQ(i)ln(\frac{Q(i)}{P(i)})\\<br>&amp; = 0.333ln(\frac{0.333}{0.36}) + 0.333ln(\frac{0.333}{0.48}) + 0.333ln(\frac{0.333}{0.16})\\<br>&amp; = -0.02596 + (-0.12176) + 0.24408\\<br>&amp; = 0.09637(nats)<br>\end{align*}<br>上面计算出来的是从$P$到$Q$的K-L散度，或者$Q$相对于$P$的相对熵。</p><h3 id="解释-v2">解释</h3><p>从$Q$到$P$的$K-L$散度表示为$D_{KL}(P||Q)$。在机器学习中，$D_{KL}(P||Q)$被称为信息增益。<br>在信息论中，$K-L$散度也被称为$P$相对于$Q$的相对熵。从信息编码角度来看，$D_{KL}(P||Q)$可以看做用估计分布$q$得到的平均编码长度比用真实分布p得到的平均编码长度多出的长度。</p><h3 id="性质-v2">性质</h3><ul><li>非负性，$D_{KL}(P||Q)\ge 0$,当且仅当$P=Q$时等号成立。</li><li>可加性，如果$P_1,P_2$的分布是独立的，即$P(x,y) = P_1(x)P_2(y)$, $Q,Q_1,Q_2$类似，那么：<br>$$D_{KL}(P||Q) = D_{KL}(P_1||Q_1) + D_{KL}(P_2||Q_2)$$</li><li>不对称性，所以K-L散度不是距离，距离需要满足对称性。</li></ul><h2 id="条件熵">条件熵</h2><h3 id="定义-v4">定义</h3><p>给定$X$，$Y$的条件熵定义如下：<br>给定离散变量${\displaystyle X}$和${\displaystyle Y}$,给定${\displaystyle X}$以后，${\displaystyle Y}$的条件熵定义为每一个${\displaystyle x}$使用权值${\displaystyle p(x)}$ 的加权和${\displaystyle \mathrm {H} (Y|X=x)}$。<br>$$H(Y|X) \equiv \sum_{x\in\boldsymbol{X} } p(x) H(Y|X=x)$$<br>可以证明它等价于下式：<br>$$H(Y|X) = -\sum_{X\in \boldsymbol{X},Y\in \boldsymbol{Y}}p(X,Y)log{\frac{p(X,Y)}{p(X)}}$$</p><h3 id="证明">证明</h3><p>\begin{align*}<br>H(Y|X) &amp;\equiv \sum_{x\in\boldsymbol{X}}p(x)H(Y|X=x)\\<br>&amp;=-\sum_{x\in\boldsymbol{X}}p(x)\sum_{y\in \boldsymbol{Y}}p(y|x)logp(y|x)\\<br>&amp;=-\sum_{x\in\boldsymbol{X}}\sum_{y\in \boldsymbol{Y}}p(x)p(y|x)logp(y|x)\\<br>&amp;=-\sum_{x\in\boldsymbol{X},y\in \boldsymbol{Y}}p(x,y)logp(y|x)\\<br>&amp;=-\sum_{x\in\boldsymbol{X},y\in \boldsymbol{Y}}p(x,y)\frac{logp(x,y)}{logp(x)}\\<br>&amp;=\sum_{x\in\boldsymbol{X},y\in \boldsymbol{Y}}p(x,y)\frac{logp(x)}{logp(x,y)}\\<br>\end{align*}</p><h3 id="属性">属性</h3><ul><li>当且仅当$Y$完全由$X$的值确定时，条件熵为$0$。</li><li>当且仅当$X$和$Y$是独立随机变量的时候，$H(Y|X) = H(Y)$。</li><li>链式法则。假设一个系统由随机变量$X,Y$确定，他们有联合熵$H(X,Y)$，我们需要$H(X,Y)$个比特去表述这个系统，如果我们已经知道了$X$的值，相当于我们已经有了$H(X)$位的信息。一旦$X$已知了，我们只需要$H(X,Y)-H(X)$位去描述整个系统。所以就有了链式法则：$H(Y|X) = H(X,Y) - H(X)$。<br>\begin{align*}<br>H(Y|X) &amp;= \sum_{X\in \boldsymbol{X}, Y\in \boldsymbol{Y}}p(X,Y)log{\frac{p(X)}{p(X,Y)}}\\<br>&amp;= - \sum_{X\in \boldsymbol{X}, Y\in \boldsymbol{Y}}p(X,Y)log{p(X,Y)}+\sum_{X\in \boldsymbol{X}, Y\in \boldsymbol{Y}}p(X,Y)log{p(X)}\\<br>&amp;=H(X,Y) +\sum_{X\in \boldsymbol{X}}p(X)log{p(X)}\\<br>&amp;=H(X,Y) - H(X)<br>\end{align*}</li><li>贝叶斯公式。$H(Y|X) = H(X|Y) - H(X) + H(Y)$。<br>证明：$H(Y|X)=H(X,Y) - H(X),H(X|Y) = H(X,Y) - H(Y)$。两个式子相减就可以得到。</li></ul><h2 id="互信息">互信息</h2><p>决策树中的信息增益指的是互信息不是KL散度。</p><h3 id="定义-v5">定义</h3><p>用$(X,Y)$表示空间$\boldsymbol{X}\times\boldsymbol{Y}$上的一对随机变量，他们的联合分布是$P_{(X,Y)}$，边缘分布是$P_X,P_Y)$，信息熵被定义为：<br>$I(X;Y) = D_{KL}(P_{(X,Y)}||P_XP_Y)$<br>对于离散变量：<br>$I(X;Y)=\sum_{X\in \boldsymbol{X},Y\in \boldsymbol{Y}}p(X,Y)log(\frac{p(X,Y)}{p(X)p(Y)})$<br>对于随机变量：<br>$I(X;Y)=\int_X\int_Y p(X,Y)log(\frac{p(X,Y)}{p(X)p(Y)})dxdy$</p><h2 id="信息熵-相对熵-交叉熵-条件熵-互信息之间的关系">信息熵，相对熵，交叉熵，条件熵，互信息之间的关系</h2><h3 id="信息论">信息论</h3><p>信息熵是对随机事件用真实的概率分布$p$进行编码的长度的期望，是最短平均编码长度。<br>交叉熵是对随机事件用估计的概率分布$q$按照其真实概率分布$p$进行编码的长度的期望（随机事件是从真实概率分布$p$中取的，但是用分布$q$进行编码），大于等于最短平均编码长度，只有$q$等于真实分布$q$时，才是最短编码长度。<br>而相对熵对随机事件用估计的概率分布$q$比用真实的概率分布$p$进行编码多用的编码长度，如果$p$和$q$相等的话，相对熵为$0$。</p><h3 id="机器学习">机器学习</h3><p>在机器学习中，交叉熵通常作为一个loss函数，用来衡量真实分布$p$和估计分布$q$之间的差异。而$K-L$散度也是用来衡量两个概率分布的差异，但是多了一个信息熵项。$K-L$散度的前半部分是交叉熵，后半部分是真实分布$p$的信息熵。(一个我自己认为的不严谨的说法是相对熵算的是相对值，而交叉熵算的是绝对值)。交叉熵正比于负的对数似然估计，最小化交叉熵等价于最大化对数似然估计。<br>如果$p$是固定的，那么随着$q$的增加相对熵也在增加，但是如果$p$是不固定的，很难说相对熵是差异的绝对量度，因为它随着$p$的增长而改变。而在机器学习领域，真实分布$p$是固定的，随着$q$的改变，$H§$是不变的,也就是信息熵是固定的。所以，从优化的角度来说，最小化交叉熵也就是最小化了相对熵。但是在其他领域，$p$可能是变化的，最小化交叉熵和最小化相对熵就不是等价的了。</p><h3 id="互信息和条件熵-相对熵的关系">互信息和条件熵，相对熵的关系</h3><p>互信息可以被等价定义为：<br>\begin{align*}<br>I(X;Y)&amp; \equiv H(X)-H(X|Y)\\<br>&amp;\equiv H(Y) - H(Y|X)\\<br>&amp;\equiv H(X)+H(Y)-H(X,Y)\\<br>&amp;\equiv H(X,Y)-H(X|Y)-H(Y|X)\\<br>\end{align*}</p><p>证明：<br>\begin{align*}<br>I(X;Y)&amp;=\sum_{X\in \boldsymbol{X},Y\in \boldsymbol{Y}}p(X,Y)log(\frac{p(Y,Y)}{p(X)p(Y)})\\<br>&amp;=\sum_{X\in \boldsymbol{X},Y\in \boldsymbol{Y}}p(X,Y)log(\frac{p(Y,Y)}{p(X)})-\sum_{X\in \boldsymbol{X},Y\in \boldsymbol{Y}}p(X,Y)logp(Y)\\<br>&amp;=\sum_{X\in \boldsymbol{X},Y\in \boldsymbol{Y}}p(X)P(Y|X)logp(Y|X)-\sum_{X\in \boldsymbol{X},Y\in \boldsymbol{Y}}p(X,Y)logp(Y)\\<br>&amp;=\sum_{X\in \boldsymbol{X}}p(X)(\sum_{Y\in \boldsymbol{Y}}P(Y|X)logp(Y|X))-\sum_{Y\in \boldsymbol{Y}}(\sum_{X\in \boldsymbol{X}}p(X,Y))logp(Y)\\<br>&amp;=\sum_{X\in \boldsymbol{X}}p(X)H(Y|X=x)-\sum_{Y\in \boldsymbol{Y}}p(Y)logp(Y)\\<br>&amp;=-H(Y|X)+H(Y)\\<br>&amp;=H(Y)-H(Y|X)<br>\end{align*}</p><p>互信息和KL散度的联系：<br>从联合分布$p(x,y)$到边缘分布$p(X)p(Y)$或者条件分布$p(X|Y)p(X)$的KL散度。</p><h2 id="参考文献-references">参考文献(references)</h2><p>1.<a href="https://en.wikipedia.org/wiki/Entropy_(information_theory)" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Entropy_(information_theory)</a><br>2.<a href="https://zh.wikipedia.org/wiki/%E7%86%B5_(%E4%BF%A1%E6%81%AF%E8%AE%BA)" target="_blank" rel="noopener">https://zh.wikipedia.org/wiki/熵_(信息论)</a><br>3.<a href="https://www.zhihu.com/question/22178202/answer/49929786" target="_blank" rel="noopener">https://www.zhihu.com/question/22178202/answer/49929786</a><br>4.<a href="https://en.wikipedia.org/wiki/Cross_entropy" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Cross_entropy</a><br>5.<a href="https://en.wikipedia.org/wiki/Kullback-Leibler_divergence" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Kullback-Leibler_divergence</a><br>6:<a href="https://www.zhihu.com/question/41252833/answer/108777563" target="_blank" rel="noopener">https://www.zhihu.com/question/41252833/answer/108777563</a><br>7.<a href="https://www.zhihu.com/question/41252833/answer/141598211" target="_blank" rel="noopener">https://www.zhihu.com/question/41252833/answer/141598211</a><br>8.<a href="https://stats.stackexchange.com/questions/265966/why-do-we-use-kullback-leibler-divergence-rather-than-cross-entropy-in-the-t-sne" target="_blank" rel="noopener">https://stats.stackexchange.com/questions/265966/why-do-we-use-kullback-leibler-divergence-rather-than-cross-entropy-in-the-t-sne</a><br>9.<a href="https://en.wikipedia.org/wiki/Conditional_entropy" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Conditional_entropy</a><br>10.<a href="https://en.wikipedia.org/wiki/Mutual_information" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Mutual_information</a><br>11.<a href="https://zhuanlan.zhihu.com/p/26551798" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/26551798</a><br>12.<a href="https://blog.csdn.net/gangyin5071/article/details/82228827#4%E7%9B%B8%E5%AF%B9%E7%86%B5kl%E6%95%A3%E5%BA%A6" target="_blank" rel="noopener">https://blog.csdn.net/gangyin5071/article/details/82228827#4相对熵kl散度</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;乡农熵-shannon-entropy&quot;&gt;乡农熵(Shannon entropy)&lt;/h2&gt;
&lt;p&gt;乡农定义了一个事件的信息量是其发生概率的负对数($-log§$)，即乡农信息量，乡农熵是信息量的期望。&lt;/p&gt;
&lt;h3 id=&quot;介绍&quot;&gt;介绍&lt;/h3&gt;
&lt;p&gt;这里的
      
    
    </summary>
    
      <category term="机器学习" scheme="http://mxxhcm.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://mxxhcm.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="熵" scheme="http://mxxhcm.github.io/tags/%E7%86%B5/"/>
    
      <category term="交叉熵" scheme="http://mxxhcm.github.io/tags/%E4%BA%A4%E5%8F%89%E7%86%B5/"/>
    
      <category term="条件熵" scheme="http://mxxhcm.github.io/tags/%E6%9D%A1%E4%BB%B6%E7%86%B5/"/>
    
      <category term="相对熵" scheme="http://mxxhcm.github.io/tags/%E7%9B%B8%E5%AF%B9%E7%86%B5/"/>
    
      <category term="KL散度" scheme="http://mxxhcm.github.io/tags/KL%E6%95%A3%E5%BA%A6/"/>
    
      <category term="互信息" scheme="http://mxxhcm.github.io/tags/%E4%BA%92%E4%BF%A1%E6%81%AF/"/>
    
  </entry>
  
  <entry>
    <title>convex optimization chapter 1 Introduction</title>
    <link href="http://mxxhcm.github.io/2018/12/22/convex-optimization-chapter-1-Introduction/"/>
    <id>http://mxxhcm.github.io/2018/12/22/convex-optimization-chapter-1-Introduction/</id>
    <published>2018-12-22T05:44:11.000Z</published>
    <updated>2019-09-04T12:51:00.437Z</updated>
    
    <content type="html"><![CDATA[<h2 id="数学优化-mathematical-optimization">数学优化(mathematical optimization)</h2><h3 id="定义">定义</h3><p>一个数学优化问题（或者称为优化问题）通常有如下的形式：<br>\begin{align*}<br>&amp;minimize \quad f_0(x)\\<br>&amp;subject \ to \quad f_i(x) \le b_i, i = 1,\cdots,m.<br>\end{align*}<br>其中$x = (x_1, \cdots, x_m)$被称为优化变量(optimization variables), 或者决策变量(decision variables)。 $f_0(x):\mathbb{R}^n \rightarrow \mathbb{R}$是目标函数(object function), $f_i(x):\mathbb{R}^n \rightarrow \mathbb{R},i =1,\cdots,m$是约束函数(constraint functions)。 常量(constraints) $b_1,\cdots,b_m$是约束的限界(limits)或者边界(bounds), $b_i$可以为$0$，这个可以通过移项构造出新的$f_i(x)$实现。如果向量$x$使得目标函数取得最小的值，并且满足所有的约束条件，那么这个向量被称为最优解$x^{*} $。</p><h4 id="线性优化-linear-program">线性优化(linear program)</h4><p>目标函数和约束函数$f_0,\cdots,f_m$是线性的, 它们满足不等式：<br>$$f_i(\alpha x+\beta y) = \alpha f_i(x) + \beta f_i(y)$$<br>对于所有的$x,y \in \mathbb{R}^n $和所有的$\alpha, \beta \in\mathbb{R}$。<br>线性优化是凸优化的一个特殊形式, 它的目标函数和约束函数都是线性的等式。</p><h4 id="非线性问题-non-linear-problem">非线性问题(non-linear problem)</h4><p>如果优化问题不是线性的，就是非线性问题。只要目标函数或者约束函数至少有一个不是线性的，那么这个优化问题就是非线性优化问题。</p><h4 id="凸问题-convex-problem">凸问题(convex problem)</h4><p>凸问题是目标函数和约束函数都是凸的的优化问题，它们满足：<br>$$f_i(\alpha x + \beta y) \le \alpha f_i(x) + \beta f_i(y)$$<br>对于所有的$x,y \in \mathbb{r}^n $和所有的$\alpha, \beta \in \mathbb{r}$且$\alpha + \beta = 1, \alpha \ge 0, \beta \ge 0$。<br>凸性比线性的范围更广，不等式取代了更加严格的等式，不等式只有在$\alpha$和$\beta$取一些特定值时才成立。凸优化和线性问题以及非线性问题都有交集，它是线性问题的超集(superset)，是非线性问题的子集(subset)。技术上来说，nonlinear problem包括convex optimization(除了linear programming), 可以用来描述不确定是非凸的问题。<br>Nonlinear program &gt; convex problem &gt; linear problem</p><h3 id="示例">示例</h3><h4 id="组合优化-portfolio-optimization">组合优化(portfolio optimization)</h4><p>变量：不同资产的投资数量<br>约束：预算，每个资产最大/最小投资数量，至少要得到的回报<br>目标：所有的风险，获利的变化</p><h4 id="电子设备的元件大小-device-sizing-in-electronic-circuits">电子设备的元件大小(device sizing in electronic circuits)</h4><p>变量：元件的宽度和长度<br>约束：生产工艺的炼制，时间要求，面积等<br>目标：节约能耗</p><h4 id="数据拟合-data-fitting">数据拟合(data fitting)</h4><p>变量：模型参数<br>约束：先验知识，参数约束条件<br>目标：错误率</p><h3 id="优化问题求解-solving-optimization-problems">优化问题求解(solving optimization problems)</h3><p>所有的问题都是优化问题。<br>绝大部分优化问题我们解不出来。</p><h4 id="一般的优化问题-general-optimization-problem">一般的优化问题(general optimization problem)</h4><ul><li>很难解出来。</li><li>做一些compromise，比如要很长时间才能解出来，或者并不总能找到解。</li></ul><h4 id="一些例外-some-exceptions">一些例外(some exceptions)</h4><ul><li>最小二乘问题(least-squares problems)</li><li>线性规划问题(linear programming problems)</li><li>凸优化问题(convex optimization problems)</li></ul><h2 id="最小二乘-least-squares-和线性规划-linear-programming">最小二乘(least-squares)和线性规划(linear programming)</h2><p>least-squares和linear programming是凸优化问题中最有名的两个子问题。</p><h3 id="最小二乘问题-least-squares-problems">最小二乘问题(least-squares problems)</h3><p>最小二乘问题是一个无约束的优化问题，它的目标函数是项$a_i^T x-b_i$的平方和。<br>\begin{align*}<br>minimize \quad f_0(x) &amp;= {||Ax-b||}^2_2 \\<br>&amp;=\sum_{i=1}^k (a_i^T x-b_i)^2<br>\end{align*}</p><h4 id="求解-solving-least-squares-problems">求解(solving least-squares problems)</h4><ul><li>最小二乘问题的解可以转换为求线性方程组$(A^T A)x = A^T b$的解。线性代数上我们学过该方程组的解析解为$x=(A^T A)^{-1} A^T b$。</li><li>时间复杂度是$n^2 k = n*k*n+n*k+n*n*n, (k &gt; n)$(转置，求逆，矩阵乘法)。</li><li>该问题具有可靠且高效的求解算法。</li><li>是一个很成熟的算法</li></ul><h4 id="应用-using-least-squares">应用(using least-squares)</h4><p>很容易就可以看出来一个问题是最小二乘问题，我们只需要验证目标函数是不是二次函数，以及对应的二次型是不是正定的即可。</p><h5 id="加权最小二乘-weighted-least-squares">加权最小二乘(weighted least-squares)</h5><p>加权最小二乘形式如下:<br>$$\sum_{i=1}^k \omega_i(a_i^T x-b_i)^2 ,$$<br>其中$\omega_1,\cdots,\omega_k$是正的，被最小化。 这里选出权重$\omega$来体现不同项$a_i^T x-b_i$的比重, 或者仅仅用来影响结果。</p><h5 id="正则化-regularization">正则化(regularization)</h5><p>目标函数中被加入了额外项, 形式如下：<br>$$\sum_{i=1}^k (a_i^T x-b_i)^2 + \rho \sum_{i=1}^n x_i^2 ,$$<br>正则项是用来惩罚大的$x$, 求出一个仅仅最小化第一个求和项的不出来的好结果。合理的选择参数$\rho$在原始的目标函数和正则化项之间做一个trade-off, 使得$\sum_{k=1}^i (a_i^T - b_i)^2 $和$\rho \sum_{k=1}^n  x_i^2 $都很小。<br>正则化项和加权最小二乘会在第六章中讲到，它们的统计解释在第七章给出。</p><h3 id="线性规划-linear-programming">线性规划(linear programming)</h3><p>线性规划问题装目标函数和约束函数都是线性的：<br>\begin{align*}<br>&amp;minimize \quad c^T x\\<br>&amp;subject \ to \quad a_i^T \le b_i, i = 1, \cdots, m.<br>\end{align*}<br>其中向量$c,a_1,\cdots,a_m \in \mathbb{R}^n $, 和标量$b_1,\cdots, b_m \in \mathbb{R}$是指定目标函数和约束函数条件的参数。</p><h4 id="求解线性规划-solving-linear-programs">求解线性规划(solving linear programs)</h4><ul><li>除了一个特例，没有解析解公式(和least-squares不同)；</li><li>有可靠且高效的算法实现；</li><li>时间复杂度是$O(n^2 m)$, m是约束条件的个数, m是维度$；</li><li>是一个成熟的方法。</li></ul><h4 id="应用-using-linear-programs">应用(using linear programs)</h4><p>一些应用直接使用线性规划的标准形式,或者其中一个标准形式。在很多时候，原始的优化问题没有一个标准的线性规划形式，但是可以被转化为等价的线性规划形式。比如切米雪夫近似问题(Chebyshev approximation problem)。它的形式如下：<br>$$minimize \quad max_{i=1,\cdots,k}|a_i^T x-b_i|$$<br>其中$x\in \mathbb{R}^n $是变量，$a_1,\cdots,a_k \in \mathbb{R}^n , b_1,\cdots,b_k \in \mathbb{R}$是实例化的问题参数,和least-squares相似的是，它们的目标函数都是项$a^T_i x-b_i$。不同之处在于，least-squares用的是该项的平方和作为目标函数，而Chebyshev approximation中用的是绝对值的最大值。Chebyshev approximation problem的目标函数是不可导的(max operation), least-squares problem的目标函数是二次的(quadratic), 因此可导的(differentiable)。</p><h2 id="凸优化-convex-optimization">凸优化(Convex optimization)</h2><p>凸优化问题是优化问题的一种,它的目标函数和优化函数都是凸的。<br>具有以下形式的问题是一种凸优化问题：<br>\begin{align*}<br>&amp;minimize \quad f_0(x)\\<br>&amp;subject \ to \quad f_i(x) \le b_i, i = 1,\cdots,m.<br>\end{align*}<br>其中函数$f_0,\cdots,f_m:\mathbb{R}^n \rightarrow \mathbb{R}$是凸的(convex), 如满足<br>$$f_i(\alpha x+ \beta y) \le \alpha f_i(x) + \beta f_i(y)$$<br>对于所有的$x,y \in \mathbb{R}^n $和所有的$\alpha, \beta \in \mathbb{R}$且$\alpha + \beta = 1, \alpha \ge 0, \beta \ge 0$。<br>或者：<br>$$f_i(\theta x+ (1-\theta) y) \le \theta f_i(x) + (1 - \theta) f_i(y)$$<br>其中$\theta \in [0,1]$。<br>课上有人问这里为$\theta$是0和1, 有没有什么物理意义，Stephen Boyd回答说这是定义，就是这么定义的。<br>The least-squares和linear programming problem都是convex optimization problem的特殊形式。线性函数(linear functions)也是convex，它们正处在边界上，它们的曲率(curvature)为0。一种方式是用正曲率去描述凸性。</p><h3 id="凸优化求解-solving-convex-optimization-problems">凸优化求解(solving convex optimization problems)</h3><ul><li>没有解析解；</li><li>有可靠且有效的算法；</li><li>时间复杂度正比于$max{n^3 , n^2 m,F},$F$是评估$f$和计算一阶导数和二阶导数的时间；</li><li>有成熟的方法，如interior-point methods。</li></ul><h3 id="凸优化的应用-using-convex-optimization">凸优化的应用(using convex optimization)</h3><p>将实际问题形式化称凸优化问题。</p><h2 id="非线性优化-nonlinear-optimization">非线性优化(Nonlinear optimization)</h2><h3 id="非线性优化">非线性优化</h3><p>非线性优化用来描述目标函数和约束函数都是非线性函数(但不是凸的)优化问题。因为凸优化问题包括least-squares和linear programming, 它们是线性的。刚开始给出的优化问题就是非线性优化问题，目前没有有效的方法解该问题。目前有一些方法来解决一般的非线性问题，但是都做了一些compromise。</p><h4 id="局部优化-local-optimization">局部优化(local optimization)</h4><p>局部优化是非线性优化的一种解法，compromise是寻找局部最优点，而不是全局最优点，在可行解附近最小化目标函数，不保证能得到一个最小的目标值。<br>局部优化需要随机初始化一个初值，这个初值很关键，很大程度的影响了局部解得到的目标值, 也就是说是一个初值敏感的算法。关于初始值和全局最优值距离有多远并没有很多有用的信息。局部优化对于算法的参数值很敏感，需要根据具体问题去具体调整。<br>使用局部优化的方法比解least-squares problems, linear program, convex optimization problem更有技巧性，因为它牵扯到算法的选择，算法参数的选择，以及初值的选取。</p><h4 id="全局优化-global-optimization">全局优化(global optimization)</h4><p>全局优化也是非线性优化的一种解法, 在全局优化中，优化目标的全局最优解被找到， compromise是效率。</p><h4 id="凸优化问题在非凸优化问题中的应用-role-of-convex-optimization-in-nonconvex-problems">凸优化问题在非凸优化问题中的应用(role of convex optimization in nonconvex problems)</h4><h5 id="初始化局部优化-initialization-for-local-optimization">初始化局部优化(initialization for local optimization)</h5><h5 id="用于非凸优化的凸的启发式搜索-convex-heuristics-for-nonconvex-optimization">用于非凸优化的凸的启发式搜索(convex heuristics for nonconvex optimization)</h5><h5 id="全局最优的边界-bounds-for-global-optimization">全局最优的边界(bounds for global optimization)</h5><h2 id="大纲-outline">大纲(outline)</h2><h3 id="理论-part-one-theory">理论(part one: Theory)</h3><p>第一部分是理论，给出一些概念和定义，第一章是Introduction, 第二章和第三章分别介绍凸集(convex set)和凸函数(convex function), 第四章介绍凸优化问题， 第五章引入拉格朗日对偶性。</p><h3 id="应用-part-two-applications">应用(part two: Applications)</h3><p>第二部分主要给出凸优化在一些领域的应用，如概率论与数理统计，经济学，计算几何以及数据拟合等领域。<br>凸优化如何应用在实践中。</p><h3 id="算法-part-three-algorithms">算法(part three: Algorithms)</h3><p>第三部分给出了凸优化的数值解法，如牛顿法(Newton’s algorithm)和内点法(interior-point)。<br>第三部分有三章，分别包含了无约束优化，等式约束优化和不等式约束优化。章节之间是递进的，解一个问题被分解为解一系列简单问题。二次优化问题(包括，如least-squares)是最底层的基石，它可以通过线性方程组精确求解。牛顿法，在第十章和第十一章介绍到，是下个层次，无约束问题或者等式约束问题被转化成一系列二次优化问题的求解。第十一章介绍了内点法，是最顶层, 这些方法将不等式约束问题转化为一系列无约束或者等式约束的问题。</p><h2 id="参考文献">参考文献</h2><p>1.stephen boyd. Convex optimization</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;数学优化-mathematical-optimization&quot;&gt;数学优化(mathematical optimization)&lt;/h2&gt;
&lt;h3 id=&quot;定义&quot;&gt;定义&lt;/h3&gt;
&lt;p&gt;一个数学优化问题（或者称为优化问题）通常有如下的形式：&lt;br&gt;
\begin{a
      
    
    </summary>
    
      <category term="凸优化" scheme="http://mxxhcm.github.io/categories/%E5%87%B8%E4%BC%98%E5%8C%96/"/>
    
    
      <category term="凸优化" scheme="http://mxxhcm.github.io/tags/%E5%87%B8%E4%BC%98%E5%8C%96/"/>
    
      <category term="convex optimization" scheme="http://mxxhcm.github.io/tags/convex-optimization/"/>
    
  </entry>
  
  <entry>
    <title>latex笔记</title>
    <link href="http://mxxhcm.github.io/2018/12/22/latex%E7%AC%94%E8%AE%B0/"/>
    <id>http://mxxhcm.github.io/2018/12/22/latex笔记/</id>
    <published>2018-12-22T02:08:26.000Z</published>
    <updated>2019-09-19T02:33:55.642Z</updated>
    
    <content type="html"><![CDATA[<h2 id="命令重命名"><a href="#命令重命名" class="headerlink" title="命令重命名"></a>命令重命名</h2><p>在写博客时也能用<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">\newcommand&#123;\mmm&#125;&#123;\mathbf&#125;</span><br><span class="line">\mmm&#123;x&#125;</span><br><span class="line">\bf&#123;x&#125;</span><br></pre></td></tr></table></figure></p><p>$\newcommand{\mmm}{\mathbf}$<br>$\mmm{x}$<br>$\bf{x}$</p><h2 id="常用Latex符号"><a href="#常用Latex符号" class="headerlink" title="常用Latex符号"></a>常用Latex符号</h2><h3 id="上标"><a href="#上标" class="headerlink" title="上标"></a>上标</h3><p>$\bar{x}$ \bar{x}<br>$\hat{x}$ \hat{x}</p><h3 id="等号"><a href="#等号" class="headerlink" title="等号"></a>等号</h3><p>$\sim$  \sim<br>$\simeq$    \simeq<br>$\approx$   \approx<br>$\cong$ \cong<br>$\equiv$    \equiv<br>$\propto$ \propto</p><h3 id="各种乘法"><a href="#各种乘法" class="headerlink" title="各种乘法"></a>各种乘法</h3><p>$\times$ \times<br>$*$ *<br>$\cdot$ \cdot<br>$\bullet$ \bullet<br>$\otimes$ \otimes<br>$\circ$ \circ<br>$\odot$ \odot</p><h3 id="上下花括号"><a href="#上下花括号" class="headerlink" title="上下花括号"></a>上下花括号</h3><p>$\overbrace{x+y}\^{1+2}=\underbrace{z}_3$ \overbrace{x+y}\^{1+2}=\underbrace{z}_3</p><h3 id="括号"><a href="#括号" class="headerlink" title="括号"></a>括号</h3><p>\left(\frac{1}{2}\right)    $\left(\frac{1}{2} \right)$<br>\left[\frac{1}{2} \right]    $\left[\frac{1}{2} \right]$<br>\left\\{\frac{1}{2} \right\\}    $\left\\{\frac{1}{2} \right\\}$<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">\begin&#123;cases&#125;x=1\\\\y=x\end&#123;cases&#125;</span><br></pre></td></tr></table></figure></p><script type="math/tex; mode=display">\begin{cases}x=1\\\\y=x\end{cases}</script><h3 id="矩阵"><a href="#矩阵" class="headerlink" title="矩阵"></a>矩阵</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">\begin&#123;matrix&#125;1&amp;2\\\\3&amp;4\end&#123;matrix&#125;</span><br></pre></td></tr></table></figure><script type="math/tex; mode=display">\begin{matrix}1&2\\\\3&4\end{matrix}</script><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">\begin&#123;pmatrix&#125;1&amp;2\\\\3&amp;4\end&#123;pmatrix&#125;</span><br></pre></td></tr></table></figure><script type="math/tex; mode=display">\begin{pmatrix}1&2\\\\3&4\end{pmatrix}</script><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">\begin&#123;bmatrix&#125;1&amp;2\\\\3&amp;4\end&#123;bmatrix&#125;</span><br></pre></td></tr></table></figure><script type="math/tex; mode=display">\begin{bmatrix}1&2\\\\3&4\end{bmatrix}</script><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">\begin&#123;Bmatrix&#125;1&amp;2\\\\3&amp;4\end&#123;Bmatrix&#125;</span><br></pre></td></tr></table></figure><script type="math/tex; mode=display">\begin{Bmatrix}1&2\\\\3&4\end{Bmatrix}</script><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">\begin&#123;vmatrix&#125;1&amp;2\\\\3&amp;4\end&#123;vmatrix&#125;</span><br></pre></td></tr></table></figure><script type="math/tex; mode=display">\begin{vmatrix}1&2\\\\3&4\end{vmatrix}</script><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">\begin&#123;Vmatrix&#125;1&amp;2\\\\3&amp;4\end&#123;Vmatrix&#125;</span><br></pre></td></tr></table></figure><script type="math/tex; mode=display">\begin{Vmatrix}1&2\\\\3&4\end{Vmatrix}</script><h3 id="希腊字母"><a href="#希腊字母" class="headerlink" title="希腊字母"></a>希腊字母</h3><p>$\alpha$ \alpha<br>$\Alpha$ \Alpha<br>$\beta$ \beta<br>$\Beta$ \Beta<br>$\Delta$ \Delta<br>$\delta$ \delta<br>$\theta$ \theta<br>$\Theta$ \Theta<br>$\gamma$ \gamma<br>$\Gamma$ \Gamma<br>$\eta$ \eta<br>$\Eta$ \Eta<br>$\lambda$ \lambda<br>$\Lambda$ \Lambda<br>$\sigma$ \sigma<br>$\Sigma$ \Sigma<br>$\pi$ \pi<br>$\Pi$ \Pi<br>$\mu$ \mu<br>$\Mu$ \Mu<br>$\psi$ \psi<br>$\Psi$ \Psi<br>$\epsilon$ \epsilon<br>$\varepsilon$ \varepsilon<br>$\phi$ \phi<br>$\varphi$ \varphi<br>$\Phi$ \Phi<br>$\nabla$ \nabla<br>$\zeta$ \zeta<br>$\xi$ \xi</p><h3 id="字体"><a href="#字体" class="headerlink" title="字体"></a>字体</h3><p>粗体<br>$\mathbf{A}$ \mathbf{A}<br>$\boldsymbol{A}$ \boldsymbol{A}<br>$\mathit{A}$ \mathit{A}<br>$\mathrm{A}$ \mathrm{A}<br>花体<br>$\mathcal{A}$ \mathcal{A}<br>$\mathcal{S}$ \mathcal{S}</p><h3 id="数学运算"><a href="#数学运算" class="headerlink" title="数学运算"></a>数学运算</h3><p>求积$\prod$ \prod<br>求和$\sum$ \sum<br>积分$\int$ \int<br>根号$\sqrt{x}$ \sqrt{x}<br>根号$\sqrt[4]{y}$ \sqrt[4]{y}<br>分数$(\frac{1}{2})$ (\frac{1}{2})<br>分数$\left(\frac{1}{2}\right)$ \left(\frac{1}{2}\right)<br>无穷$\infty$ \infty<br>期望$\mathbb{E}$ \mathbb{E}<br>范数$\Vert$ \Vert<br>$\mathbb{\pi}$ \mathbb{\pi} # 可以看出来，没有起作用，因为mathbb没有只支持大写字母。<br>$\pm$ \pm<br>$\mp$ \mp</p><h3 id="集合"><a href="#集合" class="headerlink" title="集合"></a>集合</h3><p>真含于$\subset$ \subset<br>含于$\subsetneqq$ \subsetneqq<br>真包含$\supset$ \supset<br>包含$\supsetneqq$ \supsetneqq<br>交$\cap$ \cap<br>并$\cup$ \cup<br>属于$\in$ \in<br>$\succ$ \succ<br>$\succeq$ \succeq<br>$\prec$ \prec<br>$\preceq$ \preceq<br>空集$\emptyset$ \emptyset</p><h3 id="谓词逻辑"><a href="#谓词逻辑" class="headerlink" title="谓词逻辑"></a>谓词逻辑</h3><p>否定$\neg$ \neg<br>任意$\forall$ \forall<br>存在$\exists$ \exists<br>合取$\wedge$ \wedge<br>析取$\vee$ \vee</p><h3 id="空格"><a href="#空格" class="headerlink" title="空格"></a>空格</h3><p>$a\qquad b$ a\qquad b<br>$a\quad b$ a\quad b<br>$a b$ a b<br>$a\;b$ a\;b<br>$a\,b$ a\,b<br>$ab$ ab<br>$a!b$ a!b</p><h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><p>长竖线$\big|$ \big|<br>长竖线$\Big|$ \Big|<br>长竖线$\bigg|$ \bigg|<br>长竖线$\Bigg|$ \Bigg|<br>双箭头$\Leftrightarrow$ \Leftrightarrow<br>左箭头$\leftarrow$ \leftarrow<br>右箭头$\rightarrow$ \rightarrow<br>上划线$\overline{A}$ \overline{A}<br>下划线$\underline{A}$ \underline{A}<br>$\backslash$ \backslash<br>$\sim$ \sim</p><h2 id="列表"><a href="#列表" class="headerlink" title="列表"></a>列表</h2><h3 id="有序列表"><a href="#有序列表" class="headerlink" title="有序列表"></a>有序列表</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">\begin&#123;enumerate&#125;</span><br><span class="line"> \item First.</span><br><span class="line"> \item Second.</span><br><span class="line"> \item Third.</span><br><span class="line">\end&#123;enumerate&#125;</span><br></pre></td></tr></table></figure><p>效果如下：</p><ol><li>First.</li><li>Second.</li><li>Third.</li></ol><h3 id="无序列表"><a href="#无序列表" class="headerlink" title="无序列表"></a>无序列表</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">\begin&#123;itemize&#125;</span><br><span class="line"> \item &#123;First.&#125;</span><br><span class="line"> \item &#123;Second.&#125;</span><br><span class="line"> \item &#123;Third.&#125;</span><br><span class="line">\end&#123;itemize&#125;</span><br></pre></td></tr></table></figure><p>效果如下：</p><ul><li>First.</li><li>Second.</li><li>Third.</li></ul><h2 id="跨多行公式对齐"><a href="#跨多行公式对齐" class="headerlink" title="跨多行公式对齐"></a>跨多行公式对齐</h2><p><strong>注意：不要忘了每行后面的两个\\</strong></p><h3 id="示例1"><a href="#示例1" class="headerlink" title="示例1"></a>示例1</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">\begin&#123;align*&#125;</span><br><span class="line">f(x) &amp;= (3 + 4)\^2 + 4\\</span><br><span class="line">&amp;= 7\^2 + 4\\</span><br><span class="line">&amp;= 49 + 4\\</span><br><span class="line">&amp;= 53</span><br><span class="line">\end&#123;align*&#125;</span><br></pre></td></tr></table></figure><p>效果如下：<br>\begin{align*}<br>f(x) &amp;= (3 + 4)\^2 + 4\\\\<br>&amp;= 7\^2 + 4\\\\<br>&amp;= 49 + 4\\\\<br>&amp;= 53<br>\end{align*}</p><h3 id="示例2"><a href="#示例2" class="headerlink" title="示例2"></a>示例2</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">\begin&#123;align*&#125;</span><br><span class="line">v &amp;= R + \gamma Pv\\</span><br><span class="line">(1-\gamma P) &amp;= R\\</span><br><span class="line">v &amp;= (1 - \gamma P)\^&#123;-1&#125; R</span><br><span class="line">\end&#123;align*&#125;</span><br></pre></td></tr></table></figure><p>\begin{align*}<br>v &amp;= R + \gamma Pv\\\\<br>(1-\gamma P) &amp;= R\\\\<br>v &amp;= (1 - \gamma P)\^{-1} R<br>\end{align*}</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>1.<a href="http://blog.huangyuanlove.com/2018/02/27/LaTeX笔记-六/" target="_blank" rel="noopener">http://blog.huangyuanlove.com/2018/02/27/LaTeX笔记-六/</a><br>2.<a href="https://blog.csdn.net/xxzhangx/article/details/52778539" target="_blank" rel="noopener">https://blog.csdn.net/xxzhangx/article/details/52778539</a><br>3.<a href="https://blog.csdn.net/hunauchenym/article/details/7330828" target="_blank" rel="noopener">https://blog.csdn.net/hunauchenym/article/details/7330828</a><br>4.<a href="http://geowu.blogspot.com/2012/10/latex_25.html" target="_blank" rel="noopener">http://geowu.blogspot.com/2012/10/latex_25.html</a><br>5.<a href="https://math.stackexchange.com/questions/20412/element-wise-or-pointwise-operations-notation" target="_blank" rel="noopener">https://math.stackexchange.com/questions/20412/element-wise-or-pointwise-operations-notation</a><br>6.<a href="https://xilazimu.net/" target="_blank" rel="noopener">https://xilazimu.net/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;命令重命名&quot;&gt;&lt;a href=&quot;#命令重命名&quot; class=&quot;headerlink&quot; title=&quot;命令重命名&quot;&gt;&lt;/a&gt;命令重命名&lt;/h2&gt;&lt;p&gt;在写博客时也能用&lt;br&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td
      
    
    </summary>
    
      <category term="工具" scheme="http://mxxhcm.github.io/categories/%E5%B7%A5%E5%85%B7/"/>
    
    
      <category term="Latex" scheme="http://mxxhcm.github.io/tags/Latex/"/>
    
  </entry>
  
  <entry>
    <title>reinforcement learning an introduction 第3章笔记</title>
    <link href="http://mxxhcm.github.io/2018/12/21/reinforcement-learning-an-introduction-%E7%AC%AC3%E7%AB%A0%E7%AC%94%E8%AE%B0/"/>
    <id>http://mxxhcm.github.io/2018/12/21/reinforcement-learning-an-introduction-第3章笔记/</id>
    <published>2018-12-21T07:13:38.000Z</published>
    <updated>2019-08-30T03:38:40.650Z</updated>
    
    <content type="html"><![CDATA[<h2 id="马尔科夫过程-markov-process-马尔科夫链-markov-chain">马尔科夫过程(markov process)、马尔科夫链(markov chain)</h2><p>马尔科夫过程或者马尔科夫链(markov chain)是一个tuple $\lt S,P\gt$,其中S是一个有限(或者无限)的状态集合,P是状态转移矩阵(transition probability matrix)或马尔科夫矩阵(markov matrix),$P_{ss’}= P[S_{t+1} = s’|S_t = s]$.</p><h2 id="马尔科夫奖励过程-markov-reward-process">马尔科夫奖励过程(markov reward process)</h2><p>马尔科夫奖励过程是一个tuple $\lt S,P,R,\gamma\gt$,和马尔科夫过程相比，它多了一个奖励R，R和某个具体的状态相关，MRP中的reward只和state有关,和action无关。<br>S是一个(有限)状态的集合。<br>P是一个状态转移概率矩阵。<br>R是一个奖励函数$R = \mathbb{E}[R_{t+1}|S_t = s]$, <strong>这里为什么是t+1时刻的reward?这仅仅是一个约定，为了描述RL问题中涉及到的observation，action，reward比较方便。这里可以理解为离开这个状态才能获得奖励而不是进入这个状态即获得奖励。如果改成$R_t$也是可以的，这时可以理解为进入这个状态获得的奖励。</strong><br>$\gamma$称为折扣因子(discount factor), $\gamma \epsilon [0,1]$.<strong>为什么引入$\gamma$，David Silver的公开课中提到了四个原因:(1)数学上便于计算回报(return)；(2)避免陷入无限循环；(3)长远利益具有一定的不确定性；(4)符合人类对眼前利益的追求。</strong></p><h3 id="奖励-reward">奖励(reward)</h3><p>每个状态s在一个时刻t立即可得到一个reward,reward的值需要由环境给出,这个值可正可负。目前的强化学习算法中reward都是人为设置的。</p><h3 id="回报-return">回报(return)</h3><p>回报是累积的未来的reward,其计算公式如下:<br>$$G_t = R_{t+1} + R_{t+2} + … = \sum_{k=0}^{\infty} {\gamma^k R_{t+k+1}} \tag{1}$$<br>它是一个马尔科夫链上从t时刻开始往后所有奖励的有衰减(带折扣因子)的总和。</p><h3 id="值函数-value-function">值函数(value function)</h3><p>值函数是回报(return)的期望(expected return), 一个MRP过程中某一状态的value function为从该状态开始的markov charin return的期望，即$v(s) = \mathbb{E}[G_t|S_t=s]$.<br>MRP的value function和MDP的value function是不同的, MRP的value function是对于state而言的，而MDP的value function是针对tuple $\lt$state, action$\gt$的。<br>这里为什么要取期望,因为policy是stotastic的情况时，在每个state时，采取每个action都是可能的，都有一定的概率，next state也是不确定的了，所以value funciton是一个随机变量，因此就引入期望来刻画随机变量的性质。<br>为什么在当前state就知道下一时刻的state了?对于有界的RL问题来说，return是在一个回合结束时候计算的；对于无界的RL问题来说，由于有衰减系数，只要reward有界，return就可以计算出来。</p><h3 id="马尔科夫奖励过程的贝尔曼方程-bellman-equation-for-mrp">马尔科夫奖励过程的贝尔曼方程(bellman equation for MRP)</h3><p>\begin{align*}<br>v(s) &amp;= \mathbb{E}[G_t|S_t = s]\\<br>&amp;= \mathbb{E}[R_{t+1} + \gamma R_{t+2} + … | S_t = s]\\<br>&amp;= \mathbb{E}[R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3} + …|S_t = s]\\<br>&amp;= \mathbb{E}[R_{t+1} + \gamma G_{t+1} |S_t = s]\\<br>&amp;= \mathbb{E}[R_{t+1} + \gamma v(S_{t+1})|S_t = s]\\<br>v(s) &amp;= \mathbb{E}[R_{t+1} + \gamma v(S_{t+1})|S_t = s]<br>\end{align*}<br>v(s)由两部分组成，一部分是immediate reward的期望(expectation)，$\mathbb{E}[R_{t+1}]$, 只与当前时刻state有关；另一部分是下一时刻state的value function的expectation。如果用s’表示s状态下一时刻的state，那么bellman equation可以写成：<br>$$v(s) = R_s + \gamma \sum_{s’ \epsilon S} P_{ss’}v(s’)$$<br>我们最终的目的是通过迭代使得t轮迭代时的v(s)和第t+1轮迭代时的v(s)相等。将其写成矩阵形式为：<br>$$v_t = R + \gamma P v_{t+1}$$<br>$$(v_1,v_2,…,v_n)^T = (R_1,R_2,…,R_n)^T + \gamma \begin{bmatrix}P_{11}&amp;P_{12}&amp;…&amp;P_{1n}\\P_{21}&amp;P_{22}&amp;…&amp;P_{2n}\\&amp;&amp;…&amp;\\P_{n1}&amp;P_{n2}&amp;…&amp;P_{nn}\end{bmatrix} (v_1,v_2,…,v_n)^T $$<br>MRP的Bellman方程组是线性的，可以直接求解:<br>\begin{align*}<br>v &amp;= R + \gamma Pv\\<br>(1-\gamma P) &amp;= R\\<br>v &amp;= (1 - \gamma P)^{-1} R<br>\end{align*}<br>可以直接解方程，但是复杂度为$O(n^3)$，对于大的MRP方程组不适用，可以通过迭代法求解，常用的迭代法有动态规划,蒙特卡洛算法和时序差分算法等求解(动态规划是迭代法吗？）</p><h2 id="马尔科夫决策过程-markov-decision-process">马尔科夫决策过程(markov decision process)</h2><p>马尔科夫决策过程，比markov reward process多了一个A,它也是一个tuple $\lt S,A,P,R,\gamma\gt$, 在MRP中奖励R仅仅和状态S相关，在MDP中奖励R和概率P对应的是某个状态S和某个动作A的组合。<br>\begin{align*}<br>P_{ss’}^a &amp;= P[S_{t+1} = s’ | S_t = s, A_t = a]\\<br>R_s^a &amp;= \mathbb{E}[R_{t+1} | S_t = s, A_t = a]<br>\end{align*}<br>这里的reward不仅仅与state相关，而是与tuple $\lt state，action\gt$相关。</p><h3 id="回报">回报</h3><p>MDP中的$G_t$和式子$(1)$的$G_t$是一样的，将$G_t$写成和后继时刻相关的形式如下：<br>\begin{align*}<br>G_t &amp;= R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \gamma^3 R_{t+4} + …\\<br>&amp;= R_{t+1} + \gamma (R_{t+2} + \gamma^1 R_{t+3} + \gamma^2 R_{t+4} + …)\\<br>&amp;= R_{t+1} + \gamma G_{t+1} \tag{2}<br>\end{align*}<br>这里引入$\gamma$之后，即使是在continuing情况下，只要$G_t$是非零常数，$G_t$也可以通过等比数列求和公式进行计算，即:<br>$$G_t = \sum_{k=1}^{\infty} \gamma^k = \frac{1}{1-\gamma} \tag{3}$$</p><h3 id="策略-policy">策略(policy)</h3><p>策略$\pi$的定义:给定状态时采取各个动作的概率分布。<br>$$\pi(a|s) = P[A_t = a | S_t = a] \tag{4}$$</p><h3 id="值函数-value-function-v2">值函数(value function)</h3><p>这里给出的是值函数的定义，就是这么定义的。<br>MDP的值函数有两种，状态值函数(state value function)和动作值函数(action value function), 这两种值函数的含义其实是一样的，也可以相互转换。具体来说, 值函数定义为给定一个policy $\pi$，得到的回报的期望(expected return)。<br>一个MDP的状态s对应的值函数(state value function) $v_{\pi}(s)$是从状态s开始采取策略$\pi$得到的回报的期望。<br>\begin{align*}<br>v_{\pi}(s) &amp;= \mathbb{E}_{\pi}[G_t|S_t = s]\\<br>&amp;=\mathbb{E}_{\pi}[\sum_{k=0}^{\infty} \gamma^{k} R_{t+k+1}|S_t=s] \tag{5}<br>\end{align*}<br>这里的$G_t$是式子(2)中的回报。<br>一个MDP过程中动作值函数(action value function) $q_{\pi}(s,a)$是从状态s开始,采取action a，采取策略$\pi$得到的回报的期望。<br>&lt;action value function $q_{\pi}(s,a)$ is the expected return starting from states, taking action a, and then following policy \pi.&gt;<br>\begin{align*}<br>q_{\pi}(s,a) &amp;= \mathbb{E}_{\pi}\left[G_t | S_t = s, A_t = a\right]\\<br>&amp;= \mathbb{E}_{\pi}\left[\sum_{k=0}^{\infty} \gamma^{k} R_{t+k+1}|S_t=s, A_t=a\right] \tag{6}<br>\end{align*}</p><h4 id="状态值函数-state-value-function">状态值函数(state value function)</h4><p>\begin{align*}<br>v_{\pi}(s) &amp;= \sum_{a \epsilon A} \pi(a|s) q_{\pi} (s,a) \tag{7}\\<br>v_{\pi}(s) &amp;= \sum_a \pi(a|s)\sum_{s’,r}p(s’,r|s,a) \left[r + \gamma v_{\pi}(s’) \right] \tag{8}\\<br>\end{align*}<br>式子$(7)$是$v(s)$和$q(s,a)$的关系，式子$(8)$是$v(s)$和它的后继状态$v(s’)$的关系。<br>式子$(8)$的推导如下：<br>\begin{align*}<br>v_{\pi}(s) &amp;= \mathbb{E}_{\pi}[G_t|S_t = s]\\<br>&amp;= \mathbb{E}_{\pi}\left[R_{t+1}+\gamma G_{t+1}|S_t = s\right]\\<br>&amp;= \sum_a \pi(a|s)\sum_{s’}\sum_rp(s’,r|s,a) \left[r + \gamma \mathbb{E}_{\pi}\left[G_{t+1}|S_{t+1}=s’\right]\right]\\<br>&amp;= \sum_a \pi(a|s)\sum_{s’,r}p(s’,r|s,a) \left[r + \gamma v_{\pi}(s’) \right]\\<br>\end{align*}</p><h4 id="动作值函数-action-value-function">动作值函数(action value function)</h4><p>\begin{align*}<br>q_{\pi}(s,a) &amp;= \sum_{s’}\sum_r p(s’,r|s,a)(r + \gamma  v_{\pi}(s’)) \tag{9}\\<br>q_{\pi}(s,a) &amp;= \sum_{s’}\sum_r p(s’,r|s,a)(r + \gamma  \sum_{a’}\pi(a’|s’)q(s’,a’)) \tag{10}\\<br>\end{align*}<br>式子$(9)$是$q(s,a)$和$v(s)$的关系，式子$(10)$是$q(s,a)$和它的后继状态$q(s’,a’)$的关系。<br>以上都是针对MDP来说的，在MDP中，给定policy $\pi$下，状态s下选择a的action value function，$q_{\pi}(s,a)$类似MRP里面的v(s)，而MDP中的v(s)是要考虑在state s下采率各个action后的情况。</p><h3 id="贝尔曼期望方程-bellmam-expectation-equation">贝尔曼期望方程(Bellmam expectation equation)</h3><p>\begin{align*}<br>v_{\pi}(s) &amp;= \mathbb{E}_{\pi}[R_{t+1} + \gamma v_{\pi}(S_{t+1})|S_t = s] \tag{11}\\<br>v_{\pi}(s) &amp;= \mathbb{E}_{\pi}\left[q_{\pi}(S_t,A_t)|S_t=s,A_t=a\right]\tag{12}\\<br>q_{\pi}(s,a)&amp;= \mathbb{E}_{\pi}\left[R+\gamma v_{\pi}(S_{t+1}) |S_t=s,A_t=a\right]\tag{13}\\<br>q_{\pi}(s,a) &amp;= \mathbb{E}_{\pi}[R_{t+1} + \gamma q_{\pi}(S_{t+1},A_{t+1}) | S_t = s, A_t = a] \tag{14}<br>\end{align*}</p><h4 id="矩阵形式">矩阵形式</h4><p>\begin{align*}<br>v_{\pi} &amp;= R^{\pi} + \gamma P^{\pi} v_{\pi}\\<br>v_{\pi} &amp;= (I-\gamma P^{\pi} )^{-1} R^{\pi}<br>\end{align*}</p><h2 id="最优策程的求解-how-to-find-optimal-policy">最优策程的求解(how to find optimal policy)</h2><h3 id="最优价值函数-optimal-value-function">最优价值函数(optimal value function)</h3><p>$v_{*} = max_{\pi}v_{\pi}(s)$,从所有策略产生的state value function中，选取使得state s的价值最大的函数<br>$q_{*}(s,a) = max_{\pi} q_{\pi}(s,a)$,从所有策略产生的action value function中，选取使$\lt s,a\gt$价值最大的函数<br>当我们得到了optimal value function，也就知道了每个state的最优价值，便认为这个MDP被解决了</p><h3 id="最优策略-optimal-policy">最优策略(optimal policy)</h3><p>对于每一个state s，在policy $\pi$下的value 大于在policy $\pi’$的value， 就称策略$\pi$优于策略$\pi’$， $\pi \ge \pi’$ if $v_{\pi}(s) \ge v_{\pi’}(s)$, 对于任意s都成立<br>对于任何MDP，都满足以下条件：</p><ol><li>都存在一个optimal policy，它比其他策略好或者至少相等；</li><li>所有的optimal policy的optimal value function是相同的；</li><li>所有的optimal policy 都有相同的 action value function.</li></ol><h3 id="寻找最优策略">寻找最优策略</h3><p>寻找optimal policy可以通过寻找optimal action value function来实现：<br>$${\pi}_{*}(a|s) =<br>\begin{cases}1, &amp;if\quad a = argmax\ q_{*}(s,a)\\0, &amp;otherwise\end{cases}$$</p><h3 id="贝尔曼最优方程-bellman-optimal-equation">贝尔曼最优方程(bellman optimal equation)</h3><p>*号表示最优的策略。</p><h4 id="最优状态值函数-state-value-function">最优状态值函数(state value function)</h4><p>\begin{align*}<br>v_{*}(s) &amp;= max_a q_{*}(s,a)\\<br>&amp;= max_a\mathbb{E}_{\pi_{*}}\left[G_t|S_t=s,A_t=a\right]\\<br>&amp;= max_a\mathbb{E}_{\pi_{*}}\left[R_{t+1}+\gamma G_t|S_t=s,A_t=a\right]\\<br>&amp;= max_a\mathbb{E}\left[R_{t+1} +\gamma v_{*}(S_{t+1})|S_t=s,A_t=a\right]\\<br>&amp;= max_a \left[\sum_{s’,r} p(s’,r|s,a)(r+\gamma v_{*}(s’) )\right] \tag{15}\\<br>\end{align*}</p><h4 id="最优动作值函数-action-value-function">最优动作值函数(action value function)</h4><p>\begin{align*}<br>q_{*}(s,a) &amp;= \sum_{s’,r} p(s’,r|s,a) (r + \gamma v_{*}(s’))\\<br>&amp;= \sum_{s’,r} p(s’,r|s,a) (r + \gamma max_{a’} q_{*}(s’,a’))\\<br>&amp;=\mathbb{E}\left[R_{t+1}+\gamma max_{a’}q_{*}(S_{t+1},a’)|S_t=s,A_t=a \right]\tag{16}\\<br>\end{align*}</p><h3 id="贝尔曼最优方程的求解-solution-to-bellman-optimal-equation">贝尔曼最优方程的求解(solution to Bellman optimal equation)</h3><p>Bellman equation和Bellman optimal equation相比，一个是对于给定的策略，求其对应的value function,是对一个策略的估计，而bellman optimal equation是要寻找最优策略，通过对action value function进行贪心。<br>Bellman最优方程是非线性的，没有固定的解决方案，只能通过迭代法来解决，如Policy iteration，value iteration，Q-learning，Sarsa等。</p><h2 id="参考文献">参考文献</h2><p>1.<a href="http://incompleteideas.net/book/the-book-2nd.html" target="_blank" rel="noopener">http://incompleteideas.net/book/the-book-2nd.html</a><br>2.<a href="https://www.bilibili.com/video/av32149008/?p=2" target="_blank" rel="noopener">https://www.bilibili.com/video/av32149008/?p=2</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;马尔科夫过程-markov-process-马尔科夫链-markov-chain&quot;&gt;马尔科夫过程(markov process)、马尔科夫链(markov chain)&lt;/h2&gt;
&lt;p&gt;马尔科夫过程或者马尔科夫链(markov chain)是一个tuple $\l
      
    
    </summary>
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="MDP" scheme="http://mxxhcm.github.io/tags/MDP/"/>
    
      <category term="MRP" scheme="http://mxxhcm.github.io/tags/MRP/"/>
    
      <category term="Bellman Equation" scheme="http://mxxhcm.github.io/tags/Bellman-Equation/"/>
    
  </entry>
  
  <entry>
    <title>linux 常见问题（不定期更新）</title>
    <link href="http://mxxhcm.github.io/2018/12/20/linux-%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/"/>
    <id>http://mxxhcm.github.io/2018/12/20/linux-常见问题/</id>
    <published>2018-12-20T12:30:34.000Z</published>
    <updated>2019-06-04T02:37:22.309Z</updated>
    
    <content type="html"><![CDATA[<h2 id="问题1-undefined-reference-to-pthread-create-in-linux">问题1 Undefined reference to pthread_create in Linux</h2><p>在阅读自然语言处理的一篇论文时，读到了bype pair encoding(bpe)算法。在github找到了一个实现<a href="https://github.com/glample/fastBPE" target="_blank" rel="noopener">fastBPE</a>, 算法是用C++写的，在编译的过程中遇到了问题&quot;Undefined reference to pthread_create in Linux&quot;,</p><h3 id="terminal下解决方案">terminal下解决方案</h3><p>查阅资料了解到pthread不是Linux操作系统默认的库函数，所以需要在编译的时候将pthread链接该库函数，后来在看fastBPE的文档时发现文档中已经有说明:<br>Compile with:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">g++ -std=c++11 -pthread -O3 fast.cc -o fast</span><br></pre></td></tr></table></figure><h3 id="codeblocks下解决方案">codeblocks下解决方案</h3><p>上面给出的方案是使用gcc在terminal进行编译时加入静态库，但是对于不习惯在命令行使用gdb进行调试的人来说没有用。<br>在codeblocks中，如果要链接静态库,找到Settings --&gt; Compiler… --&gt; Linker settings，点击add，添加相应的库函数即可。</p><h2 id="参考文献">参考文献</h2><p>1:<a href="https://stackoverflow.com/questions/1662909/undefined-reference-to-pthread-create-in-linux" target="_blank" rel="noopener">https://stackoverflow.com/questions/1662909/undefined-reference-to-pthread-create-in-linux</a><br>2:<a href="https://blog.csdn.net/zhaoyue007101/article/details/7705753" target="_blank" rel="noopener">https://blog.csdn.net/zhaoyue007101/article/details/7705753</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;问题1-undefined-reference-to-pthread-create-in-linux&quot;&gt;问题1 Undefined reference to pthread_create in Linux&lt;/h2&gt;
&lt;p&gt;在阅读自然语言处理的一篇论文时，读到了by
      
    
    </summary>
    
      <category term="linux" scheme="http://mxxhcm.github.io/categories/linux/"/>
    
    
      <category term="gcc" scheme="http://mxxhcm.github.io/tags/gcc/"/>
    
      <category term="linux" scheme="http://mxxhcm.github.io/tags/linux/"/>
    
      <category term="常见问题" scheme="http://mxxhcm.github.io/tags/%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/"/>
    
      <category term="codeblocks" scheme="http://mxxhcm.github.io/tags/codeblocks/"/>
    
  </entry>
  
  <entry>
    <title>随笔</title>
    <link href="http://mxxhcm.github.io/2018/12/18/%E9%9A%8F%E7%AC%94/"/>
    <id>http://mxxhcm.github.io/2018/12/18/随笔/</id>
    <published>2018-12-18T07:43:11.000Z</published>
    <updated>2019-05-12T04:02:11.044Z</updated>
    
    <content type="html"><![CDATA[<h1>目的</h1><p>看到别人在本科，硕士阶段记录了很多自己学到的东西，再看看自己，本科四年什么都没留下，现在进入实验室已经一年多了，没有沉淀下来，本来是很好的一手牌，被自己打的稀烂。今天就下定决心搭建一个自己的博客，用来记录自己的收获，一方面防止自己忘记，另一方面也确定自己是否已经懂了，能否把一个东西讲解出来。</p><h1>恩！</h1><p>爱你呦，荟荟～</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1&gt;目的&lt;/h1&gt;
&lt;p&gt;看到别人在本科，硕士阶段记录了很多自己学到的东西，再看看自己，本科四年什么都没留下，现在进入实验室已经一年多了，没有沉淀下来，本来是很好的一手牌，被自己打的稀烂。今天就下定决心搭建一个自己的博客，用来记录自己的收获，一方面防止自己忘记，另一方面也确定
      
    
    </summary>
    
      <category term="工具" scheme="http://mxxhcm.github.io/categories/%E5%B7%A5%E5%85%B7/"/>
    
    
      <category term="感悟" scheme="http://mxxhcm.github.io/tags/%E6%84%9F%E6%82%9F/"/>
    
  </entry>
  
</feed>

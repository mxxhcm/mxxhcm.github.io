<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>mxxhcm&#39;s blog</title>
  <icon>https://www.gravatar.com/avatar/e8e79984d2e37363d60a84f0f1e8cf0e</icon>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://mxxhcm.github.io/"/>
  <updated>2019-10-21T13:48:32.744Z</updated>
  <id>http://mxxhcm.github.io/</id>
  
  <author>
    <name>马晓鑫爱马荟荟</name>
    <email>mxxhcm@gmail.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>bayes classfier</title>
    <link href="http://mxxhcm.github.io/2019/10/21/bayes-classfier/"/>
    <id>http://mxxhcm.github.io/2019/10/21/bayes-classfier/</id>
    <published>2019-10-21T12:55:39.000Z</published>
    <updated>2019-10-21T13:48:32.744Z</updated>
    
    <content type="html"><![CDATA[<h2 id="贝叶斯公式">贝叶斯公式</h2><h2 id="参考文献">参考文献</h2><ol><li></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;贝叶斯公式&quot;&gt;贝叶斯公式&lt;/h2&gt;
&lt;h2 id=&quot;参考文献&quot;&gt;参考文献&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ol&gt;

      
    
    </summary>
    
      <category term="机器学习" scheme="http://mxxhcm.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://mxxhcm.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="贝叶斯" scheme="http://mxxhcm.github.io/tags/%E8%B4%9D%E5%8F%B6%E6%96%AF/"/>
    
  </entry>
  
  <entry>
    <title>algorithm-sort</title>
    <link href="http://mxxhcm.github.io/2019/10/16/algorithm-sort/"/>
    <id>http://mxxhcm.github.io/2019/10/16/algorithm-sort/</id>
    <published>2019-10-16T07:06:10.000Z</published>
    <updated>2019-10-20T13:49:12.946Z</updated>
    
    <content type="html"><![CDATA[<h2 id="排序分类">排序分类</h2><h3 id="内部排序">内部排序</h3><h4 id="基于比较的排序">基于比较的排序</h4><p>交换排序</p><ul><li>冒泡排序</li><li>快速排序</li></ul><p>插入排序</p><ul><li>简单插入排序</li><li>希尔排序</li></ul><p>选择排序</p><ul><li>简单选择排序</li><li>堆排序</li></ul><p>归并排序</p><h4 id="非比较排序">非比较排序</h4><ul><li>计数排序</li><li>桶排序</li><li>基数排序</li></ul><h3 id="外部排序">外部排序</h3><ul><li>多路合并</li><li>置换选择排序</li></ul><h2 id="交换排序">交换排序</h2><h3 id="bubble-sort">bubble sort</h3><h4 id="思路">思路</h4><p>在第$i$趟排序过程中，最后$i-1$个值是有序的，对剩余的$n-i+1$个元素，不断的交换两个相邻位置的值，使得后面的值大于前面的值，最后这$n-i+1$个元素中的最大值跑到了倒数第$i$个位置，就像冒泡一样。。<br>核心思想就是只交换两个相邻的值。</p><h4 id="属性">属性</h4><ul><li>稳定</li><li>最坏时间复杂度$O(n^2 )$</li><li>最好时间复杂度$O(n )$，在数组正序的情况下，只进行一轮排序，$n-1$次比较，在数组倒序的情况下，进行$n-1$轮排序，进行很多次比较，是$O(n^2 )$。</li><li>平均时间复杂度$O(n^2 )$</li><li>空间复杂度$O(n)$</li></ul><h4 id="代码">代码</h4><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">bubble_sort</span><span class="params">(<span class="keyword">int</span> a[], <span class="keyword">int</span> n)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n - <span class="number">1</span>; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; n - i - <span class="number">1</span>; j++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span>(a[j] &gt; a[j+<span class="number">1</span>])</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="keyword">int</span> temp = a[j];</span><br><span class="line">                a[j] = a[j+<span class="number">1</span>];</span><br><span class="line">                a[j+<span class="number">1</span>] = temp;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="quick-sort">quick sort</h3><h4 id="思路-v2">思路</h4><p>把一个数组分成两部分，然后在将这两部分的每一部分继续分下去，一直分解到每一个部分只有一个元素，这样子就有序了。</p><h4 id="属性-v2">属性</h4><ul><li>不稳定</li><li>最坏时间复杂度$O(n^2 )$，在数组基本有序的情况下退化成冒泡排序了。</li><li>最好时间复杂度$O(n\log n )$</li><li>平均时间复杂度$O(n\log n )$</li><li>最好空间复杂度$O(\log n)$，最差是$O(n)$的空间复杂度。每一次需要$O(1)$常数空间存储pivot的位置，在递归调用保存栈的时候需要空间。至多有$\log n$或者$O(n)$次，所以空间复杂度就是$O(n)$。</li></ul><h4 id="时间复杂度计算">时间复杂度计算</h4><h5 id="最坏情况下">最坏情况下</h5><p>数组有序，扫描一遍将它分解成了N-1和1，N-1分解称了N-2, 1<br>对于$N$个元素的数组，partition的时间复杂度是：<br>$T(N) = N - 1 + T(N-1)$，即扫描一遍需要比较的次数<br>$T(N-1) = N - 2 + T(N-2)$<br>$T(N-2) = N - 3 + T(N-3)$<br>$\cdots $<br>$T(3) = 2 + T(2)$<br>$T(2) = 1 + T(1)$<br>$T(1) = 0$<br>合计就是：$N-1 + N-2 + \cdots + 2 + 1 + 0= O(N^2 )$</p><h5 id="平均情况下">平均情况下</h5><p>$T(N) = 2T(\frac{N}{2}) + N$ 做了近似<br>$\frac{T(N)}{N}) = \frac{T(\frac{N}{2})}{\frac{N}{2}} + 1$<br>$\frac{T(\frac{N}{2})}{\frac{N}{2}} =\frac{T(\frac{N}{4})}{\frac{N}{4}} + 1 $<br>$\cdots$<br>$\frac{4}{4} =\frac{T(4)}{4} + 1$<br>$\frac{2}{2} =\frac{T(1)}{1} + 1$<br>而<br>$T(1) = 0$<br>$\frac{T(2)}{2} = 1$<br>$\frac{T(4)}{4} = T(2) + T(1) = 2$<br>$\frac{T(8)}{8} = T(4) + T(2) + T(1) = 3$<br>$\frac{T(16)}{16} = T(8) T(4) + T(2) + T(1) = 4$<br>所以<br>$\frac{T(N)}{N} = \log N$<br>$T(N) = N\log N$</p><h4 id="代码-v2">代码</h4><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">partition</span><span class="params">(<span class="keyword">int</span> a[], <span class="keyword">int</span> low, <span class="keyword">int</span> high)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> pivot = a[low];</span><br><span class="line">    <span class="keyword">while</span>(low &lt; high)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">while</span>(low &lt; high &amp;&amp; a[high] &gt;= pivot)</span><br><span class="line">        &#123;</span><br><span class="line">            high --;</span><br><span class="line">        &#125;</span><br><span class="line">        a[low] = a[high];</span><br><span class="line">        <span class="keyword">while</span>(low &lt; high &amp;&amp; a[low] &lt;= pivot)</span><br><span class="line">        &#123;</span><br><span class="line">            low ++;</span><br><span class="line">        &#125;</span><br><span class="line">        a[high] = a[low];</span><br><span class="line">    &#125;</span><br><span class="line">    a[low] =  pivot;</span><br><span class="line">    <span class="keyword">return</span> low;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">qsort</span><span class="params">(<span class="keyword">int</span> a[], <span class="keyword">int</span> low, <span class="keyword">int</span> high)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>( low &lt; high)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">int</span> pivot = partition(a, low, high);</span><br><span class="line">        qsort(a, low,  pivot - <span class="number">1</span>);</span><br><span class="line">        qsort(a, pivot + <span class="number">1</span>, high);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">quicksort</span><span class="params">(<span class="keyword">int</span> a[], <span class="keyword">int</span> n)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    qsort(a, <span class="number">0</span>, n<span class="number">-1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="性能测试">性能测试</h3><p>自己实现的：<br>1万数据，大概是0.004秒<br>10万数据，大概是0.013秒<br>100万数据，大概是0.13秒</p><p>C语言qsort：<br>100万数据，大概是0.03到0.06秒<br>1000万数据，大概是0.3秒左右<br>1亿数据，大概是3秒左右<br>C++sort：<br>100万数据，大概是0.13秒左右<br>1000万数据，大概是1.4秒左右<br>1亿数据，大概是16秒左右</p><h2 id="插入排序">插入排序</h2><h3 id="简单插入排序insert-sort">简单插入排序insert sort</h3><h4 id="思路分析">思路分析</h4><p>在第$i$趟排序过程中，前面$i-1$个值是有序的，将第i个数字插入前面有序的长为$i-1$的序列中，构成长为$i$的有序序列。</p><h4 id="特点">特点</h4><ul><li>稳定</li><li>最坏时间复杂度$O(n^2 )$</li><li>最好时间复杂度$O(n )$，在数组有序的情况下</li><li>平均时间复杂度$O(n^2 )$</li><li>空间复杂度$O(n)$</li></ul><h4 id="代码-v3">代码</h4><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//首先我写了下面的代码，实际上是有问题的，</span></span><br><span class="line"><span class="comment">//错误示例！！！！！！！！！！！！！！！！！！！！！！！</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">insert_sort</span><span class="params">(<span class="keyword">int</span> a[], <span class="keyword">int</span> n)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> i = <span class="number">-1</span>, j = <span class="number">-1</span>, temp = <span class="number">-1</span>;</span><br><span class="line">    <span class="keyword">for</span>(i = <span class="number">1</span>; i &lt; n; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        temp = a[i];</span><br><span class="line">        <span class="keyword">for</span>(j = i - <span class="number">1</span>; j &gt;= <span class="number">0</span>; j--)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span>(a[j] &gt; temp)</span><br><span class="line">            &#123;</span><br><span class="line">                a[j+<span class="number">1</span>] = a[j];</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span> </span><br><span class="line">            &#123;</span><br><span class="line">                <span class="comment">//如果第j论插入应该插入在0位置时，会跳过这一步的执行。</span></span><br><span class="line">                a[j+<span class="number">1</span>] = temp;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//正确示例</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">insert_sort</span><span class="params">(<span class="keyword">int</span> a[], <span class="keyword">int</span> n)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> i = <span class="number">-1</span>, j = <span class="number">-1</span>, temp = <span class="number">-1</span>;</span><br><span class="line">    <span class="keyword">for</span>(i = <span class="number">1</span>; i &lt; n; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        temp = a[i];</span><br><span class="line">        <span class="keyword">for</span>(j = i - <span class="number">1</span>; j &gt;= <span class="number">0</span>; j--)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span>(a[j] &gt; temp)</span><br><span class="line">            &#123;</span><br><span class="line">                a[j+<span class="number">1</span>] = a[j];</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span> </span><br><span class="line">            &#123;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        a[j+<span class="number">1</span>] = temp;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="希尔排序">希尔排序</h3><h4 id="思路简介">思路简介</h4><p>希尔排序是对插入排序的扩展。</p><h4 id="属性-v3">属性</h4><ul><li>不稳定</li><li>平均的时间复杂度$O(n^{1.3} )$</li><li>最坏的时间复杂度$O(n^2 )$</li><li>最好的时间复杂度$O(n )$</li><li>空间复杂度是$O(1)$</li></ul><h4 id="代码-v4">代码</h4><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">shell_sort</span><span class="params">(<span class="keyword">int</span> a[], <span class="keyword">int</span> n)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> i = <span class="number">-1</span>, j = <span class="number">-1</span>, temp = <span class="number">-1</span>;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> gap = n/<span class="number">2</span>; gap &gt; <span class="number">0</span>; gap/=<span class="number">2</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">for</span>(i = gap; i &lt; n; i++)</span><br><span class="line">        &#123;</span><br><span class="line">            temp = a[i];</span><br><span class="line">            <span class="keyword">for</span>(j = i - gap; j &gt;= <span class="number">0</span>; j -= gap)</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="keyword">if</span>(temp &lt; a[j])</span><br><span class="line">                &#123;</span><br><span class="line">                    a[j+gap] =  a[j];</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">else</span></span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            a[j+gap] = temp;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="选择排序">选择排序</h2><h3 id="简单选择排序">简单选择排序</h3><h4 id="思路简介-v2">思路简介</h4><p>在第$i$趟排序过程中，前面$i-1$个值有序，从后面$n-i+1$个值中选择第$i$小的数，记下下标min_index，如果$i$和min_index不相等的话，交换它们的值。</p><h4 id="属性-v4">属性</h4><ul><li>不稳定</li><li>最坏时间复杂度$O(n^2 )$</li><li>最好时间复杂度$O(n^2 )$</li><li>平均时间复杂度$O(n^2 )$</li><li>空间复杂度$O(n)$</li></ul><h4 id="代码-v5">代码</h4><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">selection_sort</span><span class="params">(<span class="keyword">int</span> a[], <span class="keyword">int</span> n)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n - <span class="number">1</span>; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">int</span> min_index = i;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> j = i; j &lt; n; j++)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span>(a[j] &lt; a[min_index]) </span><br><span class="line">                min_index = j;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span>(min_index != i)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">int</span> temp = a[min_index];</span><br><span class="line">            a[min_index] = a[i];</span><br><span class="line">            a[i] = temp;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="heapsort">heapsort</h2><p>1.<a href="https://www.geeksforgeeks.org/heap-sort/" target="_blank" rel="noopener">https://www.geeksforgeeks.org/heap-sort/</a><br>2.<a href="http://www.techgeekbuzz.com/heap-sort-in-c/" target="_blank" rel="noopener">http://www.techgeekbuzz.com/heap-sort-in-c/</a><br>3.<a href="http://www.techgeekbuzz.com/heap-sort-in-c/" target="_blank" rel="noopener">http://www.techgeekbuzz.com/heap-sort-in-c/</a><br>4.<a href="https://www.zentut.com/c-tutorial/c-heapsort/" target="_blank" rel="noopener">https://www.zentut.com/c-tutorial/c-heapsort/</a></p><h2 id="归并排序">归并排序</h2><h3 id="思路-v3">思路</h3><p>将数组分为原来的一半，一直分到每一个都只有一个元素，然后这两个有序数组合并到一块。</p><h3 id="属性-v5">属性</h3><ul><li>稳定</li><li>最坏时间复杂度$O(n\log n)$</li><li>最好时间复杂度$O(n\log n)$</li><li>平均时间复杂度$O(n\log n)$</li><li>空间复杂度$O(n)$</li></ul><h3 id="代码-v6">代码</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">merge_sort</span><span class="params">(<span class="keyword">int</span> a[], <span class="keyword">int</span> l, <span class="keyword">int</span> r)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(l &lt; r)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">//int middle = (l+r)/2 may be overflow, use (r-l)/2 + l </span></span><br><span class="line">        <span class="keyword">int</span> m = (r-l)/<span class="number">2</span> + l;</span><br><span class="line">        merge_sort(a, l, m);</span><br><span class="line">        merge_sort(a, m+<span class="number">1</span>, r);</span><br><span class="line">        merge(a, l , m, r);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// 这个代码的话，有问题，每次都声明一个N的数组，时间和空间消耗都很大，所以需要进行改善，只需要声明一个r-l+1的临时数组就行了。</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">merge</span><span class="params">(<span class="keyword">int</span> a[], <span class="keyword">int</span> l, <span class="keyword">int</span> m, <span class="keyword">int</span> r)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">//!!!错误</span></span><br><span class="line">    <span class="comment">//int res[N] = &#123; 0 &#125;;</span></span><br><span class="line">    <span class="keyword">int</span> res[r-l+<span class="number">1</span>] = &#123;<span class="number">0</span>&#125;;</span><br><span class="line">    <span class="keyword">int</span> i = l, j = m+<span class="number">1</span>, k = <span class="number">0</span>;</span><br><span class="line">    <span class="comment">// merge</span></span><br><span class="line">    <span class="keyword">while</span>((i &lt;= m) &amp;&amp; (j &lt;= r))</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span>(a[i] &lt; a[j])</span><br><span class="line">        &#123;</span><br><span class="line">            res[k++] = a[i++];</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">        &#123;</span><br><span class="line">            res[k++] = a[j++];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// copy left</span></span><br><span class="line">    <span class="keyword">for</span>( ; i &lt;= m; )</span><br><span class="line">    &#123;</span><br><span class="line">        res[k++] = a[i++];</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span>( ; j &lt;= r; )</span><br><span class="line">    &#123;</span><br><span class="line">        res[k++] = a[j++];</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// copy to a</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> t = l; t &lt;=r; t++)</span><br><span class="line">    &#123;</span><br><span class="line">        a[t] = res[t-l];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="性能">性能</h3><p>100万，0.1秒左右<br>10万，0.02左右<br>1万，0.003左右</p><h2 id="非比较排序-v2">非比较排序</h2><h3 id="计数排序">计数排序</h3><h4 id="思路-v4">思路</h4><p>假设输入的$n$个数都在$0-k$之间，对于输入的每个元素$x$，统计比它小的值或者和它相等的值的个数，那么这个值在排序后的位置也就确定了。</p><h4 id="排序过程">排序过程</h4><p>输入数组：[2, 5, 3, 0, 2, 3, 0, 3]<br>排序后的数组应该是：[0, 0, 2, 2, 3, 3, 3, 5]</p><ol><li>输入待排序数组a：<br>0, 1, 2, 3, 4, 5, 6, 7<br>a = [2, 5, 3, 0, 2, 3, 0, 3], n=8</li><li>使用数组c统计出现的次数：<br>0, 1, 2, 3, 4, 5<br>c = [2, 0, 2, 3, 0, 1], k=5</li><li>对c进行操作，计算每个值应该在的位置<br>0, 1, 2, 3, 4, 5<br>c = [2, 2, 4, 7, 7, 8], k=5</li><li>根据数组c和数组a给出排序后的数组output:<br>遍历数组的每一个值，给出他们应该在哪个位置<br>a[0] = 2, c[a[0]] = c[2] = 4, c[2]=3, output[4] = a[0] = 2;<br>a[1] = 5，c[a[1]] = c[5] = 8, c[5]=7, output[8] = a[1] = 5;<br>a[2] = 5，c[a[2]] = c[3] = 7, c[3]=6, output[7] = a[2] = 3; 7中放的是第一个3<br>a[3] = 5，c[a[3]] = c[0] = 2, c[0]=1, output[2] = a[3] = 0;<br>a[4] = 5，c[a[4]] = c[2] = 3, c[2]=2, output[3] = a[4] = 2;<br>a[5] = 5，c[a[5]] = c[3] = 6, c[3]=5, output[6] = a[5] = 3;<br>a[6] = 5，c[a[6]] = c[0] = 1, c[0]=0, output[1] = a[6] = 0;<br>a[7] = 5，c[a[7]] = c[3] = 5, c[3]=4, output[5] = a[7] = 3;<br><strong>正序遍历是不稳定的，倒序遍历是稳定的。上面就是正序的，可以发现不稳定</strong></li></ol><h4 id="属性-v6">属性</h4><ul><li>稳定</li><li>不基于比较</li><li>时间复杂度是$O(n+k)$</li><li>空间复杂度是$O(n+k)$</li></ul><h4 id="代码-v7">代码</h4><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">counting_sort</span><span class="params">(<span class="keyword">int</span> a[], <span class="keyword">int</span> n, <span class="keyword">int</span> k)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> output[n];</span><br><span class="line">    <span class="keyword">int</span> freq[k];</span><br><span class="line">    <span class="built_in">memset</span>(freq, <span class="number">0</span>, <span class="keyword">sizeof</span>(freq));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// freq[i] contains the number of elements equal to i</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        freq[a[i]] ++;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// freq[i] contains the number of elements equal or less to i</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= k; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        freq[i] = freq[i] + freq[i<span class="number">-1</span>];</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = n - <span class="number">1</span>; i &gt;= <span class="number">0</span> ; i--)</span><br><span class="line">    &#123;</span><br><span class="line">        output[freq[a[i]]--] = a[i];</span><br><span class="line">    &#125;</span><br><span class="line">               </span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        a[i] = output[i+<span class="number">1</span>];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="基数排序">基数排序</h3><h4 id="思路-v5">思路</h4><p>借助多关键字排序的思想对单逻辑关键字排序。简单来说，对于十进制数字，依次按照个十百千万上每个位上的数字进行排序，假设有$d$位，需要分别对这$d$位进行排序。对每一位进行排序时，可以使用计数排序，因为$k$不大。</p><h4 id="排序过程-v2">排序过程</h4><p>给出一组待排序数字：[329, 457, 657, 839, 436, 726, 255]<br>先按照个位数进行排序，<br>再按照十位数进行排序，<br>最后按照百位数进行排序</p><h4 id="属性-v7">属性</h4><ul><li>稳定</li><li>时间复杂度$O(d(n+k))$</li><li>空间复杂度$O(n+k)$</li></ul><h4 id="代码-v8">代码</h4><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">bucket_sort</span><span class="params">(<span class="keyword">int</span> a[], <span class="keyword">int</span> n)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> max = get_max(a, n);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"max=%d\n"</span>, max);</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> <span class="built_in">exp</span>=<span class="number">1</span>; max/<span class="built_in">exp</span> &gt; <span class="number">0</span>; <span class="built_in">exp</span>*=<span class="number">10</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        counting_sort(a, n, <span class="built_in">exp</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">get_max</span><span class="params">(<span class="keyword">int</span> a[], <span class="keyword">int</span> n)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> max = a[<span class="number">0</span>];</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; n ;i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span>(a[i] &gt; max)</span><br><span class="line">            max = a[i];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> max;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">counting_sort</span><span class="params">(<span class="keyword">int</span> a[], <span class="keyword">int</span> n, <span class="keyword">int</span> <span class="built_in">exp</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> count[<span class="number">10</span>];</span><br><span class="line">    <span class="keyword">int</span> output[n+<span class="number">1</span>];</span><br><span class="line">    <span class="built_in">memset</span>(count, <span class="number">0</span>, <span class="keyword">sizeof</span>(count));</span><br><span class="line"></span><br><span class="line">    <span class="comment">//329, 457, 657, 839, 436, 726, 255</span></span><br><span class="line">    <span class="comment">// 1.count</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; i ++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">int</span> temp = (a[i] / <span class="built_in">exp</span>) % <span class="number">10</span>;</span><br><span class="line">        count[temp] ++;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2.accumulate count</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; <span class="number">10</span>; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        count[i] = count[i] + count[i<span class="number">-1</span>];</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3.sort</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = n - <span class="number">1</span>; i &gt;= <span class="number">0</span>; i--)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">int</span> temp = (a[i] / <span class="built_in">exp</span>) % <span class="number">10</span>;</span><br><span class="line">        output[count[temp]--] = a[i];</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 4. copy</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        a[i] = output[i+<span class="number">1</span>];</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"%d, "</span>, a[i]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"\n"</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="桶排序">桶排序</h3><h4 id="思路-v6">思路</h4><p>计数排序是桶排序的一个特例，计数排序中使用的桶的个数和max-min+1的值相同，而通排序中桶的个数要小于等于max-min+1，等于max-min+1时，桶排序就退化成了计数排序。</p><h4 id="属性-v8">属性</h4><ul><li>稳定</li><li>最好的时间复杂度是$O(n)$</li><li>最坏的时间复杂度是$O(n^2 )$</li><li>平均时间复杂度是$O(n+k)$</li><li>空间复杂度是$O(n+k)$</li></ul><h4 id="代码-v9">代码</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">int get_min(int a[], int n)</span><br><span class="line">&#123;</span><br><span class="line">    int min = a[0];</span><br><span class="line">    for(int i=1; i &lt; n; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        if(a[i] &lt; min)</span><br><span class="line">        &#123;</span><br><span class="line">            min = a[i];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    return min;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">int get_max(int a[], int n)</span><br><span class="line">&#123;</span><br><span class="line">    int max = a[0];</span><br><span class="line">    for(int i=1; i &lt; n; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        if(a[i] &gt; max)</span><br><span class="line">        &#123;</span><br><span class="line">            max = a[i];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    return max;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">void bucket_sort(int a[], int n, int bucket_number)</span><br><span class="line">&#123;</span><br><span class="line">    // 1.创建n个桶</span><br><span class="line">    std::vector&lt;int&gt; b[bucket_number];</span><br><span class="line"></span><br><span class="line">    int max = get_max(a, n);</span><br><span class="line">    int min = get_min(a, n);</span><br><span class="line"></span><br><span class="line">    // 2.每个桶的大小</span><br><span class="line">    int bucket_size = (max - min + 1) / bucket_number;</span><br><span class="line">    for(int i = 0 ; i&lt; n; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        int bucket_index = (a[i] - min) / bucket_size;</span><br><span class="line">        b[bucket_index].push_back(a[i]);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    for(int i = 0; i &lt; bucket_number; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        sort(b[i].begin(), b[i].end());</span><br><span class="line">    &#125;</span><br><span class="line">    int count = 0;</span><br><span class="line">    for(int i = 0; i &lt; bucket_number; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        for(int j = 0; j &lt; b[i].size(); j++)</span><br><span class="line">        &#123;</span><br><span class="line">            a[count++] = b[i][j];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="快速排序vs归并排序">快速排序vs归并排序</h2><ul><li>辅助空间：快排可以使用in-place方法实现，在每一个排序过程中不需要额外的空间，但是快排的递归实现，需要保存栈调用（是常数），平均情况下是$O(\log n)$，最坏情况下是$O(n)$；<a href="https://www.geeksforgeeks.org/quicksort-tail-call-optimization-reducing-worst-case-space-log-n/" target="_blank" rel="noopener">使用尾递归的快排最坏情况下空间复杂度也是$O(\log n)$</a>。而归并排序需要临时数组存储归并后的排序数组，<a href="https://cs.stackexchange.com/a/35510" target="_blank" rel="noopener">需要$O(n)$的空间复杂度</a>。</li><li>时间复杂度：快排在最坏情况下的时间复杂度是$O(n^2 )$，但是可以使用随机选择pivot的方式避免。</li><li>归并排序更适合大的数据结构，mergesore是稳定排序，可以修改成适合链表等数据结构的算法，以及内存和网络上的排序。因为在链表中，插入的时间和空间复杂度都是$O(1)$，因此链表的归并排序可以不需要额外的辅助空间。</li><li>快排和归并的平均时间复杂度都是$O(n\log n)$，但是因为归并排序需要分配以及销毁临时数组，所以要更慢一些。</li><li>快排是cache friendly的，对于数据来说，他有locality of reference。（<a href="https://stackoverflow.com/a/70631/8939281" target="_blank" rel="noopener">即是否很大可能性从cache中读取大量元素</a>）。</li></ul><h2 id="简单排序">简单排序</h2><p>常见的三大简单排序：冒泡排序，简单选择排序，简单插入排序。<br>冒泡排序：</p><ul><li>优点:比较简单，空间复杂度较低，是稳定的；</li><li>缺点:时间复杂度太高，效率慢；</li></ul><p>选择排序：</p><ul><li>优点：一轮比较只需要一次交换；</li><li>缺点：效率慢，不稳定（举个例子2，2，1， 假设我们每趟排序都选择第一个元素作为比较值，第一趟排序会交换第一个2和1，两个2的位置已经变了）。</li></ul><p>冒泡排序和简单选择排序的区别和联系：</p><ul><li>冒泡排序每趟都是比较无序序列中相邻位置的两个数，而选择排序每趟都是选择无序序列中的最小数；</li><li>冒泡排序每一轮比较后，位置不对都需要换位置，选择排序每一趟排序都只需要一次交换；</li><li>冒泡排序是通过数去找位置，选择排序是给定位置去找数；</li></ul><h2 id="参考文献">参考文献</h2><p>1.<a href="https://www.quora.com/What-is-the-time-complexity-of-quick-sort/answer/Lakshmi-Narayana-217" target="_blank" rel="noopener">https://www.quora.com/What-is-the-time-complexity-of-quick-sort/answer/Lakshmi-Narayana-217</a><br>2.<a href="https://www.cnblogs.com/Good-good-stady-day-day-up/p/9055698.html" target="_blank" rel="noopener">https://www.cnblogs.com/Good-good-stady-day-day-up/p/9055698.html</a><br>3.<a href="https://www.cnblogs.com/onepixel/p/7674659.html" target="_blank" rel="noopener">https://www.cnblogs.com/onepixel/p/7674659.html</a><br>4.<a href="https://www.geeksforgeeks.org/radix-sort/" target="_blank" rel="noopener">https://www.geeksforgeeks.org/radix-sort/</a><br>5.<a href="https://www.geeksforgeeks.org/quicksort-better-mergesort/" target="_blank" rel="noopener">https://www.geeksforgeeks.org/quicksort-better-mergesort/</a><br>6.<a href="https://cs.stackexchange.com/a/35510" target="_blank" rel="noopener">https://cs.stackexchange.com/a/35510</a><br>7.<a href="https://www.geeksforgeeks.org/quicksort-tail-call-optimization-reducing-worst-case-space-log-n/" target="_blank" rel="noopener">https://www.geeksforgeeks.org/quicksort-tail-call-optimization-reducing-worst-case-space-log-n/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;排序分类&quot;&gt;排序分类&lt;/h2&gt;
&lt;h3 id=&quot;内部排序&quot;&gt;内部排序&lt;/h3&gt;
&lt;h4 id=&quot;基于比较的排序&quot;&gt;基于比较的排序&lt;/h4&gt;
&lt;p&gt;交换排序&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;冒泡排序&lt;/li&gt;
&lt;li&gt;快速排序&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;插入排序&lt;/p&gt;
      
    
    </summary>
    
      <category term="数据结构" scheme="http://mxxhcm.github.io/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
    
      <category term="数据结构" scheme="http://mxxhcm.github.io/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
      <category term="排序" scheme="http://mxxhcm.github.io/tags/%E6%8E%92%E5%BA%8F/"/>
    
      <category term="冒泡排序" scheme="http://mxxhcm.github.io/tags/%E5%86%92%E6%B3%A1%E6%8E%92%E5%BA%8F/"/>
    
      <category term="快速排序" scheme="http://mxxhcm.github.io/tags/%E5%BF%AB%E9%80%9F%E6%8E%92%E5%BA%8F/"/>
    
      <category term="选择排序" scheme="http://mxxhcm.github.io/tags/%E9%80%89%E6%8B%A9%E6%8E%92%E5%BA%8F/"/>
    
      <category term="堆排序" scheme="http://mxxhcm.github.io/tags/%E5%A0%86%E6%8E%92%E5%BA%8F/"/>
    
      <category term="插入排序" scheme="http://mxxhcm.github.io/tags/%E6%8F%92%E5%85%A5%E6%8E%92%E5%BA%8F/"/>
    
      <category term="希尔排序" scheme="http://mxxhcm.github.io/tags/%E5%B8%8C%E5%B0%94%E6%8E%92%E5%BA%8F/"/>
    
      <category term="计数排序" scheme="http://mxxhcm.github.io/tags/%E8%AE%A1%E6%95%B0%E6%8E%92%E5%BA%8F/"/>
    
      <category term="归并排序" scheme="http://mxxhcm.github.io/tags/%E5%BD%92%E5%B9%B6%E6%8E%92%E5%BA%8F/"/>
    
      <category term="桶排序" scheme="http://mxxhcm.github.io/tags/%E6%A1%B6%E6%8E%92%E5%BA%8F/"/>
    
      <category term="基数排序" scheme="http://mxxhcm.github.io/tags/%E5%9F%BA%E6%95%B0%E6%8E%92%E5%BA%8F/"/>
    
  </entry>
  
  <entry>
    <title>Actor-Mimic</title>
    <link href="http://mxxhcm.github.io/2019/10/14/actor-mimic/"/>
    <id>http://mxxhcm.github.io/2019/10/14/actor-mimic/</id>
    <published>2019-10-14T03:43:48.000Z</published>
    <updated>2019-10-16T02:12:44.933Z</updated>
    
    <content type="html"><![CDATA[<h2 id="actor-mimic">Actor Mimic</h2><p>本文提出了Actor-Mimic，一个multitask和transfer learning方法，使用多个expert DQN指导训练一个可以在多个taskes上使用的单个policy network，并且可以将这些经验迁移到新的taskes上。</p><h2 id="policy-regression-objective">Policy Regression Objective</h2><p>给定多个sources games $S_1, \cdots, S_N$，我们的第一个目标是获得一个能玩任何source games，并且尽可能和expert DQN性能相近的single multitask policy network。为了训练这样一个网络，使用$N$个expert DQN $E_1, \cdots, E_N$进行指导。一个可能的方法是定义student network和expert network之间$Q$值的均方根误差。因为expert values funcitons在不同的游戏之间可能变化很大，所以作者首先将$Q$值经过softmax变成了policies，softmax的输出都在$0$和$1$之间，所以可以提高训练的稳定性。我们可以把softmax看成让student更多的关注expert DQN在每个state选择的action（DQN选择的是Q值最大的action），经过softmax相当于让它更sharp了。<br>最后得到了一个actor，或者说是一个policy，它模仿了所有DQN experts的decisions。比如，在$Q$值上计算Boltzman分布：<br>$$\pi_{E_i} (a|s) = \frac{ e^{\tau^{-1} Q_{E_i}(s,a) } }{\sum_{a’\in A_{E_i} } e^{\tau^{-1} Q_{E_i}(s,a) } } \tag{1}$$<br>其中$\tau$是温度，$A_{E_i}$是expert DQN $E_i$使用的action space。给定$S_i$的一个state s，定义multitask  network的policy objective是expert network’s policy和currnet multitask policcy的cross-entropy:<br>$$L^i_{policy}(\theta) = \sum_{a\in A_{E_i} }\pi_{E_i} (a|s) \log \pi_{AMN}(a|s;\theta) \tag{2}$$<br>其中$\pi_{AMN}(a|s;\theta) $是$\theta$参数化的multitask Actor Mimic Network policy。和Q-learning把自身当做target value相比，AMN得到了一个stable supervised training signal (expert network)指导 multitask network训练。<br>为了获得训练数据，可以sample expert network后者使用AMN action outputs生成trajectories。即使AMN还在学习过程中，也能得到好的结果。至少在AMN是linear function approximator时，可以证明AMN会收敛到expert policy。</p><h2 id="feature-regression-objective">Feature Regression Objective</h2><p>除了对policy进行回归以外，还可以对feature进行回归。用$h_{AMN}(s)$和$h_{E_i}(s)$分别表示AMN和第$i$个expert network在state s处feature的hidden activation，他们两个的dimension不一定要相等。使用一个feature回归网络$f_i(h_{AMN}(s))$，预测$s$处$h_{E_i}(s)$到$h_{AMN}(s)$的映射，映射$f_i$的结构是随意的，可以使用以下的回归loss进行训练：<br>$$L^i_{FeatureRegression}(\theta, \theta_{f_i}) = || f_i(h_{AMN}(s;\theta); \theta_{f_i}) - h_{E_i}(s) ||^2_2 \tag{3}$$<br>其中$\theta$是AMN的参数，$\theta_{f_i}$是第$i$个特征回归网络的参数。使用这个loss训练，最终我们的目标是得到一个multitask network能够包含多个expert network的features。</p><h2 id="actor-mimic-objective">Actor-Mimic Objective</h2><p>将policy objective和feature objective结合在一起，就得到了actor-mimic objective：<br>$$ L^i_{ActorMimic}(\theta, \theta_{f_i}) = L^i_{policy}(\theta) + \beta L^i_{FeatureRegression}(\theta, \theta_{f_i}) \tag{4}$$<br>$\beta$用来控制两个objective的权重。直观上来说，我们可以把policy objective看成expert network教会AMN该怎么act（模仿expert的action），而feature objective类似于expert network教会AMN为什么这样act，模仿expert的思考过程（特征提取过程）。</p><h2 id="transfering-knowledge">Transfering Knowledge</h2><p>通过优化actor-mimic objective，我们得到一个在所有source target上都表现不错的expert network，接下来我们可以把它迁移到相关的target task上。为了迁移到新的task上，首先移除掉AMN的final softmax layer，然后用AMN的参数初始化一个DQN在新的task上继续训练，接下来和标准的DQN训练方式一样。Multitask pretaining可以看成学习了related tasks中对于policies definition相当有效的特征，然后初始化DQN。如果source和target tasks很像的话，pretained features对于target task是相当有效的。</p><h2 id="multitask-experiments">Multitask experiments</h2><h3 id="简介">简介</h3><p>Multitask任务中并不进行transfer，仅仅使用policy regression objective同时在multitask上训练一个AMN。</p><h3 id="baselines">baselines</h3><ul><li>Multitask DQN: 使用多个games训练一个DQN，只有最后的full-connected layer不同。</li><li>Multitask Convolutions DQN: 使用多个games训练一个DQN，但是只共享convolutional layer，每个game都有自己的全连接层和softmax层。</li></ul><h3 id="网络架构：">网络架构：</h3><p>32个步长为$4$的$8\times 8$filters<br>64个步长为$2$的$4\times 4$filters<br>64个步长为$1$的$3\times 3$filters<br>$512$ fully-connected units<br>$18$个actions<br>除了最后一层都有一个relu。</p><h3 id="实验数据采集">实验数据采集</h3><h4 id="amn和dqn-expert对比">AMN和DQN expert对比</h4><p>DQN训练到收敛，使用的是训练到收敛过程中的max test reward，收敛过程中最后10个epochs的mean test reward。<br>AMN在每个source game上训练100个epochs， 每一个epoch是250000 frames，总共有2500万 frames。图中展示了AMN在100个epochs中最大的test reward和最后100个epochs的mean test reward。</p><h4 id="amn和mdqn-mcdqn对比">AMN和MDQN，MCDQN对比</h4><p>AMN，MDQN和MCDQN在每个source game上训练40个epochs， 每一个epoch是250000 frames，总共有2500万frames，每一个training epoch之后进行一个$125000$ frames的tesing epoch。最后图中展示了AMN,MDQN以及MCDQN在每个tesing epoch的test average episode rewrad。</p><h2 id="transfer-experiments">Transfer experiments</h2><p>小的AMN（和DQN expert架构相同）的AMN能够学习多个source tasks的knowledge，而大一些的AMN能够更容易的迁移。在transfer实验中，使用了比DQN expert更复杂的AMN model，能够同时玩13个source games。为了防止过拟合，AMN在每个source game上训练400万个frames。<br>然后用训练完的AMN当做新任务上DQN的初始化权重。仅仅使用policy regression objective的叫做AMN-policy，而使用feature和policy objective的叫做AMN-feature。将AMN-feature以及AMN-policy的结果和随机初始化的DQN baseline进行比较。<br>每隔4个traing epoches，每个training epoch后都有一个testing epoch，输出这4个epoches的average test reward。<br>使用的网络架构：<br>256个步长为$4$的$8\times 8$filters<br>512个步长为$2$的$4\times 4$filters<br>512个步长为$1$的$3\times 3$filters<br>512个步长为$1$的$3\times 3$filters<br>$2048$ fully-connected units<br>$1024$ fully-connected units<br>$18$个actions<br>除了最后一层都有一个relu。</p><h2 id="amn的细节">AMN的细节</h2><p>所有的AMN使用Adam优化器，有一个18-unit的output layer，每一个对应atari 18个actions中可能的一个，使用18个actions简化了不同游戏有不同的action subsets。训练一个特定的游戏时，mask那些not valid的actions，然后在valid actions上使用softmax。AMN每个game使用100000大小的replay memeory。<br>在feature regression objective中，设置$\beta$是$0.01$，设置$f_i$是第$i$个expert feature的线性投影。在训练过程中，AMN使用的$\epsilon$-greedy policy中$\epsilon$是常数$0.1$。在训练过程中，基于AMN而不是expert DQN选择actions。<br>在实验中使用的DQN，使用RMSProp优化，和nature-DQN的架构，超参数以及训练过程都一样。Replay memory总共有1000000 frames。</p><h2 id="参考文献">参考文献</h2><ol><li></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;actor-mimic&quot;&gt;Actor Mimic&lt;/h2&gt;
&lt;p&gt;本文提出了Actor-Mimic，一个multitask和transfer learning方法，使用多个expert DQN指导训练一个可以在多个taskes上使用的单个policy networ
      
    
    </summary>
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="rl papers" scheme="http://mxxhcm.github.io/tags/rl-papers/"/>
    
  </entry>
  
  <entry>
    <title>python ptan</title>
    <link href="http://mxxhcm.github.io/2019/10/12/python-ptan/"/>
    <id>http://mxxhcm.github.io/2019/10/12/python-ptan/</id>
    <published>2019-10-12T12:36:40.000Z</published>
    <updated>2019-10-21T11:49:41.769Z</updated>
    
    <content type="html"><![CDATA[<h2 id="PyTorch-Agent-Net-library"><a href="#PyTorch-Agent-Net-library" class="headerlink" title="PyTorch Agent Net library"></a>PyTorch Agent Net library</h2><h1 id><a href="#" class="headerlink" title="#"></a>#</h1><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>1.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;PyTorch-Agent-Net-library&quot;&gt;&lt;a href=&quot;#PyTorch-Agent-Net-library&quot; class=&quot;headerlink&quot; title=&quot;PyTorch Agent Net library&quot;&gt;&lt;/a&gt;PyTorch Age
      
    
    </summary>
    
      <category term="python" scheme="http://mxxhcm.github.io/categories/python/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="pytorch" scheme="http://mxxhcm.github.io/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>python iteration-iterable and iterator</title>
    <link href="http://mxxhcm.github.io/2019/10/12/python-iteration/"/>
    <id>http://mxxhcm.github.io/2019/10/12/python-iteration/</id>
    <published>2019-10-12T07:51:26.000Z</published>
    <updated>2019-10-12T10:19:42.861Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Iteration"><a href="#Iteration" class="headerlink" title="Iteration"></a>Iteration</h2><p>Iteration并不是一个具体的东西，它是一个抽象的名词，指的是一个接一个的取某个对象的每一个项。包含隐式的，显式的loop，即while，do, for等，这叫iteration。</p><h2 id="Iterable和iterator"><a href="#Iterable和iterator" class="headerlink" title="Iterable和iterator"></a>Iterable和iterator</h2><p>而在python中，有iterator和iterable。<br>一个iterable object是实现了<strong>iter</strong>方法的object或者定义了<strong>getitem</strong>方法。一个iteratable object是一个可以得到iterator的object，但是它自己并不一定是iterator object。<br>而iterator是一个实现了<strong>next</strong>和<strong>iter</strong>方法的object。<br><strong>iterable object不一定是iterator，iterator一定是iterable object。</strong><br><strong>可以使用for循环的都是ieterable object，比如str，list，但是它们不是itertor，可以使用iter()方法得到iterator</strong><br><strong>可以next()的都是iterator</strong></p><h2 id="iter和-iter"><a href="#iter和-iter" class="headerlink" title="iter和__iter__"></a>iter和__iter__</h2><p>所有实现了<strong>iter</strong>方法的object，都是iterable object，可以通过iter()方法产生iterator object。<br>具体示例<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">from collections import Iterator</span><br><span class="line">from collections import Iterable</span><br><span class="line"></span><br><span class="line">class Fibs:</span><br><span class="line">    def __init__(self, a, b):</span><br><span class="line">        self.a = a</span><br><span class="line">        self.b = b</span><br><span class="line"></span><br><span class="line">    def __iter__(self):</span><br><span class="line">        a = self.a</span><br><span class="line">        b = self.b</span><br><span class="line">        while True:</span><br><span class="line">            yield a</span><br><span class="line">            a, b = b, a + b</span><br><span class="line"></span><br><span class="line">real_fibs = Fibs(0,1)</span><br><span class="line"></span><br><span class="line">print(&quot;real_fibs is iterator? &quot;, isinstance(real_fibs, Iterator))</span><br><span class="line">print(&quot;real_fibs is iterable? &quot;, isinstance(real_fibs, Iterable))</span><br><span class="line">print(&quot;iter(real_fibs) is iterator? &quot;, isinstance(iter(real_fibs), Iterator))</span><br><span class="line">print(&quot;iter(real_fibs) is iterable? &quot;, isinstance(iter(real_fibs), Iterable))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">for idx, i in enumerate(real_fibs):</span><br><span class="line">    print(i)</span><br><span class="line">    if idx &gt; 10:</span><br><span class="line">        break</span><br></pre></td></tr></table></figure></p><p>其中出现了yield关键字。yield关键字的作用是每次迭代执行到该行代码时，就返回一个值，并且记住相应的位置，在下次迭代时继续从该行位置开始执行。</p><h2 id="next和-next"><a href="#next和-next" class="headerlink" title="next和__next__"></a>next和__next__</h2><p>代码示例<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Iterator</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">A</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, max_v=<span class="number">5</span>)</span>:</span></span><br><span class="line">        self.max_v = max_v</span><br><span class="line">        self.v = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__next__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># if self.v &lt;= self.nax_v</span></span><br><span class="line">        self.v += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> self.v</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    a = A()</span><br><span class="line">    <span class="keyword">for</span> idx, v <span class="keyword">in</span> enumerate(a):</span><br><span class="line">        print(idx, v)</span><br><span class="line">        <span class="keyword">if</span> (idx &gt;= <span class="number">10</span>):</span><br><span class="line">            <span class="keyword">break</span></span><br></pre></td></tr></table></figure></p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>1.<a href="https://stackoverflow.com/questions/9884132/what-exactly-are-iterator-iterable-and-iteration" target="_blank" rel="noopener">https://stackoverflow.com/questions/9884132/what-exactly-are-iterator-iterable-and-iteration</a><br>2.<a href="https://stackoverflow.com/a/46411740/8939281" target="_blank" rel="noopener">https://stackoverflow.com/a/46411740/8939281</a><br>3.<a href="https://www.jianshu.com/p/f9b547874a14" target="_blank" rel="noopener">https://www.jianshu.com/p/f9b547874a14</a><br>4.<a href="https://www.jianshu.com/p/1b0686bc166d" target="_blank" rel="noopener">https://www.jianshu.com/p/1b0686bc166d</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Iteration&quot;&gt;&lt;a href=&quot;#Iteration&quot; class=&quot;headerlink&quot; title=&quot;Iteration&quot;&gt;&lt;/a&gt;Iteration&lt;/h2&gt;&lt;p&gt;Iteration并不是一个具体的东西，它是一个抽象的名词，指的是一个接一个的取某个
      
    
    </summary>
    
      <category term="python" scheme="http://mxxhcm.github.io/categories/python/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="iteration" scheme="http://mxxhcm.github.io/tags/iteration/"/>
    
      <category term="iterable" scheme="http://mxxhcm.github.io/tags/iterable/"/>
    
      <category term="iterator" scheme="http://mxxhcm.github.io/tags/iterator/"/>
    
  </entry>
  
  <entry>
    <title>python pickle</title>
    <link href="http://mxxhcm.github.io/2019/10/08/python-pickle/"/>
    <id>http://mxxhcm.github.io/2019/10/08/python-pickle/</id>
    <published>2019-10-08T09:56:48.000Z</published>
    <updated>2019-10-11T05:30:51.354Z</updated>
    
    <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>pickle是一个序列化模块，它能将python对象序列化转换成二进制串再反序列化成python对象。</p><h2 id="常用函数"><a href="#常用函数" class="headerlink" title="常用函数"></a>常用函数</h2><h3 id="pickle-dump"><a href="#pickle-dump" class="headerlink" title="pickle.dump()"></a>pickle.dump()</h3><h4 id="API"><a href="#API" class="headerlink" title="API"></a>API</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pickle.dump(obj, file, [,protocol])</span><br></pre></td></tr></table></figure><h4 id="作用"><a href="#作用" class="headerlink" title="作用"></a>作用</h4><p>将python对象obj以二进制字符串形式保存到文件file中，使用protocol。</p><h4 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"></span><br><span class="line">dictionary = &#123;<span class="string">"name"</span>: <span class="string">"mxx"</span>, <span class="string">"age"</span>: <span class="number">23</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">"test.txt"</span>, <span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    pickle.dump(dictionary, f)</span><br></pre></td></tr></table></figure><h3 id="pickle-load"><a href="#pickle-load" class="headerlink" title="pickle.load()"></a>pickle.load()</h3><h4 id="API-1"><a href="#API-1" class="headerlink" title="API"></a>API</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pickle.load(file)</span><br></pre></td></tr></table></figure><h4 id="作用-1"><a href="#作用-1" class="headerlink" title="作用"></a>作用</h4><p>从文件file中读取二进制字符串，将其反序列成python对象。</p><h4 id="示例-1"><a href="#示例-1" class="headerlink" title="示例"></a>示例</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">"test.txt"</span>, <span class="string">'rb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    b = pickle.load(f)</span><br><span class="line"></span><br><span class="line">print(b)</span><br><span class="line">print(type(b))</span><br></pre></td></tr></table></figure><h3 id="pickle-dumps"><a href="#pickle-dumps" class="headerlink" title="pickle.dumps()"></a>pickle.dumps()</h3><h4 id="API-2"><a href="#API-2" class="headerlink" title="API"></a>API</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pickle.dumps(obj, [,protocol])</span><br></pre></td></tr></table></figure><h4 id="作用-2"><a href="#作用-2" class="headerlink" title="作用"></a>作用</h4><p>将python对象obj转化成二进制字符串，返回一个字符串</p><h4 id="示例-2"><a href="#示例-2" class="headerlink" title="示例"></a>示例</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"></span><br><span class="line">dictionary = &#123;<span class="string">"name"</span>: <span class="string">"mxx"</span>, <span class="string">"age"</span>: <span class="number">23</span>&#125;</span><br><span class="line"></span><br><span class="line">s = pickle.dumps(dictionary)</span><br><span class="line">print(s)</span><br><span class="line">print(type(s))</span><br><span class="line"><span class="comment"># 输出</span></span><br><span class="line"><span class="comment"># b'\x80\x03&#125;q\x00(X\x04\x00\x00\x00nameq\x01X\x03\x00\x00\x00mxxq\x02X\x03\x00\x00\x00ageq\x03K\x17u.'</span></span><br><span class="line"><span class="comment"># &lt;class 'bytes'&gt;</span></span><br></pre></td></tr></table></figure><h3 id="pickle-loads"><a href="#pickle-loads" class="headerlink" title="pickle.loads()"></a>pickle.loads()</h3><h4 id="API-3"><a href="#API-3" class="headerlink" title="API"></a>API</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pickle.loads(string)</span><br></pre></td></tr></table></figure><h4 id="作用-3"><a href="#作用-3" class="headerlink" title="作用"></a>作用</h4><p>从二进制字符串中返回序列化前的python obj对象。</p><h4 id="示例-3"><a href="#示例-3" class="headerlink" title="示例"></a>示例</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"></span><br><span class="line">dictionary = &#123;<span class="string">"name"</span>: <span class="string">"mxx"</span>, <span class="string">"age"</span>: <span class="number">23</span>&#125;</span><br><span class="line"></span><br><span class="line">s = pickle.dumps(dictionary)</span><br><span class="line">b = pickle.loads(s)</span><br><span class="line">print(b)</span><br><span class="line">print(type(b))</span><br></pre></td></tr></table></figure><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>1.<a href="https://www.jianshu.com/p/cf91849064e3" target="_blank" rel="noopener">https://www.jianshu.com/p/cf91849064e3</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h2&gt;&lt;p&gt;pickle是一个序列化模块，它能将python对象序列化转换成二进制串再反序列化成python对象。&lt;/p&gt;
&lt;h2 id=&quot;常用函数&quot;&gt;
      
    
    </summary>
    
      <category term="python" scheme="http://mxxhcm.github.io/categories/python/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="pickle" scheme="http://mxxhcm.github.io/tags/pickle/"/>
    
  </entry>
  
  <entry>
    <title>python mpi4py</title>
    <link href="http://mxxhcm.github.io/2019/10/08/python-mpi4py/"/>
    <id>http://mxxhcm.github.io/2019/10/08/python-mpi4py/</id>
    <published>2019-10-08T09:25:46.000Z</published>
    <updated>2019-10-11T05:30:51.354Z</updated>
    
    <content type="html"><![CDATA[<h2 id="MPI"><a href="#MPI" class="headerlink" title="MPI"></a>MPI</h2><p>MPI全名是Message Passing Interface，它是一个标准，而不是一个实现，专门为进程间通信实现的。它的工作原理很简单，启动一组进程，在同一个通信域中的不同进程有不同的编号，可以给不同编号的进程分配不同的任务，最终实现整个任务。<br>MPI4PY就是python中MPI的实现。在python中有很多种方法实现多进程以及进程间通信，比如multiprocessing，但是multiprocessing进程间通信不够方便，mpi4py的效率更高一些。<br>mpi4py提供了点对点通信，点对面，面对点通信。点对点通信又包含阻塞和非阻塞等等，通信的内容包含python内置对象，也包含numpy数组等。</p><h2 id="mpi4py简单对象和方法介绍"><a href="#mpi4py简单对象和方法介绍" class="headerlink" title="mpi4py简单对象和方法介绍"></a>mpi4py简单对象和方法介绍</h2><p>MPI.COMM_WORLD是一个通信域，在这个通信域中有不同的进程，每个进程的编号以及进程的数量都可以通过这个通信域获得。具体看以下comm_world.py代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mpi4py <span class="keyword">import</span> MPI</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获得多进程通信域</span></span><br><span class="line">comm = MPI.COMM_WORLD</span><br><span class="line"><span class="comment"># 获得当前进程通信域中进程数量</span></span><br><span class="line">size = comm.Get_size()</span><br><span class="line"><span class="comment"># 获得当前进程在通信域中的编号</span></span><br><span class="line">rank = comm.Get_rank()</span><br></pre></td></tr></table></figure></p><blockquote><blockquote><blockquote><p>mpiexec -np 3 python comm_world.py</p></blockquote></blockquote></blockquote><h2 id="点对点通信"><a href="#点对点通信" class="headerlink" title="点对点通信"></a>点对点通信</h2><h3 id="阻塞通信"><a href="#阻塞通信" class="headerlink" title="阻塞通信"></a>阻塞通信</h3><h4 id="python对象"><a href="#python对象" class="headerlink" title="python对象"></a>python对象</h4><h5 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h5><p>comm.send(data, dest, tag)<br>comm.recv(source, tag)<br>send和recv都是阻塞方法，即调用这个方法之后，等到该函数调用结束之后再返回。dest是目的process编号，source是发送的process编号。data是要发送的数据，需要是python的内置对象，即可以pickle的对象。</p><h5 id="代码示例"><a href="#代码示例" class="headerlink" title="代码示例"></a>代码示例</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mpi4py <span class="keyword">import</span> MPI</span><br><span class="line"></span><br><span class="line">comm = MPI.COMM_WORLD</span><br><span class="line">rank = comm.Get_rank()</span><br><span class="line">size = comm.Get_size()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> rank == <span class="number">0</span>:</span><br><span class="line">    data = &#123;<span class="string">'name'</span>: <span class="string">"mxx"</span>, <span class="string">"age"</span>: <span class="number">23</span>&#125;</span><br><span class="line">    comm.send(data, dest=<span class="number">1</span>, tag=<span class="number">10</span>)</span><br><span class="line">    print(<span class="string">"data has sent."</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    data = comm.recv(source=<span class="number">0</span>, tag=<span class="number">10</span>)</span><br><span class="line">    print(<span class="string">"data has been receieved."</span>)</span><br></pre></td></tr></table></figure><h4 id="numpy数组"><a href="#numpy数组" class="headerlink" title="numpy数组"></a>numpy数组</h4><h5 id="简介-1"><a href="#简介-1" class="headerlink" title="简介"></a>简介</h5><p>comm.Send(data, dest, tag)<br>comm.Recv(source, tag)<br>Send和Recv都是阻塞方法，即调用这个方法之后，等到该函数调用结束之后再返回。dest是目的process编号，source是发送的process编号。data是要发送的数据，需要是numpy对象，和c语言的效率差不多。</p><h5 id="代码示例-1"><a href="#代码示例-1" class="headerlink" title="代码示例"></a>代码示例</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mpi4py <span class="keyword">import</span> MPI</span><br><span class="line"></span><br><span class="line">comm = MPI.COMM_WORLD</span><br><span class="line"></span><br><span class="line">rank = comm.Get_rank()</span><br><span class="line">size = comm.Get_size()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> rank == <span class="number">0</span>:</span><br><span class="line">    data = &#123;<span class="string">'name'</span>: <span class="string">"mxx"</span>, <span class="string">"age"</span>: <span class="number">23</span>&#125;</span><br><span class="line">    comm.isend(data, dest=<span class="number">1</span>, tag=<span class="number">10</span>)</span><br><span class="line">    print(<span class="string">"data has sent."</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    data = comm.irecv(source=<span class="number">0</span>, tag=<span class="number">10</span>)</span><br><span class="line">    print(<span class="string">"data has been receieved."</span>)</span><br></pre></td></tr></table></figure><h3 id="非阻塞通信"><a href="#非阻塞通信" class="headerlink" title="非阻塞通信"></a>非阻塞通信</h3><h4 id="简介-2"><a href="#简介-2" class="headerlink" title="简介"></a>简介</h4><p>comm.isend(data, dest, tag)<br>comm.irecv(source, tag)<br>isend和irecv都是非阻塞方法，即调用这个方法之后，调用该函数之后立即返回，无需等待它执行结束。dest是目的process编号，source是发送的process编号。data要是python对象，可以被pickle处理的。</p><h4 id="代码示例-2"><a href="#代码示例-2" class="headerlink" title="代码示例"></a>代码示例</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mpi4py <span class="keyword">import</span> MPI</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">comm = MPI.COMM_WORLD</span><br><span class="line"></span><br><span class="line">rank = comm.Get_rank()</span><br><span class="line">size = comm.Get_size()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> rank == <span class="number">0</span>:</span><br><span class="line">    data = np.ones((<span class="number">3</span>, <span class="number">4</span>), dtype=<span class="string">'i'</span>)</span><br><span class="line">    comm.Send([data, MPI.INT], dest=<span class="number">1</span>, tag=<span class="number">10</span>)</span><br><span class="line">    print(<span class="string">"data has sent."</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    data = np.empty((<span class="number">3</span>, <span class="number">4</span>), dtype=<span class="string">'i'</span>)</span><br><span class="line">    data = comm.Recv([data, MPI.INT], source=<span class="number">0</span>, tag=<span class="number">10</span>)</span><br><span class="line">    print(<span class="string">"data has been receieved."</span>)</span><br></pre></td></tr></table></figure><h2 id="组通信"><a href="#组通信" class="headerlink" title="组通信"></a>组通信</h2><h3 id="bcast"><a href="#bcast" class="headerlink" title="bcast"></a>bcast</h3><h4 id="简介-3"><a href="#简介-3" class="headerlink" title="简介"></a>简介</h4><p>将一个process中的数据发送给所有在通信池中的process。<br>comm.bcast(data, dest, tag)</p><h4 id="代码示例-3"><a href="#代码示例-3" class="headerlink" title="代码示例"></a>代码示例</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> mpi4py</span><br><span class="line"><span class="keyword">from</span> mpi4py <span class="keyword">import</span> MPI</span><br><span class="line"></span><br><span class="line">comm = MPI.COMM_WORLD</span><br><span class="line">size = comm.Get_size()</span><br><span class="line">rank = comm.Get_rank()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> rank == <span class="number">1</span>:</span><br><span class="line">    data = &#123;<span class="string">"name"</span>: <span class="string">"mxx"</span>, <span class="string">"age"</span>: <span class="number">23</span>&#125;</span><br><span class="line">    print(<span class="string">"data bcast to others"</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    data = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">data = comm.bcast(data, root=<span class="number">1</span>)</span><br><span class="line">print(<span class="string">"process &#123;&#125; has received data"</span>.format(rank))</span><br></pre></td></tr></table></figure><h3 id="scatter"><a href="#scatter" class="headerlink" title="scatter"></a>scatter</h3><h4 id="简介-4"><a href="#简介-4" class="headerlink" title="简介"></a>简介</h4><p>将一个process的数据拆分成n份，发送给所有在通信池中的process每个一份，和bcast的区别在于，bcast发送的数据对于每一个process都是一样的，而scatter是将一份数据拆分成n份分别发送给每个process。<br>comm.scatter(data, dest, tag)</p><h4 id="代码示例-4"><a href="#代码示例-4" class="headerlink" title="代码示例"></a>代码示例</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> mpi4py</span><br><span class="line"><span class="keyword">from</span> mpi4py <span class="keyword">import</span> MPI</span><br><span class="line"></span><br><span class="line">comm = MPI.COMM_WORLD</span><br><span class="line">size = comm.Get_size()</span><br><span class="line">rank = comm.Get_rank()</span><br><span class="line"></span><br><span class="line">recv_data = <span class="literal">None</span></span><br><span class="line"><span class="keyword">if</span> rank == <span class="number">1</span>:</span><br><span class="line">    send_data = range(size) </span><br><span class="line">    print(<span class="string">"data bcast to others"</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    send_data = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">recv_data = comm.scatter(send_data, root=<span class="number">1</span>)</span><br><span class="line">print(<span class="string">"process &#123;&#125; has received data &#123;&#125;"</span>.format(rank, recv_data))</span><br></pre></td></tr></table></figure><h3 id="gather"><a href="#gather" class="headerlink" title="gather"></a>gather</h3><h4 id="简介-5"><a href="#简介-5" class="headerlink" title="简介"></a>简介</h4><p>和comm.bcast相反，将每个process中的数据收集到一个process中。<br>comm.gather(data, dest, tag)</p><h4 id="代码示例-5"><a href="#代码示例-5" class="headerlink" title="代码示例"></a>代码示例</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> mpi4py</span><br><span class="line"><span class="keyword">from</span> mpi4py <span class="keyword">import</span> MPI</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">comm = MPI.COMM_WORLD</span><br><span class="line">size = comm.Get_size()</span><br><span class="line">rank = comm.Get_rank()</span><br><span class="line"></span><br><span class="line">send_data = rank</span><br><span class="line">print(<span class="string">"process &#123;&#125; send data &#123;&#125; to root."</span>.format(rank, send_data))</span><br><span class="line"></span><br><span class="line">recv_data = comm.gather(send_data, root=<span class="number">9</span>)</span><br><span class="line"><span class="keyword">if</span> rank == <span class="number">9</span>:</span><br><span class="line">    print(<span class="string">"process &#123;&#125; gather all data &#123;&#125; to others."</span>.format(rank, recv_data))</span><br></pre></td></tr></table></figure><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>1.<a href="https://zhuanlan.zhihu.com/p/25332041" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/25332041</a><br>2.<a href="https://www.jianshu.com/p/f497f3a5855f" target="_blank" rel="noopener">https://www.jianshu.com/p/f497f3a5855f</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;MPI&quot;&gt;&lt;a href=&quot;#MPI&quot; class=&quot;headerlink&quot; title=&quot;MPI&quot;&gt;&lt;/a&gt;MPI&lt;/h2&gt;&lt;p&gt;MPI全名是Message Passing Interface，它是一个标准，而不是一个实现，专门为进程间通信实现的。它的工作原理很
      
    
    </summary>
    
      <category term="python" scheme="http://mxxhcm.github.io/categories/python/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="mpi4py" scheme="http://mxxhcm.github.io/tags/mpi4py/"/>
    
      <category term="多进程" scheme="http://mxxhcm.github.io/tags/%E5%A4%9A%E8%BF%9B%E7%A8%8B/"/>
    
  </entry>
  
  <entry>
    <title>gradient method deep deterministic policy gradient</title>
    <link href="http://mxxhcm.github.io/2019/10/06/gradient-method-deep-deterministic-policy-gradient/"/>
    <id>http://mxxhcm.github.io/2019/10/06/gradient-method-deep-deterministic-policy-gradient/</id>
    <published>2019-10-06T02:17:25.000Z</published>
    <updated>2019-10-06T12:17:08.471Z</updated>
    
    <content type="html"><![CDATA[<h2 id="ddpg">ddpg</h2><p>论文名称：CONTINUOUS CONTROL WITH DEEP REINFORCEMENT LEARNING<br>论文地址：<a href="https://arxiv.org/pdf/1509.02971.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1509.02971.pdf</a></p><h3 id="摘要">摘要</h3><p>本文将DQN的思路推广到continuous action domain上。DQN是离散空间，DDPG是连续空间。</p><h3 id="简介">简介</h3><p>强化学习的目标是学习一个policy最大化$J=\mathbb{E}_{r_i,s_i\sim E, a_i\sim \pi}\left[R_1\right]$的expected return。<br>简要回顾以下action-value的定义，它的定义是从状态s开始,采取action a，采取策略$\pi$得到的回报的期望。<br>$$Q{\pi}(s_t,a_t) = \mathbb{E}_{r_{i\ge t}, s_{i \gt t}\sim E,a_{i\gt t}\sim \pi}\left[R_t|s_t,a_t\right] \tag{1}$$<br>（注意，这里$R$的下标和reinforcement learning an introduction中的定义不一样，但是这个无所谓，只要在用的时候保持统一就好了。）<br>许多rl方法使用bellman方程递归的更新Q:<br>$$Q{\pi}(s_t,a_t) = \mathbb{E}_{r_t,s_{t+1}\sim E}\left[r(s_t,a_t) + \gamma\mathbb{E}_{a_{t+1}\sim\pi}\left[Q^{\pi} (s_{t+1},a_{t+1})\right]\right]\tag{2}$$<br>如果target policy是deterministic的话，用$\mu$表示，那么就可以去掉式子里面的期望，action是deterministic的而不是服从一个概率分布：<br>$$Q{\mu}(s_t,a_t) = \mathbb{E}_{r_t,s_{t+1}\sim E}\left[r(s_t,a_t) + \gamma Q^{\mu} (s_{t+1},\mu(s_{t+1}))\right] \tag{3}$$<br>而第一个期望只和environment相关。这就意味着可以使用off-policy方法学习$Q{\mu}$。<br>在DQN中，作者使用replay buffer和target network缓解了non-linear funnction approximator不稳定的问题，作者在这篇文章将它们推广到了DDPG上面。</p><h3 id="ddpg-v2">DDPG</h3><p>直接将Q-learning推广到continuous action space是不可行的，因为action是continuous的，对其进行max等greedy操作是不可行的。这种优化方法只适合trival action spaces的情况。所以这里使用的是DPG(deterministic policy gradient)，将其推广到non-linear case，DPG是一种actor-critic的方法。<br>DPG使用一个参数化的actor function $\mu(s|\theta{\mu})$作为当前的policy，它将一个states直接mapping到一个specific action。$Q(s,a)$作为critic使用Q-learning中的Bellman公式进行更新。Actor的更新直接应用chain rule到$J$的expected reutrn ，更新actor的参数如下：<br>\begin{align*}<br>\nabla_{\theta{\mu}} &amp;\approx \mathbb{E}_{s_t\sim \rho^{\beta} }\left[\nabla_{\theta^{\mu} }Q(s,a|\theta^Q )|_{s=s_t, a= \mu(s_t|\theta^{\mu} )}\right]\\<br>&amp;= \mathbb{E}_{s_t\sim \rho{\beta}}\left[\frac{\partial Q(s,a|\theta^Q )}{\partial\theta^{\mu} }|_{s=s_t, a= \mu(s_t|\theta^{\mu} )}\right]\\<br>&amp;= \mathbb{E}_{s_t\sim \rho{\beta}}\left[\frac{\partial Q(s,a|\theta^Q )}{\partial a}|_{s=s_t, a= \mu(s_t)}\frac{\partial \mu(s_t|\theta^{\mu} )}{\partial\theta^{\mu} }|_{s=s_t}\right]\\<br>&amp;= \mathbb{E}_{s_t\sim \rho{\beta}}\left[\nabla_a Q(s,a|\theta^Q )|_{s=s_t, a= \mu(s_t)} \nabla_{\theta_{\mu}} \mu(s|\theta_{\mu})|_{s=s_t}\right]\\ \tag{4}<br>\end{align*}<br>中间的两行是我自己加的，不知道对不对，DPG论文中有证明，还没有看到，等到读完以后再说补充把。</p><h4 id="contributions">Contributions</h4><p>本文的几个改进：</p><ol><li>使用replay buffer，</li><li>使用target network解决不稳定的问题。</li><li>使用了batch-normalization。</li><li>exploration。off policy的一个优势就是target policy和behaviour policy可以不同。本文使用的behaviour policy $\mu’$ 添加了一个从noise process $N$中采样的noise：<br>$$\mu(s_t) = \mu(s_t|\theta_t{\mu}) + N \tag{5}$$</li></ol><h4 id="算法">算法</h4><p>算法1 DDPG<br>随机初始化critic 网络$Q(s,a |\theta Q)$，和actor网络$\mu(s|\theta^{\mu} )$的权重$\theta^Q $和$\theta^{\mu} $<br>初始化target networks　$Q’$和$\mu’$的权重$\theta{Q’}\leftarrow \theta^Q ,\theta^{\mu’} \leftarrow \theta^{\mu} $<br>初始化replay buffer $R$<br><strong>for</strong> episode = 1, M <strong>do</strong><br>初始化一个随机process $N$用于exploration<br>receive initial observation state $s_1$<br>for $t=1, T$ do<br>根据behaviour policy选择action $a_t = \mu(s_t| \theta{\mu}) + N_t$<br>执行action $a_t$，得到$r_t$和$s_{t+1}$<br>将transition $s_t, a_t, r_t, s_{t+1}$存到$R$<br>从$R$中采样$N$个transition $s_i, a_i, r_i, s_{i+1}$<br>设置target value $y_i = r_i + \gamma Q’(s_{i+1}, \mu’(s_{i+1}|\theta{\mu’})|\theta^{Q’} )$<br>使用$L = \frac{1}{N}\sum_i(y_i-Q(s_i,a_i|\theta Q))^2 $更新critic<br>使用sampled policy gradient 更新acotr:<br>$$\nabla_{\theta{\mu}}\approx \frac{1}{N}\sum_i\nabla_a Q(s,a|\theta^Q )|_{s=s_i, a=\mu(s_i)}\nabla_{\theta^{\mu} }\mu(s|\theta^{\mu} )|_{s_i}$$<br>更新target networks:<br>$$\theta’\leftarrow \tau \theta + (1-\tau) \theta’$$<br>end for<br>end for</p><h3 id="实验">实验</h3><p>所有任务中，都使用了low-dimensional state和high-dimensional renderings。在DQN中，为了让问题在high dimensional environment中fully observable，使用了action repeats。在agent的每一个timestep中，进行$3$个timesteps的仿真，包含repeating action以及rendering。因此agent的observation包含$9$个feature maps（RGB，每一个有3个renderings），可以让agent推理不同frames之间的differences。frames进行下采样，得到$64\times 64$的像素矩阵，然后$8$位的RGB值转化为$[0,1]$之间的float points。<br>在训练的时候，周期性的进行test，test时候的不需要exploration noise。实验表明，去掉不同的组件，即contribution中的几点之后，结果都会比原来差。没有使用target network的话，结果尤其差。<br>作者使用了两个baselines normalized scores，第一个是naive policy，在action space中均匀的采样action得到的mean return，第二个是iLQG。normalized之后，naive policy的mean score是0，iLQG的mean score是$1$。DDPG能够学习到好的policy，在某些任务上甚至比iLQG还要好。</p><h2 id="参考文献">参考文献</h2><p>1.<a href="https://arxiv.org/pdf/1509.02971.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1509.02971.pdf</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;ddpg&quot;&gt;ddpg&lt;/h2&gt;
&lt;p&gt;论文名称：CONTINUOUS CONTROL WITH DEEP REINFORCEMENT LEARNING&lt;br&gt;
论文地址：&lt;a href=&quot;https://arxiv.org/pdf/1509.02971.pdf&quot; 
      
    
    </summary>
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="http://mxxhcm.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="policy gradient" scheme="http://mxxhcm.github.io/tags/policy-gradient/"/>
    
      <category term="ddpg" scheme="http://mxxhcm.github.io/tags/ddpg/"/>
    
  </entry>
  
  <entry>
    <title>reinforcement learning why use baseline ?</title>
    <link href="http://mxxhcm.github.io/2019/10/04/reinforcement-learning-why-use-baseline/"/>
    <id>http://mxxhcm.github.io/2019/10/04/reinforcement-learning-why-use-baseline/</id>
    <published>2019-10-04T07:46:00.000Z</published>
    <updated>2019-10-04T07:46:00.448Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>reinforcement learning importance sampling</title>
    <link href="http://mxxhcm.github.io/2019/09/27/reinforcement-learning-importance-sampling/"/>
    <id>http://mxxhcm.github.io/2019/09/27/reinforcement-learning-importance-sampling/</id>
    <published>2019-09-27T15:41:36.000Z</published>
    <updated>2019-09-30T04:27:02.181Z</updated>
    
    <content type="html"><![CDATA[<h2 id="importance-sampling">Importance Sampling</h2><p>Importance sampling是使用一个分布近似估计另一个分布期望的方法，即通过分布$q$计算分布$p$下$f(x)$的期望。通过从$q$中采样而不是从$p$中采样近似：<br>$$\mathbb{E}_p\left[f(x)\right] = \mathbb{E}_q\left[ \frac{p(x)f(x)}{q(x)}\right] \tag{1}$$<br>使用采样分布$q$估计分布$p$下的期望：<br>$$\mathbb{E}_p\left[f(x)\right] \approx \frac{1}{n} \sum_{i=1}^n \frac{p(x_i)f(x_i)}{q(x_i)} x_i\sim q\tag{2}$$<br>上面的公式需要满足$p(x_i)$不为$0$时，$q(x_i)$也不为$0$。直接计算$\mathbb{E}_p\left[f(x)\right]$和$\mathbb{E}_q\left[f(x)\right]$，一般来说是不同的，通过importance ratio调整权重，就可以使用$q$分布估计$p$分布的期望了。举个例子：<br>$$f(1) = 2, f(2) = 3, f(3) = 4, otherwise 0 \tag{3}$$<br>概率分布$p$为：$p(x=1) = 0, p(x=2) = \frac{1}{3},p(x=3) = \frac{2}{3}$，概率分布$q$为：$q(x=1) = \frac{1}{3}, q(x=2) = \frac{1}{3}, q(x=3) = \frac{1}{3}$。计算期望，$\mathbb{E}_p\left[f(x)\right] = \frac{11}{3}$，$\mathbb{E}_q\left[f(x)\right] = 3$<br>使用importance ratio进行权重调整：<br>\begin{align*}<br>\mathbb{E}_p\left[f(x)\right] &amp; = \mathbb{E}_q\left[\frac{q(x)}{p(x)}f(x)\right] \\<br>&amp; = \mathbb{E}_q\left[\frac{p(x=1)}{q(x=1)}f(x=1) \right] + \mathbb{E}_q\left[\frac{p(x=2)}{q(x=2)}f(x=2) \right] + \mathbb{E}_q\left[\frac{p(x=3)}{q(x=3)}f(x=3) \right] \\<br>&amp; = \frac{1}{3}*0 + \frac{1}{3}\frac{\frac{1}{3}}{\frac{1}{3}}*3 + \frac{1}{3}\frac{\frac{2}{3}}{\frac{1}{3}}*4\\<br>&amp; =\frac{11}{3}\\<br>\end{align*}<br>可以看出来，我们使用分布$q$估计除了分布$p$的期望。通过使用一个简单分布$q$进行采样，可以计算出$p$的期望。在RL中，通常通过复用old policy的sample trajectory学习current policy。</p><h2 id="optimal-importance-sampling">Optimal Importance Sampling</h2><p>Importance sampling使用采样近似估计$\mathbb{E}_p\left[f(x)\right]\approx \frac{1}{N}\sum_i \frac{p(x_i)}{q(x_i)}f(x_i)$近似计算$\mathbb{E}_p\left[f(x)\right]$。随着样本数量$N$的增加，期望值越准确。但是这种方法的方差很大，为了减少方差，样本分布$q$应该满足：<br>$$q(x) \propto p(x)\vert f(x)\vert \tag{4}$$<br>简单来说，为了减少方差，我们需要采样return更大的点。</p><h2 id="normalized-importanct-sampling">Normalized importanct sampling</h2><p>上面介绍的方法叫做unnormalized importance sampling。可以使用下里面的公式将unnormalized importance sampling转换为normalized importance sampling。<br>$$p(x) = \frac{\hat{p}(x)}{Z}\tag{5}$$<br>许多ML方法属于贝叶斯网络或者马尔科夫随机场，对于贝叶斯网络中，$p$很容易计算。但是当$p$是马尔科夫随机场时，$\sum\hat{p}(x)$是很难计算的。<br>\begin{align*}<br>\mathbb{E}_p\left[f(x)\right] &amp; = \int f(x) p(x) dx\\<br>&amp; = \int f(x) \frac{\hat{p}(x)}{Z} \frac{q(x)}{q(x)} dx\\<br>&amp; = \frac{\int f(x) \hat{p}(x) \frac{q(x)}{q(x)}dx}{Z}\\<br>&amp; = \frac{\int f(x) \hat{p}(x) \frac{q(x)}{q(x)} dx}{\int \hat{p}(x) dx}\\<br>&amp; = \frac{\int f(x) \hat{p}(x) \frac{q(x)}{q(x)} dx}{\int \hat{p}(x)\frac{q(x)}{q(x)} dx}\\<br>&amp; = \frac{\int f(x) q(x)\frac{\hat{p}(x)}{q(x)} dx}{\int q(x)\frac{\hat{p}(x)}{q(x)} dx}\\<br>&amp; = \frac{\int f(x) r(x)q(x) dx}{\int r(x)q(x) dx}\qquad\qquad 记r(x) = \frac{\hat{p}(x)}{q(x)}\\<br>\end{align*}<br>接下来用采样样本的求和近似积分求期望：<br>\begin{align*}<br>\mathbb{E}_p\left[f(x)\right] &amp; = \frac{\int f(x) r(x)q(x) dx}{\int r(x)q(x) dx}\qquad\qquad 记r(x) = \frac{\hat{p}(x)}{q(x)}\\<br>&amp; \approx \frac{\sum_i f(x^i) r^i }{\sum r^i}\qquad\qquad 其中 r^i = \frac{\hat{p}(x^i ) }{q(x^i ) }\\<br>&amp; = \sum_i f(x^i) r^i  \frac{r^i}{\sum_i r^i}\\<br>\end{align*}<br>通过计算<br>这就避免了计算$Z$，这种方法叫做normalized importance sampling。但是需要付出一定代价，unnormalized importance sampling是无偏的，而normalized importance是有偏的但是方差更小。</p><h2 id="importance-sampling-in-rl">Importance sampling in RL</h2><p>我们可以使用importance sampling方法从old policy $\pi’$采样估计new policy $\pi$的值函数。计算一个action的returns的代价很高，但是如果新的action和老的action很接近，importance sampling可以帮助我们利用old calculation计算新的returns。举个例子，在MC方法中，无论何时更新$\theta$，都需要根据新的trajectories计算returns。<br>$$\nabla_{\theta}J(\theta) = \frac{1}{N}\sum_{i=1}^T \left(\sum_{t=1}^T \nabla_{\theta} \log \pi_{\theta}(a_{i,t}|s_{i,t})\right)\left(\prod_{t=1}^T R(s_{i,t},a_{i,t})\right) \tag{6}$$<br>一个trajectory可以有几百个steps，单个的更新是非常低效的。有了importance sampling之后，我们可以基于old samples计算新的return。然而，如果两个policy差的太远，accuracy会降低。因此周期性的同步policy是非常必要的。<br>使用importance sampling，重写policy gradient的等式：<br>$$\nabla_{\theta}J(\theta) = \mathbb{E}_{\tau\sim\bar{\pi}(\tau)}\left[\sum_{t=1}^T \nabla_{\theta} \log \pi_{\theta}(a_t|s_t)\left(\prod_{t’=1}^T \frac{\pi_{\theta}(a_{t’}|s_{t’})}{\hat{\pi}_{\theta}(a_{t’}|s_{t’})}\right)\left(\prod_{t’=t}^T R(s_{t’},a_{t’})\right)\right]\tag{7}$$<br>为了约束policy的变化，可以加入trust region约束条件，在这个region内，我们认为使用importance sampling得到的结果是可信的：<br>$$\max_{\theta} \hat{\mathbb{E}}_t\left[\frac{\pi_{\theta}(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t}\hat{A}_t\right]\tag{8}$$<br>$$s.t. \hat{\mathbb{E}}_t\left[\text{KL}\left[\pi_{\theta_{old}}(\cdot|s_t),\pi_{\theta}(\cdot|s_t)\right]\right]\tag{9}$$</p><h2 id="参考文献">参考文献</h2><p>1.<a href="https://medium.com/@jonathan_hui/rl-importance-sampling-ebfb28b4a8c6" target="_blank" rel="noopener">https://medium.com/@jonathan_hui/rl-importance-sampling-ebfb28b4a8c6</a><br>2.<a href="http://webee.technion.ac.il/people/shimkin/MC15/MC15lect4-ImportanceSampling.pdf" target="_blank" rel="noopener">http://webee.technion.ac.il/people/shimkin/MC15/MC15lect4-ImportanceSampling.pdf</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;importance-sampling&quot;&gt;Importance Sampling&lt;/h2&gt;
&lt;p&gt;Importance sampling是使用一个分布近似估计另一个分布期望的方法，即通过分布$q$计算分布$p$下$f(x)$的期望。通过从$q$中采样而不是从$p$
      
    
    </summary>
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="important sampling" scheme="http://mxxhcm.github.io/tags/important-sampling/"/>
    
      <category term="重要性采样" scheme="http://mxxhcm.github.io/tags/%E9%87%8D%E8%A6%81%E6%80%A7%E9%87%87%E6%A0%B7/"/>
    
  </entry>
  
  <entry>
    <title>gradient method proximal policy optimization</title>
    <link href="http://mxxhcm.github.io/2019/09/23/gradient-method-proximal-policy-optimization/"/>
    <id>http://mxxhcm.github.io/2019/09/23/gradient-method-proximal-policy-optimization/</id>
    <published>2019-09-23T08:57:43.000Z</published>
    <updated>2019-09-27T02:33:39.187Z</updated>
    
    <content type="html"><![CDATA[<h2 id="abstract">Abstract</h2><p>标准的policy gradietn每一次更新需要一个样本，本文提出的PPO能够使用minibatch更新。PPO有着TRPO的优势，但是更容易实现，更普遍，更好的采样复杂度。</p><h2 id="introduction">Introduction</h2><h3 id="policy-gradient">Policy Gradient</h3><p>目标函数：<br>$$ L^{PG} (\theta) = \hat{\mathbb{E}}_t \left[\log \pi_{\theta}(a_t|s_t)\hat{A}_t \right] \tag{1}$$<br>约束条件：<br>$$\vert d\theta\vert^2 \le \delta \tag{2}$$</p><h3 id="natural-policy-gradient">Natural Policy Gradient</h3><p>目标函数：<br>$$ L^{NPG} (\theta) = \hat{\mathbb{E}}_t \left[\log \pi_{\theta}(a_t|s_t)\hat{A}_t  \right]\tag{3}$$<br>约束条件：<br>$$\hat{\mathbb{E}}_t\left[\text{KL}\left[\pi_{old}(\cdot|s_t), \pi_{\theta}(\cdot|s_t)\right] \right] \tag{4}$$<br>等价于<br>$$\frac{1}{2} d\theta^T \text{H} d\theta \le \delta \tag{5}$$</p><h3 id="trust-region-policy-optimization">Trust Region Policy Optimization</h3><p>目标函数：<br>$$ L^{PG} (\theta) = \hat{\mathbb{E}}_t \left[\frac{\pi_{\theta}(a_t|s_t)}{\pi_{old}(a_t|s_t)}\hat{A}_t \right]\tag{6}$$<br>约束条件：<br>$$\hat{\mathbb{E}}_t\left[\text{KL}\left[\pi_{old}(\cdot|s_t), \pi_{\theta}(\cdot|s_t)\right] \right] \tag{7}$$</p><h3 id="proximal-policy-optimization">Proximal Policy Optimization</h3><p>目标函数：<br>$$L^{PPO}(\theta) =\hat{\mathbb{E}}_t \left[L_t^{CLIP+VF+S}(\theta) - \beta\text{KL}\left[\pi_{old}(\cdot|s_t), \pi_{\theta}(\cdot|s_t) \right] \right] \tag{8}$$<br>其中$S$表示entropy bonus。<br>$$L_t^{CLIP}(\theta) = \hat{\mathbb{E}}_t \left[\min(\frac{\pi_{\theta}(\cdot|s_t)}{\pi_{old}(\cdot|s_t)},\ clip(\frac{\pi_{\theta}(\cdot|s_t)}{\pi_{old}(\cdot|s_t)}, 1-\epsilon, 1+\epsilon) \hat{A}_t) \right]\tag{9}$$<br>$$L_t^{VF} = (V_{\theta}(s_t) - V_t^{targ} )^2 \tag{10}$$</p><h2 id="参考文献">参考文献</h2><p>1.<a href="https://arxiv.org/pdf/1707.06347.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1707.06347.pdf</a><br>2.<a href="https://medium.com/@jonathan_hui/rl-proximal-policy-optimization-ppo-explained-77f014ec3f12" target="_blank" rel="noopener">https://medium.com/@jonathan_hui/rl-proximal-policy-optimization-ppo-explained-77f014ec3f12</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;标准的policy gradietn每一次更新需要一个样本，本文提出的PPO能够使用minibatch更新。PPO有着TRPO的优势，但是更容易实现，更普遍，更好的采样复杂度。&lt;/p&gt;
&lt;h2 id=&quot;intr
      
    
    </summary>
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="policy gradient" scheme="http://mxxhcm.github.io/tags/policy-gradient/"/>
    
      <category term="proximal policy optimization" scheme="http://mxxhcm.github.io/tags/proximal-policy-optimization/"/>
    
      <category term="ppo" scheme="http://mxxhcm.github.io/tags/ppo/"/>
    
  </entry>
  
  <entry>
    <title>Asymptotically Efficient 渐进有效性</title>
    <link href="http://mxxhcm.github.io/2019/09/18/asymptotically-efficient-%E6%B8%90%E8%BF%9B%E6%9C%89%E6%95%88%E6%80%A7/"/>
    <id>http://mxxhcm.github.io/2019/09/18/asymptotically-efficient-渐进有效性/</id>
    <published>2019-09-18T06:55:50.000Z</published>
    <updated>2019-09-18T07:50:19.665Z</updated>
    
    <content type="html"><![CDATA[<h2 id="无偏估计的方差下界-cramer-rao-bound">无偏估计的方差下界 cramer-rao bound</h2><p>理论上可以证明，任何无偏估计的方差都有一个下界，这个下界叫做cramer-rao bound。具体的证明好复杂，这里只是简单说一下。如果证明算法无偏估计量的方差的下界是cramer-rao bound，说明这个算法的下界已经没有优化了。。。这个下界其实很多时候不知道什么时候能取到，到时能给我们一定的信息，就像期望一样。</p><p>它的最简单形式是：任何无偏估计的方差至少大于fisher information的倒数。</p><h2 id="efficient">Efficient</h2><p>Efficient说的是在所有的无偏估计方法中，如果某种方法中无偏估计的方差等于cramer-rao bound，那么这个方法就是efficient。（应该是这样子吧。。。）</p><h2 id="asymptotically">Asymptotically</h2><p>在样本有效的情况下，统计量的方差不好计算。但是当样本不断增大时，方差会逐渐接近一个定值。用asymptotically修饰不断增大趋向于无穷的样本数量。</p><h2 id="asymptotically-efficient">Asymptotically  Efficient</h2><p>Asymptotically efficient指得是某种方法在小样本时可能不是efficient的，但是随着样本数量不断增加，变成了efficient的，这种方式就是asymptotically efficient的。</p><h2 id="参考文献">参考文献</h2><p>渐进有效性<br>1.<a href="https://www.zhihu.com/question/285834087/answer/446120288" target="_blank" rel="noopener">https://www.zhihu.com/question/285834087/answer/446120288</a><br>2.<a href="https://bbs.pinggu.org/thread-2139008-1-1.html" target="_blank" rel="noopener">https://bbs.pinggu.org/thread-2139008-1-1.html</a><br>3.<a href="https://www.zhihu.com/question/28908532/answer/254617423" target="_blank" rel="noopener">https://www.zhihu.com/question/28908532/answer/254617423</a><br>4.<a href="https://en.wikipedia.org/wiki/Efficiency_(statistics)#Asymptotic_efficiency" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Efficiency_(statistics)#Asymptotic_efficiency</a><br>5.<a href="https://cs.stackexchange.com/questions/69819/what-does-it-mean-by-saying-asymptotically-more-efficient" target="_blank" rel="noopener">https://cs.stackexchange.com/questions/69819/what-does-it-mean-by-saying-asymptotically-more-efficient</a><br>cramer-rao bound<br>6.<a href="https://www.zhihu.com/question/24710773/answer/117796031" target="_blank" rel="noopener">https://www.zhihu.com/question/24710773/answer/117796031</a><br>7.<a href="http://www2.math.ou.edu/~kmartin/stats/cramer-rao.pdf" target="_blank" rel="noopener">http://www2.math.ou.edu/~kmartin/stats/cramer-rao.pdf</a><br>8.<a href="https://www.zhihu.com/question/311561435/answer/607730638" target="_blank" rel="noopener">https://www.zhihu.com/question/311561435/answer/607730638</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;无偏估计的方差下界-cramer-rao-bound&quot;&gt;无偏估计的方差下界 cramer-rao bound&lt;/h2&gt;
&lt;p&gt;理论上可以证明，任何无偏估计的方差都有一个下界，这个下界叫做cramer-rao bound。具体的证明好复杂，这里只是简单说一下。如果证
      
    
    </summary>
    
      <category term="概率论" scheme="http://mxxhcm.github.io/categories/%E6%A6%82%E7%8E%87%E8%AE%BA/"/>
    
    
      <category term="渐进有效性" scheme="http://mxxhcm.github.io/tags/%E6%B8%90%E8%BF%9B%E6%9C%89%E6%95%88%E6%80%A7/"/>
    
      <category term="asymptotically efficient" scheme="http://mxxhcm.github.io/tags/asymptotically-efficient/"/>
    
      <category term="cramer-rao bound" scheme="http://mxxhcm.github.io/tags/cramer-rao-bound/"/>
    
  </entry>
  
  <entry>
    <title>位运算</title>
    <link href="http://mxxhcm.github.io/2019/09/13/bit_manipulation/"/>
    <id>http://mxxhcm.github.io/2019/09/13/bit_manipulation/</id>
    <published>2019-09-13T01:53:35.000Z</published>
    <updated>2019-09-28T14:37:29.419Z</updated>
    
    <content type="html"><![CDATA[<h2 id="按位异或">按位异或</h2><h3 id="定义">定义</h3><p>两个运算对象，相同为$0$，不同为$1$。</p><h3 id="应用">应用</h3><ul><li>任意两个相等的数亦或都为$0$。</li><li>$0$与任何数亦或都等于那个数。</li><li>两个有符号数异或，如果符号相同，结果大于$0$，否则小于$0$。</li></ul><p>只要谨记这两条规则就好。</p><h3 id="示例">示例</h3><p>$12 ^{} 7 = 11$</p><p>$12 = 1100$<br>$7 = 0111$<br>按位异或得到<br>$1011 = 11$</p><h3 id="交换律">交换律</h3><p>$a$^$b$ = $b$^$a$</p><h3 id="结合律">结合律</h3><p>$a$ ^ $b$ ^ $c$ = $a$ ^ ($b$ ^ $c$)</p><h4 id="扩展">扩展</h4><p>$a$ ^ $b$ ^ $c$ ^ $d$ ^ $a$ ^ $b$ ^ $c$ ^ $d$ ^ $e$ = $a$ ^ $a$ ^ $b$ ^ $b$ ^ $c$ ^ $c$ ^ $d$ ^ $d$ ^ $e$</p><h4 id="示例-v2">示例</h4><p>为什么呢？<br>$1$ ^ $2$ ^ $3$ ^ $4$ ^ $1$ ^ $2$ ^ $3$ ^ $4$ ^ $5$ = $1$ ^ $1$ ^ $2$ ^ $2$ ^ $3$ ^ $3$ ^ $4$ ^ $4$ ^ $5$<br>就相当于<br>$1 = 0001$<br>$2 = 0010$<br>$3 = 0011$<br>$4 = 0100$<br>$1 = 0001$<br>$2 = 0010$<br>$3 = 0011$<br>$4 = 0100$<br>$5 = 0101$<br>分别对每一位异或，对于每一位来说，其实他们都是没有顺序的，只需要统计每一位有多少个$0$和$1$即可了，重复偶数次的值都相互抵消了，剩下的就是没有抵消的那些。</p><h2 id="移位">移位</h2><h3 id="左移位">左移位</h3><p>$1 \ll 2 $<br>相当于<br>$0001 \ll 2 = 0100 = 4$</p><h3 id="右移位">右移位</h3><p>$8 \gg 2$<br>相当于<br>$1000 \gg 2 = 0010 = 2$</p><h2 id="按位与">按位与</h2><h3 id="定义-v2">定义</h3><p>两个运算对象，相同为$0$，不同为$1$。</p><h3 id="示例-v3">示例</h3><p>十进制的<br>$0, 1, 2, 3, 4, 5, 6, 7, 8$<br>对应的二进制为<br>$0000, 0001, 0010, 0011, 0100, 0101, 0110, 0111, 1000$<br>判断十进制数中每一个二进制位是否是$1$。<br>$1$分别左移$i=0,1,2,3$位，然后与要判断的二进制数进行按位与，如果不是$0$，则说明第$i$位为$1$。</p><h3 id="应用-v2">应用</h3><p>判断一个数是不是$2$的幂，再进一步就是统计一个数二进制位为$1$的个数。<br>$2$的幂有一个特征，就是只有一个二进制是$1$，其余的所有位都是$0$。<br>而$2$的幂减去$1$会得到所有的二进制位都是$1$，他们按位与，得到所有的二进制位是$0$，即$(2$ ^ $n)$&amp;$(2$ ^ $n-1) = 0$，实际上，这个公式会消去二进制位最右边的一个$1$。</p><h2 id="移位和与">移位和与</h2><p>移位和与结合起来判断某一位二进制位是否是$1$。</p><h3 id="示例-v4">示例</h3><p>判断$13$从右数的的第$3$个二进制位是否为$1$。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span>(<span class="number">13</span> &gt;&gt; <span class="number">1</span> &amp; <span class="number">1</span>)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"true\n"</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;按位异或&quot;&gt;按位异或&lt;/h2&gt;
&lt;h3 id=&quot;定义&quot;&gt;定义&lt;/h3&gt;
&lt;p&gt;两个运算对象，相同为$0$，不同为$1$。&lt;/p&gt;
&lt;h3 id=&quot;应用&quot;&gt;应用&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;任意两个相等的数亦或都为$0$。&lt;/li&gt;
&lt;li&gt;$0$与任何数亦或都等于
      
    
    </summary>
    
      <category term="算法" scheme="http://mxxhcm.github.io/categories/%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="按位与" scheme="http://mxxhcm.github.io/tags/%E6%8C%89%E4%BD%8D%E4%B8%8E/"/>
    
      <category term="按位或" scheme="http://mxxhcm.github.io/tags/%E6%8C%89%E4%BD%8D%E6%88%96/"/>
    
      <category term="按位异或" scheme="http://mxxhcm.github.io/tags/%E6%8C%89%E4%BD%8D%E5%BC%82%E6%88%96/"/>
    
      <category term="非" scheme="http://mxxhcm.github.io/tags/%E9%9D%9E/"/>
    
      <category term="移位" scheme="http://mxxhcm.github.io/tags/%E7%A7%BB%E4%BD%8D/"/>
    
      <category term="逻辑运算" scheme="http://mxxhcm.github.io/tags/%E9%80%BB%E8%BE%91%E8%BF%90%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>gradient method trust region policy optimization</title>
    <link href="http://mxxhcm.github.io/2019/09/08/gradient-method-trust-region-policy-optimization/"/>
    <id>http://mxxhcm.github.io/2019/09/08/gradient-method-trust-region-policy-optimization/</id>
    <published>2019-09-08T06:24:37.000Z</published>
    <updated>2019-10-07T11:33:08.455Z</updated>
    
    <content type="html"><![CDATA[<h2 id="trust-region-policy-optimization">Trust Region Policy Optimization</h2><p>作者提出了optimizing policies的一个迭代算法，理论上保证可以以non-trivial steps单调改善plicy。对经过理论验证的算法做一些近似，产生一个实用算法，叫做Trust Region Policy Optimization(TRPO)。这个算法和natural policy gradient很像，并且在大的非线性网络优化问题上有很高的效率。TRPO有两个变种，single-path方法应用在model-free环境中，vine方法，需要整个system能够能够从特定的states重启，通常在仿真环境中可用。</p><h2 id="introduction">Introduction</h2><p>为什么要有TRPO？</p><ol><li>policy gradient计算的是expected rewards梯度的最大方向，然后朝着这个方向更新policy的参数。因为梯度使用的是一阶导数，梯度太大时容易fail，梯度太小的话更新太慢。</li><li>学习率很难选择，学习率固定，梯度大容易失败，梯度小更新太慢。</li><li>如何限制policy，防止它进行太大的move。然后如何将policy的改变转换到model parameter的改变上。</li><li>采样效率很低。对整个trajectory进行采样，但是仅仅用于一次policy update。在一个trajectory中的states是很像的，尤其是用pixels表示时。如果在每一个timestep都改进policy的话，会一直在某一个局部进行更新，训练会变得很不稳定。</li></ol><h2 id="motivation">Motivation</h2><p>我们想要每一次策略$\pi$的更新，都能使得$\eta(\pi)$单调递增。要是能将它写成old poliy $\pi_{old}$和new policy $\pi_{new}$的关系式就好啦。给出这样一个关系式：<br>$$\eta(\pi_{new}) = \eta(\pi_{old}) + \mathbb{E}_{s_0, a_0, \cdots \sim \pi_{new}} \left[\sum_{t=0}^{\infty} \gamma^t A^{\pi_{old}}(s_t,a_t)\right] \tag{1}$$<br>证明：<br>\begin{align*}<br>\mathbb{E}_{s_0, a_0,\cdots\sim \pi_{new} }\left[\sum_{t=0}^{\infty} \gamma^t A^{\pi_{old}} (s_t,a_t) \right] &amp;=\mathbb{E}_{s_0, a_0,\cdots\sim \pi_{new}}\left[\sum_{t=0}^{\infty} \gamma^t (Q^{\pi_{old}} (s_t,a_t) - V^{\pi_{old}} (s_t))\right]  \\<br>&amp;=\mathbb{E}_{s_0, a_0,\cdots\sim \pi_{new}} \left[\sum_{t=0}^{\infty} \gamma^t ( R_{t+1} + \gamma V^{\pi_{old}} (s_{t+1}) -  V^{\pi_{old}} (s_t))\right]  \\<br>&amp;=\mathbb{E}_{s_0, a_0,\cdots\sim \pi_{new}} \left[\sum_{t=0}^{\infty} \gamma^t R_{t+1} + \sum_{t=0}^{\infty} \gamma^t (\gamma V^{\pi_{old}} (s_{t+1}) -  V^{\pi_{old}} (s_t))\right]  \\<br>&amp;=\mathbb{E}_{s_0, a_0,\cdots\sim \pi_{new}} \left[\sum_{t=0}^{\infty} \gamma^t R_{t+1} \right]+ \mathbb{E}_{s_0, a_0,\cdots\sim \pi_{new}} \left[\sum_{t=0}^{\infty} \gamma^t (\gamma V^{\pi_{old}} (s_{t+1}) -  V^{\pi_{old}} (s_t))\right]  \\<br>&amp;=\eta(\pi_{new}) + \mathbb{E}_{s_0, a_0,\cdots\sim \pi_{new}} \left[ -  V^{\pi_{old}} (s_0))\right]  \\<br>&amp;=\eta(\pi_{new}) - \mathbb{E}_{s_0, a_0,\cdots\sim \pi_{new}} \left[ V^{\pi_{old}} (s_0))\right]  \\<br>&amp;=\eta(\pi_{new}) - \eta(\pi_{old})\\<br>\end{align*}<br>将new policy $\pi_{new}$的期望回报表示为old policy $\pi_{old}$的期望回报加上另一项，只要保证这一项是非负的即可。其中$\mathbb{E}_{s_0, a_0,\cdots, \sim \pi_{new}}\left[\cdots\right]$表示actions是从$a_t\sim\pi_{new}(\cdot|s_t)$得到的。</p><h2 id="用求和代替期望">用求和代替期望</h2><p>代入$s$的概率分布$\rho_{\pi}(s) = P(s_0 = s) +\gamma P(s_1=s) + \gamma^2 P(s_2 = s)+\cdots, s_0\sim \rho_0$，并将期望换成求和：<br>\begin{align*}<br>\eta(\pi_{new}) &amp;= \eta(\pi_{old}) + \mathbb{E}_{s_0, a_0, \cdots \sim \pi_{new}} \left[\sum_{t=0}^{\infty} \gamma^t A^{\pi_{old}}(s_t,a_t)\right]\\<br>&amp;=\eta(\pi_{old}) +\sum_{t=0}^{\infty} \sum_s P(s_t=s|\pi_{new}) \sum_a \pi_{new}(a|s)\gamma^t A^{\pi_{old}}(s,a)\\<br>&amp;=\eta(\pi_{old}) +\sum_s\sum_{t=0}^{\infty} \gamma^t P(s_t=s|\pi_{new}) \sum_a \pi_{new}(a|s)A^{\pi_{old}}(s,a)\\<br>&amp;=\eta(\pi_{old}) + \sum_s \rho_{\pi_{new}}(s) \sum_a \pi_{new}(a|s) A^{\pi_{old}} (s,a) \tag{2}\\<br>\end{align*}<br>从上面的推导可以看出来，任何从$\pi_{old}$到$\pi_{new}$的更新，只要保证每个state $s$处的expected advantage是非负的，即$\sum_a \pi_{new}(a|s) A_{\pi_{old}}(s,a)\ge 0$，就能说明$\pi_{new}$要比$\pi_{old}$好，在$s$处，新的policy $\pi_{new}$:<br>$$\pi_{new}(s) = \arg\ \max_a A^{\pi_{old}} (s,a) \tag{3}$$<br>直到所有$s$处的$A^{\pi_{old}} (s,a)$为非正停止。当然，在实际应用中，因为各种误差，可能会有一些state的expected advantage是负的。</p><h2 id="rho-pi-old-s-近似-rho-pi-new-s-第一次近似">$\rho_{\pi_{old}}(s)$近似$\rho_{\pi_{new}}(s)$（第一次近似）</h2><p>式子$(2)$中包含的$\rho_{\pi_{new}}$依赖于未知的$\pi_{new}$，而我们已知的是$\pi_{old}$，忽略因为policy改变导致的state访问频率的改变，在$L_{\pi_{old}}(\pi_{new} )$中用$\rho_{\pi_{old}}(s)$近似$\rho_{\pi_{new}}(s)$。<br>\begin{align*}<br>\eta(\pi_{new}) &amp;= \eta(\pi_{old}) + \sum_s\rho_{\pi_{new}}(s)\sum_a\pi_{new}(a|s)A^{\pi_{old}} (s,a)\\<br>&amp; = \eta(\pi_{old}) + \mathbb{E}_{s\sim \pi_{new}, a\sim \pi_{new}}A^{\pi_{old}}(s,a)\\<br>&amp; = \eta(\pi_{old}) + \mathbb{E}_{s\sim \pi_{new}, a\sim \pi_{old}}\left[\frac{\pi_{new}(a|s)}{\pi_{old}(a|s)}A^{\pi_{old}}(s,a)\right]\tag{4}\\<br>\end{align*}</p><p>\begin{align*}<br>L_{\pi_{old}}(\pi_{new}) &amp; = \eta(\pi_{old}) + \sum_s\rho_{\pi_{old}}(s)\sum_a\pi_{new}(a|s)A^{\pi_{old}} (s,a)\\<br>&amp; = \eta(\pi_{old}) +\mathbb{E}_{s\sim \pi_{old}, a\sim \pi_{new}}A^{\pi_{old}}(s,a)\\<br>&amp; = \eta(\pi_{old}) +\mathbb{E}_{s\sim \pi_{old}, a\sim \pi_{old}}\left[\frac{\pi_{new}(a|s)}{\pi_{old}(a|s)}A^{\pi_{old}}(s,a)\right]\tag{5}\\<br>\end{align*}</p><p>用$\pi_{\theta}(a|s)$表示可导policy，用$\theta_{old}$表示$\pi_{old}$的参数。当$\pi_{new} = \pi_{old}$时，即$\theta=\theta_{old}$时，$L_{\pi_{old}}(\pi_{new})$和$\eta(\pi_{new})$的一阶导相等：<br>$$L_{\pi_{old}}(\pi_{new}) = \eta(\pi_{old}) + \sum_s\rho_{\pi_{old}}(s)\sum_a\pi_{old}(a|s)A^\pi_{old}(s,a) = \eta(\pi_{new})\tag{6}$$<br>$$\nabla_{\theta} L_{\pi_{old}}(\pi_{new})|_{\theta=\theta_{old}} = \mathbb{E}_{s\sim \pi_{old}, a\sim \pi_{old}}\left[\frac{\nabla_{\theta}\pi_{new}(a|s)}{\pi_{old}(a|s)}A^{\pi_{old}}(s,a)\right]|_{\theta_{old}}\tag{7} $$<br>$$\nabla_{\theta} \eta(\pi_{new})|_{\theta=\theta_{old}} =\mathbb{E}_{s\sim \pi_{new}, a\sim \pi_{old}}\left[\nabla_{\theta}\log\pi_{new}(a|s)A^{\pi_{old}}(s,a)\right]|_{\theta_{old}} \tag{8}$$<br>证明：<br>式子$(6)$将$\pi_{new}=\pi_{old}$代入即可。我对于式子$7$和$8$相等有疑问，为什么？<br>也就是说当$\pi_{new} = \pi_{old}$时，$L_{\pi_{old}}(\pi_{new})$和$\eta(\pi_{new})$是相等的，在$\pi_{old}$对应的参数$\theta$周围的无穷小范围内，可以近似认为它们依然相等，$\theta$进行足够小的step更新到达新的policy $\pi_{new}$，相应参数为$\theta_{\pi_{new}}$，在改进$L_{\pi_{old}}$同时也改进了$\eta$，但是这个足够小的step是多少是不知道的。</p><h2 id="conservative-policy-iteration">conservative policy iteration</h2><p>为了求出这个step到底是多少，有人提出了conservative policy iteration算法，该算法提供了$\eta$提高的一个lower bound。用$\pi_{old}$表示current policy，用$\pi’$表示使得$L_{\pi_{old}}$取得最大值的policy，$\pi’ = \arg\ \min_{\pi’} L_{\pi_{old}}(\pi’)$，新的policy $\pi_{new}$定义为：<br>$$\pi_{new}(a|s) = (1-\alpha) \pi_{old}(a|s)+\alpha\pi’(a|s) \tag{9}$$<br>可以证明，新的policy $\pi_{new}$和老的policy $\pi_{old}$之间存在以下关系：<br>$$\eta(\pi_{new})\ge L_{\pi_{old}}(\pi_{new}) - \frac{2\epsilon \gamma}{(1-\gamma(1-\alpha))(1-\gamma)}\alpha^2 $$<br>$$\epsilon = \max_s \vert\mathbb{E}_{a\sim\pi’}\left[A^{\pi} (s,a)\right]\vert \tag{10}, \alpha,\gamma\in [0,1]$$<br>证明：<br>进行缩放得到：<br>$$\eta(\pi_{new})\ge L_{\pi_{old}}(\pi_{new}) - \frac{2\epsilon \gamma}{(1-\gamma)^2 }\alpha^2 \tag{11}$$</p><h2 id="通用随机策略单调增加的证明">通用随机策略单调增加的证明</h2><p>从公式$9$我们可以看出来，改进右边就一定能改进真实的performance $\eta$。然而，这个bound只适用于通过公式$7$生成的混合policy，在实践中，这类policy很少用到，而且限制条件很多。所以我们想要的是一个适用于任何stochastic policy的lower bound，通过提升这个bound提升$\eta$。<br>作者使用$\pi_{old}$和$\pi_{new}$之间的一个距离代替$\alpha$，将公式$8$扩展到了任意stochastic policy，而不仅仅是混合policy。这里使用的distance measure，叫做total variation divergence，对于离散的概率分布$p,q$来说，定义为：<br>$$D_{TV}(p||q) = \frac{1}{2} \sum_i \vert p_i -q_i \vert \tag{12}$$<br>定义$D_{TV}^{max}(\pi_{old}, \pi_{new})$为：<br>$$D_{TV}^{max} (\pi_{old}, \pi_{new}) = \max_s D_{TV}(\pi_{old}(\cdot|s) || \pi_{new}(\cdot|s))\tag{13}$$<br>让$\alpha = D_{TV}^{max}(\pi_{old}, \pi_{new})$，新的bound如下：<br>$$\eta(\pi_{new})\ge L_{\pi_{old}}(\pi_{new}) - \frac{4\epsilon \gamma}{(1-\gamma)^2 }\alpha^2 , \qquad\epsilon = \max_{s,a} \vert A^{\pi_{old}}(s,a)\vert \tag{14}$$<br>证明：<br>…</p><p>Total variation divergence和KL散度之间有这样一个关系：<br>$$D_{TV}(p||q)^2 \le D_{KL}(p||q) \tag{15}$$<br>证明：<br>…<br>让<br>$$D_{KL}^{max}(\pi_{old}, \pi_{new}) = \max_s D_{KL}(\pi_{old}(\cdot|s)||\pi_{new}(\cdot|s)) \tag{16}$$<br>从公式$12$中可以直接得到：<br>\begin{align*}<br>\eta(\pi_{new}) &amp;\ge L_{\pi_{old}}(\pi_{new}) - \frac{4\epsilon \gamma}{(1-\gamma)^2 }\alpha^2 \\<br>&amp;\ge L_{\pi_{old}}(\pi_{new}) - \frac{4\epsilon \gamma}{(1-\gamma)^2 }D_{KL}^{max}(\pi_{old}, \pi_{new}) \\<br>&amp; \ge L_{\pi_{old}}(\pi_{new}) - CD_{KL}^{max}(\pi_{old}, \pi_{new})\\<br>C &amp; =\frac{4\epsilon \gamma}{(1-\gamma)^2} \tag{17}<br>\end{align*}<br>根据公式$12$，我们能生成一个单调非递减的sequence：$\eta(\pi_0)\le \eta(\pi_1) \le \eta(\pi_2) \le \cdots$，记$M_i(\pi) = L_{\pi_i}(\pi) - CD_{KL}^{max}(\pi_i, \pi)$，有：<br>因为：<br>$$\eta(\pi_{i+1}) \ge M_i(\pi_{i+1})\tag{18}$$<br>$$\eta(\pi_i) = M_i(\pi_i)\tag{19}$$<br>上面的第一个式子减去第二个式子得到：<br>$$\eta(\pi_{i+1}) - \eta(\pi_i)\ge M_i(\pi_{i+1})-M_i(\pi_i) \tag{20}$$<br>在每一次迭代的时候，确保$M_i(\pi_{i+1}) - M_i(\pi_i)\ge 0$就能够保证$\eta$是非递减的，最大化$M_i$就能实现这个目标，$M_i$是miorize $\eta$的近似目标。这种算法是minorizaiton maximization的一种。</p><h2 id="参数化策略的优化-第二次近似">参数化策略的优化（第二次近似）</h2><p>前面几小节考虑的optimization问题时没有考虑$\pi$的参数化，并且假设所有的states都可以被evaluated。这一节介绍如何在有限的样本下和任意的参数化策略下，从理论基础推导出一个实用的算法。<br>用$\theta$表示参数化策略$\pi_{\theta}(a|s)$的参数$\theta$，将目标表示成$\theta$而不是$\pi$的函数，即用$\eta(\theta)$表示原来的$\eta(\pi_\theta)$，用$L_{\theta}(\hat{\theta})$表示$L_{\pi_{\theta}}(\pi_{\hat{\theta}})$，用$D_{KL}(\theta||\hat{\theta})$表示$D_{KL}(\pi_{\theta}||\pi_{\hat{\theta}})$。用$\theta_{old}$表示我们想要改进的policy参数。<br>上一小节我们得到$\eta(\theta) \ge L_{\theta_{old}}(\theta) - CD_{KL}^{max}(\theta_{old}, \theta)$，当$\theta = \theta_{old}$时取等。通过最大化等式右边，可以提高$\eta$的下界：<br>$$\max_{\theta}\left[L_{\theta_{old}}(\theta) - CD_{KL}^{max}(\theta_{old}, \theta)\right]\tag{21}$$<br>在实践中，如果使用上述理论中的penalty coefficient $C$，会导致steps size很小。一种方法是使用new policy 和old policy之间的KL散度进行约束，可以采取更大的steps，这个约束叫做trust region constraint:<br>$$\max_{\theta} L_{\theta_{old}} (\theta)$$<br>$$ s.t. D_{KL}^{max}(\theta_{old},\theta) \le \delta \tag{22}$$<br>这样会在state space的每一个state都有一个KL散度约束。由于约束太多，这个问题还是不能解。这里使用average KL divergence进行近似:<br>$$\bar{D}_{KL}^{\rho}(\theta_1, \theta_2) = \mathbb{E}_{s\sim \rho}\left[D_{KL}(\pi_{\theta_1}(\cdot|s) || \pi_{\theta_2}(\cdot|s))\right] \tag{23}$$<br>公式$22$变成：<br>$$\max_{\theta} L_{\theta_{old}} (\theta)$$<br>$$s.t. \bar{D}_{KL}^{\rho_{\theta_{old}}}(\theta_{old},\theta) \le \delta \tag{24}$$</p><h2 id="目标函数和约束的采样估计-第三次近似">目标函数和约束的采样估计（第三次近似）</h2><p>上一节介绍的是关于policy parameter的有约束优化问题，约束条件为每一次policy更新时限制policy变化的大小，优化expected toral reward $\eta$的一个估计值。这一节使用Monte Carlo仿真近似目标和约束函数。<br>代入$L_{\theta_{old}}$的等式，得到：<br>$$\max_{\theta}\sum_s \rho_{\theta_{old}}(s) \sum_a\pi_{\theta}(a|s)A_{\theta_{old}}(s,a)$$<br>$$s.t. \bar{D}_{KL}^{\rho_{\theta_{old}}}(\theta_{old},\theta) \le \delta \tag{25}$$<br>首先用期望$\frac{1}{1-\gamma}\mathbb{E}_{s\sim \rho_{\theta_{old}}}\left[\cdots\right]$代替目标函数中的$\sum_s\rho_{\theta_{old}}(s) \left[\cdots\right]$。接下来用$Q$值$Q_{\theta_{old}}$代替advantage $A_{\theta_{old}}$，结果多了一个常数项，不影响。最后使用importance smapling代替actions上的求和。使用$q$表示采样分布，$q$分布中单个的$s_n$对于loss函数的贡献在于：<br>$$\sum_a \pi_{\theta}(a|s_n) A_{\theta_{old}}(s_n,a) = \mathbb{E}_{a\sim q}\left[\frac{\pi_{\theta} (a|s_n) }{q(a|s_n)}A_{\theta_{old}}(s_n,a) \right]\tag{26}$$<br>上面的公式就是使用importance sampling代替求和。将$A$展开：<br>\begin{align*}<br>\sum_a \pi_{\theta}(a|s) A_{\theta_{old}}(s,a) &amp;= \sum_a \pi_{\theta}(a|s)\left( Q_{\theta_{old}}(s,a)  - V_{\theta_{old}}(s)\right)\\<br>&amp;= \sum_a \pi_{\theta}(a|s)Q_{\theta_{old}}(s,a)- \sum_a \pi_{\theta}(a|s)V_{\theta_{old}}(s)\\<br>&amp;= \sum_a \pi_{\theta}(a|s)Q_{\theta_{old}}(s,a)- V_{\theta_{old}}(s)\\<br>\end{align*}<br>将公式$25$的优化问题转化为：<br>$$\max_{\theta} \mathbb{E}_{s\sim\rho_{\theta_{old}}, a\sim q}\left[\frac{\pi_{\theta} (a|s) }{q(a|s)}Q_{\theta_{old}}(s,a)\right]$$<br>$$s.t. \mathbb{E}_{s\sim \rho_{\theta_{old}}}\left[D_{KL}(\pi_{\theta_{old}}(\cdot|s)||\pi_{\theta}(\cdot|s))\right]\le \delta \tag{27}$$<br>好了，前面给出各种证明和近似，终于给出了我们要解决的问题的数学公式，这部分是为了帮助我们理解。我们实际需要的是解这个有约束的优化问题，这也是代码中要实现的部分，具体怎么做，一句话，采样然后估计。用采样代替期望，用经验估计代替$Q$值。<br>介绍两种方法进行估计。第一个叫做single path，通常用在policy gradient estimation，基于单个轨迹的采样。第二个叫做vine，构建一个rollout set，从rollout set的每一个state处执行多个actions。这种方法经常用在policy iteration方法上。</p><h3 id="single-path">Single Path</h3><p>采样$s_0\sim \rho_0$，模拟policy $\pi_{\theta_{old}}$一些timesteps生成一个trajectory $s_0, a_0, s_1, a_1, \cdots, s_{T-1}, a_{T-1}, s_T$，因此$q(a|s) = \pi_{\theta_{old}}(a|s)$。根据trajectory对每一个state action pair $(s_t,a_t)$计算$Q_{\theta_{old}}(s,a)$。</p><h3 id="vine">Vine</h3><p>采样$s_0\sim \rho_0$，模拟policy $\pi_{\theta_i}$生成一系列trajectories。在这些trajectories选择一个具有$N$个states的子集，表示为$s_1, c\dots, s_N$，这个集合称为rollout set。对于rollout set中的每一个state $s_n$，根据$a_{n,k}\sim q(\cdot|s_n)$采样$K$个actions。任何$q(\cdot|s_n)$都行，在实践中，$q(\cdot|s_n) = \pi_{\theta_i}(\cdot|s_n)$适用于contionous problems，像机器人运动；而均匀分布适用于离散任务，如Atari游戏。<br>对于$s_n$处的每一个action $a_{n,k}$，从$s_n$和$a_{n,k}$处进行rollout，估计$\hat{Q}_{\theta_i}(s_n, a_{n,k})$。在小的有限action spaces情况下，我们可以对从给定状态任何可能的action生成一个rollout，单个$s_n$对$L_{\theta_{old}}$的贡献如下：<br>$$L_n(\theta) = \sum_{k=1}^K \pi_{\theta} (a_k|s_n) \hat{Q}(s_n, a_k)\tag{28}$$<br>其中action space是$\mathcal{A} = {a_1, a_2,\cdots, a_K}$。在大的连续state space中，可以使用importance sampling构建一个新的目标近似。从$s_n$处计算的$L_{\theta_{old}}$的self-normalized 估计是：<br>$$L_n(\theta) = \frac{\sum_{k=1}^K \frac{\pi_{\theta}(a_{n,k}|s_n)}{\pi_{\theta_{old}}(a_{n,k}|s_n)}\hat{Q}(s_n, a_{n,k})}{\sum_{k=1}^K \frac{\pi_{\theta}(a_{n,k}|s_n)}{\pi_{\theta_{old}}(a_{n,k}|s_n)}}\tag{29}$$<br>假设在$s_n$处执行了$K$个actions $a_{n,1}, a_{n,2}, \cdots, a_{n,K}$。Self-normalized 估计去掉了$Q$值baseline的需要。在$s_n\sim \rho(\pi)$上做平均，可以得到$L_{\theta_{old}}$和它的gradient的估计。<br>Vine比single path好的地方在于，给定相同数量的$Q$样本，目标函数的局部估计有更低的方差，也就是vine能更好的估计advantage。Vine的缺点在于，需要执行更多steps的模拟计算相应的advantage。此外，vine方法需要对rollout set 中的每一个state都生成多个trajectories，这就需要整个system可以重置到任意的一个state，而single path算法不需要，可以直接应用在真实的system中。</p><h2 id="实用算法">实用算法</h2><p>使用上面介绍的single path或者vine进行采样，给出两个算法。重复执行以下步骤：</p><ol><li>使用single path或者vine算法产生一系列state-action pairs，使用Monte Carlo估计相应的$Q$值；</li><li>利用样本计算公式$(27)$中目标函数和约束函数的估计值</li><li>使用共轭梯度和line search求出有约束优化问题的近似解，更新policy参数$\theta$，。</li></ol><p>在第$3$步中，使用KL散度的Hessian矩阵而不是协方差矩阵的梯度计算Fisher information matrix，即使用$\frac{1}{N}\sum_{n=1}^N \frac{\partial^2}{\partial \theta_j}D_{KL}(\pi_{\theta_{old}}(\cdot|s_n)||\pi_{\theta}(\cdot|s_n))$近似$A_{ij}$而不是$\frac{1}{N}\sum_{n=1}^N \frac{\partial}{\partial \theta_i}log(\pi_{\theta}(a_n|s_n))\frac{\partial}{\partial \partial_j}log(\pi_{\theta}(a_n|s_n))$。<br>这个实用算法和前面的理论关联如下：</p><ol><li>验证了优化使用KL散度进行约束的目标函数可以保证policy improvement是单调递增的。如果penalty系数$C$很大step会很小，我们想要减小这个系数。经验上来讲，很难选择一个鲁邦的penalty系数，所以我们使用一个KL散度上的一个hard constraint而不是一个penalty。</li><li>$D_{KL}^{max}(\theta_{old}, \theta)$是很难计算和估计的，所以将约束条件改成对期望$\bar{D}_{KL}(\theta_{old}, \theta)$进行约束。</li><li>本文的理论忽略了advantage function的近似误差。</li></ol><h2 id="和policy-gradient以及natural-policy-gradient的对比">和policy gradient以及natural policy gradient的对比</h2><p>Policy gradient和natural policy gradient可以看成特殊的trpo，它们可以统一在policy update框架下。<a href="http://mxxhcm.github.io/2019/09/07/gradient-method-natural-policy-gradient/">The natural policy gradient</a>可以看成公式$(24)$的一个特例：使用$L$的一个linear approximation，和$\bar{D}_{KL}$的一个二次估计，就变成了下面的优化问题：<br>$$\max_{\theta} \left[\nabla_{\theta}L_{\theta_{old}}(\theta)|_{\theta=\theta_{old}}\cdot (\theta-\theta_{old}) \right]$$<br>$$s.t. \frac{1}{2}(\theta_{old}-\theta)^T A(\theta_{old})(\theta_{old} - \theta)\le\delta\tag{30}$$<br>其中$A(\theta_{old})_{ij} = \frac{\partial}{\partial\theta_i}\frac{\partial}{\partial \theta_j}\mathbb{E}_{s\sim \rho_{\pi}}\left[D_{KL}(\pi(\cdot|s, \theta_{old})||\pi(\cdot|s, \theta))\right]_{\theta=\theta_{old}}$，更新公式是$\theta_{new} = \theta_{old}+\frac{1}{\lambda}A(\theta_{old})^{-1} \nabla_{\theta}L(\theta)|_{\theta=\theta_{old}}$，其中步长$\frac{1}{\lambda}$可以看成算法参数。这和trpo不同，在每一次更新都有constraint。尽管这个差别很小，实验表明它能改善在更大规模问题上算法的性能。<br>同样，也可以使用$l2$约束，推导出标准的<a href="http://mxxhcm.github.io/2019/09/07/gradient-method-policy-gradient/">policy gradient</a>如下：<br>$$\max_{\theta} \left[\nabla_{\theta} L_{\theta_{old}}(\theta)|_{\theta=\theta_{old}}\cdot (\theta - \theta_{old})\right] $$<br>$$s.t. \frac{1}{2}\vert \theta-\theta_{old}\vert^2 \le \delta\tag{31}$$</p><h2 id="trpo算法">TRPO算法</h2><p>TRPO应用了conjugate gradient方法到natural policy gradient，此外，natural policy gradient的trusted region很小，作者将它换成了一个更大的可以调节的值。二次近似可能会降低accuracy，这些可能会对policy的更新引入其他问题，造成performance的degrade。一种可能的解决办法是在进行更新之前先进行验证：</p><ul><li>新的policy和老的policy之间的的$\text{KL}$散度是不是小于$\delta$</li><li>$L(\theta) \ge 0$</li></ul><p>如果验证失败了，使用衰减因子$0\lt \alpha \lt 1$，减小natural policy gradient直到满足要求即可。下面的算法介绍了这种思想的line search solution：<br>算法 Line Search for TRPO<br>计算$\Delta_k = \alpha \hat{\text{F}}_k^{-1} \nabla\eta$<br>for $j=0,1,2,\cdots, t$ do<br>$\qquad$计算$\theta = \theta_k + \alpha^j \Delta_k$<br>$\qquad$If $L_{\theta_k}(\theta) \ge 0$或者$\bar{D}_{KL}(\theta||\theta_k) \le \delta$ then<br>$\qquad\qquad$接受这个更新， $\theta_{k+1} = \theta_k + \alpha^j \Delta_k$<br>$\qquad\qquad$break<br>$\qquad$end if<br>end for<br>TRPO将truncated natural policiy gradient(使用conjugate gradient)和line search结合起来：<br>算法 Trust Region Policy Optimization<br>输入：初始的policy参数$\theta_0$<br>for $k=0,1,2,\cdots$ do<br>$\qquad$使用policy $\pi_k = \pi(\theta_k)$收集trajectories到集合$\mathbb{D}_k$<br>$\qquad$估计优势函数$\hat{A}_t^{\pi_k}$<br>$\qquad$计算样本估计：<br>$\qquad\qquad$使用优势函数估计policy gradient $\nabla \eta(\theta)$<br>$\qquad\qquad$计算$\text{KL}$散度的海塞矩阵（fisher informaction matrix）$\text{H}$<br>$\qquad$使用共轭梯度算法计算$\hat{\nabla}\eta(\theta) \approx \text{H}^{-1} \nabla\eta(\theta)$<br>$\qquad$更新$\theta_{k+1} = \theta_k + \alpha \hat{\nabla}\eta(\theta)$<br>end for</p><h2 id="trpo的缺点">TRPO的缺点</h2><p>TRPO通过最小化二次泛函近似$\text{F}$的逆，很大程度减少了计算量。但是每一次更新参数还需要计算$\text{F}$。TRPO和其他policy gradient方法相比，采样效率很低，并且扩展性不好，对于很深的网络不适用，这就有了后来的<a href="https://mxxhcm.github.io/2019/09/23/gradient-method-proximal-policy-optimization/">PPO</a>和ACKTR。</p><h2 id="minorize-maximization-mm算法"><a href="https://mxxhcm.github.io/2019/09/25/mm/">Minorize-Maximization MM算法</a></h2><p><img src="/2019/09/08/gradient-method-trust-region-policy-optimization/mm.jpeg" alt="mm"><br>如上图所示，通过迭代的最大化下界函数局部地逼近expected reward。更详细的来说，随机的初始化$\theta$，在当前$\theta$下，找到下界$M$最接近expected reward $\eta$的点，然后将$M$的最优点作为下一次的$\theta$。不断的迭代，直到收敛到optimal policy。这样做有一个条件，就是$M$要比$\eta$容易优化。比如$M$是二次函数：<br>$$ax^2 + bx+c\tag{32}$$<br>用向量形式表示是：<br>$$g\cdot(\theta- \theta_{old}) - \frac{\beta}{2} (\theta- \theta_{old})^T F(\theta - \theta_{old})\tag{33}$$<br>是一个convex function。<br>为什么MM算法会收敛到optimal policy，如果$M$是下界的话，它不会跨过红线$\eta$。假设新的$\eta$中的new policy更低，那么blue线一定会越过$\eta$，和$M$是下界冲突。</p><h2 id="trust-region">Trust Region</h2><p>有两种优化方法：line search和trust region。Gradient descent是line search方法。首先确定下降的方向，然后超这个方向移动一步。而trust region中，首先确定我们想要探索的step size，然后直到在trust region中的optimal point。用$\delta$表示初始的maximum step size，作为trust region的半径：<br>$$max_{s\in \mathbb{R}^n} m_k(s), \qquad s.t. \vert s\vert \le \delta\tag{34}$$<br>$m$是原始目标函数$f$的近似，我们的目标是找到半径$\delta$范围$m$的最优点，迭代下去直到最高点。在运行时可以根据表面的曲率延伸或者压缩$\delta$控制学习的速度。如果在optimal point，$m$是$f$的一个poor approximator，收缩trust region。如果approximatation很好，就expand trust region。如果policy改变太多的话，可以收缩trust region。</p><h2 id="参考文献">参考文献</h2><p>Trust Region Policy Optimization<br>1.<a href="http://joschu.net/docs/thesis.pdf" target="_blank" rel="noopener">http://joschu.net/docs/thesis.pdf</a><br>2.<a href="https://arxiv.org/pdf/1502.05477.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1502.05477.pdf</a><br>3.<a href="https://medium.com/@jonathan_hui/rl-trust-region-policy-optimization-trpo-explained-a6ee04eeeee9" target="_blank" rel="noopener">https://medium.com/@jonathan_hui/rl-trust-region-policy-optimization-trpo-explained-a6ee04eeeee9</a><br>4.<a href="https://medium.com/@jonathan_hui/rl-trust-region-policy-optimization-trpo-part-2-f51e3b2e373a" target="_blank" rel="noopener">https://medium.com/@jonathan_hui/rl-trust-region-policy-optimization-trpo-part-2-f51e3b2e373a</a><br>5.<a href="https://people.eecs.berkeley.edu/~pabbeel/cs287-fa09/readings/KakadeLangford-icml2002.pdf" target="_blank" rel="noopener">https://people.eecs.berkeley.edu/~pabbeel/cs287-fa09/readings/KakadeLangford-icml2002.pdf</a><br>6.<a href="https://drive.google.com/file/d/0BxXI_RttTZAhMVhsNk5VSXU0U3c/view" target="_blank" rel="noopener">https://drive.google.com/file/d/0BxXI_RttTZAhMVhsNk5VSXU0U3c/view</a><br>7.<a href="https://zhuanlan.zhihu.com/p/26308073" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/26308073</a><br>8.<a href="https://zhuanlan.zhihu.com/p/60257706" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/60257706</a><br>9.<a href="http://rll.berkeley.edu/deeprlcourse/docs/lec5.pdf" target="_blank" rel="noopener">http://rll.berkeley.edu/deeprlcourse/docs/lec5.pdf</a><br>10.<a href="https://www.zhihu.com/question/316004388" target="_blank" rel="noopener">https://www.zhihu.com/question/316004388</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;trust-region-policy-optimization&quot;&gt;Trust Region Policy Optimization&lt;/h2&gt;
&lt;p&gt;作者提出了optimizing policies的一个迭代算法，理论上保证可以以non-trivial steps
      
    
    </summary>
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="reinforcement learning" scheme="http://mxxhcm.github.io/tags/reinforcement-learning/"/>
    
      <category term="gradient method" scheme="http://mxxhcm.github.io/tags/gradient-method/"/>
    
      <category term="trust region policy optimization" scheme="http://mxxhcm.github.io/tags/trust-region-policy-optimization/"/>
    
      <category term="trpo" scheme="http://mxxhcm.github.io/tags/trpo/"/>
    
  </entry>
  
  <entry>
    <title>gradient method natural policy gradient</title>
    <link href="http://mxxhcm.github.io/2019/09/07/gradient-method-natural-policy-gradient/"/>
    <id>http://mxxhcm.github.io/2019/09/07/gradient-method-natural-policy-gradient/</id>
    <published>2019-09-07T11:38:03.000Z</published>
    <updated>2019-09-27T07:57:58.938Z</updated>
    
    <content type="html"><![CDATA[<h2 id="a-natural-policy-gradient">A Natural Policy Gradient</h2><p>论文名称：A Natural Policy Gradient<br>论文地址：<a href="https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf" target="_blank" rel="noopener">https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf</a></p><h2 id="abstract">Abstract</h2><p>作者基于参数空间的底层结构提出了natural gradient方法，找出下降最快方向。尽管gradient方法不能过大的改变参数，它还是能够朝着选择greedy optimal action而不是更好的action方向移动。基于兼容值函数的policy iteration，在每一个improvement step选择greedy optimal action。</p><h2 id="introduction">Introduction</h2><p>直接的policy gradient在解决大规模的MDPs时很有用，这种方法基于future reward的梯度在满足约束条件的一类polices中找一个$\pi$，但是这种方法是non covariant的，简单来说，就是左右两边的维度不一致。<br>这篇文章基于policy的底层参数结构定义了一个metric，提出了一个covariant gradient方法，通过将它和policy iteration联系起来，可以证明natural gradient朝着选择greedy optimal action的方向移动。通过在简单和复杂的MDP中进行测试，结果表明这种方法中没有出现严重的plateau phenomenon。</p><h2 id="a-natural-gradient">A Natural Gradient</h2><p>定义average reward $\eta(\pi)$为：<br>$$\eta(\pi) = \sum_{s,a}\rho^{\pi} (s) \pi(a;s) R(s, a) \tag{1}$$<br>其中$R(s,a) = \mathbb{E}\left[R_{t+1}\right|s_t=s, a_t = a]$，state action value和value function定义如下：<br>$$Q^{\pi} (s,a) = \sum_{t=0}^{\infty} \mathbb{E}\left[R_t - \eta(\pi)|s_0=s,a_0=a,\pi\right], \forall s\in S, a\in A \tag{2}$$<br>$$V^{\pi} (s) = \mathbb{E}_{\pi(a’;s)}\left[Q^{\pi}(s,a’)\right] \tag{3}$$<br>计算average reward的精确梯度是（可以看<a href="http://mxxhcm.github.io/2019/09/22/gradient-method-policy-gradient/">policy gradient</a>的推导）：<br>$$\nabla\eta(\theta) = \sum_{s,a} \rho^{\pi} (s) \nabla \pi(a;s,\theta) Q^{\pi} (s,a) \tag{4}$$<br>使用$\eta(\theta)$代替了$\eta(\pi_{\theta})$。本文中定义$d\theta$的平方长度$\vert d\theta\vert^2 $和一个正定矩阵$\text{G}(\theta)$有关：<br>$$\vert d\theta\vert^2 = \sum_{ij} \text{G}_{ij} (\theta)d\theta_i d\theta_j = d\theta^T \text{G}(\theta) d\theta  \tag{5}$$<br>在$d\theta$的平方长度$\vert d\theta\vert^2 $ 等于一个常数时，求使得$\eta(\theta+d\theta)$下降的最快的$d\theta$方向。可以证明，最快的梯度下降方向是$\text{G}^{-1} \nabla \eta(\theta)$。标准的policy gradient假设$\text{G}=\text{I}$，所以最陡的下降方向是$\nabla\eta(\theta)$。本文作者的想法是选择一个其他的$\text{G}$，这个新的$G$对应的metric不根据坐标轴的变化而变化，而是跟着坐标参数化的mainfold变化，根据新的metric定义natural gradient。<br>给出策略$\pi(a;s,\theta)$的fisher information：<br>$$\text{F}_s(\theta) = \mathbb{E}_{\pi(a;s,\theta)} \left[\frac{\partial \log \pi(a;s,\theta)}{\partial \theta_i} \frac{\partial \log \pi(a;s,\theta)}{\partial \theta_j}\right] \tag{6}$$<br>显然$\text{F}_s$是正定矩阵，可以证明，FIM是概率分布参数空间上的一个invariant metric。不论两个点的坐标怎么选择，它都能计算处相同的distance，所以说它是invariant。当然，$\text{F}_s$使用了单个的$s$，而在计算average reward时，使用的是一个分布，定义$\text{F}$：<br>$$\text{F}(\theta) = \mathbb{E}_{\rho^{\pi} (s)} \left[\mathbb{F}_s (\theta)\right] \tag{7}$$<br>每一个$s$对应的单个$\text{F}_s$都和MDP的transition model没有关系，期望操作引入了对transition model参数的依赖。直观上来说，$\text{F}_s$测量的是在$s$上的probability manifold的距离，$\text{F}(\theta)$对它们进行了平均。对应的下降最快的方向是：<br>$$\hat{\nabla}\eta(\theta) =\text{F}(\theta)^{-1} \nabla\eta(\theta)  \tag{8}$$<br>为什么natural gradient下降最快的方向是这个方向，接下来我们进行证明。其实上面就是说的这些就是使用$\text{KL}$散度当做metric，而不是使用欧几里得metric。然后对$\text{KL}$散度进行约束，要找到使得目标函数$\eta(\theta)$最大的$d\theta$，需要知道哪个方向的$\text{KL}$散度上升的最快，目标函数：<br>$$d\theta^{*} = \arg \max \eta(\theta +d\theta) \tag{9}$$<br>$$s.t. \text{KL}\left[p_{\theta}||p_{\theta’}\right] = c \tag{10}$$<br>其中$c$是常数，确保更新在一定范围内，不受curvature的影响。目标函数的一阶泰勒展开公式如下：<br>\begin{align*}<br>\eta_{\theta’}(\theta) &amp; = \eta_{\theta’}(\theta’) + \left[\nabla_{\theta}\eta_{\theta’}(\theta)|_{\theta=\theta’}\right]^T (\theta’ + d\theta - \theta’) + \cdots \\<br>&amp; = \eta_{\theta’}(\theta’) + \left[\nabla_{\theta}\eta_{\theta’}(\theta)|_{\theta=\theta’}\right]^T d\theta + \cdots  \tag{11}\\<br>\end{align*}</p><p>引理$1$：$\text{KL}$散度在$\theta=\theta’$附近$\theta’ +d\theta, d\theta\rightarrow 0$处的二阶泰勒展开是：<br>$$\text{KL}\left[p(x|\theta’)||p(x|\theta’+d\theta)\right] \approx \frac{1}{2}d\theta^T \text{F}d\theta \tag{12}$$<br>证明：<br>\begin{align*}<br>\text{KL}\left[p_{\theta’}||p_{\theta’+d\theta}\right] &amp;\approx \text{KL}\left[p_{\theta’}||p_{\theta’}\right] + (\nabla_{\theta}\text{KL}\left[p_{\theta}||p_{\theta’}\right]|_{\theta=\theta’})^T (\theta’+d\theta -\theta’) \\<br>&amp;\qquad\qquad\qquad\qquad + \frac{1}{2} (\theta’ +d\theta -\theta’)^T (\nabla_{\theta}^2 \text{KL}\left[p_{\theta}||p_{\theta’}\right]|_{\theta=\theta’})(\theta’+d\theta-\theta’)\tag{13}\\<br>&amp; = \text{KL}\left[p_{\theta’}||p_{\theta’}\right] + (\nabla_{\theta}\text{KL}\left[p_{\theta}||p_{\theta’}\right]|_{\theta=\theta’})^T d\theta \\<br>&amp;\qquad\qquad\qquad\qquad + \frac{1}{2} d\theta^T (\nabla_{\theta}^2 \text{KL}\left[p_{\theta}||p_{\theta’}\right]|_{\theta=\theta’}) d\theta\tag{14}\\<br>&amp; = \text{KL}\left[p_{\theta’}||p_{\theta’}\right] + (\int_x p(x|\theta’)\nabla \log (p|\theta)|_{\theta=\theta’} dx)^T d\theta \\<br>&amp;\qquad\qquad\qquad\qquad + \frac{1}{2} d\theta^T (\nabla_{\theta}^2 \text{KL}\left[p_{\theta}||p_{\theta’}\right]|_{\theta=\theta’}) d\theta\tag{15}\\<br>&amp; = \text{KL}\left[p_{\theta’}||p_{\theta’}\right] + (\mathbb{E}_{p(x|\theta’)} \nabla\log p(x|\theta) dx|_{\theta=\theta’})^T d\theta \\<br>&amp;\qquad\qquad\qquad\qquad + \frac{1}{2} d\theta^T (\nabla_{\theta}^2 \text{KL}\left[p_{\theta}||p_{\theta’}\right]|_{\theta=\theta’}) d\theta\tag{16}\\<br>&amp; = 0 + 0 + \frac{1}{2} d\theta^T (\nabla_{\theta}^2 \text{KL}\left[p_{\theta}||p_{\theta’}\right]|_{\theta=\theta’}) d\theta\tag{17}\\<br>&amp; = \frac{1}{2} d\theta^T (\nabla_{\theta}^2 \text{KL}\left[p_{\theta}||p_{\theta’}\right]|_{\theta=\theta’}) d\theta\tag{18}\\<br>&amp; = \frac{1}{2} d\theta^T \text{H} d\theta\tag{19}\\<br>&amp; = \frac{1}{2} d\theta^T \text{F} d\theta\tag{20}\\<br>\end{align*}<br>这也是为什么$\vert d\theta\vert^2 $定义为$d\theta^T\text{G}\theta$的原因。使用拉格朗日乘子法将$\text{KL}$散度约束条件带入目标函数$\eta$：<br>\begin{align*}<br>d\theta^{*} &amp; = {\arg \min}_{d\theta} \eta(\theta’+d\theta) + \lambda(\text{KL}\left[p_{\theta’}||p_{\theta’+d\theta}\right] -c)\\<br>&amp; = {\arg \min}_{d\theta} L_{\theta’}(\theta’) + \left[\nabla_{\theta}L_{\theta’}(\theta)|_{\theta=\theta’}\right]^T d\theta + \lambda(\left[\frac{1}{2} d\theta^T \text{F} d\theta\right] -c)\tag{21}\\<br>\end{align*}<br>对$d\theta$求导，令其等于$0$，得：<br>\begin{align*}<br>&amp;0 + \nabla_{\theta}\eta_{\theta’}(\theta)|_{\theta=\theta’} + \text{F}d\theta + 0\\<br>=&amp; \nabla_{\theta}\eta_{\theta’}(\theta)|_{\theta=\theta’} + \text{F}d\theta \tag{22}\\<br>=&amp; 0\\<br>\end{align*}<br>求解得到：<br>$$d\theta= - \frac{1}{\lambda}\text{F}^{-1} \nabla_{\theta} \eta_{\theta’}(\theta) \tag{23}$$<br>所以natural gradient定义为：<br>$$\hat{\nabla}\eta(\theta) = \text{F}^{-1} \nabla_{\theta}\eta(\theta) \tag{24}$$</p><h2 id="the-natural-gradient-和-policy-iteration">The Natural Gradient 和 Policy Iteration</h2><p>使用$\omega$参数化的值函数$f^{\pi} (s,a;\omega)$近似$Q^{\pi} (s,a)$。</p><h3 id="natural-gradient-with-approximation-使用近似的自然梯度">Natural Gradient with Approximation（使用近似的自然梯度）</h3><p>定义：<br>$$\psi(s,a)^{\pi} = \nabla \log \pi(a;s, \theta)$$<br>$$f^{\pi} (s,a;\omega) = \omega^T \psi^{\pi} (s,a) \tag{25}$$<br>其中$\left[\nabla \log \pi(a;s, \theta)\right]_i = \frac{\partial \log \pi(a;s, \theta)}{\partial \theta_i}$。找到最小化均方根误差函数的$\omega$，记为$\hat{\omega}$：<br>$$\epsilon(\omega, \pi) = \sum_{s,a}\rho^{\pi} (s)\pi(a;s,\theta)(f^{\pi} (s,a;\omega) - Q^{\pi} (s,a))^2 \tag{26}$$<br>如果使用$f^{\pi} $代替$Q$计算出来的grdient还是exact的，就称$f$是兼容的。</p><h4 id="定理1">定理1</h4><p>如果$\hat{\omega}$是使得均方误差$\epsilon(\omega,\pi_\theta)$最小的$\omega$，可以证明：<br>$$\hat{\omega} = \hat{\nabla} \eta(\theta) =\text{F}(\theta)^{-1} \nabla\eta(\theta) =\text{F}(\theta)^{-1} \nabla\eta(\theta) \tag{27}$$<br>证明：<br>因为$\hat{\omega}$使得$\epsilon$最小，所以当$\omega = \hat{\omega}$时，$\frac{\partial \epsilon}{\partial \omega} = 0$，有：<br>$$\frac{\partial \epsilon}{\partial \omega} = \sum_{s,a}\rho^{\pi} (s) \pi(a|s;\theta) \psi^{\pi} (s,a) (\psi^{\pi} (s,a)^T \hat{\omega} - Q^{\pi} (s,a)) = 0 \tag{28}$$<br>移项合并同类项得：<br>$$\sum_{s,a}\rho^{\pi} (s) \pi(a|s;\theta) \psi^{\pi} (s,a) \psi^{\pi} (s,a)^T \hat{\omega} = \sum_{s,a}\rho^{\pi} (s) \pi(a|s;\theta) \psi^{\pi} (s,a)  Q^{\pi} (s,a) \tag{29}$$<br>根据定义$\psi(s,a)^{\pi} = \nabla \log \pi(a;s, \theta)$，而根据log-derativate trick：$\pi(a|s) \nabla \log \pi(a|s;\theta) = \nabla \pi(a|s;\theta)$，所以式子$(29)$右面就是$\nabla \eta$，而式子左面$\sum_{s,a}\rho^{\pi} (s) \pi(a|s;\theta) \psi^{\pi} (s,a) \psi^{\pi} (s,a)^T = \text{F}(\theta)$。最后得到：<br>$$ \text{F}(\theta)\hat{\omega} = \nabla\eta(\theta)$$</p><h3 id="greedy-policy-improvement">Greedy Policy Improvement</h3><p>在greedy policy improvement的每一步，在$s$处，选择$a\in \arg \max_{a’} f^{\pi}(s, a’;\hat{\omega})$。这一节介绍natural gradient能够找到best action，而不仅仅是一个good action。<br>首先考虑指数函数：$\pi(s;a,\theta) \propto e^{\theta^T \phi_{sa}}$，其中$\phi_{sa} \in \mathbb{R}^m $是特征向量。为什么使用指数函数，因为它是affine geometry。简单来说，就是$\pi(a;s,\theta)$的probability manifold可以被弯曲。接下来证明policy在natrual gradient方向上改进的一大步等价于进行一步greedy policy improvement的policy。</p><h4 id="定理2">定理2</h4><p>假设$\pi(s;a,\theta) \propto e^{\theta^T \phi_{sa}} $，$\hat{\nabla}\eta(\theta)$是非零的，并且$\hat{\omega}$是最小化均方误差的$\omega$。令<br>$$\pi_{\infty}(a;s) = lim_{\alpha\rightarrow \infty}\pi(a;s,\theta + \alpha\hat{\nabla}\eta(\theta)) \tag{30}$$<br>当且仅当$a\in \arg\max_{a’} f^{\pi} (s,a’;\hat{\omega})$时，有$\pi_{\infty}(a;s)\neq 0$。<br>证明：<br>根据定义：$f^{\pi} (s,a,\omega) = \omega^T \psi^{\pi} (s,a)$，由定理$1$可知：$\hat{\omega} = \text{F}^{-1} \nabla \eta(\theta) = \hat{\nabla} \eta(\theta)$，所以$f^{\pi}(s,a,\hat{\omega}) = \hat{\nabla}\eta(\theta)^T \psi^{\pi} (s,a)$。而根据定义$\psi^{\pi} (s,a) = \nabla \log \pi(a|s;\theta) = \phi_{sa} - \mathbb{E}_{\pi(a’|s;\theta)}(\phi_{sa’})$，$\mathbb{E}_{\pi(a’|s;\theta)}(\phi_{sa’})$不是$a$的函数，所以就有：<br>$$\arg\max_{a’}f^{\pi} (s,a’;\hat{\omega}) = \arg\max_{a’} \hat{\nabla}\eta(\theta)^T \phi_{sa}\tag{31}$$<br>和$\mathbb{E}_{\pi(a’|s;\theta)}(\phi_{sa’})$无关。。经过一个gradient step：<br>$$\pi(a|s;\theta+\alpha \hat{\nabla}\eta(\theta)) \propto e^{(\theta+\alpha \hat{\nabla}\eta(\theta))^T \phi_{sa}} \tag{32}$$<br>因为$\hat{\nabla}\eta(\theta) \neq 0$，很明显，当$\alpha\rightarrow \infty$时，$\hat{\nabla}\eta(\theta)^T\phi_{sa}$会dominate，所以只有当且仅当$a\in \arg\max_{a’} f^{\pi} (s,a’;\hat{\omega})$时，有$\pi_{\infty}(a;s)\neq 0$。<br>可以看出来natural gradient趋向于选择最好的action，而普通的gradient方法只能选出来一个更好的action。<br>使用指数函数的目的只是为了展示在极端情况下－－有无限大的learning rate情况下的结果，接下来给出的是普通的参数化策略的结果，natural gradient可以根据$Q^{\pi} (s,a)$的局部近似估计$f^{\pi}(s,a;\hat{\omega})$，近似找到局部best action。</p><h4 id="定理3">定理3</h4><p>假如$\hat{\omega}$最小化估计误差，使用$\theta’ = \theta + \alpha \hat{\nabla}\eta(\theta)$更新参数，可以得到：<br>$$\pi(a;s,\theta’) = \pi(a;s,\theta)(1+f^{\pi}(a,s,\hat{\omega})) + O(\alpha^2)\tag{33}$$<br>证明：<br>根据定理$1$，得到$\Delta \theta = \alpha\hat{\nabla}\eta(\theta) = \alpha\hat{\omega}$，然后利用一阶泰勒展开：<br>\begin{align*}<br>\pi(a|s;\theta’) &amp;= \pi(a|s;\theta) + \frac{\partial \pi(a|s;\theta)^T }{\partial\theta}\Delta\theta + O(\theta^2 ) \\<br>&amp;= \pi(a|s;\theta) + \frac{\partial\log \pi(a|s;\theta)^T }{\partial\theta}\pi(a|s;\theta)\Delta\theta + O(\theta^2 ) \\<br>&amp;= \pi(a|s;\theta)(1 + \frac{\partial\log \pi(a|s;\theta)^T }{\partial\theta}\Delta\theta) + O(\theta^2 ) \\<br>&amp;= \pi(a|s;\theta)(1 +  \psi(s, a)^T \Delta\theta) + O(\theta^2 ) \\<br>&amp;= \pi(a|s;\theta)(1 +  \psi(s, a)^T \alpha\hat{\omega}) + O(\alpha^2 ) \\<br>&amp;= \pi(a|s;\theta)(1 +  \alpha f^{\pi} (s, a, \hat{\omega})) + O(\alpha^2 ) \\<br>\end{align*}<br>这个相当于是根据$f^{\pi}(s,a) $选择每个state的action。当然，并不是选择greedy action就一定会改善policy，还有许多例外，这里就不细说了。</p><h2 id="metrics和curvatures">Metrics和Curvatures</h2><p>在不同的参数空间中，<a href="https://mxxhcm.github.io/2019/09/16/fisher-information/">fisher information</a>都可以收敛到<a href="https://mxxhcm.github.io/2019/09/10/Jacobian-matrix-and-Hessian-matrix/">海塞矩阵</a>，因此，它是<a href="https://mxxhcm.github.io/2019/09/18/asymptotically-efficient-%E6%B8%90%E8%BF%9B%E6%9C%89%E6%95%88%E6%80%A7/">aymptotically efficient</a>，即到达了cramer-rao bound。<br>$\text{F}$是$\log \pi$对应的fisher information。Fisher information 和海塞矩阵有关系，但是都需要和$\pi$联系起来。是这里考虑$\eta(\theta)$的海塞矩阵，它和$\text{F}$两个之间有一定联系，但是不一样。<br>事实上，定义的新的$\text{F}$并不会收敛到海塞矩阵。但是因为海塞矩阵一般不是正定的，所以在非局部最小处附近，它提供的curvature信息用处不大。在局部最小处使用conjugate methods会更好。</p><h2 id="truncated-natural-policy-gradient">Truncated Natural Policy Gradient</h2><p>Natural policy gradient需要计算$\delta \theta = \alpha \hat{\nabla}\eta(\theta) = \alpha\text{F}^{-1}\nabla(\eta)$。<br>需要计算费舍尔信息矩阵（$\text{KL}$散度的海塞矩阵）$\text{F}$以及逆矩阵$\text{F}^{-1} $。寻找deep networks逆的代价很大，而且通常是数值不稳定的，我们想要不计算FIM的逆，而直接计算：<br>$$\hat{\nabla}\eta(\theta) = \text{F}^{-1} \nabla\eta(\theta)$$<br>进而转化成求解：<br>$$\text{F}^{-1} \hat{\nabla}\eta(\theta) = \nabla\eta(\theta)$$<br>因为$\text{F}$是一个对称矩阵，将原问题转化为：<br>$$\min_{x\in \mathbb{R}^n } \frac{1}{2}x^T \text{F}x - g^T x$$<br>这个问题可以使用<a href="https://mxxhcm.github.io/2019/09/23/conjugate-gradient/">conjugate method</a>求解。<br>即用求解出来的$x$近似$\hat{\nabla}\eta(\theta) = \text{F}^{-1}\nabla(\eta)$，大大减少了计算量。</p><h2 id="参考文献">参考文献</h2><p>1.<a href="https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf" target="_blank" rel="noopener">https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf</a><br>2.<a href="https://wiseodd.github.io/techblog/2018/03/14/natural-gradient/" target="_blank" rel="noopener">https://wiseodd.github.io/techblog/2018/03/14/natural-gradient/</a><br>3.<a href="https://medium.com/@jonathan_hui/rl-trust-region-policy-optimization-trpo-part-2-f51e3b2e373a" target="_blank" rel="noopener">https://medium.com/@jonathan_hui/rl-trust-region-policy-optimization-trpo-part-2-f51e3b2e373a</a><br>4.<a href="https://medium.com/@jonathan_hui/rl-natural-policy-gradient-actor-critic-using-kronecker-factored-trust-region-acktr-58f3798a4a93" target="_blank" rel="noopener">https://medium.com/@jonathan_hui/rl-natural-policy-gradient-actor-critic-using-kronecker-factored-trust-region-acktr-58f3798a4a93</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;a-natural-policy-gradient&quot;&gt;A Natural Policy Gradient&lt;/h2&gt;
&lt;p&gt;论文名称：A Natural Policy Gradient&lt;br&gt;
论文地址：&lt;a href=&quot;https://papers.nips.cc
      
    
    </summary>
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="gradient method" scheme="http://mxxhcm.github.io/tags/gradient-method/"/>
    
      <category term="natural policy gradient" scheme="http://mxxhcm.github.io/tags/natural-policy-gradient/"/>
    
  </entry>
  
  <entry>
    <title>gradient method policy gradient</title>
    <link href="http://mxxhcm.github.io/2019/09/07/gradient-method-policy-gradient/"/>
    <id>http://mxxhcm.github.io/2019/09/07/gradient-method-policy-gradient/</id>
    <published>2019-09-07T11:37:52.000Z</published>
    <updated>2019-10-07T14:19:39.778Z</updated>
    
    <content type="html"><![CDATA[<h2 id="policy-gradient">Policy Gradient</h2><p>强化学习有三种常用的方法，第一种是基于值函数的，第二种是policy gradient，第三种是derivative-free的方法，即不利用导数的方法。基于值函数的方法在理论上证明是很难的。这篇论文提出了policy gradient的方法，直接用函数去表示策略，根据expected reward对策略参数的梯度进行更新，REINFORCE和actor-critic都是policy gradient的方法。<br>本文给出了policy gradient theorem的证明，使用近似的action-value function或者advantage函数，梯度可以表示成experience的估计。同时证明了任意可导的函数表示的policy通过policy iteration都可以收敛到locl optimal policy。</p><h3 id="值函数方法的缺点">值函数方法的缺点</h3><p>基于值函数的方法，在估计出值函数之后，每次通过greedy算法选择action。这种方法有两个缺点。</p><ul><li>基于值函数的方法会找到一个deterministic的策略，但是很多时候optimal policy可能是stochastic的。</li><li>某个action的估计值函数稍微改变一点就可能导致这个动作被选中或者不被选中，这种不连续是保证值函数收敛的一大障碍。</li></ul><h3 id="用函数表示stochastic-policy">用函数表示stochastic policy</h3><p>Policy gradient用函数表示stochastic policy。比如用神经网络表示的一个policy，输入是state，输出是每个action选择的概率，神经网络的参数是policy的参数。用$\mathbf{\theta}$表示policy参数，用$J$表示该策略的performance measure。然后参数$\mathbf{\theta}$的更新正比于以下梯度：<br>$$\nabla\mathbf{\theta} \approx \alpha \frac{\partial J}{\partial \mathbf{\theta}} \tag{1}$$<br>其中$\alpha$是正定的step size，按照式子$(1)$进行更新，可以确保$\theta$收敛到$J$的局部最优值对应的local optimal policy。和value based方法相比，$\mathbf{\theta}$的微小改变只能造成policy和state分布的微小改变。</p><h3 id="使用值函数辅助学习policy">使用值函数辅助学习policy</h3><p>使用满足特定属性的辅助近似值函数，利用之前的experience就可以得到式子$(1)$的一个无偏估计。REINFORCE方法也找到了式子$(1)$的一个无偏估计，但没有使用辅助值函数，此外它的速度要比使用值函数的方法慢很多。学习一个值函数，并用它取减少方差对快速学习是很重要的。</p><h3 id="证明policy-iteration收敛性">证明policy iteration收敛性</h3><p>本文还证明了基于actor-critic和policy-iteration架构方法的收敛性。在这篇文章中，他们只证明了使用通用函数逼近的policy iteration可以收敛到local optimal policy。</p><h2 id="objective-function">Objective Function</h2><h3 id="三种形式">三种形式</h3><p>智能体每一步的action由policy $\pi$决定：$\pi(s,a,\mathbf{\theta})=Pr\left[a_t=a|s_t=s,\mathbf{\theta}\right],\forall s\in S, \forall a\in A,\mathbf{\theta}\in \mathbb{R}^l $。为了方便，通常把$\pi(s,a,\mathbf{\theta})$简写为$\pi(s,a)$。假设$\pi$是可导的，即$\frac{\partial\pi(s,a)}{\partial\mathbf{\theta}}$存在。有三种方式定义智能体的objective：</p><ul><li>计算policy $\pi$下从初始状态$s_0$开始的accumulated reward：<br>$$J(\theta) = V^{\pi}(s_0) = \mathbb{E}_{\pi}\left[G_0\right] = \mathbb{E}_{\pi} \left[\sum_{t=0}^{\infty} \gamma^{t-1} R_t | s_0 \right] \tag{2}$$<br>其中$0 \le \gamma \le 1$，在continuing case中，$0 \le \gamma \lt 1$，而在episodic情况下，$\gamma$能取到$1$，$0 \le \gamma \le 1$。</li><li>计算policy $\pi$每个timestep的immediate reward的均值，即average reward。<br>$$J(\theta) = \mathbb{E}_{\pi}\left[R(s, a)\right] = \sum_s d(s) \sum_a\pi(s, a)R(s,a) \tag{3}$$<br>在connuting problems情况下，没有episode boundaries，如果不使用discount factor，可以这么做；事实上，我觉得在episodic情况下，也能这么做。</li><li>当没有明确的初始状态时，计算policy $\pi$下所有state value的均值：<br>$$J(\theta) = \sum_s d(s) V^{\pi} (s) = \sum_s d(s) \sum_a\pi(s, a) Q^{\pi} (s, a) = \mathbb{E}_{\pi}\left[Q^{\pi}(s, a)\right] \tag{4}$$</li></ul><p>其中$R(s,a) = \mathbb{E}\left[R_{t+1}|s_t=s, a_t=a\right]$，$d (s) = lim_{t\rightarrow \infty} Pr\left[s_t=s|s_0,\pi\right]$是策略$\pi$下的stationary distribution。<a href="http://mxxhcm.github.io/2019/07/31/markov-matrices/">Stationary distribution</a>的意思是就是不论初始状态是什么，经过很多步之后，都会达到一个stable state。其实这三个目标本质上都是一样的，都是要最大化每个时刻agent得到的reward。</p><h3 id="accumated-reward-from-designated-state-从指定状态开始的累计奖励">Accumated Reward from Designated State(从指定状态开始的累计奖励)</h3><p>我们可以指定一个初始状态$s_0$，计算从这个初始状态开始得到的accumulated reward：<br>$$\eta(\pi) = V^{\pi} (s_0) = \mathbb{E}_{\pi}\left[\sum_{t=0}^{\infty} \gamma^{t-1} R_t|s_0\right] = \mathbb{E}_{\pi}\left[G_0 \right]\tag{5}$$<br>定义return $G_t$如下：<br>$$ G_t = \sum_{k=0}^{\infty} R_{t+k+1} \tag{6}$$<br>定义state-action value function和state value function如下：<br>\begin{align*}<br>Q^{\pi} (s,a) = \mathbb{E}_{\pi}\left[G_t|s_t=s, a_t=a\right] &amp; = \mathbb{E}_{\pi}\left[\sum_{k=0}^{\infty} R_{t+k+1}|s_t=s,a_t=a\right] \\<br>V^{\pi} (s) = \mathbb{E}_{\pi}\left[G_t|s_t=s\right] &amp; = \mathbb{E}_{\pi}\left[\sum_{k=0}^{\infty} R_{t+k+1}|s_t=s\right]\\<br>\tag{7}<br>\end{align*}<br>其中$\gamma\in[0,1]$是折扣因子，只有在episodic任务中才允许取$\gamma=1$。它们之间的关系如下：<br>\begin{align*}<br>V^{\pi} (s) = \mathbb{E}_{\pi}\left[Q^{\pi} (s,a)|S_t=s\right] &amp; = \sum_a \pi(a|s) Q^{\pi} (s,a) \\<br>Q^{\pi} (s, a) = \mathbb{E}_{\pi}\left[R_{t+1} + \gamma V^{\pi}(s)|S_t=s, A_t=a\right] &amp; = \sum_{s’,r}p(s’,r|s,a) (r+\gamma V^{\pi} (s’))\\<br>\tag{8}<br>\end{align*}<br>定义$\rho^{\pi} (s)$是从指定的初始状态$s_0$开始，执行策略$\pi$在$t=\infty$之间的任意时刻所有能到达state $s$的折扣概率之和：<br>$$\rho^{\pi} (s) = \sum_{t=1}^{\infty} \gamma^t Pr\left[s_t = s|s_0,\pi\right]  =  \sum_{t=0}^{\infty} \gamma^{t} p(s_0\rightarrow s, t,\pi) \tag{9}$$<br>把$\rho^{\pi} (s) $换一种写法就容易理解了：<br>$$\rho^{\pi} (s) = P(s_0 = s) +\gamma P(s_1=s) + \gamma^2 P(s_2 = s)+\cdots \tag{10}$$</p><h3 id="average-reward-平均奖励">Average Reward(平均奖励)</h3><p>Average reward是根据每一个step的的expected reward $\eta(\pi)$对不同的policy进行排名：<br>$$\eta(\pi) = lim_{t\rightarrow \infty}\frac{1}{t}\mathbb{E}\left[R_1+R_2+\cdots+R_t|\pi\right] = \int_S d(s) \int_A \pi(s,a) R(s,a)dads \tag{11}$$<br>第一个等号中，$R_t$表示$t$时刻的immediate reward，所以第一个等号表示的是在策略$\pi$下$t$个时间步的imediate reward平均值的期望。第二个等号后，第一个积分是对$s$积分，相当于求的是$s$的期望；然后对$a$的积分，求的是每一个$s$处对应各个动作$a$出现概率的期望，所以第二个等式后面求的其实就是每一步$R(s,a)$平均值的期望。<br>Return的定义和accumulated reward不同：<br>$$G_t = \sum_{k=0}^{\infty} \left[R_{t+k+1} - \eta(\pi)\right] \tag{12}$$<br>因为$G_t$不同，State-action value $Q^{\pi} (s,a)$以及state value $V^{\pi} (s)$的定义和accumulated reward也就不同了：<br>\begin{align*}<br>Q^{\pi} (s,a) = \mathbb{E}_{\pi}\left[G_t|s_t=s, a_t=a\right] &amp; = \mathbb{E}_{\pi}\left[\sum_{k=0}^{\infty}\left( R_{t+k+1} - \eta(\pi)\right)|s_t=s,a_t=a\right]\\<br>V^{\pi} (s) = \mathbb{E}_{\pi}\left[G_t|s_t=s\right] &amp; = \mathbb{E}_{\pi}\left[\sum_{k=0}^{\infty} \left(R_{t+k+1} - \eta(\pi) \right)|s_t=s\right] \\<br>\tag{13}<br>\end{align*}<br>$Q^{\pi} (s,a)$和$V^{\pi} (s,a)$之间的关系满足：<br>\begin{align*}<br>Q^{\pi} (s,a) = \sum_{s’, r}p(s’,r|s,a)(r - \eta(\pi) + V(s’)) \tag{14}<br>\end{align*}</p><h3 id="state-value的均值">State value的均值</h3><p>这个和上面的accumulated reward有一定关联，accumulated计算的是$V^{\pi} (s_0)$，而这里计算的是$V^{\pi} (s_0)$的期望（均值）：<br>$$ \eta(\pi) = \sum_s \rho_0(s_0) V^{\pi} (s) \tag{15}$$<br>State action value function和state value function的定义和accumulated reward一样。<br>定义$\rho^{\pi} $为从任意初始状态$s_0$经过$t$步之后state $s$出现的概率：<br>$$\rho^{\pi} (s) =\int_S \sum_{t=0}^{\infty} \gamma^t \rho_0^{\pi} (s_0) Pr\left[s_t = s|s_0,\pi\right] ds_0  = \int_S \sum_{t=0}^{\infty} \gamma^{t} \rho_0^{\pi} (s_0)p(s_0\rightarrow s, t,\pi)ds_0 \tag{16}$$</p><h3 id="policy-gradient-v2">Policy Gradient</h3><p>对于单步的MDP，从分布$\rho^{\pi} (s)$中采样得到$s$，采取action $a$，得到immediate reward $R=R(s,a)$，结束。上面三种目标函数是一样的：<br>\begin{align*}<br>J(\theta) &amp; = \mathbb{E}_{\pi}\left[R\right]\\<br>&amp; = \sum_s d(s) \sum_a \pi(s,a) R(s,a) \tag{17}\\<br>\end{align*}<br>求导有问题！！！！怎么求导得到的。。。<br>\begin{align*}<br>\nabla_{\theta} J(\theta) &amp; = \sum_s d(s) \sum_a \nabla_{\theta}\pi(s,a) R(s,a)\\<br>&amp; = \sum_s d(s) \sum_a\pi(s,a) \nabla_{\theta}\log \pi(s,a) R(s,a)\\<br>&amp; = \mathbb{E}_{\pi}\left[\nabla_{\theta}\log \pi(s,a) R(s,a)\right] \tag{18}\\<br>\end{align*}<br>对于多步的MDP，只需要将$R$换成$Q^{\pi} (s, a)$就行了，上面三种目标函数最后都能够得到：<br>$$\nabla_{\theta} J(\theta) = \sum_s d(s) \sum_a\pi(a|s) \nabla_{\theta} \log\pi(s,a) Q^{\pi} (s,a) = \mathbb{E}_{\pi} \left[\nabla_{\theta} \log\pi(s,a) Q^{\pi} (s,a)\right] \tag{19}$$<br>其中$Q$是根据不同的目标函数定义的state-action value function，目标函数不同，$Q$定义也不同。在其他论文中，$\nabla_{\theta} \log\pi_{\theta}(s,a)$不变，可以把$Q$换成其他目标函数，GAE这篇论文对不同的目标函数进行了总结。</p><h2 id="policy-gradient-theorem">Policy Gradient Theorem</h2><p>对于任何MDP，不论是average reward还是accumulated reward的形式，都有：<br>\begin{align*}<br>\nabla_{\theta} \eta &amp; = \sum_s \rho^{\pi} (s)\sum_a{\nabla_{\theta}\pi(s,a)}Q^{\pi} (s,a) \\<br>&amp; = \sum_s \rho^{\pi} (s)\sum_a{\pi(s,a)\nabla_{\theta}\log\pi(s,a)}Q^{\pi} (s,a), \tag{20}\\<br>&amp; = \mathbb{E}_{\pi}\left[\nabla_{\theta}\log\pi(s,a)Q^{\pi} (s,a)\right], \tag{21}\\<br>\end{align*}<br>证明：<br>\begin{align*}<br>\nabla V_{\pi}(s) &amp;= \nabla \left[ \sum_a \pi(a|s)Q_{\pi}(s,a)\right], \forall s\in S \\<br>&amp;= \sum_a \left[\nabla\pi(a|s)Q_{\pi}(s,a)\right], \forall s\in S \\<br>&amp;= \sum_a\left[\nabla\pi(a|s)Q_{\pi}(s,a) + \pi(a|s)\nabla Q_{\pi}(s,a)\right] \\<br>&amp;= \sum_a\left[\nabla\pi(a|s)Q_{\pi}(s,a) + \pi(a|s)\nabla \left[\sum_{s’,r}p(s’,r|s,a)(R+\gamma V_{\pi}(s’))\right]\right] \\<br>&amp;= \sum_a\left[\nabla\pi(a|s)Q_{\pi}(s,a) + \pi(a|s)\nabla \left[\sum_{s’,r}p(s’,r|s,a)R +\sum_{s’,r}p(s’,r|s,a)\gamma V_{\pi}(s’)\right]\right] \\<br>&amp;= \sum_a\left[\nabla\pi(a|s)Q_{\pi}(s,a) + \pi(a|s)\left[0 +\sum_{s’,r}p(s’,r|s,a)\gamma\nabla V_{\pi}(s’)\right]\right] \\<br>&amp;= \sum_a\left[\nabla\pi(a|s)Q_{\pi}(s,a) + \pi(a|s)\sum_{s’,r}p(s’,r|s,a)\gamma \nabla V_{\pi}(s’))\right] \\<br>&amp;= \sum_a\left[\nabla\pi(a|s)Q_{\pi}(s,a) + \pi(a|s)\sum_{s’}\gamma p(s’|s,a)\nabla V_{\pi}(s’) \right] \\<br>&amp;= \sum_a\\<br>&amp;\left[\nabla\pi(a|s)Q_{\pi}(s,a) + \pi(a|s)\sum_{s’}\gamma p(s’|s,a)\left( \sum_{a’} \nabla\pi(a’|s’)Q_{\pi}(s’,a’) + \pi(a’|s’)\sum_{s’’}\gamma p(s’’|s’,a’)\nabla V_{\pi}(s’’))\right) \right] \tag{22}\\<br>&amp;= \sum_{x\in S}\sum_{k=0}^{\infty} Pr(s\rightarrow x, k,\pi)\sum_a\nabla\pi(a|x)Q_{\pi}(x,a) \tag{23}\\<br>&amp;= \sum_{x\in S}\rho^{\pi} (x)\sum_a\nabla \pi(a|x) Q_{\pi}(x,a) \tag{24}\\<br>\end{align*}</p><p>式子$(23)$中的$Pr(s\rightarrow x, k, \pi)$是在策略$\pi$下从state $s$经过$k$步转换到state $x$的概率，对第$(14)$步进行展开以后，从状态$s$开始，在每一个$k$都有可能到达状态$x$，如果不能到$x$，概率为$0$就是了。</p><h3 id="指定初始状态-s-0-的accumulated-reward">指定初始状态$s_0$的accumulated reward</h3><p>证明思路，在上面我们已经求得了$V^{\pi} (s)$的梯度，而accumulated reward其实就是$V^{\pi} (s_0)$，取$J(\mathbf{\theta}) = V_{\pi}(s_0)$，则：<br>\begin{align*}<br>\nabla J(\mathbf{\theta}) &amp;= \nabla_{\theta}V_{\pi}(s_0)\\<br>&amp;= \sum_{s\in S}( \sum_{k=0}^{\infty} Pr(s_0\rightarrow s,k,\pi) ) \sum_a\nabla{\pi}(a|s)Q_{\pi}(s,a)\qquad\qquad\qquad;\rho(s) = \sum_{k=0}^{\infty} Pr(s_0\rightarrow s,k,\pi) \tag{25}\\<br>&amp;=\sum_{s\in S}\rho(s)\sum_a \nabla{\pi}(a|s)Q_{\pi}(s,a)\tag{26}\\<br>&amp;=\sum_{s’\in S}\rho(s’)\sum_s\frac{\rho(s)}{\sum_{s’}\rho(s’)}\sum_a \nabla{\pi}(a|s)Q_{\pi}(s,a) \qquad\qquad\qquad\qquad; \text{normalize } \rho(s) \tag{27}\\<br>&amp;=\sum_{s’\in S}\rho(s’)\sum_sd(s)\sum_a \nabla{\pi}(a|s)Q_{\pi}(s,a) \tag{28} \qquad\qquad\qquad\qquad\qquad; d(s) = \frac{\rho(s) }{\sum_{s’} \rho(s’)} \text{ is stationary distribution}\\<br>&amp;\propto \sum_{s\in S}d(s)\sum_a\nabla\pi(a|s)Q_{\pi}(s,a)\tag{29},\qquad\qquad\qquad\qquad\qquad\qquad\qquad; \sum_s\rho(s)\text{是常数}\\<br>&amp; = \sum_{s\in S}d(s)\sum_a\pi(s, a)\nabla\log\pi(a|s)Q_{\pi}(s,a) \tag{30}\\<br>&amp; = \mathbb{E}_{\pi}\left[\nabla\log\pi(a|s)Q_{\pi}(s,a)\right] \tag{31}\\<br>\end{align*}<br>其中$\mathbb{E}_{\pi}$表示$\mathbb{E}_{s\sim d_{\pi}, a\sim \pi}$，即state和action distributions都遵守policy $\pi$。</p><h3 id="average-reward">Average Reward:</h3><p>证明思路，首先求$V$的梯度，带入$Q, V$和$\eta$的关系进行转换，能够得到$\eta$的梯度和$Q,V$梯度之间的关系，最后进行一系列化简即可。<br>\begin{align*}<br>\nabla V_{\pi}(s) &amp;= \nabla \left[ \sum_a \pi(a|s)Q_{\pi}(s,a)\right], \forall s\in S \tag{32}\\<br>&amp;= \sum_a \left[\nabla\pi(a|s)Q_{\pi}(s,a)\right], \tag{33} \\<br>&amp;= \sum_a \left[\nabla\pi(a|s)Q_{\pi}(s,a) + \pi(a|s)\nabla Q_{\pi}(s,a)\right] \tag{34}\\<br>&amp;= \sum_a\left[\nabla\pi(a|s)Q_{\pi}(s,a) + \pi(a|s)\nabla \left[\sum_{s’,r}p(s’,r|s,a)\left(R(s,a)-\eta(\pi)+V_{\pi}(s’)\right)\right]\right] \tag{35}\\<br>&amp;= \sum_a\left[\nabla\pi(a|s)Q_{\pi}(s,a) + \pi(a|s)\left[-\nabla \eta(\pi)+ \sum_{s’,r}p(s’,r|s,a) \nabla V_{\pi}(s’)\right]\right], \nabla R(s,a) = 0\tag{36}\\<br>&amp;= \sum_a\left[\nabla\pi(a|s)Q_{\pi}(s,a) - \pi(a|s)\nabla \eta(\pi)+ \pi(a|s) \sum_{s’,r}p(s’,r|s,a) \nabla V_{\pi}(s’)\right]\tag{37}\\<br>&amp;= \sum_a\nabla\pi(a|s)Q_{\pi}(s,a) - \sum_a \pi(a|s)\nabla \eta(\pi) + \sum_a \pi(a|s) \sum_{s’,r}p(s’,r|s,a) \nabla V_{\pi}(s’) \tag{38}\\<br>&amp;= \sum_a\nabla\pi(a|s)Q_{\pi}(s,a) -\nabla \eta(\pi)+ \sum_a \pi(a|s)\sum_{s’,r}p(s’,r|s,a)\nabla V_{\pi}(s’), \sum_s\pi(s,a)=1\tag{39}\\<br>\end{align*}<br>移项合并同类项得：<br>$$\nabla \eta(\pi) = \sum_a\nabla\pi(a|s)Q_{\pi}(s,a) + \sum_a\pi(s,a) \sum_{s’,r}p(s’,r|s,a) \nabla V_{\pi}(s’) - \nabla V_{\pi}(s) \tag{40}$$<br>同时在上式两边对$d(s)$进行求和，得到：<br>\begin{align*}<br>\sum_s d(s)\nabla \eta(\pi) &amp;= \sum_s d(s)\sum_a \nabla\pi(a|s)Q_{\pi}(s,a) \\<br>&amp;\qquad\qquad\qquad + \sum_s d(s) \sum_a\pi(a|s) \sum_{s’,r}p(s’,r|s,a) \nabla V_{\pi}(s’)\\<br>&amp;\qquad\qquad\qquad - \sum_s d(s)\nabla V_{\pi}(s) \tag{41}\\<br>\nabla \eta(\pi) &amp;= \sum_s d(s)\sum_a \nabla\pi(a|s)Q_{\pi}(s,a) + \sum_s d(s’) \nabla V_{\pi}(s’) - \sum_s d(s)\nabla V_{\pi}(s) \tag{42}\\<br>&amp;= \sum_s d(s)\sum_a \nabla\pi(a|s)Q_{\pi}(s,a) \tag{43}\\<br>&amp;= \sum_s d(s)\sum_a \pi(s,a) \nabla\log\pi(a|s)Q_{\pi}(s,a) \tag{44}\\<br>&amp; = \mathbb{E}_{\pi}\left[\nabla_{\theta}\log\pi(s,a) Q_{\pi}(s,a)\right] \tag{45}\\<br>\end{align*}<br>式子$(41)$到式子$(42)$其实就是$\sum_s d(s) \sum_a\pi(a|s) \sum_{s’,r}p(s’,r|s,a) = \sum_{s’}d(s’)$，根据$\rho^{\pi} (s)$表示的意义，显然这是成立的。<br>\begin{align*}<br>\end{align*}</p><h3 id="state-value的期望">State value的期望</h3><p>还不会证明。</p><h3 id="结论">结论</h3><p>从这两种情况的证明可以看出来，policy gradient和$\frac{\partial \rho^{\pi} (s)}{\partial\mathbf{\theta}}$无关：即可以通过计算，让policy的改变不影响states distributions，这非常有利于使用采样来估计梯度。举个例子来说，如果$s$是根据policy $\pi$的从$\rho$中采样得到的，那么$\sum_a\frac{\partial\pi(s,a)}{\partial\mathbf{\theta}}Q^{\pi} (s,a)$就是$\frac{\partial{\rho}}{\partial\mathbf{\theta}}$的一个无偏估计。通常$Q^{\pi}(s,a)$也是不知道的，需要估计。一种方法是使用returns近似，即$G_t = \sum_{k=0}^{\infty} R_{t+k+1}-\rho(\pi)$或者$R_t = \sum_{k=0}^{\infty} \gamma^{t} R_{t+k+1}$（在指定初始状态条件下），这就是REINFROCE方法。$\nabla\mathbf{\theta}\propto\frac{\partial\pi(s_t,a_t)}{\partial\mathbf{\theta}}R_t\frac{1}{\pi(s_t,a_t)}$,$\frac{1}{\pi(s_t,a_t)}$纠正了$\pi$的oversampling）。</p><h2 id="policy-gradient-with-approximation-使用近似的策略梯度">Policy Gradient with Approximation(使用近似的策略梯度)</h2><p>因为$Q^{\pi} $是不知道的，我们希望用函数近似式子$(21)$中的$Q^{\pi} $，大致求出梯度的方向。用$f_w:S\times A \rightarrow \mathbb{R}$表示$Q^{\pi} $的估计值。在策略$\pi$下，更新$w$的值:<br>$$\Delta w_t\propto \frac{\partial}{\partial w}\left[\hat{Q}^{\pi} (s_t,a_t) - f_w(s_t,a_t)\right]^2 \propto \left[\hat{Q}^{\pi} (s_t,a_t) - f_w(s_t,a_t)\right]\frac{\partial f_w(s_t,a_t)}{\partial w} \tag{67}$$<br>$\hat{Q}^{\pi} (s_t,a_t)$是$Q^{\pi} (s_t,a_t)$的一个无偏估计，当这样一个过程收敛到local optimum，$Q^{\pi} (s,a)$和$f_w(s,a)$的均方误差最小时：<br>$$\epsilon(\omega, \pi) = \sum_{s,a}\rho^{\pi} (s)\pi(a|s;\theta)(Q^{\pi} (s,a))^2 - f^{\pi} (s,a;\omega) \tag{68}$$<br>即导数等于$0$:<br>$$\sum_s \rho^{\pi} (s)\sum_a\pi(a|s;\theta)\left[Q^{\pi} (s,a) -f_w (s,a;w)\right]\frac{\partial f_w(s,a)}{\partial w}  = 0\tag{69}$$</p><h3 id="定理2：policy-gradient-with-approximation-theorem">定理2：Policy Gradient with Approximation Theorem</h3><p>如果$f_w$的参数$w$满足式子$(69)$，并且：<br>$$\frac{\partial f_w(s,a)}{\partial w} = \frac{\partial \pi(s,a)}{\partial \mathbf{\theta}}\frac{1}{\pi(s,a)} = \frac{\partial \log \pi(s,a)}{\partial \mathbf{\theta}}\tag{70}$$<br>那么使用$f_w(s,a)$计算的gradient和$Q^{\pi} (s,a)$计算的gradient是一样的：<br>$$\frac{\partial \rho}{\partial \theta} = \sum_s\rho^{\pi} (s)\sum_a\frac{\partial \pi(s,a)}{\partial \mathbf{\theta}}f_w(s,a)\tag{71}$$</p><p>证明：<br>将式子$(70)$代入$(69)$得到：<br>\begin{align*}<br>&amp;\sum_s\rho^{\pi} (s)\sum_a\pi(s,a)\left[Q^{\pi} (s,a) -f_w(s,a)\right]\frac{\partial f_w(s,a)}{\partial w}\\<br>= &amp;\sum_s\rho^{\pi} (s)\sum_a\pi(s,a)\left[Q^{\pi} (s,a) -f_w(s,a)\right]\frac{\partial \pi(s,a)}{\partial \mathbf{\theta}}\frac{1}{\pi(s,a)}\\<br>= &amp;\sum_s\rho^{\pi} (s)\sum_a\frac{\partial \pi(s,a)}{\partial \mathbf{\theta}}\left[Q^{\pi} (s,a) -f_w(s,a)\right] \tag{72}\\<br>= &amp; 0 \\<br>\end{align*}<br>将式子$72$带入式子$(21)$：<br>\begin{align*}<br>\frac{\partial \eta}{\partial \mathbf{\theta}} &amp; = \sum_a \rho^{\pi} (s)\sum_a\frac{\partial\pi(s,a)}{\partial\mathbf{\theta}}Q^{\pi} (s,a)\\<br>&amp;= \sum_a \rho^{\pi} (s)\sum_a\frac{\partial\pi(s,a)}{\partial\mathbf{\theta}}Q^{\pi} (s,a) - \sum_s\rho^{\pi} (s)\sum_a\frac{\partial \pi(s,a)}{\partial \mathbf{\theta}}\left[Q^{\pi} (s,a) -f_w(s,a)\right]\\<br>&amp;= \sum_a \rho^{\pi} (s)\sum_a\frac{\partial\pi(s,a)}{\partial\mathbf{\theta}} \left[Q^{\pi} (s,a) - Q^{\pi} (s,a) +f_w(s,a)\right]\\<br>&amp;= \sum_a \rho^{\pi} (s)\sum_a\frac{\partial\pi(s,a)}{\partial\mathbf{\theta}} f_w(s,a) \tag{73}\\<br>\end{align*}<br>得证$\sum_a \rho^{\pi} (s)\sum_a\frac{\partial\pi(s,a)}{\partial\mathbf{\theta}}Q^{\pi} (s,a) = \sum_a \rho^{\pi} (s)\sum_a\frac{\partial\pi(s,a)}{\partial\mathbf{\theta}} f_w(s,a) $。</p><h2 id="application-to-deriving-algorithms-and-advantages">Application to Deriving Algorithms and Advantages</h2><p>给定一个参数化的policy，可以利用定理2推导出参数化value function的形式。比如，考虑在features上进行线性组合的Gibbs分布构成的policy：<br>$$\pi(a|s) = \frac{e^{\theta^T \phi_{sa} } }{\sum_b e^{\theta^T \phi_{sb} }} , \forall s \in S, \forall a \in A \tag{74}$$<br>其中$\phi_{s,a}$是state-action pair $s,a$的特征向量。满足式子$(70)$的公式如下：<br>$$\frac{\partial f_w(s,a)}{\partial w} = \frac{\partial \pi(a|s)}{\partial \theta}\frac{1}{\pi(a|s)} = \phi_{sa} - \sum_b\pi(b|s)\phi_{sb}\tag{75}$$<br>所以：<br>$$f_w(s,a) = w^T \left[\phi_{sa} - \sum_b\pi(b|s)\phi_{sb} \right]\tag{76}$$<br>也就是说，$f_w$和policy $\pi$都是feature的线性组合，只不过每一个state处$f_w$的均值都为$0$，$\sum_a\pi(a|s)f_w(s,a) = 0,\forall s\in S$。所以，其实我们可以认为$f_w$是对advantage function $A^{\pi} (s,a) = Q^{\pi} (s,a)- V^{\pi} (s)$而不是$Q^{\pi} (s,a)$的一个近似。式子$(70)$中$f_w$其实是一个相对值而不是一个绝对值。事实上，他们都可对以推广变成一个function加上一个value function。比如式子$(71)$可以变成$\frac{\partial\eta}{\partial \theta} = \sum_s\rho^{\pi}(s) \sum_a \frac{\partial \pi(a|s)}{\partial \theta}\left[f_w(s,a) + v(s)\right]$，其中$v$是一个function，$v$的选择不影响理论结果，但是会影响近似梯度的方差。</p><h2 id="convergence-of-policy-iteration-with-function-approximation-使用函数近似的策略迭代的收敛性">Convergence of Policy Iteration with Function Approximation(使用函数近似的策略迭代的收敛性)</h2><h3 id="定理3：policy-iteration-with-function-approximation">定理3：Policy Iteration with Function Approximation</h3><p>用$\pi$和$f_w$表示policy和value function的可导函数，并且满足式子$(70)$。$\max_{\theta,s,a,i,j} \vert\frac{\partial^2 \pi(a|s)}{\partial\theta_i \partial\theta_j} \vert\lt B\lt \infty$，假设$\left[\alpha_k\right]_{k=0}^{\infty}$是步长sequence，$\lim_{k\rightarrow \infty}\alpha_k = 0$，$\sum_k \alpha_k = \infty$。对于任何有界rewards的MDP来说，任意$\theta_0$，$\pi_k=\pi(\cdot, \theta_k)$定义的$\left[\eta(\pi_k)\right]_{k=0}^{\infty}$，并且$w_k = w$满足：<br>$$\sum_s\rho^{\pi_k} (s) \sum_a\pi_k(a|s)\left[Q^{\pi_k} (s,a)-f_w(s,a) \right]\frac{\partial f_w(s,a)}{\partial w}=0 \tag{77}$$<br>$$\theta_{k+1} = \theta_k + \alpha_k \sum_s\rho^{\pi_k}(s) \sum_a\frac{\partial\pi_k(s,a)}{\partial \theta}f_{w_k}(s,a) \tag{78}$$<br>一定收敛：$\lim_{k\rightarrow \infty}\frac{\partial \rho(\pi_k)}{\partial \theta} = 0$。</p><h2 id="policy-gradient-algorithms">Policy Gradient Algorithms</h2><h3 id="reinforce">REINFORCE</h3><p>REINFORCE使用Monte Carlo方法近似return $G_t$，因为$Q^{\pi} (s,a) = \mathbb{E}_{\pi}\left[G_t|s_t=s, a_t=a\right]$使用$G_t$代替policy gradient theorem中的$Q$：<br>\begin{align*}<br>\nabla_{\theta}J(\theta) &amp; = \mathbb{E}_{\pi}\left[Q^{\pi} (s,a) \nabla_{\theta}\log\pi_{\theta}(a|s)\right]\\<br>&amp; = \mathbb{E}_{\pi} \left[G_t\nabla_{\theta}\log\pi_{\theta}(a|s)\right]\\<br>\end{align*}<br>接下来进行sampling，使用Monte Carlo方法计算$G_t$即可。完整算法如下：<br><strong>REINFORCE 算法</strong><br>输入：policy $\pi$的初始化参数$\theta$，step-size $\alpha$<br>Loop<br>$\qquad$使用$\pi_{\theta}$生成一个trajectory $S_0, A_0, R_1, S_1, A_1, \cdots$<br>$\qquad$for $t=1, 2, \cdots, T$<br>$\qquad\qquad$估计$G_t = \sum_{k=t}^T \gamma^{k-t} R_{t+1}$<br>$\qquad\qquad$更新$\theta \leftarrow \theta + \alpha \gamma^t G_t \log\pi_{\theta}(a_t|s_t)$<br>$\qquad$end for</p><h3 id="reinforce-with-baseline">REINFORCE with Baseline</h3><p>REINFROCE的一类变种是在$G_t$的基础上减去一个和$\theta$无关的baseline，作用是在不改变bais的前提下减少方差：<br>$$\sum_a \nabla_{\theta}\pi(a|s) b(s) = b(s)  \nabla_{\theta}\sum_a\pi(a|s) = b(s) \nabla_{\theta} 1 = 0$$<br>即加了一个baseline之后，梯度更新的期望值依然保持不变，但是可以减少方差。具体的证明可以见<a href>why use baselinse in policy gradient</a>。在MDPs中，有的state可能所有的actions都有很高的values，这时候需要一个high baseline，而有的actions可能都有低的values，这时候需要一个low baseline，一个常用的baseline可以选择$V(s)$，可以使用$G_t - V^{\pi} (s,a)$计算梯度。<br>直观上来首，RL感兴趣的是那些比平均值好的action。如果returns都是正的$(R(\tau)\ge 0)$，PG总是会提高这个trajectory发生的概率，即使它比其他的trajectory要低。考虑以下两个例子：</p><ul><li>Trajectory $A$的return是$10$，trajectory $B$的reward是$-10$</li><li>Trajectory $A$的return是$10$，trajectory $B$的reward是$1$</li></ul><p>在第一个例子中，PG会提高$A$发生的概率，降低$B$发生的概率。在第二个例子中，PG会提高$A$和$B$的概率。然而，对于我们来说，在两个例子中，我们都想要降低$B$发生的概率，提高$A$发生的概率。通过引入一个baseline，比如$V$，我们就可以实现这样的目的。<br>完整算法如下：<br><strong>REINFORCE with Baseline 算法</strong><br>输入：可导的policy $\pi$的初始化参数$\theta$，可导的state value function $\hat{v}(s, \mathbf{w})$，step-size $\alpha^{\theta} \gt 0, \alpha^{w} \gt 0 $<br>Loop<br>$\qquad$使用$\pi_{\theta}$生成一个trajectory $S_0, A_0, R_1, S_1, A_1, \cdots$<br>$\qquad$for $t=1, 2, \cdots, T$<br>$\qquad\qquad$近似$G_t = \sum_{k=t}^T \gamma^{k-t} R_{t+1}$<br>$\qquad\qquad$近似$\delta \leftarrow G-\hat{v}(s_t, w)$<br>$\qquad\qquad$更新$w\leftarrow w + \alpha^{w} \delta \nabla\hat{v}(s_t, w)$<br>$\qquad\qquad$更新$\theta \leftarrow \theta + \alpha^{\theta} \gamma^t\delta\log\pi_{\theta}(a_t|s_t)$<br>$\qquad$end for<br>和介绍的原理不同的是，REINFORCE with Baselien使用了近似的$\hat{v}(s, \mathbf{w}) \approx V(s)$，因为我们不知道真实的$Q$，前面也已经证明了。。</p><h3 id="actor-critic">Actor-Critic</h3><p>Policy gradient中两个常用的components是policy和value function，在学习policy的同时学习一个value function是非常有用的，value function可以辅助policy进行更新，比如vanilla policy gradient使用value function辅助policy减小方差，我们把这类方法统称为actor-critic方法。Value function和policy可以共享参数：</p><ul><li>Critic利用mean squared error更新value function $Q_w(s,a)$或者$V_w(s)$的参数$w$；</li><li>Actor根据critic给出的更新方向更新policy $\pi_{\theta}(a|s)$的参数$\theta$。</li></ul><p>One-step actor-critic方法使用one-step return代替了full return。完整的算法按如下：<br><strong>One-step actor critic 算法</strong><br>输入：policy $\pi$的参数$\theta$，初始化state $s_0$<br>采样$a\sim \pi(a|s)$<br>Loop<br>$\qquad$for $t= 1,\cdots, T$:<br>$\qquad\qquad$采样reward $r_t \sum R(s,a)$和next state $s’\sim P(s’|s,a)$<br>$\qquad\qquad$采样next action $s’\sim \pi(a’|s’)$<br>$\qquad\qquad$更新policy参数$\theta \leftarrow \theta + \alpha_{\theta} Q_w(s,a) \nabla_{\theta}\log\pi_{\theta}(a|s)$<br>$\qquad\qquad$计算timestep $t$时刻的TD-error：<br>$\qquad\qquad\qquad \delta_t = r_t + \gamma Q_w(s’, a’) - Q_w(s,a)$<br>$\qquad\qquad\qquad$使用mean squared error更新$Q$函数：<br>$\qquad\qquad\qquad w\leftarrow w + \alpha_w \delta_t \nabla_w Q_w(s, a)$<br>$\qquad\qquad\qquad a\leftarrow a’, s\leftarrow s’$<br>$\qquad$end for</p><p>REINFORCE with baseline的方法同时学习了policy $\pi$和state value function $V$，但是我们一般不把它叫做actor-critic方法，因为$V$在这里是一个baseline而不是一个critic。它没有被用作bootstraping，只是用作待更新state的一个baseline。bootstraping和state representation引入了bias，但是能减小方差和加快学习速度。REINFORCE with baseline是无偏的，并且收敛到local minimum，但是像所有的MC方法一样，它都收敛的很慢，也有很大的方差，并且很难应用到online和continuing问题。使用TD方法可以解决这个问题，并且通过multi-step可以控制boostrapping的度。为了得到policy gradient的方法，可以使用boostrapping critic的actor critic方法。</p><h3 id="off-policy-policy-gradient">Off-Policy Policy Gradient</h3><p>REINFORCE和one-step actor-critic都是on-policy的，behaviour policy和target policy是相同的，很低效。Off-polciy相对于on-policy有几个好处：</p><ul><li>可以使用过去的experience，即experience replay提高采样效率；</li><li>behaviour policy和target policy不同，能够更好的进行exploration。</li></ul><p>如何使用计算off policy graadient，这就牵涉到了<a href>importance sampling</a>。用$\beta(a|s)$表示behaviour oplicy，目标函数为：<br>$$J(\theta) = \sum_s d^{\beta} (s) \sum_a Q^{\pi} (s,a) \pi(a|s) = \mathbb{E}_{s\sim d^{\beta} } \left[\sum_a Q^{\pi} (s,a) \pi(a|s)\right]$$<br>其中$d^{\beta} (s)$是behaviour policy $\beta$的stationary distrbution， $\pi$是target policy。<br>实际上$a\sim \beta(a|s)$，对$J(\theta)$求偏导得到：<br>\begin{align*}<br>\nabla_{\theta}J(\theta) &amp; = \nabla_{\theta} \mathbb{E}_{s\sim d^{\beta} }\left[ \sum_a Q^{\pi} (s, a) \pi_{\theta} (a|s)\right]\\<br>&amp; = \mathbb{E}_{s\sim d^{\beta} }\left[ \sum_a\left(\nabla_{\theta} Q^{\pi} (s, a) \pi_{\theta} (a|s) + Q^{\pi} (s, a) \nabla_{\theta}\pi_{\theta} (a|s)\right)\right]\\<br>&amp; \approx \mathbb{E}_{s\sim d^{\beta} }\left[ \sum_a Q^{\pi} (s, a) \nabla_{\theta}\pi_{\theta} (a|s)\right]\\<br>&amp; \approx \mathbb{E}_{s\sim d^{\beta} }\left[\sum_a \beta(a|s)\frac{ \pi(a|s)}{\beta(a|s)} Q^{\pi} (s, a) \frac{\nabla_{\theta}\pi_{\theta} (a|s)}{\pi_{\theta}(a|s)}\right]\\<br>&amp; \approx \mathbb{E}_{\beta}\left[\sum_a \frac{ \pi(a|s)}{\beta(a|s)} Q^{\pi} (s, a) \nabla_{\theta}\log\pi_{\theta} (a|s)\right]\\<br>\end{align*}<br>其中$\frac{ \pi(a|s)}{\beta(a|s)}$称作importance sampling ratio。式子$(55)$到式子$(44)$忽略了第二项，有人狰狞了即使忽略了这一项，最终结果还会收敛到局部最优。<br>即通过importance sampling可以将过去policy的experience用于新policy的训练。</p><h3 id="a3c">A3C</h3><p>详细的解释可以见<a href="http://mxxhcm.github.io/2019/04/19/a3c/">A3C</a>。<br>A3C是Asynchronous advantage actor-critic，是并行的policy gardient，就是为并行训练设计的。在A3C中，多个actors并行采样进行训练，一个critic学习value function。<br>A3C算法的实质就是使用多个线程同步训练。分为主网络和线程中的网络，主网络不需要训练，主要用来存储和传递参数，每个线程中的网络用来训练参数。总的来说，多个线程同时训练提高了效率，另一方面，减小了数据之间的相关性，比如，线程$1$和$2$中都用主网络复制来的参数计算梯度，但是同一时刻只能有一个线程更新主网络的参数，比如线程$1$更新主网络的参数，那么线程$2$利用原来主网络参数计算的梯度会更新在线程$1$更新完之后的主网络参数上。</p><p><strong>A3C算法－－每个actor-learn线程的伪代码</strong><br>用$\theta, w$表示全局共享参数，用$T=0$表示全局共享计数器，<br>用$\theta’,w’$表示每个线程中的参数<br>初始化线程步计数器$t\leftarrow 1$，<br><strong>while</strong> $T\le T_{max}$<br>$\qquad$重置梯度$d\theta\leftarrow 0, dw\leftarrow 0$，<br>$\qquad$同步线程参数$\theta’=\theta,w’=w$<br>$\qquad t_{start}=t$<br>$\qquad$采样初始状态$s_t$，<br>$\qquad$ <strong>while</strong> $s_t \neq$ terminal且$t-t_{start} \le t_{max}$<br>$\qquad\qquad$根据策略选择action$a_t \sim \pi_{\theta’}(a_t|s_t;\theta’)$，<br>$\qquad\qquad$接收下一个状态$s_{t+1}$和reward $r_{t+1}$，<br>$\qquad\qquad T\leftarrow T+1, t\leftarrow t+1$<br>$\qquad$设置奖励$R=\begin{cases}0,&amp;for\ terminal\ s_t\\ V(s_t,\theta’_v), &amp;for\ non-terminal\ s_t\end{cases}$<br>$\qquad$<strong>for</strong> $i\in{t-1,\cdots,t_{start}}$ do<br>$\qquad\qquad R\leftarrow r_i+\gamma R$<br>$\qquad\qquad$累计和$\theta’$相关的梯度：$d\theta \leftarrow d\theta+\frac{\partial (y-Q(s,a;\theta))^2}{\partial \theta}$<br>$\qquad\qquad$累计和$\theta’_v$相关的梯度：$d\theta_v \leftarrow d\theta_v+\frac{\partial (R-V(s_i;\theta’_v))^2}{\partial \theta’_v}$<br>$\qquad$<strong>end for</strong><br>$\qquad$使用$d\theta$异步更新$\theta$，使用$d\theta_v$异步更新$\theta_v$.</p><p>累计梯度$dw$和$d\theta$其实可以看成是mini-batch的sgd。</p><h3 id="a2c">A2C</h3><p>A2C是A3C的同步版本。在A3C中每一个agent独立的和global parameters进行沟通，所以可能在某些时候，不同的thread使用的policy可能都会不同，thread1从global拿了参数，计算梯度，thread2从global拿了参数，计算梯度，thread1更新了global，而这个时候thread2还在计算梯度，thread1就拿到了新的global参数，计算梯度，更新。这时候thread2还没计算完，就产生了不一致。<br>A2C就是为了解决这个问题的，A2C使用一个调度器，等待所有的actors完成相应的工作，然后更新global的参数，保证在下一次更新的时候每一个actor使用的都是相同的policy。</p><h3 id="dpg">DPG</h3><p>完整解释见<a href="http://localhost:4000/2019/07/16/gradient-method-deterministic-policy-gradient/" target="_blank" rel="noopener">deterministic policy gardient</a>。<br>Deterministic policy gradient theorem：<br>\begin{align*}<br>J(\mu_{\theta}) &amp; = \int_S\rho^{\mu} (s) R(s, \mu_{\theta}(s)) da ds\tag{48}\\<br>&amp; = \mathbb{E}_{s\sim \rho^{\mu} } \left[R(s, \mu_{\theta}(s) \right]\tag{49}\\<br>\end{align*}</p><p>\begin{align*}<br>\nabla_{\theta} J(\mu_\theta) &amp; = \int_S\rho^{\mu} (s)\nabla_{\theta}\mu_{\theta}(s) \nabla_a Q^{\mu} (s, a)|_{\mu_{\theta}(s)} da ds\tag{50}\\<br>&amp; = \mathbb{E}_{s\sim \rho^{\mu} (s)} \left[ \nabla_{\theta}\mu_{\theta}(s) \nabla_a Q^{\mu} (s, a)|_{\mu_{\theta}(s)} \right]\tag{51}\\<br>\end{align*}</p><h3 id="ddpg">DDPG</h3><p>DDPG是一个model-free off-plicy actor critic方法，将DPG和DQN的思想相结合。DQN使用replay buffer和target network稳定学习过程，但是DQN只有在discrete space空间中起作用，DDPG将actor-critic框架扩展到了continous空间，学习deterministic policy。为了更好的exploration，使用$\mu$和noise $\mathbf{N}$构造exploration policy $\mu’$：<br>$$\mu’(s) = \mu(s) + \mathbf{N} \tag{}$$<br>此外，DDPG对actor和critic实行soft update，即$\theta’ \leftarrow \tau \theta+(1-\tau) \theta’$。同时使用batch normalizion对每层的输入进行处理，完整的算法如下：<br><strong>DDPG算法</strong><br>使用$\theta^Q $和$\theta^\mu $ 随机初始化critic网络$Q(s,a|\theta^Q )$和actor网络$\mu(s|\theta^\mu )$。<br>使用$\theta^ Q$和$\theta^\mu $初始化target network参数$\theta^{Q’} \leftarrow \theta^Q, \theta^{\mu’} \leftarrow \theta^\mu$。<br>初始化replay buffer<br>for episode $= 1, \cdots, M$ do<br>$\qquad$初始化随机过程$\mathbf{N}$用于exploration<br>$\qquad$获得初始状态$s_0$<br>$\qquad$for $t=0, \cdots, T$ do<br>$\qquad\qquad$选择action $a_t = \mu(s_t|\theta^\mu ) + \mathbf{N}_t$<br>$\qquad\qquad$执行$a_t$，获得$r_{t+1}, s_{t+1}$<br>$\qquad\qquad$将$(s_t, a_t, r_{t+1}, s_{t+1})$存入buffer<br>$\qquad\qquad$从buffer中获取一个大小为$N$的batch，$(s_i, a_i, r_{i+1}，s_{i+1})$<br>$\qquad\qquad$使用target network计算TD target：$y_i = r_i + \gamma Q’(s_{i+1}, \mu’(s_{i+1}|\theta^{\mu’} ) | \theta^{Q’} )$<br>$\qquad\qquad$使用TD-error loss更新critic： $L=\frac{1}{N} \sum_i (y_i - Q(s_i,a_i|\theta^Q ) )^2 $<br>$\qquad\qquad$使用样本计算policy gradient更新actor：<br>$$\nabla_{\theta^{\mu} } J \approx \frac{1}{N} \sum_i \nabla_a Q(s, a|\theta^Q ) |_{s=s_i,a=\mu(s_i)} \nabla_{\theta^{\mu} }\mu(s|\theta^{\mu} )$$<br>$\qquad\qquad$更新target networks:<br>$$\theta^{Q’} \leftarrow \tau \theta^Q + (1 - \tau) \theta^{Q’} $$<br>$$\theta^{\mu’} \leftarrow \tau \theta^\mu + (1 - \tau) \theta^{\mu’} $$<br>$\qquad$end for<br>end for</p><h3 id="maddpg">MADDPG</h3><p>MADDPG将DDPG扩展到multi agents问题上。多个只有local informaction的agents合作完成任务，从单个agent来看，环境是non-stationary，因为其他agents的polices是未知的。MADDPG就是解决这样一类问题的方法。<br>对于$N$个agetns的MADDPG算法，每一个agent都有一个decentralized actor和一个centralized critic。每一个decentralized actor输入为当前agent的observation，输出为它的action，每一个centralized critic输入为所有agents的observation，输出为当前agent的$Q$值，和每个智能体的reward相关。</p><p>完整的算法如下：<br><strong>N个agents的MADDPG算法</strong><br>for episode $= 1, \cdots, M$ do<br>$\qquad$初始化随机过程$\mathbf{N}$用来exploration<br>$\qquad$获得初始状态$\mathbf{s}$<br>$\qquad$for $t=1, \cdots , T$ do<br>$\qquad\qquad$for $i = 1, \cdots, N$<br>$\qquad\qquad\qquad a_i = \mu_{\theta_i}(o_i) +\mathbf{N}_t$<br>$\qquad\qquad$end for<br>$\qquad\qquad$执行actions $\mathbf{a} = (a_1, \cdots, a_N)$，获得$\mathbf{r}$和$\mathbf{s’}$<br>$\qquad\qquad$将$(\mathbf{s},\mathbf{a},\mathbf{r},\mathbf{s’})$存入buffer<br>$\qquad\qquad \mathbf{s}\leftarrow \mathbf{s’}$<br>$\qquad\qquad$for $i= 1, \cdots, N$ do<br>$\qquad\qquad\qquad$从buffer中采样$S$个samples $(\mathbf{s}^j ,\mathbf{a}^j ,\mathbf{r}^j ,\mathbf{s’}^j )$<br>$\qquad\qquad\qquad$计算TD target $\mathbf{y}^j_i = \mathbf{r}^j_i + \gamma Q^{\mu’}_i (\mathbf{x’}^j, a_1^{’},\cdots, a_N^{’} )|_{a_k^{’} = \mu_k^{’} (o_k^j ) }$<br>$\qquad\qquad\qquad$使用均方误差更新critic：<br>$$L(\theta_i) = \frac{1}{S} \sum_j \left( y^j - Q_i^{\mu} (\mathbf{x}^j , a_1^j ,\cdots, a_N^j ) \right)^2 $$<br>$\qquad\qquad\qquad$使用样本近似计算policy gradient：<br>$$\nabla_{\theta_i} J\approx \frac{1}{S} \sum_j\nabla_{\theta_i}\mu_i(o_i^j ) Q_i^{\mu} (\mathbf{x}^j , a_1^j ,\cdots, a_i^j, a_N^j)|_{a_i = \mu_i(o_i^j)} $$<br>$\qquad\qquad$end for<br>$\qquad\qquad$更新每个agent $i$的target network<br>$$\qquad\qquad \theta_i^{’} \leftarrow \tau\theta_i + (1- \tau) \theta_i^{’}$$<br>$\qquad$end for<br>end for</p><h3 id="d4pg">D4PG</h3><h3 id="natural-pg">Natural PG</h3><h3 id="trpo">TRPO</h3><p>详细介绍可以查看<a href="http://mxxhcmg.github.io/2019/09/08/gradient-method-trust-region-policy-optimization/" target="_blank" rel="noopener">trust region policy optimization</a>。<br>TRPO将policy的更新表示为两个policy的performance的一个公式：<br>\begin{align*}<br>\mathbb{E}_{s_0, a_0,\cdots\sim \pi_{new} }\left[\sum_{t=0}^{\infty} \gamma^t A^{\pi_{old}} (s_t,a_t) \right] &amp;=\mathbb{E}_{s_0, a_0,\cdots\sim \pi_{new}}\left[\sum_{t=0}^{\infty} \gamma^t (Q^{\pi_{old}} (s_t,a_t) - V^{\pi_{old}} (s_t))\right]  \\<br>&amp;=\mathbb{E}_{s_0, a_0,\cdots\sim \pi_{new}} \left[\sum_{t=0}^{\infty} \gamma^t ( R_{t+1} + \gamma V^{\pi_{old}} (s_{t+1}) -  V^{\pi_{old}} (s_t))\right]  \\<br>&amp;=\mathbb{E}_{s_0, a_0,\cdots\sim \pi_{new}} \left[\sum_{t=0}^{\infty} \gamma^t R_{t+1} + \sum_{t=0}^{\infty} \gamma^t (\gamma V^{\pi_{old}} (s_{t+1}) -  V^{\pi_{old}} (s_t))\right]  \\<br>&amp;=\mathbb{E}_{s_0, a_0,\cdots\sim \pi_{new}} \left[\sum_{t=0}^{\infty} \gamma^t R_{t+1} \right]+ \mathbb{E}_{s_0, a_0,\cdots\sim \pi_{new}} \left[\sum_{t=0}^{\infty} \gamma^t (\gamma V^{\pi_{old}} (s_{t+1}) -  V^{\pi_{old}} (s_t))\right]  \\<br>&amp;=\eta(\pi_{new}) + \mathbb{E}_{s_0, a_0,\cdots\sim \pi_{new}} \left[ -  V^{\pi_{old}} (s_0))\right]  \\<br>&amp;=\eta(\pi_{new}) - \mathbb{E}_{s_0, a_0,\cdots\sim \pi_{new}} \left[ V^{\pi_{old}} (s_0))\right]  \\<br>&amp;=\eta(\pi_{new}) - \eta(\pi_{old})\\<br>\end{align*}<br>我们的目标就是想要最大化<br>$$\mathbb{E}_{s_0, a_0,\cdots\sim \pi_{new} }\left[\sum_{t=0}^{\infty} \gamma^t A^{\pi_{old}} (s_t,a_t) \right]$$<br>用$\rho_{\pi_{old}}(s)$近似$\rho_{\pi_{new}}(s)$得到<br>$$\mathbb{E}_{s_0, a_0,\cdots\sim \pi_{new} }\left[\sum_{t=0}^{\infty} \gamma^t A^{\pi_{old}} (s_t,a_t) \right]$$</p><p>最后得到目标函数：<br>$$J = \mathbb{E}_{s\sim\rho_{\theta_{old}}, a\sim q}\left[\frac{\pi_{\theta} (a|s) }{q(a|s)}Q_{\theta_{old}}(s,a)\right] \tag{}$$</p><p>为了训练的稳定性，我们应该避免在一个step内policy改变太大。TRPO通过添加一个KL散度约束每一次迭代中，policy改变的大小。<br>$$s.t. \mathbb{E}_{s\sim \rho_{\theta_{old}}}\left[D_{KL}(\pi_{\theta_{old}}(\cdot|s)||\pi_{\theta}(\cdot|s))\right]\le \delta \tag{}$$</p><h3 id="ppo">PPO</h3><h3 id="acer">ACER</h3><h3 id="actkr">ACTKR</h3><h3 id="sac">SAC</h3><h3 id="sac-with-automatically-adjusted-temperature">SAC with Automatically Adjusted Temperature</h3><h3 id="td3">TD3</h3><h3 id="svpg">SVPG</h3><h2 id="另一种policy-gradient的方法">另一种policy gradient的方法</h2><p>这是CS294上的方法，感觉和前面的有一些不成体系，而且有一些地方的证明让我不能接受。<br>目标函数$J$如下：<br>\begin{align*}<br>J(\theta) &amp; = \mathbb{E}_{\tau \sim \pi_{\theta}(\tau)} \left[R(\tau)\right] \tag{46}\\<br>&amp; = \mathbb{E}_{\tau \sim \pi_{\theta}(\tau)} \left[\sum_t R(s_t, a_t)\right] \tag{47}\\<br>&amp; = \int \pi_{\theta}(\tau) R(\tau) d\tau \tag{48}\\<br>&amp; = \int \pi_{\theta}(\tau)\sum_t R(s_t, a_t) d\tau \tag{49}\\<br>&amp; \approx \frac{1}{N}\sum_i \sum_t R(s_{i,t}, a_{i,t}) \tag{50}\\<br>\end{align*}<br>其中$\tau = s_0, a_0, s_1, a_1,\cdots \sim \pi_{\theta}$表示一个episode的trajectory，$R(\tau)$表示这个trajectory的returns($G_0$)。Policy gradient变成下式，（为什么？？？还是不懂！！CS294上面的推导！！！为什么$\nabla$可以写进积分号里面，$R(\tau)$不也和$\pi_{\theta}$有关？？？）<br>\begin{align*}<br>\nabla_{\theta}J(\theta) &amp; = \int \nabla_{\theta} \pi_{\theta}(\tau) R(\tau)d\tau\\<br>&amp; = \int \pi_{\theta}(\tau) \nabla_{\theta}\log\pi_{\theta}(\tau) R(\tau)d\tau\\<br>&amp; = \mathbb{E}_{\tau\sim \pi_{\theta}(\tau)} \left[\nabla_{\theta} \log\pi_{\theta}(\tau) R(\tau) d\tau\right] \tag{51}<br>\end{align*}<br>可以将policy gradient表示成期望的形式，然后就可以采样进行估计。对$R(\tau)$进行采样，但是不进行求导。Returns不直接受$\pi_{\theta}$的影响，$\tau$受$\pi_{\theta}$的影响，下面是$\log\pi(\tau)$的偏导数计算。<br>$\pi(\tau)$定义为：<br>$$\pi_{\theta}(s_0,a_0,\cdots, s_T,a_T) = p(s_0) \prod_{t=0}^T \pi_{\theta}(a_t|s_t)p(s_{t+1}|s_t,a_t) \tag{52}$$<br>取$\log$：<br>$$\log\pi_{\theta}(s_0,a_0,\cdots, s_T,a_T) = \log p(s_0) + \sum_{t=0}^T\log \pi_{\theta}(a_t|s_t) + \log p(s_{t+1}|s_t,a_t) \tag{53}$$<br>对$\theta$求偏导，得到：<br>$$\nabla_{\theta} \log\pi(\tau) = \nabla_{\theta}\left[\sum_{t=0}^T \log\pi_{\theta}(a_t|s_t) \right] = \left[\sum_{t=0}^T \nabla_{\theta} \log\pi_{\theta}(a_t|s_t) \right]\tag{54}$$<br>所以，policy gradient：<br>\begin{align*}<br>\nabla_{\theta}J(\theta) &amp;= \mathbb{E}_{\tau \sim \pi_{\theta}(\tau)}\left[\nabla_{\theta}\log\pi_{\theta}(\tau) R(\tau) \right]\tag{55}\\<br>&amp; = \mathbb{E}_{\tau \sim \pi_{\theta}(\tau)} \left[ \left(\sum_{t=1}^T\nabla_{\theta} \log\pi_{\theta}(a_{i,t}|s_{i,t})\right) \left(\sum_{t=1}^T R(s_{i,t}, a_{i,t})\right) \right] \tag{56}\\<br>&amp; \approx \frac{1}{N}\sum_{i=1}^N \left(\sum_{t=1}^T\nabla_{\theta} \log\pi_{\theta}(a_{i,t}|s_{i,t})\right) \left(\sum_{t=1}^T R(s_{i,t}, a_{i,t})\right)\tag{57}\\<br>\end{align*}<br>$$ \theta \leftarrow \theta + \alpha \nabla_{\theta} J(\theta)\tag{58}$$<br>即用多个trajectories近似计算policy gradietn，更新$\theta$。</p><h3 id="reinforce-policy-gradient-with-monte-carlo-rollouts">REINFORCE: Policy Gradient with Monte Carlo rollouts</h3><p>REINFROCE使用Monte Carlo近似returns，$\nabla_{\theta}J(\theta)$近似为：</p><p>$$\nabla_{\theta} J(\theta) \approx \frac{1}{N}\sum_{i=1}^N \left(\sum_{t=1}^T\nabla_{\theta} \log\pi_{\theta}(a_{i,t}|s_{i,t})\right) \left(\sum_{t=1}^T R(s_{i,t}, a_{i,t})\right)$$<br>完整的算法如下：<br>REINFORCE 算法<br>Loop<br>$\qquad 1.$使用policy $\pi_{\theta}(a_t|s_t)$生成一个trajectory $\left[\tau^i \right]$<br>$\qquad$估计$\nabla_{\theta}J(\theta) \approx \sum_i (\sum_t \nabla_{\theta} \log\pi_{\theta}(a_t^i |s_t^i )) (\sum_t R(s_t^i , a_t^i ))$<br>$\qquad \theta\leftarrow \theta+\alpha\nabla_{\theta}J(\theta)$<br>Until 收敛</p><h3 id="intution">Intution</h3><p>$\nabla_{\theta} \log\pi_{\theta}(a_{i,t}|s_{i,t})$是最大对数似然，表示的是对应的trajectory在当前的policy下发生的可能性。将它和returns相乘，如果产生high positive reward，增加policy的可能性，如果是high negetive reward，减少policy的可能性。在一个trajectory中的states具有很强的相关性，这个trajectory发生的概率定义为：<br>$$\pi(\tau) = p(s_0) \prod_{t=0}^T \pi_{\theta}(a_t|s_t)p(s_{t+1}|s_t,a_t) \tag{59}$$<br>但是连续的乘法可能会产生梯度消失或者梯度爆炸问题。policy gradient将连乘变成了连加。</p><h3 id="policy-gradients-improvements">Policy Gradients Improvements</h3><p>Policy gradient的方差很大，而且很难收敛，这是一大问题。<br>MC方法根据整个trajectory计算exact rewards，但是stochastic policy可能会在不同的episode采取不同的actions，一个小的改变可能会完全改变结果，MC方法没有bias但是有很大的方差。方差会影响深度学习的优化，一个采样的reward可能想要增加似然，另一个样本rewards可能想要减少似然，给出了冲突的梯度方向，影响收敛性。为了减少选择action造成的方差，我们需要减少样本rewards的方差：<br>$$\left( \sum_{t=1}^T R(s_{i,t}, a_{i,t})\right)\tag{60}$$<br>增大PG中的batch size会减少方差。<br>但是增大batch size会降低sample efficiency。所以batch size不能增加太多，我们需要想其他的方法减少方差：</p><h4 id="causality">Causality</h4><p>未来的action不应该改变过去的decisions：<br>$$\nabla_{\theta}J(\theta) \approx \frac{1}{N} \sum_{i=1}^N \sum_{t=1}^T \nabla_{\theta}\log\pi_{\theta}(a_{i,t}|s_{i,t}) \left(\sum_{t’=t}^T R(s_{i,t’};a_{i,t’})\right)\tag{61}$$<br>可以用$Q$代替$\sum_t R(s,a)$，<br>$$\nabla_{\theta}J(\theta) \approx \frac{1}{N} \sum_{i=1}^N \sum_{t=1}^T \nabla_{\theta}\log\pi_{\theta}(a_{i,t}|s_{i,t}) Q_{i,t}\tag{62}$$<br>为什么会减少方差？</p><h4 id="baseline">Baseline</h4><p>$$\nabla_{\theta}J(\theta) \approx \frac{1}{N}\sum_{i=1}^N \left(\sum_{t=1}^T\nabla_{\theta} \log\pi_{\theta}(a_{i,t}|s_{i,t}\right) \left(\sum_{t=1}^T R(s_{i,t}, a_{i,t})\right)\tag{63}$$<br>中$\sum_{t=1}^T R(s_{i,t}, a_{i,t})$其实就是$Q(s,a)$，我们可以在上面减去一项，只要这一项和$\theta$无关就好，所以我们可以减去$V(s)$：<br>\begin{align*}<br>\nabla_{\theta} J(\theta) &amp; \approx \frac{1}{N} \sum_{i=1}^N \sum_{t=1}^T \nabla_{\theta}\log\pi_{\theta}(a_{i,t}|s_{i,t})\left(Q(s_{i,t}, a_{i,t}) - V(s_{i,t})\right)\\<br>&amp; = \frac{1}{N} \sum_{i=1}^N \sum_{t=1}^T \nabla_{\theta}\log\pi_{\theta}(a_{i,t}|s_{i,t})\left(A(s_{i,t}, a_{i,t})\right)\\<br>\end{align*}</p><h4 id="vanilla-policy-gradient">Vanilla Policy Gradient</h4><p>给出一个使用baseline $b$的通用算法：<br>$$ \nabla\approx \hat{g} = \frac{1}{m} \sum_{i=1}^m \nabla_{\theta}\log P(\tau^{(i)} ;\theta)(R(\tau^{(i)} )-b)\tag{64}$$<br>Vanilla policy gradient算法<br>初始化policy 参数$\theta$，baselien $b$<br>for $i = 1, 2, \cdots$ do<br>$\qquad$使用当前policy $\pi_{\theta}$收集trajectories<br>$\qquad$在每个trajectory的每一个timestep，计算<br>$\qquad\qquad$return $G_t = \sum_{t’=t}^{T-1} \gamma^{t’-t} R_{t’}$<br>$\qquad\qquad$advantage的估计值$\hat{A}_t = R_t - b(s_t)$<br>$\qquad$重新拟合baseline，最小化$\vert b(s_t) - G_t\vert^2 $<br>$\qquad$在所有trajectories和timesteps上求和估计$\hat{g}$<br>$\qquad$使用policy gradient estimate $\hat{g}$的估计$\hat{g}$更新policy 参数<br>end for</p><h4 id="reward-discount">Reward discount</h4><p>加上折扣因子：<br>$$Q^{\pi,\gamma}(s, a) \leftarrow r_0 + \gamma r_1 + \gamma^2 r_2 + \cdots|s_0 = s, a_0 = a \tag{65}$$<br>得到：<br>$$\nabla_{\theta}J(\theta) \approx \frac{1}{N} \sum_{i=1}^N \left(\sum_{t=1}^T \nabla_{\theta}\log\pi_{\theta}(a_{i,t}|s_{i,t}) \right) \left(\sum_{t’=t}^T \gamma^{t’-t} R(s_{i,t’};a_{i,t’})\right)\tag{66}$$</p><h2 id="参考文献">参考文献</h2><p>1.<a href="https://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf" target="_blank" rel="noopener">https://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf</a><br>2.<a href="https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf" target="_blank" rel="noopener">https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf</a><br>3.<a href="https://medium.com/@jonathan_hui/rl-policy-gradients-explained-9b13b688b146" target="_blank" rel="noopener">https://medium.com/@jonathan_hui/rl-policy-gradients-explained-9b13b688b146</a><br>4.<a href="https://medium.com/@jonathan_hui/rl-policy-gradients-explained-advanced-topic-20c2b81a9a8b" target="_blank" rel="noopener">https://medium.com/@jonathan_hui/rl-policy-gradients-explained-advanced-topic-20c2b81a9a8b</a><br>5.<a href="https://www.jianshu.com/p/af668c5d783d" target="_blank" rel="noopener">https://www.jianshu.com/p/af668c5d783d</a><br>6.<a href="https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html#what-is-policy-gradient" target="_blank" rel="noopener">https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html#what-is-policy-gradient</a><br>7.<a href="https://drive.google.com/file/d/0BxXI_RttTZAhY216RTMtanBpUnc/view" target="_blank" rel="noopener">https://drive.google.com/file/d/0BxXI_RttTZAhY216RTMtanBpUnc/view</a><br>8.<a href="https://danieltakeshi.github.io/2017/03/28/going-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients/" target="_blank" rel="noopener">https://danieltakeshi.github.io/2017/03/28/going-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients/</a><br>9.<a href="https://danieltakeshi.github.io/2017/04/02/notes-on-the-generalized-advantage-estimation-paper/" target="_blank" rel="noopener">https://danieltakeshi.github.io/2017/04/02/notes-on-the-generalized-advantage-estimation-paper/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;policy-gradient&quot;&gt;Policy Gradient&lt;/h2&gt;
&lt;p&gt;强化学习有三种常用的方法，第一种是基于值函数的，第二种是policy gradient，第三种是derivative-free的方法，即不利用导数的方法。基于值函数的方法在理论上证明
      
    
    </summary>
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="policy gradient" scheme="http://mxxhcm.github.io/tags/policy-gradient/"/>
    
      <category term="gradient method" scheme="http://mxxhcm.github.io/tags/gradient-method/"/>
    
  </entry>
  
  <entry>
    <title>convex optimization chapter 3 convex functions</title>
    <link href="http://mxxhcm.github.io/2019/09/04/convex-optimization-chapter-3-convex-functions/"/>
    <id>http://mxxhcm.github.io/2019/09/04/convex-optimization-chapter-3-convex-functions/</id>
    <published>2019-09-04T12:49:37.000Z</published>
    <updated>2019-09-04T13:44:30.488Z</updated>
    
    <content type="html"><![CDATA[<h2 id="凸函数-convex-functions">凸函数(convex functions)</h2><h2 id="参考文献">参考文献</h2><p>1.stephen boyd. Convex optimization</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;凸函数-convex-functions&quot;&gt;凸函数(convex functions)&lt;/h2&gt;
&lt;h2 id=&quot;参考文献&quot;&gt;参考文献&lt;/h2&gt;
&lt;p&gt;1.stephen boyd. Convex optimization&lt;/p&gt;

      
    
    </summary>
    
      <category term="凸优化" scheme="http://mxxhcm.github.io/categories/%E5%87%B8%E4%BC%98%E5%8C%96/"/>
    
    
      <category term="凸优化" scheme="http://mxxhcm.github.io/tags/%E5%87%B8%E4%BC%98%E5%8C%96/"/>
    
      <category term="convex optimization" scheme="http://mxxhcm.github.io/tags/convex-optimization/"/>
    
      <category term="convex functions" scheme="http://mxxhcm.github.io/tags/convex-functions/"/>
    
      <category term="凸函数" scheme="http://mxxhcm.github.io/tags/%E5%87%B8%E5%87%BD%E6%95%B0/"/>
    
  </entry>
  
  <entry>
    <title>eigenvalues and eigenvectors（特征值和特征向量）</title>
    <link href="http://mxxhcm.github.io/2019/08/28/linear-algebra-eigenvalues-and-eigenvectors/"/>
    <id>http://mxxhcm.github.io/2019/08/28/linear-algebra-eigenvalues-and-eigenvectors/</id>
    <published>2019-08-28T09:21:43.000Z</published>
    <updated>2019-09-09T08:53:24.996Z</updated>
    
    <content type="html"><![CDATA[<h2 id="特征值和特征向量">特征值和特征向量</h2><p>这里介绍的东西都是针对于方阵来说的。</p><h3 id="定义">定义</h3><p>$Ax=\lambda x $，满足该式子的$x$称为矩阵$A$的特征向量，相应的$\lambda$称为特征值。</p><h3 id="求解">求解</h3><p>将$Ax=\lambda x$进行移项，得到$(A-\lambda I) x =0$，其中$A-\lambda I$必须是sigular（即不可逆），如果$A - \lambda I$是非奇异矩阵，也就是说它的列向量相互独立，那么只有零解，无意义。令$det (A-\lambda I)=0$，求出相应的$\lambda$和$x$。</p><h3 id="属性">属性</h3><ol><li>$n$个特征值的乘积等于行列式。</li><li>$n$个特征值之和等于对角线元素之和。</li></ol><h2 id="迹">迹</h2><h3 id="定义-v2">定义</h3><p>主对角线元素之和叫做迹（trace）。<br>$$\lambda_1 +\cdots + \lambda_n = trace = a_{11} + \cdots + a_{nn}$$</p><h2 id="矩阵对角化">矩阵对角化</h2><h3 id="定义-v3">定义</h3><p>如果合适的使用矩阵$A$的特征向量，可以把$A$转换成一个对角矩阵。<br>假设$n\times n$的矩阵$A$有$n$个线性独立的特征向量$x_1,\cdots, x_n$，把它们当做列向量，构成一个新的矩阵$S=\left[x_1, \cdots, x_n\right]$。<br>$$AS = A\left[x_1, \cdots, x_n\right] = \left[\lambda_1 x_1, \cdots, \lambda_n x_n\right] = \left[x_1, \cdots, x_n\right] \begin{bmatrix} \lambda_1 &amp;\cdots &amp; 0 \\ \vdots&amp;\lambda_i &amp; \vdots\\ 0&amp; \cdots &amp; \lambda_n\end{bmatrix} = S\Lambda$$<br>即$AS = S\Lambda$，所以有$S^{-1} AS = \Lambda, A = S\Lambda S^{-1} $，这里我们假设$A$的$n$个特征向量都是线性无关的。$A, \Lambda$的特征值相同，特征向量不同。$A$的特征向量用来对角化$A$。</p><h3 id="属性-v2">属性</h3><p>如果一个矩阵有$n$个不同的实特征值，那么它一定可对角化。<br>如果存在重复的特征值，可能但不一定可对角化，单位矩阵就有重复特征值，但可对角化。</p><h2 id="可逆和对角化">可逆和对角化</h2><p>矩阵可逆和矩阵可对角化之间没有关联。<br>矩阵可逆和特征值是否为$0$有关，而矩阵可对角化与特征向量有关，是否有足够的线性无关的特征向量。</p><h2 id="矩阵的幂">矩阵的幂</h2><h3 id="矩阵幂">矩阵幂</h3><p>$A= S\Lambda S^{-1} $,<br>$A^2 = S\Lambda S^{-1}S\Lambda S^{-1} = S\Lambda^2 S^{-1} $,<br>$A^k = S\Lambda^k S^{-1}$<br>所以，$A^k $和$A$的特征向量相同，特征值是$\Lambda^k $。当$k\rightarrow \infty$时，如果所有的特征值$\lambda_i \lt 1$，那么$A^k \rightarrow 0$。</p><h3 id="以解方程组-u-k-1-au-k">以解方程组$u_{k+1} = Au_k$</h3><p>从给定的向量$u_0$开始，$u_1 = Au_0, u_2 = Au_1, u_k = A^k u_0$<br>假设$u_0 = c_1 x_1 + c_2 x_2 + \cdots + c_nx_n$，$x_1, \cdots, x_n$是一组正交基。<br>$Au_0 =  c_1 \lambda_1 x_1 + \cdots + c_n\lambda_n x_n$<br>$u_{100} = A^{100} u_0 = c_1 \lambda_1^{100} x_1 + \cdots + c_n \lambda_n^{100} x_n$<br>$u_{100} = A^{100} u_0 = \Lambda^{100} S c$</p><h2 id="微分方程">微分方程</h2><h2 id="指数矩阵">指数矩阵</h2><h2 id="markov-matrices">Markov Matrices</h2><h3 id="定义-v4">定义</h3><p>马尔科夫矩阵满足两个条件</p><ol><li>所有元素大于$0$</li><li>行向量之和为$1$</li></ol><h3 id="属性-v3">属性</h3><ol><li>$\lambda = 1$是一个特征值，对应的特征向量的所有分量大于等于$0$。可以直接验证，假设$A = \begin{bmatrix}a&amp;b\\c&amp;d\\ \end{bmatrix}, a + b = 1, c + d = 1$，$A-\lambda I =  \begin{bmatrix}a - 1&amp;b\\c&amp;d - 1\\ \end{bmatrix}$，所有元素加起来等于$0$，即$(A-I)(1, \cdots, 1)^T = 0$，所以这些向量线性相关，因为存在一组不全为$0$的系数使得他们的和为$0$。所以$A-I$是奇异矩阵，也就是说$1$是$A$的一个特征值。</li><li>所有其他的特征值小于$1$。</li></ol><h3 id="马尔科夫矩阵的幂">马尔科夫矩阵的幂</h3><p>$u_k = A^k u_0 = c_1 \lambda_1^k x_1 + c_2 \lambda_2^k x_2 + \cdots$<br>如果只有一个特征值为$1$，所有其他特征值都小于$1$，幂运算之后$\lambda^k \rightarrow 0, k\rightaroow \infty, \lambda_k \neq 1$。</p><h2 id="对称矩阵">对称矩阵</h2><h3 id="定义-v5">定义</h3><p>满足$A= A^T $的矩阵$A$被称为对称矩阵。</p><h3 id="属性-v4">属性</h3><ol><li>实对称矩阵的特征值都是实数<br>证明：由$Ax= \lambda x$，得到$A\bar{x} = \bar{\lambda} \bar{x}$，$\bar{x}$是$x$的共轭，转置得：<br>$$\bar{x}^T A^T = \bar{x}^T A = \bar{x}^T \bar{\lambda}$$<br>$Ax = \lambda x$的左边乘上$\bar{x}^T $，在$\bar{x}^T A = \bar{x}^T \lambda$的右边同时乘上$x$：<br>$$\bar{x}^T Ax = \bar{x}^T \lambda x = \bar{x}^T A x= \bar{x}^T \bar{\lambda} x$$<br>即$\bar{x}^T \lambda x = \bar{x}^T \bar{\lambda} x$，而$\bar{x}^T x= \vert x\vert \ge 0 $，如果$x\neq 0$，则$\lambda = \bar{\lambda}$，即$\lambda$的虚部为$0$，即特征值都是实数。</li><li>对称矩阵有单位正交的特征向量。<br>证明：假设$S = \left[v_1, \cdots, v_i, \cdots, v_n\right]$是矩阵$A$的特征向量矩阵，根据矩阵对角化公式：<br>$$A = S \Lambda S^{-1}  $$<br>而$A=A^T $，所以得到<br>$$S\Lambda S^{-1} = A = A^T = \left(S \Lambda S^{-1} \right)^T = S^{-T} \Lambda^T S^T = S^{-T} \Lambda S^T $$<br>可以得出$S^T = S^{-1} $，所以$S S^T = I$，即$v_i^T v_i = 1, v_i^T v_j = 0, \forall i\neq j$。</li><li>所有的对称矩阵都是可对角化的。</li></ol><h3 id="谱定理-spectral-theorem">谱定理（Spectral Theorem）</h3><p>对称矩阵的对角化可以从$A=S\Lambda S^{-1} $变成$A=Q\Lambda Q^{-1} =Q\Lambda Q^T $。<br>谱定理：每一个对称矩阵都有以下分解$A = Q\Lambda Q^T $，$\Lambda$是实特征值，$Q$是单位正交向量矩阵。<br>$$A = Q\Lambda Q^{-1} = Q\Lambda Q^T $$<br>$A$是对称的，$Q \Lambda Q^T $也是对称的。</p><h2 id="正定矩阵">正定矩阵</h2><p>正定矩阵，负定矩阵，半正定矩阵，半负定矩阵都是对于对称矩阵来说的。</p><h3 id="定义-v6">定义</h3><p>如果对于所有的非零向量$x$，$x^T Ax$都是大于$0$的，我们称矩阵$A$是正定矩阵。</p><h3 id="属性-v5">属性</h3><ol><li>所有的$n$个特征值都是正的</li><li>所有的$n$个左上行列式都是正的</li><li>所有的$n$个主元都是正的</li><li>对于任意$x\neq 0$，$x^T A x$大于$0$。</li><li>$A=R^T R$，$R$是一个具有$n$个独立column的矩阵。</li></ol><p>如果任意矩阵$A$拥有以上属性中的任意一个，那么它就有其他四个性质，或者说上面五个属性都可以用来判定矩阵是否为正定矩阵。</p><h3 id="半正定矩阵">半正定矩阵</h3><p>如果对于所有的非零向量$x$，$x^T Ax$都是大于等于$0$的，我们称矩阵$A$是半正定矩阵。</p><h3 id="属性-v6">属性</h3><p>对于任何矩阵$A$，$A^T A$和$A A^T $都是对称矩阵，并且它们一定是半正定矩阵。<br>假设$A = \begin{bmatrix} a&amp;b\\c&amp;d\end{bmatrix}$，如何判断$A^T A$是不是正定的？根据定义，判断$x^T (A^T A) x$的符号：<br>$$x^T (A^T A) x = x^T A^T Ax = (Ax)^T (Ax) = \vert Ax \vert $$<br>相当于计算向量$Ax$的模长，它一定是大于等于$0$的。<br>同理$A A^T $的二次型相当于计算$A^T x$的模长，大于等于$0$。</p><h2 id="参考文献">参考文献</h2><p>1.MIT线性代数公开课<br>2.<a href="http://maecourses.ucsd.edu/~mdeolive/mae280a/lecture11.pdf" target="_blank" rel="noopener">http://maecourses.ucsd.edu/~mdeolive/mae280a/lecture11.pdf</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;特征值和特征向量&quot;&gt;特征值和特征向量&lt;/h2&gt;
&lt;p&gt;这里介绍的东西都是针对于方阵来说的。&lt;/p&gt;
&lt;h3 id=&quot;定义&quot;&gt;定义&lt;/h3&gt;
&lt;p&gt;$Ax=\lambda x $，满足该式子的$x$称为矩阵$A$的特征向量，相应的$\lambda$称为特征值。&lt;/p
      
    
    </summary>
    
      <category term="线性代数" scheme="http://mxxhcm.github.io/categories/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/"/>
    
    
      <category term="线性代数" scheme="http://mxxhcm.github.io/tags/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/"/>
    
      <category term="特征值" scheme="http://mxxhcm.github.io/tags/%E7%89%B9%E5%BE%81%E5%80%BC/"/>
    
      <category term="特征向量" scheme="http://mxxhcm.github.io/tags/%E7%89%B9%E5%BE%81%E5%90%91%E9%87%8F/"/>
    
  </entry>
  
  <entry>
    <title>determinants（行列式）</title>
    <link href="http://mxxhcm.github.io/2019/08/28/linear-algebra-determinants/"/>
    <id>http://mxxhcm.github.io/2019/08/28/linear-algebra-determinants/</id>
    <published>2019-08-28T06:45:18.000Z</published>
    <updated>2019-09-09T08:53:24.996Z</updated>
    
    <content type="html"><![CDATA[<h2 id="行列式-determinants">行列式（Determinants）</h2><p>这一章介绍行列式相关知识，行列式的一些属性等等。</p><ul><li>矩阵不可逆，行列式为$0$。</li><li>主元的乘积是行列式。</li><li>交换任意两行和两列，行列式的符号改变。</li><li>行列式的绝对值等于这个矩阵描述的space的体积。</li><li>行列式的计算公式有三个：<ol><li>主元公式，就是所有主元的乘积</li><li>big formula</li><li>cofact formula</li></ol></li></ul><h2 id="定义以及性质">定义以及性质</h2><p>行列式用det表示，给出以下的几个属性：</p><ol><li>$n\times n$的单位矩阵$I$的行列式$det I = I$<br>$$\begin{vmatrix}1&amp;0\\0&amp;1 \end{vmatrix} = 1$$</li><li>交换任意两行，行列式符号取反。<br>$$\begin{vmatrix}a&amp;b\\c&amp;d \end{vmatrix} = - \begin{vmatrix}c&amp;d\\a&amp;b \end{vmatrix}$$</li><li>行列式是每一行的线性函数<br>$$\begin{vmatrix}ta&amp;tb\\c&amp;d \end{vmatrix} = t \begin{vmatrix}a&amp;b\\c&amp;d \end{vmatrix}$$<br>$$\begin{vmatrix}a+a’&amp;b+b’\\c&amp;d \end{vmatrix} = \begin{vmatrix}a’&amp;b’\\c&amp;d \end{vmatrix}+\begin{vmatrix}a&amp;b\\c&amp;d \end{vmatrix}$$</li></ol><p>以上的三个属性是行列式的性质，事实上，它们定义了行列式是什么，从这几个基本属性出发，我们能推导出更多的属性。</p><ol start="4"><li>如果$A$的两行相等，那么$det A=0$，交换两行，还是矩阵$A$，行列式变号，所以行列式只能为$0$。<br>假设$A = \begin{bmatrix}a&amp;b\\a&amp;b \end{bmatrix}$，$\begin{vmatrix}a&amp;b\\a&amp;b \end{vmatrix}= - \begin{vmatrix}a&amp;b\\a&amp;b \end{vmatrix}$</li><li>从某一行减去其他行的倍数，行列式不变<br>$$\begin{vmatrix}a&amp;b\\c- la&amp;d-lb \end{vmatrix}= \begin{vmatrix}a&amp;b\\c&amp;d \end{vmatrix} -l \begin{vmatrix}a&amp;b\\a&amp;b\end{vmatrix}   = \begin{vmatrix}a&amp;b\\c&amp;d \end{vmatrix}$$</li><li>某一行为$0$矩阵，行列式为$0$。<br>$$\begin{vmatrix}0&amp;0\\c&amp;d \end{vmatrix} = \begin{vmatrix}c&amp;d\\c&amp;d \end{vmatrix} = 0$$</li><li>如果$A$是三角矩阵，行列式的值等于对角元素乘积。<br>$$\begin{vmatrix}a&amp;b\\0&amp;d \end{vmatrix} = \begin{vmatrix}a&amp;0\\c&amp;d \end{vmatrix} = \begin{vmatrix}a&amp;0\\0&amp;d \end{vmatrix} = ad \begin{vmatrix}1&amp;0\\0&amp;1 \end{vmatrix} = ad$$</li><li>当且仅当$A$不可逆的时候，$det A\neq 0$<br>$det A = det U$，如果$A$不可逆，$U$中有零行，从$6$我们知道，行列式为$0$。如果$A$可逆，行列式的值等于主元的乘积。</li><li>矩阵$AB$的行列式等于矩阵$A$的行列式以及矩阵$B$的行列式。<br>$$\begin{vmatrix}a&amp;b\\c&amp;d \end{vmatrix}\begin{vmatrix}p&amp;q\\r&amp;s \end{vmatrix} = \begin{vmatrix}ap+br&amp;aq+bs\\cp+dr&amp;cq+ds \end{vmatrix}$$<br>证明：<br>对于$2\times 2$的情况，有$\begin{vmatrix}A\end{vmatrix}\begin{vmatrix}B\end{vmatrix} = (ad - bc) ( ps - qr) = (ap + br) (cq+ds) - (aq+bs)(cp+dr) \begin{vmatrix}AB \end{vmatrix}$<br>当$B$是$A^{-1} $的时候，有$det (A A^{-1}) = det (I) = 1 = det (A) det(A^{-1} )$，所以$det A^{-1} = \frac{1}{det A}$</li><li>$A^T$和$A$的行列式相同。<br>$PA=LU, A^T P^T = U^T L^T$，$det L = det L^T = 1, det U = det U^T $，$L$是对角线元素为$1$的对角矩阵，$U$是对角矩阵，$P$是置换矩阵，$P^T P = I$，$det P det P^T = 1$，则$det P = det P^T = 1$，这个为什么？我有点不明明白。最后有$det A = det A^T $。</li></ol><h2 id="行列式的计算">行列式的计算</h2><h3 id="主元公式">主元公式</h3><p>行列式等于主元的乘积。</p><h3 id="大公式">大公式</h3><p>$n=2$的情况下：<br>$$A= \begin{bmatrix} a &amp; b\\c&amp;d\\\end{bmatrix}$$<br>$$det A = \begin{vmatrix}a&amp;0\\c&amp;d\end{vmatrix}+\begin{vmatrix}0&amp;b\\c&amp;d\end{vmatrix} = \begin{vmatrix}a&amp;0\\c&amp;0\end{vmatrix}+\begin{vmatrix}a&amp;0\\0&amp;d\end{vmatrix}+\begin{vmatrix}0&amp;b\\c&amp;0\end{vmatrix} \begin{vmatrix}0&amp;b\\0&amp;d\end{vmatrix} = ad - bc$$<br>$n=3$的情况下，最后有六项不为$0$的取值，$3!= 3\times 2\times 1= 6$<br>在$n$的情况下，有$n!$个项，将它们加起来求和。</p><h3 id="代数余子式-cofactors">代数余子式（Cofactors）</h3><h4 id="定义">定义</h4><p>用$C$表示代数余子式，用$M_{ij}$表示划去$i$行，$j$列的子矩阵，<br>$$C_{ij} = (-1)^{i+j} det  M_{ij} $$</p><h4 id="计算行列式">计算行列式</h4><p>行列式可以沿着任意一行或者任意一列，利用代数余子式进行计算，<br>沿着第$i$行计算的公式如下：<br>$$ det A = \sum_{j=1}^n a_{ij} C_{ij}$$<br>沿着第$j$列计算的公式如下：<br>$$ det A = \sum_{i=1}^m a_{ij} C_{ij}$$<br>可以递归下去进行计算。</p><h2 id="参考文献">参考文献</h2><p>1.MIT线性代数公开课视频</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;行列式-determinants&quot;&gt;行列式（Determinants）&lt;/h2&gt;
&lt;p&gt;这一章介绍行列式相关知识，行列式的一些属性等等。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;矩阵不可逆，行列式为$0$。&lt;/li&gt;
&lt;li&gt;主元的乘积是行列式。&lt;/li&gt;
&lt;li&gt;交换任意两行
      
    
    </summary>
    
      <category term="线性代数" scheme="http://mxxhcm.github.io/categories/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/"/>
    
    
      <category term="线性代数" scheme="http://mxxhcm.github.io/tags/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/"/>
    
      <category term="行列式" scheme="http://mxxhcm.github.io/tags/%E8%A1%8C%E5%88%97%E5%BC%8F/"/>
    
      <category term="determinants" scheme="http://mxxhcm.github.io/tags/determinants/"/>
    
  </entry>
  
  <entry>
    <title>orthogonality（正交性）</title>
    <link href="http://mxxhcm.github.io/2019/08/27/linear-algebra-orthogonality/"/>
    <id>http://mxxhcm.github.io/2019/08/27/linear-algebra-orthogonality/</id>
    <published>2019-08-27T11:21:44.000Z</published>
    <updated>2019-09-09T08:53:24.996Z</updated>
    
    <content type="html"><![CDATA[<h2 id="正交性-orthogonality">正交性（Orthogonality）</h2><p>这一章主要介绍正交性相关的内容。包括正交向量，投影，正交子空间，正交基以及如果求一组正交基，最后介绍QR分解求线性方程组。</p><h2 id="正交向量-orthogonal-vectors">正交向量（Orthogonal vectors）</h2><p>给定向量$v,w$，如果$v^T w = 0$，那么这两个向量就是正交向量。</p><h2 id="正交子空间-orthogonal-subspaces">正交子空间（Orthogonal subspaces）</h2><p>如果对于$\forall v\in V, \forall w\in W$，都有$v^T w = 0$，那么我们称subspaces $V,W$是orthogonal subspaces。</p><h2 id="column-space-nullsapce-row-space-left-nullspace的正交性">Column space, nullsapce, row space, left nullspace的正交性</h2><ol><li><p>Row space和nullspace是正交的。<br>举个例子来证明吧，有$A= \begin{bmatrix}c1&amp;c2\end{bmatrix} = \begin{bmatrix}r1\\r2\end{bmatrix} = \begin{bmatrix}1&amp;1&amp;2&amp;4\\0&amp;0&amp;1&amp;3\end{bmatrix}$<br>因为row space是row vector的linear combination，即$c_1 r_1+c_2 r_2$，而nullspace是$Ax=0$的所有解，即$x_1 c_1+x_2c_2  = 0$，这里的$0$是向量，可以推出来$r_1x = 0, r_2x =0 $，所以$c_1 r_1 x =0, c_2 r_2x = 0$，也就是说row space中的任意vector和nullspace中的vector都正交。<br>使用数值方法证明：<br>$x$表示$Ax=0$中的$x$，$A^Ty$表示row space，那么有<br>$$x^T (A^T y) = (Ax)^T y = 0^T y = 0$$</p></li><li><p>Column space和nullspace是正交的。</p></li></ol><h2 id="正交补-orthogonal-complements">正交补（Orthogonal complements）</h2><h3 id="定义">定义</h3><p>如果一个subspace包含所有和subspace $V$正交的向量，称这个subspace是$V$的orthogonal complements（正交补）。</p><h3 id="示例">示例</h3><p>Nullspace是row space的正交补（$\mathbb{R}^n$上）。<br>Left nullspace是column space的正交补（$\mathbb{R}^m$上）。</p><h2 id="投影-projections">投影（Projections）</h2><p>如下图所示，左边是投影到一条直线上的结果，右边是投影到一个subspace上的结果<br><img src="/2019/08/27/linear-algebra-orthogonality/projection.jpg" alt="projection"></p><h2 id="a-t-a">$A^T A$</h2><p>$A^T A$是可逆的，当且仅当$A$有linear independent columns时<br>证明：<br>$A^TA$是一个$n\times n$的方阵，$A$的nullspace和$A^T A$的nullspace相等。<br>如果$Ax= 0$，那么$A^T Ax = 0$，所以$x$也在$A^T A$的nullspace中。如果$A^T Ax=0$，那么我们要证明$Ax=0$，在左右两边同乘$x^T $得$x^T A^T Ax=0$，则$(AX)^T AX =0$，所以$\vert Ax\vert^2 =0$。也即是说如果$A^T Ax=0$，那么$Ax$的长度为$0$，也就是$Ax=0$。<br>如果$A^T A$的columns是独立的，也就是说nullspace为空，所以$A$的columns也是独立的；同理，如果$A$的columns是独立的，那么$A^T $的columns也是独立的。</p><h2 id="最小二乘法-least-squares-approximations">最小二乘法（Least Squares Approximations）</h2><p>$Ax=b$无解的情况，通常是等式个数大于未知数的个数，即$m\gt n$，$b$不在$A$的column space内。我们的目标是想让$e=b-Ax$为$0$，当这个目标不能实现的时候，可以在方程左右两边同时乘上$A^T$，求出一个近似的$\hat{x}$：<br>$$A^TAx = A^Tb$$<br>如何推导出这个结果，有以下几种方法：</p><h3 id="最小化误差">最小化误差</h3><ol><li><p>几何上<br>$Ax=b$的最好近似是$A\bar{x} = p$，最小的可能误差是$e=b-p$，$b$上的点的投影都在$p$上，而$p$在$A$的column space上，从直线拟合的角度上来看，$\bar{x}$给出了最好的结果。</p></li><li><p>代数上<br>$b=p+e$，$e$在$A$的nullspace上，$Ax=b=p+e$我们解不出来，$A\bar{x} = p$我们可以解出来。</p></li><li><p>积分</p></li></ol><h3 id="直线拟合">直线拟合</h3><h3 id="抛物线拟合">抛物线拟合</h3><h2 id="正交基-orthogonal-bases">正交基（Orthogonal Bases）</h2><h3 id="定义-v2">定义</h3><p>一组向量$q_1, q_2, \cdots , q_n$如果满足以下条件：<br>$$q_i^T q_j\begin{cases}0, i\neq j \\1, i=j\end{cases}$$<br>我们称这一组向量是正交向量，由正交column vectors构成的矩阵用一个特殊字母$Q$表示。如果这组正交向量同时还是单位向量，我们叫它单位正交向量。如果columns仅仅正交，而不是单位向量的话，点乘仍然会得到一个对角矩阵，但是它的性质没有那么好。</p><h3 id="性质">性质</h3><ol><li>满足$Q^T Q=I$。</li><li>如果$Q$是方阵，那么$Q^T = Q^{-1}$，即转置等于逆。</li><li>如果$Q$是方阵的话，$QQ^T = Q^T Q= I$。</li><li>如果$Q$是rectangular的话，$QQ^T =I$不成立，而$Q^T Q =I$依然成立。</li></ol><h2 id="用-q-取代-a-进行正交投影">用$Q$取代$A$进行正交投影</h2><p>假设矩阵$A$的所有column vectors都是orthonormal的，$a$就变成了$q$，$A^T A$就变成了$Q^T Q=I$，所以$Ax=b$的解变成了$\bar{x} = Q^T b$，而投影矩阵变成了$P=QQ^T $。</p><h2 id="gram-schmidi正交化">Gram-Schmidi正交化</h2><p>Gram-Schmidt正交化过程就相当于是在不断的进行投影，这个方法的想法是从$n$个独立的column vector出发，构建$n$个正交向量，然后再单位化。拿$3$个过程举个例子。用$a,b,c$表示初始的$3$个独立向量，$A,B,C$表示三个正交向量，$q_1, q_2,q_3$表示三个正交单位向量。<br>第一个正交向量，直接对第一个向量单位化<br>$$A=a, q_1 = \frac{A}{\vert A\vert}$$<br>第二个正交向量，将第二个向量投影到第一个向量上，计算出一个和第二个向量正交的向量。<br>$$B=b-\frac{A^T B}{A^T A}A , q_2 = \frac{B}{\vert B\vert}$$<br>第三个正交向量，将第三个向量分别投影到第一个和第二个正交向量上，计算处第三个正交向量。<br>$$C=c - \frac{A^T C}{A^T A}A - \frac{B^T C}{B^T B}B , q_2 = \frac{C}{\vert C\vert}$$<br><img src="/2019/08/27/linear-algebra-orthogonality/gram_schmidi.jpg" alt="gram_schmidi"></p><h2 id="qr分解">QR分解</h2><p>假设一个矩阵$A$的列向量分别为$a,b,c$，最后经过一个三角矩阵$R$化简成一个正交矩阵$Q$，相应的列向量分别为$q_1,q_2,q_3$。<br>首先根据Gram-Schmidi计算处一组正交基$Q = \begin{bmatrix}q_1&amp;q_2&amp;q_3 \end{bmatrix}$。根据$A$，能直接计算出$Q$，那么如何得到$R$呢？我们假设$A=QR$，在$A$和$Q$已知的情况下，并且满足$Q^T Q = I$，我们可以左右两边同时乘上$Q^T $，就有$Q^T A = Q^T QR = R$，即$R=Q^T A$。</p><h2 id="参考文献">参考文献</h2><p>1.MIT线性代数公开课</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;正交性-orthogonality&quot;&gt;正交性（Orthogonality）&lt;/h2&gt;
&lt;p&gt;这一章主要介绍正交性相关的内容。包括正交向量，投影，正交子空间，正交基以及如果求一组正交基，最后介绍QR分解求线性方程组。&lt;/p&gt;
&lt;h2 id=&quot;正交向量-orthogo
      
    
    </summary>
    
      <category term="线性代数" scheme="http://mxxhcm.github.io/categories/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/"/>
    
    
      <category term="线性代数" scheme="http://mxxhcm.github.io/tags/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/"/>
    
      <category term="正交" scheme="http://mxxhcm.github.io/tags/%E6%AD%A3%E4%BA%A4/"/>
    
      <category term="orthogonality" scheme="http://mxxhcm.github.io/tags/orthogonality/"/>
    
  </entry>
  
  <entry>
    <title>vector spaces和subspaces（向量空间和子空间）</title>
    <link href="http://mxxhcm.github.io/2019/08/26/linear-algebra-vector-spaces%E5%92%8Csubspaces/"/>
    <id>http://mxxhcm.github.io/2019/08/26/linear-algebra-vector-spaces和subspaces/</id>
    <published>2019-08-26T11:17:41.000Z</published>
    <updated>2019-09-09T08:53:25.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="向量空间和子空间-vector-spaces-and-subspaces">向量空间和子空间（vector spaces and subspaces）</h2><p>这一件介绍和space相关的概念以及很多其他的基础知识。</p><h2 id="线性组合-linear-of-combinations">线性组合（Linear of Combinations）</h2><p>线性组合有两种：加法和数乘。</p><h3 id="定义">定义</h3><p>如果$v$和$w$是column vectors，$c,d$是标量，那么$cv+dw$是$v$和$w$的线性组合。</p><h2 id="向量空间-vector-spaces">向量空间（Vector Spaces）</h2><p>Vector spaces是向量的集合，通常表示为$\mathbb{R}^1 , \mathbb{R}^2 , \mathbb{R}^n $。$\mathbb{R}^5 $表示所有$5$维的column vectors。</p><h3 id="定义-v2">定义</h3><p>Space $\mathbb{R}^n $是所有$n$维column vectors $v$组成的space。</p><h2 id="子空间-subspaces">子空间（Subspaces）</h2><h3 id="定义-v3">定义</h3><p>某个vector space的subspace是满足以下条件的vectors的集合，如果$v$和$w$是subspace中的vectors，并且$c$是任意的salar，</p><ol><li>$v+w$还在subspace中</li><li>$cv$还在subspace</li></ol><p>也就是说subspace是对加法和数乘封闭的vectors set，所有的线性组合仍然还在这个subspace。</p><h3 id="例子">例子</h3><ol><li>所有的subspace都包括zero vector。</li><li>通过原点的直线都是subspace。</li><li>包含$v$和$w$的subspace一定得包含所有的线性组合$cv+dw$</li><li>给定两个subspace $S,T$<ol><li>$S\cup T$不是一个subspace</li><li>$S\cap T$是一个subspace，证明<br>假设$v,w$是$S\cap T$的，则$v,w\in S, v,w\in T$，$v+w\in S, v+w\in T, cv+dw \in S, cv+dw \in T$，所以$cv+dw \in S\cap T$</li></ol></li></ol><h2 id="列空间-column-space">列空间（Column Space）</h2><h3 id="创建矩阵的subspace">创建矩阵的subspace</h3><p>取矩阵$A$的column vectors，计算它们的所有线性组合，借得到了一个subspace</p><h3 id="定义-v4">定义</h3><p>给定矩阵$A$，$A$的所有column vectors的linear combinations组成的subspace称为column space，用$C(A)$表示。$C(A)$由$Ax$的所有可能取值构成。</p><h3 id="性质">性质</h3><ol><li>当且仅当$b$在$A$的column space中，$Ax=b$才有解。</li><li>假设$A$是$m\times n$矩阵，$A$的column space是$\mathbb{R}^m $的subspace。</li></ol><h2 id="零空间-nullspace">零空间（Nullspace）</h2><h3 id="定义-v5">定义</h3><p>矩阵$A$的nullspace是所有$Ax=0$的解构成的vector space，用$N(A)$表示。$N(A)$是$\mathbb{R}^n $的subspace，因为$x$是在$\mathbb{R}^n $中的$n$维向量，所以是$\mathbb{R}^n $的subspace。</p><h2 id="special-solution-主元-自由变量-special-solution-pivot-variables和free-variables-pivot-columns和free-columns">special solution，主元，自由变量（Special solution, Pivot variables和free variables, Pivot columns和free columns）</h2><h3 id="special-solution">special solution</h3><p>如果方程数量小于未知数数量，那么这个方程（组）有无穷多个解，为了表示这个方程组，指定special solution来表示它。<br>如方程组<br>\begin{cases}x_1+2x_2=0\\3x_1+6x_2 = 0 \end{cases}<br>上面的方程组其实是一个方程$x_1+2x_2=0$，两个未知数。随便的选择一个变量，让它的值为$1$，求出另一个$x$。比如令$x_2 = 1$，那么$x_1 = -2$。我们就称$(x_1=-2,x_2=1)$为一个special solution。<br>再给一个例子，$x+2y+3z=0$，有两个special solution，随机选择两个变量，分别让其中一个取$1$，剩余的另一个取$0$，求解出来最后的一个变量。</p><h3 id="主元-主元列-自由变量-自由列">主元，主元列，自由变量，自由列</h3><p>我们通常把选定的两个变量叫做free variables，其他的那些变量叫做pivot variables。比如第一个例子中，$x_2$是free variable，$x_1$是pivot variable。第二个例子中，$x_2, x_3$是free variables，$x_1$是pivot variables。主元所在的column叫做pivot column，free variables所在的column叫做free columns。</p><h2 id="秩-rank">秩（rank）</h2><p>矩阵$A$的秩（rank），用$r(A)$表示，它等于pivots的数量，等于column space的维度，等于row space的维度。</p><h2 id="消元法解-ax-0">消元法解$Ax=0$</h2><p>两个步骤：</p><ol><li>将矩阵$A$化为三交矩阵$U$</li><li>解$Ux=0$或者$Rx=0$</li></ol><h3 id="示例">示例</h3><p>矩阵$A= \begin{bmatrix}1&amp;1&amp;2&amp;3\\2&amp;2&amp;8&amp;10\\ 3&amp;3&amp;10&amp;13\end{bmatrix}$化成三角矩阵为：$U= \begin{bmatrix}1&amp;1&amp;2&amp;3\\0&amp;0&amp;4&amp;4\\ 0&amp;0&amp;0&amp;0\end{bmatrix}$，第一列和第三列是pivot columns，第二列和第四列是free columns，然后求出special solutions，再计算出通解。<br>对于每个free variabled都有一个special solution，$Ax=0$共有$r$个pivots，以及$n-r$个free variables，$A$的nullspace $N(A)$包含$n-r$个special solutions，$N(A)$具有如下的形式：<br>$$N = \begin{bmatrix} -F\\I\end{bmatrix}$$<br>其中$F$为free variables取特值的时候，pivtos的取值，$I$为free variables的取值。</p><h2 id="行简化阶梯形矩阵-thre-reduced-row-echelon-matrix">行简化阶梯形矩阵（Thre reduced row echelon matrix）</h2><p>行简化阶梯形矩阵是pivot colunmn恰好构成单位矩阵的矩阵，如：<br>$$U = \begin{bmatrix}1&amp;1&amp;0&amp;1\\0&amp;0&amp;1&amp;1\\0&amp;0&amp;0&amp;0\end{bmatrix}$$<br>所有的pivots都是$1$，主元所在列的其余位置都是$0$。<br>行简化阶梯形矩阵给出了很多有用信息：</p><ol><li>pivot columns</li><li>pivot rows</li><li>pivots是$1$</li><li>zero rows显示这一行是其他rows的lihear combination</li><li>free columns</li></ol><h2 id="ax-b-的通解">$Ax=b$的通解</h2><p>先求出particular solution，让所有的free variables取$0$，求解出pivots，得到一个solution，我们称它为particular solution。然后求出所有的special solutions，则$Ax=b$的通解可以表示为：<br>$$x = x_p + a x_{special_solution_1} + b x_{special_solution} + \cdots=x_p+x_n$$<br>即particular solution加上nullspace组成的新的vector sets。当没有free variables的时候，也就没有special solutions，nullspace为空。</p><!--当$b=0$的时候，我们可以求出通解$x$，当$b\neq 0$时，有点难。通过使用增广矩阵：$\left[A\ b\right]$，然后进行消元，得到$\left[R\ d\right]$，$R$是行间化阶梯形矩阵，$d$是$b$做了和$A$一样的变换后的结果。--><h2 id="列满秩">列满秩</h2><h3 id="定义-v6">定义</h3><p>对于$m\times n$的矩阵$A$，每一列都有一个pivot，rank $r=n$，matrix是瘦高的$(m\ge n)$，其实就相当于每一个column vector都用到了，没有多余的column vector。可以用以下的形式表示：<br>$$R = \begin{bmatrix}I\\0\end{bmatrix}=\begin{bmatrix}n\times n 单位矩阵\\m-n行零向量\end{bmatrix}$$</p><h3 id="属性">属性</h3><p>当$A$列满秩的时候，有以下结论：</p><ol><li>$A$的所有columns都是pivot columns</li><li>没有free variables，free columns和special solutions</li><li>Nullspace只有$x=0$</li><li>如果$Ax=b$有解，那么它只有一个解，或者一个解都没有</li></ol><h2 id="行满秩">行满秩</h2><h3 id="定义-v7">定义</h3><p>对于$m\times n$的矩阵$A$，如果$r=m$的话，$A$是一个矮胖的矩阵$(m\le n)$，每一行都有一个pivot。<br>$$R = \begin{bmatrix}I&amp;F\end{bmatrix}=\begin{bmatrix}m\times m 单位矩阵&amp;F\end{bmatrix}$$</p><h3 id="属性-v2">属性</h3><p>当$A$行满秩的时候，有以下结论：</p><ol><li>$A$的所有row都有pivots，$R$没有$0$向量</li><li>对于任何$b$，$Ax=b$都有解</li><li>column space就是整个$\mathbb{R}^m$</li><li>总共有$n-r= n-m$个special solutions。</li></ol><h2 id="秩和方程解个数之间的关系">秩和方程解个数之间的关系</h2><ol><li>$r=m, r=n$,可逆方阵，$Ax=b$有且只有一个解，$R=\begin{bmatrix}I\end{bmatrix}$</li><li>$r=m, r\lt n$,矮胖，$Ax=b$有无穷多个解，一个particular solution加上nullspace中的无穷个，$R=\begin{bmatrix}I&amp;F\end{bmatrix}$</li><li>$r\lt m, r=m$,瘦高，$Ax=b$没有或者只有一个解，如果$b$恰好在$A$的column space中有一个解，如果$b$恰好不在$A$的column space中无解，因为column vectors是相互独立的，所以$Ax=0$只有零解，$R=\begin{bmatrix}I\\0\end{bmatrix}$</li><li>$r\lt m, r\lt n$,并不满秩，$Ax=b$无解或者有无穷多个解，无解的情况是不在$A$的column space中，有解的情况是 在$A$的column space中，而在这部分中，又有无穷多个零解，所以要不无解要不无穷多个解。$R=\begin{bmatrix}I&amp;F\\0&amp;0\end{bmatrix}$</li></ol><h2 id="线性独立-linear-independence">线性独立（Linear independence）</h2><p>矩阵$A$的columns是linear independent的，当且仅当$Ax=0$的唯一解是$x=0$时。也就是说$A$的nullspace只有零向量的时候。</p><h3 id="定义-v8">定义</h3><p>给定一系列向量$v_1, \cdots, v_n$，$c_1 v_1 +\cdots+c_n v_n=0$当且仅当$c_1, \cdots, c_n=0$时候成立。</p><h2 id="生成-span">生成（Span）</h2><h3 id="定义-v9">定义</h3><p>使用一系列vectors生成space的过程就叫做span。</p><h2 id="行空间-row-space">行空间（Row Space）</h2><h3 id="定义-v10">定义</h3><p>使用矩阵的row vector生成的subspace就叫做row space，表示维$C(A<sup>T)$，它和$A</sup>T$的column space是相同的。</p><h2 id="基-basis">基（Basis）</h2><h3 id="定义-v11">定义</h3><p>生成space的最小vectors的independent vectors叫做这个space的一组basis，basis不是唯一的。</p><h3 id="示例-v2">示例</h3><p>矩阵的pivot columns是它的column space的一组basis。</p><h2 id="维度-dimension">维度（Dimension）</h2><h3 id="定义-v12">定义</h3><p>Space的dimension指的是每组basis中向量的个数。对于一个space，不同的baisis，它们的vectors不同，但是向量的个数都是相同的，这是space的属性。</p><h2 id="秩和维度的关系">秩和维度的关系</h2><p>和矩阵$A$相关的主要有四个subspace，分别是column space, nullspace, row space以及left nullspace。它们四个具有的属性如下所示：</p><ol><li>row space和column space的dimension都是$r$</li><li>nullspace和left nullspace的dimension是$n-r, m-r$，为什么是$n-r,m-r$，解$Ax=0$得到$x$是$n$维向量，也就是nullspace是$\mathbb{R}^n$的subspace，$A$的column space的dimension是$r$，free variables，free columns的个数就是$n-r$，special solutions的个数就是$n-r$，而nullspace的basis其实就是所有的special solutions，所以nullspace的dimension就是$n-r$，$m-r$同理。</li></ol><h2 id="a-和-r-的维度和基的关系">$A$和$R$的维度和基的关系</h2><p>这里的$A$是矩阵，$R$是行间化阶梯形矩阵</p><ol><li>$A$和$R$的row space相同，dimension相同，为$r$，basis相同</li><li>$A$和$R$的column space不同，dimension相同，也为$r$，basis不同</li><li>$A$和$R$的nullspace相同，dimension相同，为$n-r$,basis相同</li><li>$A$和$R$的left nullspace不同，dimension相同，为$m-r$</li></ol><p>Row space和nullspace都是$\mathbb{R}^n $的subspace，它们的dimension加起来等于n，但是这两个subspace加起来并不是$\mathbb{R}^n $。Row space是对$r$个$n$维pivot row vectors进行linear combination，而nullspace是对$n-r$个$n$维的解向量$x$进行linear combination，这里虽然都出现了$n$，但是第一个$n$是row vector的长度$n$，而第二个$n$是解向量的$n$。<br>同理，可以得column space和left nullspace都是$\mathbb{R}^m$的subspace。</p><h2 id="参考文献">参考文献</h2><p>1.MIT线性代数公开课</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;向量空间和子空间-vector-spaces-and-subspaces&quot;&gt;向量空间和子空间（vector spaces and subspaces）&lt;/h2&gt;
&lt;p&gt;这一件介绍和space相关的概念以及很多其他的基础知识。&lt;/p&gt;
&lt;h2 id=&quot;线性组合-li
      
    
    </summary>
    
      <category term="线性代数" scheme="http://mxxhcm.github.io/categories/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/"/>
    
    
      <category term="线性代数" scheme="http://mxxhcm.github.io/tags/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/"/>
    
      <category term="spaces" scheme="http://mxxhcm.github.io/tags/spaces/"/>
    
      <category term="subspaces" scheme="http://mxxhcm.github.io/tags/subspaces/"/>
    
  </entry>
  
  <entry>
    <title>reinforcement learning an introduction 第8章笔记</title>
    <link href="http://mxxhcm.github.io/2019/08/07/reinforcement-learning-an-introduction-%E7%AC%AC8%E7%AB%A0%E7%AC%94%E8%AE%B0/"/>
    <id>http://mxxhcm.github.io/2019/08/07/reinforcement-learning-an-introduction-第8章笔记/</id>
    <published>2019-08-07T08:32:52.000Z</published>
    <updated>2019-08-20T13:05:53.387Z</updated>
    
    <content type="html"><![CDATA[<h2 id="planning-and-learning-with-tabular-methods">Planning and Learning with Tabular Methods</h2><p>前面几章，介绍了MC算法，TD算法，再用$n$-step TD框架把它们统一了起来，此外，这些算法都属于model free的方法。这一章要介绍model based方法，model based方法主要用于planning，而model free的方法主要用于learning。</p><h2 id="models和planning">Models和Planning</h2><h3 id="model">Model</h3><h4 id="定义">定义</h4><p>Environment的model是agents用来预测environment会如何对agent的action做出响应的任何东西。给定一个state和一个action，model可以预测下一个state和action。如果model是stochastic的话，有多个可能的next states和rewards，每一个都有一定概率。Distribution models计算所有可能性以及相应概率，sample models根据概率进行采样，只计算采样的结果。</p><h4 id="用处">用处</h4><p>Model可以用来模仿或者仿真。给定一个start state和action，sample model产生一个transition，distribution model产生所有可能的transitions，并使用概率进行加权。给定start state和一个policy，sample model产生一个episode，distribution model产生所有可能的episodes以及相应的概率。Model被用来模拟environment或者产生simulated experience。</p><h4 id="示例">示例</h4><p>DP中使用的$p(s’, r|s,a)$就是distribution model，第五章中blackjack例子中使用的模型是sample model。Distribution model可以用来产生samples，但是sample model要比distribution models好获得。考虑投掷很多骰子的和，distribution models计算所有可能值和相应的概率，而sample model只计算根据概率产生的一个样本。</p><h3 id="planning">Planning</h3><h4 id="定义-v2">定义</h4><p>Planning的定义是给定model，不断的与environment交互生成或者改进poilcy的过程。有两种planning的方法，state space在state space中寻找一个optimal policy或者optimal path，这本书中介绍的方法都是这类。Plan-space planning在plans space中search。Plan-sapce 方法很难高效的应用到stochastic sequential decision problems，在这本书中不做过多介绍。<br>这一章要介绍的state-space planning方法具有相同的结构，这个结构在learing和planning中都有。基本的想法是：计算value funtions，通过应用simulated experience的update以及backup操作计算value functions。</p><h4 id="示例-v2">示例</h4><p>如DP方法，扫描整个state space，然后生成每一个state可能的transitions的distribution，用于计算update target，更新state’s estimated value。这一章介绍的其他方法，也满足这个结构，只不过计算target的方式顺序以及长度不同而已。</p><h4 id="planning和learning的关系">planning和learning的关系</h4><p>将planning方法表示成这种形式主要是为了强调planning方法和learning方法之间的联系。learning和planning的重点都是使用backup update op更新estimations of value functions。区别在于plannning使用了model生成的simulate experience，而learning使用environment生成的real experience。同时perfomance measure以及experience的灵活性也不同，但是由于相同的结构，许多learning的方法可以直接应用到planning上去，使用simulated experience代替real experience即可。<br>给出一个利用sample model和Q-learning结合起来的planning算法：</p><h3 id="learning算法示例">learning算法示例</h3><p>Random-sample one-step tabular Q-planning<br>Loop forever<br>$\qquad 1.$随机选择初始state $S\in \mathbb{S}, A\in \mathbb{A}$<br>$\qquad 2.$将$S,A$发送给sample model，得到next state和reward $S’, R$<br>$\qquad 3.$应用one-step Q-learning更新公式：<br>$\qquad\qquad Q(S,A) \leftarrow Q(S,A) + \alpha \left[R+\gamma mx_a Q(S’, a) - Q(S, A)\right]$</p><h2 id="dyna">Dyna</h2><h3 id="简介">简介</h3><p>Online的planning更新需要不断的与environment交互，从交互中获得的information可能会改变model，以及与environment的交互，所以可能model也需要不断的学习。这一节主要介绍Dyna-Q，将online planning需要的内容都整合了起来，Dyna算法中包含了planning, acting以及learning。<br>Planning中real experience至少有两个作用，一个是改进model，叫做model-learning，另一个是使用learning的方法直接改进value function和policy，叫做direct reinforcement learning。相应的关系如下图所示：<br><img src="/2019/08/07/reinforcement-learning-an-introduction-第8章笔记/model_learning.png" alt="model_learning"><br>如图所示，experience可以直接改进value function，也可以通过model间接改进value fucntion，叫做indirect reinforcement learning。Direct learning和indirect learning各有优势，indirect方法能够充分利用有限的experience，得到一个更好的policy；direct方法更简单，不会受到model bias的影响。</p><h3 id="dyna-q-简介">Dyna-$Q$简介</h3><p>Dyna-$Q$包含planning, acting, model-learning和direct RL。planning是random-sample one-step tabular Q-planning；direct RL就是one-step tabular Q-learning，model-learning是table-based并且假设environment是deterministic，对于每一个transition $S_t,A_t\rightarrow R_{t+1}, S_{t+1}$，model用表格的形式记录下$S_t,A_t$的prediction值是$R_{t+1}, S_{t+1}$。如下图是Dyna算法的整体框架图（Dyna-Q是一个示例）。<br><img src="/2019/08/07/reinforcement-learning-an-introduction-第8章笔记/dyna.png" alt="dyna_q"><br>左边使用real experience进行direct RL，右边是model-based的方法，从real experience中学习出model，然后利用model生成simulated experience。search control指的是从model生成的simulated experiences中选择指定starting state和actions的experience。最后，planning使用simulated experience更新value function。从概念上来讲，Dyna agents中planning, acting, model-learning以及direct RL几乎同时发生。但是在实现的时候，还是需要串行的进行。Dyna-$Q$中，计算量主要集中在planning上。具体的算法如下：</p><h3 id="tabular-dyna-q">Tabular Dyna-Q</h3><p>初始化$Q(s,a), Model(s,a), s\in \mathbb{S}, a\in \mathbb{A}$<br>Loop forever<br>$\qquad (a)S\leftarrow$ current state<br>$\qquad (b)A\leftarrow \epsilon$-greedy$(S,Q)$<br>$\qquad ( c )$采取action $A$，得到下一时刻的state $S’$和reward $R$<br>$\qquad (d)Q(S,A) \leftarrow Q(S,A) + \alpha\left[R+\gamma max_a Q(S’, a) -Q(S,A)\right]$<br>$\qquad (e)Model(S,A)\leftarrow R,S’$（假设deterministic environment)<br>$\qquad (f)$Loop repeat $n$ 次<br>$\qquad\qquad S\leftarrow$ 任意之前的观测状态<br>$\qquad\qquad A\leftarrow$ 在$S$处采取的任意action $A$<br>$\qquad\qquad R,S’\leftarrow Model(S,A)$<br>$\qquad\qquad Q(S,A) \leftarrow Q(S,A) + \alpha\left[R+\gamma max_a Q(S’, a) -Q(S,A)\right]$</p><p>其中$Model(s,a)$表示预测$(s,a)$的next state。(d)是direct RL，(e)是model-learning，(f)是planning。如果忽略了(e,f)，就是one-step tabular Q-learning。</p><h2 id="如果model出错">如果model出错</h2><p>前面给的例子很简单，model是不会错的。但是如果environment是stochastic的，或者samples很少的话，或者function approximation效果不好，或者environment刚刚改变，新的behaviour还没有被观测到，model就可能会出错。当model是错的话，就可能会产生suboptimal policy。在一些情况下，suboptimal会发现并且纠正model的error。当模型预测的结果比真实的结果好的时候就会发生这种情况。这里给出两个例子。一个environment变坏，一个environment变好。<br>第一个例子，有一个迷宫，如图所示。<br><img src="/2019/08/07/reinforcement-learning-an-introduction-第8章笔记/blocking_maze.png" alt="blocking mase"><br>刚开始，路在右边，1000步之后，右边的路被堵上了，左边有一条新的路。<br><img src="/2019/08/07/reinforcement-learning-an-introduction-第8章笔记/shortcut_maze.png" alt="blocking mase"><br>第二个例子，刚开始路在左边，3000步之后，右边有一条新的路，左边的路也被保留。<br>这又是一个exploration和exploitation问题。在planning中，exploration意味着尝试那些让model变得更好的actions，而exploitation意味着给定当前的model，选择optimal action。我们想要agents能够explore environment的变化，但是不影响performance。</p><h3 id="启发式搜索">启发式搜索</h3><p>Dyna-Q+ 使用了一个简单有效的heuristics，agent记录每一个state-action pair 在real environment中从上次使用到现在经历了多少个time steps，累计的时间步越多，说明这个pair改变的可能性越大，model不对的可能性越大。为了鼓励使用很久没有用的action，这里加了一个bonus reward，如果一个transition的reward是$r$，这个transition已经有$\tau$步没有试过，在planning进行update的时候，用一个新的reward $r + k\sqrt{\tau}$，$k$很小。不过新添加的bonus会有一定的cost，而且会对value function造成影响，但是在上面的两个例子中，这个cost比performance的提升要好。</p><h2 id="优先级">优先级</h2><p>在Dyna进行planning时，所有的state-action pair被选中的概率是一样的，显然是不合理的，planning的效率太低，可以有效的集中在某些state-action pair中。举个例子，在dyna_maze例子中，在第二个episode开始的时候，只有goal state前的那个state会产生正的reward，其余的都仍然是$0$，这里意味着很多updates都是无意义的。从一个value为$0$的state转移到另一个value为$0$的state，这个updates是没有意义的。只有那些在goal前面的state才会被更新，或者，那些value不为$0$的state的前面的state的value的更新才有意义。在planning过程中，有用的更新变多了，但是离effcient还差得多。在真实应用中，states可能相当大，这种没有重点的更新是非常低效的。。</p><h3 id="backward-focusing">backward focusing</h3><p>Dyna maze的例子给了我们一个提示，从goal state backward的进行更新。这里的goal state不是一个具体的goal state，指的是抽象的goal state。一般来说，包括goal state以及value改变的state。假设model的value都是正确的，如果agent发现了environment的一个变化以及相应state value的变化，首先更新这个state的predecessor states value，然后一直往前利用value改变的state进行更新就行了。这种想法叫做backward focusing。对于那些低效的state，不更新就是了。<br>Backward过程很快，会有很多state-action被更新，但并不是所有的pair是等价的。有的state value变化很大，有的变化很小。在stochastic环境中，对transiton probability变化的估计也会导致change大小的变化和紧急性的变化。所以，根据紧急性对不同的pair排一个优先级，然后根据这个优先级进行更新。用一个queue记录如果更新某个state-action pair的话，它的estimated value会变多少，根据这个值的大小排优先级。队头的元素取出来进行更新以后，计算它的predecessor pair变化大小。如果这个大小大于某个阈值，就使用新的优先级，把它加入队列，如果queue中有这个pair了，queue中保存大的优先级。完整的算法如下所示：</p><h3 id="deteriministic-environment下的优先级">deteriministic environment下的优先级</h3><p>初始化$Q(s,a), Model(s,a), \forall s, a$，置$PQueue$为空<br>Loop forever<br>$\qquad(a) S\leftarrow$ currnet state<br>$\qquad(b) A\leftarrow policy(S,Q)$<br>$\qquad( c )$采取action $A$；得到下一个reward $R$和state $S’$<br>$\qquad(d)Model(S,A) \leftarrow R, S’$<br>$\qquad(e) P\leftarrow |R+\gamma max_aQ(S’,a) - Q(S,A)|$<br>$\qquad$(f)如果$P\gt \theta$，将$S,A$以$P$的优先级插入$PQueue$<br>$\qquad$(g) Loop repeat $n$次，当$PQueue$非空的时候<br>$\qquad S, A\leftarrow fisrt(PQueue)$<br>$\qquad R,S’\leftarrow Model(S,A)$<br>$\qquad Q(S,A) \leftarrow Q(S,A) +\alpha \left[R+\gamma max_a Q(S’,a) -Q(S,A)\right]$<br>$\qquad$Loop for all 预计能到$S$的$\bar{S}, \bar{A} $<br>$\qquad\qquad \bar{R} \leftarrow $predicted reward for $\bar{S}, \bar{A}, S$<br>$\qquad\qquad P\leftarrow |\bar{R}+\gamma max_aQ(S,a) -Q(\bar{S}, \bar{A})$<br>$\qquad\qquad$如果$P\gt \theta$，将$\bar{S},\bar{A}$以$P$的优先级插入到$PQueue$</p><p>推广到stochastic environment上，用model记录state-action pair experience的次数，以及next state，计算出概率，然后进行expected update。Expected update会计算许多低概率transition上，浪费资源，所以可以使用sample update。<br>Sample update相对于expected update的好处，相当于将完整的backup分解成单个更小的transition。这个idea是从van Seijen和Sutton(2013)的一篇论文中得到的，叫做&quot;small backups&quot;，使用单个的transition进行更新，像sample update，但是使用的是这个transition的概率，而不是sampling的$1$。</p><h2 id="expected-updates和sample-updates">Expected updates和Sample updates</h2><p>这本书介绍了不同的value function updates，就one-step方法来说，主要有三个binary dimensions。第一个是估计state values还是action values；第二个是估计optimal policy还是random policy的value，对应四种组合:$q_{*}, v_{*}, q_{\pi}, v_{\pi}$；第三个是使用expected updates还是sample updates，也是这节的内容。总共有八种组合，其中七种对应具体的算法，如下图所示：<br><img src="/2019/08/07/reinforcement-learning-an-introduction-第8章笔记/seven_backup.png" alt="seven_bakcup"><br>这些算法都可以用来planning，之前介绍的Dyna-$Q$使用的是$q_{*}$ sample update，也可以用$q_{*}$ expected update，还可以用$q_{\pi}$ sample updates。</p><h3 id="简介和比较">简介和比较</h3><p>Expected update对于每一个$(s,a)$ pair，考虑所有可能的next state和next action $(s’,a’)$，需要distributions model进行精确计算；而sample update仅仅需要sample model，考虑一个next state，会有采样误差。所以expected upadte一定要比sample update好，但是需要的计算量也大。当环境是deteriministic的话，expected udpaet和sample update其实是一样的，只有在stochastic环境下才有区别。<br>假设每一个state有$b$个next state，expected upadte要比sample update的计算量大$b$倍。如果有足够的时间进行完全的expected update，进行一次完全的expected update一定比进行$b$次sample update好，因为虽然计算次数相等，但是sample update有sampling error；如果没有足够的时间的话，在计算次数小于$b$次的时候，sample update要比expected update好，sample update至少进行了一部分improvement，而sample update只有完全进行$b$次计算后才会得到正确的value function。而当state-action pairs很多的情况下，进行完全的expected update是不可能的，sample update是一个可行的方案。<br>给定一个state-action pair，到底是进行一些（小于$b$次）expected update好呢，还是进行$b$次sample updates好呢？如下图所示，展示了expected update和sample update在不同的$b$下，estimation error关于计算次数的函数。<br><img src="/2019/08/07/reinforcement-learning-an-introduction-第8章笔记/fig_8_7.png" alt="fig_8_7"><br>在这个例子中，所有$b$个后继状态出现的可能性相等，开始的error为$1$。假设所有的next state value都是正确的，expected update完成之后，error从$1$变成了$0$。而sample updates根据$\sqrt{\frac{b-1}{bt}}$减少error。对于$b$很大的值来说，进行$b$的很小比例次数的更新，error就会下降很快。</p><h3 id="sample-updates的好处">Sample updates的好处</h3><p>上图中，sample update的结果可能要比实际结果差一些。在实际问题中，后继状态的value会被它们自身更新。他们的estimate value会更精确，从后继状态进行backup也会更精确。</p><h3 id="结论">结论</h3><p>在stochastic环境下，如果每个state处可能的next state数量非常多，并且有很多个states，那么sample update可能要比expected update好。</p><h2 id="trajectory-sampling">Trajectory Sampling</h2><p>这里比较两种distributing updates。<br>DP进行update的时候，每一次扫描整个state spaces，很多state其实是没有用的，没有focus，所有的states地位一样。原则上，只要确保收敛，任何distributed方法都行，然而在实际上常用的是exhaustive sweep。<br>第二种是根据一些distributions从state space或者state action space中进行采样。Dyna-$Q$使用均匀分布采样，和exhaustive sweep问题一样，没有focus。使用on-policy distribution是一种很不错的想法，根据当前的policy不断的与model交互，产生一个trajectory。Sample state transitions以及reward是model给出来的，sample action是当前的policy给出来的。这种方法叫做generating experience和update trajectory sampling。</p><h3 id="原因">原因</h3><p>如果on-policy的distribution是已知的，可以根据这个distribution对所有的states进行加权，但是这和exhaustive sweeps的计算量差不多。或者从distribution中采样state-action paris，这比simulating trajectories好在哪里呢？事实上我们不知道distribution，当policy改变，计算distribution的计算量和policy evaluation相等。</p><h3 id="分析">分析</h3><p>使用on-policy distribution可以去掉很多我们不感兴趣的内容，或者一遍又一遍的进行无用的重复更新。通过一个小例子分析它的效果。使用one-step expected updates，公式如下：<br>$$Q(s,a) \leftarrow \sum_{s’,r} \hat{p}(s’,r|s,a)\left[r+\gamma max_{a’}Q(s’,a’)\right]$$<br>使用均匀分布的时候，对所有的state-action pair进行in place的expected update更新；使用on-policy的时候，使用当前$\epsilon$-greedy policy（$\epsilon=0.1$）直接生成episodes，对episodes中出现的state-action pair进行expected update。也就是说一个是随机选的，一个是on-policy中出现的，选择的方式不一样，但是都是进行expected update。<br>简单介绍一下environment。每个state都有两个action，等可能性的到达$b$个next states，$b$对于所有state-action pair都是一样的，所有的transition都有$0.1$的概率到达terminal state。Expected reward服从均值为$0$,方差为$1$的高斯分布。<br>在planning过程中的任何一步都可以停止，根据当前估计的action value，计算greedy policy $\hat{\pi}$下start state的value $v_{\hat{\pi}}(s_0)$，也就是说告诉我们使用greedy重新开始一个新的episodes，agent的表现会怎么样。（假设model是正确的）。<br><img src="/2019/08/07/reinforcement-learning-an-introduction-第8章笔记/fig_8_8.png" alt="fig_8_8"><br>上半部分是进行了$200$次sample任务，$1000$个states，$b$分别为$1,3,10$的结果。在图中根据on-policy 采样更新的方法，一开始很快，时间一长，就慢下来了。当$b$越小的时候，效果越好，越快。另一个实验中表明，随着states数量的增加，on-policy distribution采样的效果也在变好。<br>使用on-policy distribtion进行采样，能够帮助我们关注start state的后继状态。如果states很多，并且$b$很小，这个效果很好并且会持续很久。在长时间的计算中，使用on-policy distribution采样可能会有副作用，因为一直在更新那些value已经正确的states。对它们进行采样没用了，还不如采样一些其他的states。这就是为什么在长时间的运行中使用均匀分布进行采样的效果更好。</p><h2 id="real-time-dp">Real-time DP</h2><p>Real time DP是DP的on-policy trajectory sampling版本。RLDP和上一节介绍的on-policy expected update的区别在update的方式不同，上一届实际上使用的是state-action pair，而这一节DP用的是state，它的更新公式是：<br>$$v_{k+1}(s) = \max_a \sum_{s’,r}p(s’,r|s,a) \left[r+\gamma v_k(s’)\right]$$<br>一个是更新action value function，一个是更新state valut function。</p><h3 id="irrelevant-states">irrelevant states</h3><p>如果trajectories可以仅仅从指定的states开始，我们感兴趣的仅仅是给定policy下states的value，on-policy trajectory sampling可以完全跳过给定policy下不能到达的states。这些states跟prediction问题无关。对于control问题，目标是找到optimal policy而不是evaluating一个给定的policy，可能存在无论从哪个start states开始都无法到达的states，所以就没有必要给出这些无关states的action。我们需要的是一个optimal partial policy，对于relevant states，给出optimal policy，而对于irrelevant states，给出任意的actions。<br>找到这样一个policy，按道理来说需要访问所有的state-action pairs无数次，包括那些无关状态。<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.137.1955&amp;rep=rep1&amp;type=pdf" target="_blank" rel="noopener">Korf</a>证明了在满足以下条件的时候，能够在很少访问relevant states甚至不访问的时候，找到这样一个policy。</p><ol><li>goal state的初始value为$0$</li><li>存在至少一个policy保证从任何start state都能到达goal state</li><li>所有到达的非goal states的reward是严格为负的</li><li>所有states的初始value要大于等于optimal values（可以设置为$0$，一定比负值大）</li></ol><p>如果满足这些条件，RTDP一定会收敛到relevant states的optimal policy。</p><h2 id="决策时进行planning">决策时进行planning</h2><h3 id="backgroud-planning">backgroud planning</h3><p>进行planning至少有两种方法。第一种是已经介绍的DP和Dyna这些算法，使用planning基于从model得到的simulated experience不断的改进policy和value function。通过比较某一个state处不同state-action pairs value值的大小选择action。在action被选择之前，planning更新所有的$Q$值。这里planning的结果被很多个states用来选择action。这种planning叫做background planning。</p><h3 id="decision-time-planning">decision-time planning</h3><p>第二种方法是使用planning输出单个state的action，遇到一个新的state $S_t$，输出是单个的action $A_t$，然后再下一个时间步根据$S_{t+1}$继续计算$A_{t+1}$。最简单的一个例子是当只有states value可以使用的时候，通过比较model预测每一个action能够到达的后继state的value（也就是使用after state value）选择相应的action，。这种方法叫做decision-time planning。<br>事实上，decision-time planning和background planning的流程是一样的，都是使用simulated experience到backup values再到policy。只不过decision-time planning只是对当前可访问的单个state和action的value和policy进行planning。在许多decision-time planning的过程中，用于选择当前state对应的action时，使用到的value和policy用过以后都被丢弃了，这并不会造成太大的计算损失，因为在大部分任务中很多states在短时间内都不会被再次访问到。</p><h3 id="应用场景">应用场景</h3><p>decision-time planning适用于不需要快速实时相应的场景，比如各种下棋。如果需要快速响应的，那么最好使用backgroud planning计算一个policy可以快速应用到新的states。</p><h2 id="启发式搜索-v2">启发式搜索</h2><p>AI中经典的state-space方法都是decision-time planning方法，统称为heuristic search。在启发式搜索中，会使用树进行搜索，近似的value function应用到叶子节点，进行backup到根节点（当前state），相应的backup diagram和optimal的expected updates类似。根据计算的backed-up值，选择相应的action之后，丢弃这些值。<br>传统的启发式搜索并不保存backup value，它更像是多步的greedy policy，目的是更好的选择actions。如果我们有一个perfect model以及imperfect action value function，搜索的越深结果越好。如果search沿着所有可能的路一直到episode结束，那么imperfect value function的结果被抵消了，选出来的action也是optimal。如果搜索的深度$k$足够深，$\gamma^k$接近于$0$,那么找到的action也是近乎于最优的。当然，搜索的深度越深，需要的计算资源就越多。<br>启发式搜索的的updates集中在current state，它的search tree集中在接下来可能的states和actions，这也是它的结果为什么很好的原因。在某些特殊的情况下，我们可以将具体的启发式搜索算法构建出一个tree，自底向上的执行one-step update，如下图所示：<br><img src="/2019/08/07/reinforcement-learning-an-introduction-第8章笔记/heuristic_search_tree.png" alt="heuristic_search_tree"><br>如果update是有序的，并且使用tabular representation，整个updates可以看成深搜，所有的state-space搜索可以看成很多个one-step updates的组合。我们得出了一个结论，搜索深度越深，性能越好的原因不是multistep updates的使用，因为它实际上使用的是多个one-step update。真正的原因是更新都集中在current state downstream的states和actions上，所有的计算都集中在candidate actions相关的states和actions上。</p><h2 id="rollout算法">Rollout算法</h2><h3 id="什么是rollout算法">什么是Rollout算法</h3><p>Rollout算法是将Monte Carlo Control应用到从current state开始的simulate trajectories上的decision-time planning算法。Rollout算法根据给定的policy，这个policy叫做rolloutpolicy，从当前state可能采取的所有action开始生成很多simulated trajectories，对得到的returns进行平均估计aciton values。当action value估计的足够精确的时候，选择最大的那个action执行。<br>和MC Control的区别在于，MC Control的目的是估计整个action value function $q_{\pi}$或者$q_{*}$，而Rollout算法的目的是对于每一个current state，在一个给定policy下估计每一个可能的action的value。Rollout是decision-time planning算法，计算完相应的estimate action value之后，就丢弃它们。</p><h3 id="rollout算法做了什么">Rollout算法做了什么</h3><p>Rollout算法和policy iteration差不多。在policy improvement theorem理论中，如果在一个state处采取新的action，它的value要比原来的value高，那么就说这个新的policy要比老的policy好。Rollout算法在每个current state处，估计不同的state action value，然后选择最好的，其实就相当于one-step的policy iteration，或者更像on-step的asynchronous value iteration。<br>也就是说，rollout算法的目的是改进rollout policy，而不是寻找最优的policy。Rollout算法非常有效，但是它的效果也取决于rollout policy，roloout policy越好，最后算法生成的policy就越好。</p><h3 id="如何选择好的rollout-policy">如何选择好的rollout policy</h3><p>更好的rollout policy也就需要更多的资源，因为是decision-time算法，时间约束一定要满足，rollout算法的计算时间取决于每一个decision需要选择的action数量，sample trajectories的长度，rollout policy做决策的事件，以及足够的sample trajectories的数量。接下来给出几种方法去权衡这些影响因素：<br>第一个方法，MC trials都是独立的，所以可以使用多个分开的处理器运行多个trials。第二个方法是在simulated trajectories结束之前截断，通过一个分类评估函数对truncated returns进行修正。第三个方法是剪枝，剪掉那些不可能是最优的actions，或者那些和当前最优结果没啥差别的acitons。</p><h3 id="rollout算法和learning算法的关系">rollout算法和learning算法的关系</h3><p>Rollout算法并不是learning算法，因为它没有保存values和polices。但是rollout算法具有rl很多好的特征。作为MC Control的应用，他们使用sample trajectories，避免了DP的exhausstive sweeps，同时不需要使用distributin models，使用sample models。最后，rollout算法还使用了policy improvement property，即选择当前estimate action values最大的action。</p><h2 id="monte-carlo-tree-search">Monte Carlo Tree Search</h2><p>MCTS是decision-time planning算法，实际上，MCTS是一个rollout算法，它在上一节介绍的rollout算法上，加上了acucumulating value estimates的均值。MCTS是AlphaGO的基础算法。<br>每到达一个新的state，MCTS根据state选择action，到达新的state，再选择action，持续下去。在大多数情况下，simulated trajectories使用rollout policy生成actions。当model和rollout policy都不需要大量计算的时候，可以在短时间内生成大量simulated trajectories。只保留在接下来的几步内最后可能访问到的state-action pairs的子集，形成一棵树，如下图所示。任意simulated trajectory都会经过这棵树，并且从叶子节点退出。在tree的外边，使用rollout policy选择actions，在tree的内部使用一个新的policy，称为tree policy，平衡exploration和exploitation。Tree policy可以使用$\epsilon$-greedy算法。<br><img src="/2019/08/07/reinforcement-learning-an-introduction-第8章笔记/mcts.png" alt="mcts"><br>MCTS总共有四个部分，一直在迭代进行，第一步是Selection，根据tree policy生成一个episode的前半部分；第二步是expansion，从选中的叶子节点处的上一个节点探索其他没有探索过的节点；第三步是simulation，从选中节点，或者第二步中增加的节点处，使用rollout policy生成一个episode的后半部分；第四步是backup，从第一二三步得到的episode进行backup。这四步一直迭代下去，等到资源耗尽，或者没有time的时候，就退出，然后根据生成的tree中的信息选择相应的aciton，比如可以选择root node处action value最大的action，也可以选择最经常访问的action。<br>MCTS是一种rollout算法，所以它拥有online，incremental，sample-based和policy improvement等优点。同时，它保存了tree边上的estimate action value，并且使用sample update进行更新。优势是让trivals的初始部分集中在之前simulated的high-return trajectories的公共部分。然后不断的expanding这个tree，高效增长相关的action value table，通过这样子，MCTS避免了全局近似value funciton，同时又能够利用过去的experience指定exploraion。</p><h2 id="参考文献">参考文献</h2><p>1.《reinforcement learning an introduction》第二版</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;planning-and-learning-with-tabular-methods&quot;&gt;Planning and Learning with Tabular Methods&lt;/h2&gt;
&lt;p&gt;前面几章，介绍了MC算法，TD算法，再用$n$-step TD框架把它们统
      
    
    </summary>
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="model based" scheme="http://mxxhcm.github.io/tags/model-based/"/>
    
  </entry>
  
  <entry>
    <title>python selenium</title>
    <link href="http://mxxhcm.github.io/2019/08/06/python-selenium%E5%AE%89%E8%A3%85/"/>
    <id>http://mxxhcm.github.io/2019/08/06/python-selenium安装/</id>
    <published>2019-08-06T03:24:09.000Z</published>
    <updated>2019-08-06T07:18:18.592Z</updated>
    
    <content type="html"><![CDATA[<h2 id="ubuntu-安装chrome-driver"><a href="#ubuntu-安装chrome-driver" class="headerlink" title="ubuntu 安装chrome driver"></a>ubuntu 安装chrome driver</h2><ol><li>下载chrome driver<br><a href="http://chromedriver.chromium.org/downloads" target="_blank" rel="noopener">http://chromedriver.chromium.org/downloads</a><br>根据自己的操作系统和chrome下载相应的chrome driver</li><li>解压<br>将解压后的chromedriver放置在PATH环境变量中的任意目录即可，我是放置在了/usr/local/bin</li></ol><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>1.<a href="https://www.jianshu.com/p/dd848e40c7ad" target="_blank" rel="noopener">https://www.jianshu.com/p/dd848e40c7ad</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;ubuntu-安装chrome-driver&quot;&gt;&lt;a href=&quot;#ubuntu-安装chrome-driver&quot; class=&quot;headerlink&quot; title=&quot;ubuntu 安装chrome driver&quot;&gt;&lt;/a&gt;ubuntu 安装chrome driv
      
    
    </summary>
    
      <category term="python" scheme="http://mxxhcm.github.io/categories/python/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="selenium" scheme="http://mxxhcm.github.io/tags/selenium/"/>
    
  </entry>
  
  <entry>
    <title>python-spider</title>
    <link href="http://mxxhcm.github.io/2019/08/06/python-spider/"/>
    <id>http://mxxhcm.github.io/2019/08/06/python-spider/</id>
    <published>2019-08-06T02:53:56.000Z</published>
    <updated>2019-08-09T08:37:45.484Z</updated>
    
    <content type="html"><![CDATA[<h2 id="requests"><a href="#requests" class="headerlink" title="requests"></a>requests</h2><p>获取网页<br>response = requests.post(url, headers)<br>返回的response包含有网页返回的内容。<br>response.text以文字方式访问。</p><h2 id="beautiful-soup"><a href="#beautiful-soup" class="headerlink" title="beautiful soup"></a>beautiful soup</h2><p>soup.find_all()</p><h2 id="selenium"><a href="#selenium" class="headerlink" title="selenium"></a>selenium</h2><ol><li><p>xpath查找<br>查找class为”wos-style-checkbox”，type为”checkbox”的任意elements。可以把*号换成input，就变成查找满足上述条件的input elemetns。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">driver.find_elements_by_xpath(<span class="string">"//*[@type='checkbox'][@class='wos-style-checkbox']"</span>)</span><br></pre></td></tr></table></figure></li><li><p>访问元素的内容<br>element.text</p></li><li><p>复选框选中与取消选中</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">checkbox = driver.find_element_by_id(<span class="string">"checkbox"</span>)</span><br><span class="line"><span class="keyword">if</span> checkbox.is_selected():  <span class="comment">#如果被选中</span></span><br><span class="line">    checkbox.click()    <span class="comment"># 再点击一次，就变成了取消选中</span></span><br><span class="line"><span class="keyword">else</span>:   <span class="comment"># 如果没有被选中</span></span><br><span class="line">    checkbox.click()    <span class="comment"># 再点击一次，就变成了选中</span></span><br></pre></td></tr></table></figure></li><li><p>提交表单</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">keywords = driver.find_element_by_id(<span class="string">"input"</span>)</span><br><span class="line"><span class="comment"># 清空表单默认值</span></span><br><span class="line">keywords.clear()</span><br><span class="line"><span class="comment"># 提交表单</span></span><br><span class="line">keywords.send_keys(values)</span><br></pre></td></tr></table></figure></li><li><p>输完表单内容需要回车<br>直接在表单内容中添加\n即可。</p></li><li><p>下拉框<br>使用Select对象</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">from selenium.webdriver.support.ui import Select</span><br><span class="line">from selenium import webdriver</span><br><span class="line">s = Select(driver.find_element_by_id(&quot;databases&quot;))</span><br><span class="line"></span><br><span class="line">s.select_by_value(&quot;value&quot;)</span><br><span class="line">s.select_by_index(index)</span><br><span class="line">s.select_by_visible_text(&quot;visible_text&quot;)</span><br></pre></td></tr></table></figure></li><li><p>模拟鼠标操作</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">from selenium.webdriver.common.action_chains import ActionChains</span><br><span class="line"></span><br><span class="line"># 定位element</span><br><span class="line">arrow = driver.find_element_by_id(&quot;next&quot;)</span><br><span class="line"># 单击</span><br><span class="line">ActionChains(driver).click(arrow).perform()</span><br><span class="line"># 双击</span><br><span class="line">ActionChains(driver).double_click(arrow).perform()</span><br></pre></td></tr></table></figure></li><li><p>display:none和visible: hidden<br>display:none表示完全不可见，不占据页面空间，而visible: hidden仅仅隐藏了element的显示效果，仍然占据页面空间，并且是可以被定位到，但是无法被访问（如selenium的click,clear以及send_keys等，会报错ElementNotVisibleException’: Message: Element is not currently visible and so may not be interacted with）。</p></li></ol><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>1.<a href="https://www.w3schools.com/xml/xml_xpath.asp" target="_blank" rel="noopener">https://www.w3schools.com/xml/xml_xpath.asp</a><br>2.<a href="https://devhints.io/xpath" target="_blank" rel="noopener">https://devhints.io/xpath</a><br>3.<a href="https://zhuanlan.zhihu.com/p/31604356" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/31604356</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;requests&quot;&gt;&lt;a href=&quot;#requests&quot; class=&quot;headerlink&quot; title=&quot;requests&quot;&gt;&lt;/a&gt;requests&lt;/h2&gt;&lt;p&gt;获取网页&lt;br&gt;response = requests.post(url, headers)
      
    
    </summary>
    
      <category term="python" scheme="http://mxxhcm.github.io/categories/python/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="spider" scheme="http://mxxhcm.github.io/tags/spider/"/>
    
  </entry>
  
  <entry>
    <title>reinforcement learning an introduction 第7章笔记</title>
    <link href="http://mxxhcm.github.io/2019/07/30/reinforcement-learning-an-introduction-%E7%AC%AC7%E7%AB%A0%E7%AC%94%E8%AE%B0/"/>
    <id>http://mxxhcm.github.io/2019/07/30/reinforcement-learning-an-introduction-第7章笔记/</id>
    <published>2019-07-30T01:59:50.000Z</published>
    <updated>2019-10-06T13:59:10.417Z</updated>
    
    <content type="html"><![CDATA[<h2 id="n-step-bootstrapping">n-step Bootstrapping</h2><p>这章要介绍的n-step TD方法，将MC和one-step TD用一个统一的框架整合了起来。在一个任务中，两种方法可以无缝切换。n-step方法生成了一系列方法，MC在一头，one-step在另一头。最好的方法往往不是MC也不是one-step TD。One-step TD每隔一个timestep进行bootstrapping，在许多需要及时更新变化的问题，这种方法很有用，但是一般情况下，经历了长时间的稳定变化之后，bootstrap的效果会更好。N-step TD就是将one-step TD方法中time interval的one改成了n。N-step方法的idea和eligibility traces很像，eligibility traces同时使用多个不同的time intervals进行bootstarp。</p><h2 id="n-step-td-prediction">n-step TD Prediction</h2><p>对于使用采样进行policy evaluation的方法来说，不断使用policy $\pi$生成sample episodes，然后估计$v_{\pi}$。MC方法用的是每一个episode中每个state的return进行更新，即无穷步reward之和。TD方法使用每一个state执行完某一个action之后的下一个reward加上下一个state的估计值进行更新。N-step方法使用的是每一个state接下来n步的reward之和加上第n步之后的state的估计值。相应的backup diagram如下所示：<br><img src="/2019/07/30/reinforcement-learning-an-introduction-第7章笔记/n_step_backup_diagram.png" alt="n_step_diagram"></p><p>n-step方法还是属于TD方法，因为n-step的更新还是基于时间维度上不同estimate的估计进行的。</p><blockquote><p>n-step updates are still TD methods because they still chagne an erlier estimate based on how it differs from a later estimate. Now the later estimate is not one step later, but n steps later。</p></blockquote><p>正式的，假设$S_t$的estimated value是基于state-reward sequence $S_t, R_{t+1}, S_{t+1}, R_{t+2}, \cdots, R_T,S_T$得到的。<br>MC方法更新基于completer return：<br>$$G_T = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots + \gamma^{T-t-1}R_T \tag{1}$$<br>one-step TD更新基于one-step return：<br>$$G_{t:t+1} = R_{t+1}+ \gamma V_t(S_{t+1}) \tag{2}$$<br>two-step TD更新基于two-step return：<br>$$G_{t:t+2} = R_{t+1}+ \gamma V_t(S_{t+1}) + \gamma^2 V_{t+1}(S_{t+2}) \tag{3}$$<br>n-step TD更新基于n-step return：<br>$$G_{t:t+n} = R_{t+1}+ \gamma V_t(S_{t+1}) + \gamma^2 V_{t+1}(S_{t+2}) +\cdots + \gamma^{n-1} R_{t+n} + \gamma^n V_{t+n-1}(S_{t+n}), n\ge1, 0\le t\le T-n \tag{4}$$<br>所有的n-step方法都可以看成使用n steps的rewards之和加上$V_{t+n-1}(S_{t+n})$近似其余项的rewards近似return。如果$t+n \ge T$时，$T$步以后的reward以及estimated value当做$0$，相当于定义$t+n \ge T$时，$G_{t:t+1} = G_t$。<br>当$n &gt; 1$时，只有在$t+n$时刻之后，$R_{t+n},S_{t+n}$也是已知的，所以不能使用real time的算法。使用$t+n-1$时刻的$V$近似估计$V_{t+n-1}(S_{t+n})$，将n-step return当做n-step TD的target，得到的更新公式如下：<br>$$V_{t+n}(S_t) = V_{t+n-1} (S_t) + \gamma(G_{t:t+n} - V_{t+n-1}(S_{t}))\tag{5}$$<br>当更新$V_{t+n}(S_t)$时，所有其他states的$V$不变，用公式来表示是$V_{t+n}(s) = V_{t+n-1}(s), \forall s\neq S_t$。在每个episode的前$n-1$步中，不进行任何更新，为了弥补这几步，在每个episode结束以后，即到达terminal state之后，仍然继续进行更新，直到下一个episode开始之前，依然进行updates。完整的n-step TD算法如下：</p><h3 id="n-step-td-prediction伪代码">n-step TD prediction伪代码</h3><p>n-step TD估计$V\approx v_{\pi}$<br>输入：一个policy $\pi$<br>算法参数：step size $\alpha \in (0, 1]$，正整数$n$<br>随机初始化$V(s), \forall s\in S$<br>Loop for each episode<br>$\qquad$初始化 $S_0 \neq terminal$<br>$\qquad$$T\leftarrow \infty$<br>$\qquad$Loop for $t=0, 1, 2, \cdots:$<br>$\qquad$ IF $t\lt T$, THEN<br>$\qquad\qquad$ 根据$\pi(\cdot|S_t)$执行action<br>$\qquad\qquad$ 接收并记录$R_{t+1}, S_{t+1}$<br>$\qquad\qquad$ 如果$S_{t+1}$是terminal ，更新$T\leftarrow t+1$<br>$\qquad$ END IF<br>$\qquad$ $\tau \leftarrow t - n + 1, \tau$是当前更新的时间<br>$\qquad$ If $\tau \ge 0$, then<br>$\qquad\qquad$ $G\leftarrow \sum_{i=\tau+1}^{min(\tau+n, T)} \gamma^{i-\tau -1} R_i$<br>$\qquad\qquad$ 如果$\tau+n \lt T$，那么$G\leftarrow G+ \gamma^n V(S_{\tau+n})$<br>$\qquad\qquad$ $V(S_{\tau}) \leftarrow V(S_{\tau}) +\alpha [G-V(S_{\tau})]$<br>$\qquad$ End if<br>Until $\tau = T-1$<br>n-step return有一个重要的属性叫做error reduction property，在最坏的情况下，n-step returns的期望也是一个比$V_{t+n-1}$更好的估计：<br>$$max_{s}|\mathbb{E}_{\pi}\left[G_{t:t+n}|S_t = s\right]- v_{\pi}(s)| \le \gamma^n max_s | V_{t+n-1}(s)-v_{\pi}(s)| \tag{6}$$<br>所以所有的n-step TD方法都可以收敛到真实值，MC和one-step TD是其中的一种特殊情况。</p><h2 id="n-step-sarsa">n-step Sarsa</h2><p>介绍完了prediction算法，接下来要介绍的就是control算法了。因为TD方法是model-free的，所以，还是要估计action value function。上一章介绍了one-step Sarsa，这一章自然介绍一下n-step Sarsa。n-step Sarsa的backup图如下所示：<br><img src="/2019/07/30/reinforcement-learning-an-introduction-第7章笔记/n_step_sarsa.png" alt="n_step_sarsa"><br>就是将n-step TD的state换成state-action就行了。定义action value的n-step returns如下：<br>$$G_{t:t+n} = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+1} + \cdots + \gamma^{n-1} R_{t+n} + \gamma^n Q_{t+n-1}(S_{t+n},A_{t+n}), n\ge 1, 0\le t\le T-n\tag{7}$$<br>如果$t+n\ge T$，那么$G_{t:t+n} = G_t$。<br>完整的$n$-step Sarsa如下所示：</p><h3 id="n-step-sarsa算法伪代码">n-step Sarsa算法伪代码</h3><p>n-step Sarsa算法，估计$Q\approx q_{*}$<br>随机初始化$Q(s,a),\forall s\in S, a\in A$<br>初始化$\pi$是相对于$Q$的$\epsilon$-greedy policy，或者是一个给定的不变policy<br>算法参数：step size $\alpha \in (0,1], \epsilon \gt 0$，一个正整数$n$<br>Loop for each episode<br>$\qquad$初始化$S_0\neq$ terminal<br>$\qquad$ 选择action $A_0= \pi(\cdot| S_0)$<br>$\qquad$ $T\leftarrow \infty$<br>$\qquad$ Loop for $t=0,1,2,\cdots$<br>$\qquad\qquad$ If $t\lt T$,then:<br>$\qquad\qquad\qquad$ 采取action $A_t$，<br>$\qquad\qquad\qquad$ 接收rewared $R_{t+1}$以及下一个state $S_{t+1}$<br>$\qquad\qquad$ 如果$S_{t+1}$是terminal，那么<br>$\qquad\qquad$ $T\leftarrow t+1$<br>$\qquad\qquad$ 否则选择$A_{t+1} = \pi(\cdot|S_{t+1})$<br>$\qquad\qquad$End if<br>$\qquad\qquad$ $\tau \leftarrow t-n+1$<br>$\qquad\qquad$ If $\tau \ge 0$<br>$\qquad\qquad\qquad$ $G\leftarrow \sum_{i=\tau +1}^{min(\tau+n, T)} \gamma^{i-\tau -1} R_i$<br>$\qquad\qquad\qquad$ If $\tau+n \le T$,then<br>$\qquad\qquad\qquad$ $G\leftarrow G+\gamma^n Q(S_{\tau+n}, A_{\tau+n})$<br>$\qquad\qquad\qquad$ $Q(S_{\tau}, Q_{\tau}) \leftarrow Q(S_{\tau}, Q_{\tau}) + \alpha \left[G-Q(S_{\tau}, Q_{\tau})\right]$<br>$\qquad\qquad$End if<br>$\qquad$ Until $\tau =T-1$</p><h3 id="n-step-expected-sarsa">n-step Expected Sarsa</h3><p>n-step Expected Sarsa的n-step return定义为：<br>$$G_{t:t+n} = R_{t+1} +\gamma R_{t+1} + \cdots +\gamma^{n-1} R_{t+n} + \gamma^n \bar{V}_{t+n-1}(S_{t+n}) \tag{8}$$<br>其中$\bar{V}_t(s) = \sum_{a}\pi(a|s) Q_t(s,a), \forall s\in S$<br>如果$s$是terminal，它的期望是$0$。</p><h2 id="n-step-off-policy-learning">n-step Off-policy Learning</h2><p>Off-policy算法使用behaviour policy采样的内容得到target policy的value function，但是需要使用他们在不同policy下采取某个action的relavtive probability。在$n$-step方法中，我们感兴趣的只有对应的$n$个actions，所以最简单的off-policy $n$-step TD算法，$t$时刻的更新可以使用importance sampling ratio $\rho_{t:t+n-1}$：<br>$$V_{t+n}(S_t) = V_{t+n-1} S_t + \alpha \rho_{t:t+n-1}\left[G_{t:t+n} - V_{t+n-1}(S_t)\right], 0\le t\le T \tag{9}$$<br>$\rho_{t:t+n-}$计算的是从$A_t$到$A_{t+n-1}$这$n$个action在behaviour policy和target policy下的relative probability，计算公式如下：<br>$$\rho_{t:h} = \prod_{k=t}^{min(h, T-1)} \frac{\pi(A_k|S_k)}{b(A_k|S_k)} \tag{10}$$<br>如果$\pi(A_k|S_k) = 0$，那么对应的$n$-step return对应的权重就应该是$0$，如果policy $pi$下选中某个action的概率很大，那么对应的return就应该比$b$下的权重大一些。因为在$b$下出现的概率下，很少出现，所以权重就该大一些啊。如果是on-policy的话，importance sampling ratio一直是$1$，所以公式$9$可以将on-policy和off-policy的$n$-step TD概括起来。同样的，$n$-step的Sarsa的更新公式也可以换成：<br>$$Q_{t+n}(S_t,A_t) = Q_{t+n-1}(S_t,A_t) + \alpha \rho_{t+1:t+n}\left[G_{t:t+n} - Q_{t+n-1} (S_t,A_t)\right], 0\le t\le T \tag{11}$$<br>公式$11$中的importance sampling ratio要比公式$9$中计算的晚一步。这是因为我们是在更新一个state-action pair，我们并不关心有多大的概率选中这个action，我们现在已经选中了它，importance sampling只是用于后续actions的选择。这个解释也让我理解了为什么Q-learning和Sarsa为什么没有使用importance sampling。完整的伪代码如下。</p><h3 id="off-policy-n-step-sarsa-估计-q">Off-policy $n$-step Sarsa 估计$Q$</h3><p>输入：任意的behaviour policy $b$, $b(a|s)\gt 0, \forall s\in S, a\in A$<br>随机初始化$Q(s,a), \forall s\in S, a\in A$<br>初始化$\pi$是相对于$Q$的greedy policy<br>算法参数：步长$\alpha \in (0,1]$，正整数$n$<br>Loop for each episode<br>$\qquad$初始化$S_0\neq terminal$<br>$\qquad$选择并存储$A_0 \sim b(\cdot|S_0)$<br>$\qquad T\leftarrow \infty$<br>$\qquad$Loop for $t=0,1,2,\cdots$<br>$\qquad\qquad$IF $t\lt T$,then<br>$\qquad\qquad\qquad$执行action $A_t$，接收$R_{t+1}, S_{t+1}$<br>$\qquad\qquad\qquad$如果$S_{t+1}$是terminal，那么$T\leftarrow t+1$<br>$\qquad\qquad\qquad$否则选择并记录$A_{t+1} \sim b(\cdot| S_{t+1})$<br>$\qquad\qquad$END IF<br>$\qquad\qquad \tau \leftarrow t-n +1$  (加$1$表示下标是从$0$开始的)<br>$\qquad\qquad$ IF $\tau \ge 0$<br>$\qquad\qquad\qquad \rho \leftarrow \prod_{i=\tau+1}^{min(\tau+n,T)} \frac{\pi(A_i|S_i)}{b(A_i|S_i)}$ （计算$\rho_{\tau+1:\tau+n}$，这里是不是写成了Expected Sarsa公式）<br>$\qquad\qquad\qquad G\leftarrow \sum_{i=\tau+1}^{min(\tau+n, T)}\gamma^{i-\tau -1}R_i$ （计算$n$个reward, $R_{\tau+1}+\cdots+R_{\tau+n}$）<br>$\qquad\qquad\qquad$如果$\tau+n \lt T$，$G\leftarrow G + \gamma^n Q(S_{\tau+n},A_{\tau+n})$ （因为没有$Q(S_T,A_T)$）<br>$\qquad\qquad\qquad Q(S_{\tau}, A_{\tau}) \leftarrow Q(S_{\tau}, A_{\tau})+\alpha \rho \left[G-Q(S_{\tau}, A_{\tau})\right]$（计算$Q(S_{\tau+n},A_{\tau+n})$）<br>$\qquad\qquad\qquad$确保$\pi$是相对于$Q$的greedy policy<br>$\qquad\qquad$ END IF<br>$\qquad$Until $\tau = T-1$<br>上面介绍的算法是$n$-step的Sarsa，计算importance ratio使用的$\rho_{t+1:t+n}$，$n$-step的Expected Sarsa计算importance ratio使用的是$\rho_{t+1:t+n-1}$，因为在估计$\bar{V}_{t+n-1}$时候，考虑到了last state中所有的actions。</p><h2 id="n-step-tree-backup算法">$n$-step Tree Backup算法</h2><p>这一章介绍的是不适用importance sampling的off-policy算法，叫做tree-backup algorithm。如下图所示，是一个$3$-step的tree-backup diaqgram。<br>沿着中间这条路走，有三个sample states和rewards以及两个sample actions。在旁边的是没有被选中的actions。对于这些没有选中的actions，因为没有采样，所以使用bootstrap计算target value。因为它的backup diagram有点像tree，所以就叫做tree backup upadte。或者更精确的说，backup update使用的是所用tree的叶子结点action value的估计值进行计算的。非叶子节点的action对应的是采样的action，不参与更新，所有叶子节点对于target的贡献都有一个权重正比于它在target policy下发生的概率。在$S_{t+1}$处的除了$A_{t+1}$之外所有action权重是$\pi(a|S_{t+1})$，$A_{t+1}$一点贡献没有；在$S_{t+2}$处所有没有被选中的action的权重是$\pi(A_{t+1}|S_{t+1})\pi(a’|S_{t+2})$；在$S_{t+3}$处所有没有被选中的action的权重是$\pi(A_{t+1}|S_{t+1})\pi(A_{t+2}|S_{t+2})\pi(a’’|S_{t+3})$<br>one-step Tree backup 算法其实就是Expected Sarsa：<br>$$G_{t:t+1} = R_{t+1} + \gamma \sum_a\pi(a|S_{t+1})Q(a, S_{t+1}), t\lt T-1 \tag{12}$$<br>two-step Tree backup算法return计算公式如下：<br>\begin{align*}<br>G_{t:t+2} &amp;= R_{t+1} + \gamma \sum_{a\neq A_{t+1}}\pi(a|S_{t+1})Q(a, S_{t+1}), t\lt T-1$ + \gamma\pi(A_{t+1}|S_{t+1})(R_{t+2} + \sum_{a} \pi(a|S_{t+2})Q(a, S_{t+2})\\<br>&amp;=R_{t+1} + \gamma \sum_{a\neq A_{t+1}}\pi(a|S_{t+1})Q(a, S_{t+1}), t\lt T-1$ + \gamma\pi(A_{t+1}|S_{t+1})G_{t+1:t+2}, t \lt T-2<br>\end{align*}<br>下面的形式给出了tree-backup的递归形式如下：<br>$$G_{t:t+n} = R_{t+1} + \gamma \sum_{a\neq A_{t+1}}\pi(a|S_{t+1})Q(a,S_{t+1}) + \gamma\pi(A_{t+1}|S_{t+1}) G_{t+1:t+n}, n\ge 2, t\lt T-1, \tag{13}$$<br>当$n=1$时除了$G_{T-1:t+n} = R_T$，其他的和式子$12$一样。使用这个新的target代替$n$-step Sarsa中的target就可以得到$n$-step tree backup 算法。<br>完整的算法如下所示：</p><h3 id="n-step-tree-backup-算法">$n$-step Tree Backup 算法</h3><p>随机初始化$Q(s,a),\forall s\in S, a\in A$<br>初始化$\pi$是相对于$Q$的$\epsilon$-greedy policy，或者是一个给定的不变policy<br>算法参数：step size $\alpha \in (0,1], \epsilon \gt 0$，一个正整数$n$<br>Loop for each episode<br>$\qquad$初始化$S_0\neq$ terminal<br>$\qquad$ 根据$S_0$随机选择action $A_0$<br>$\qquad$ $T\leftarrow \infty$<br>$\qquad$ Loop for $t=0,1,2,\cdots$<br>$\qquad\qquad$ If $t\lt T$,then:<br>$\qquad\qquad\qquad$ 采取action $A_t$，<br>$\qquad\qquad\qquad$ 接收rewared $R_{t+1}$以及下一个state $S_{t+1}$<br>$\qquad\qquad$ 如果$S_{t+1}$是terminal，那么<br>$\qquad\qquad$ $T\leftarrow t+1$<br>$\qquad\qquad$ 根据$S_{t+1}$随机选择action $A_{t+1}$<br>$\qquad\qquad$End IF<br>$\qquad\qquad$ $\tau \leftarrow t-n+1$<br>$\qquad\qquad\qquad$ IF $\tau+n\ge T$ then<br>$\qquad\qquad\qquad\qquad G\leftarrow R_T$<br>$\qquad\qquad\qquad$ ELSE<br>$\qquad\qquad\qquad\qquad G\leftarrow R_{t+1}+\gamma \sum_a\pi(a|S_{t+1})Q(a, S_{t+1})$<br>$\qquad\qquad\qquad$ END IF<br>$\qquad\qquad\qquad$Loop for $k = min(t, T-1)$ down $\tau+1$<br>$\qquad\qquad\qquad G\leftarrow R_k+\gamma^n\sum_{a\neq A_k}\pi(a|S_k)Q(a, S_k) + \gamma \pi(A_k|S_k) G$<br>$\qquad\qquad\qquad$ $Q(S_{\tau}, Q_{\tau}) \leftarrow Q(S_{\tau}, Q_{\tau}) + \alpha \left[G-Q(S_{\tau}, Q_{\tau})\right]$<br>$\qquad\qquad$End if<br>$\qquad$ Until $\tau =T-1$</p><h2 id="n-step-q-sigma">$n$-step $Q(\sigma)$</h2><p>现在已经讲了$n$-step Sarsa，$n$-step Expected Sarsa，$n$-step Tree Backup，$4$-step的一个backup diagram如下所示。它们其实有很多共同的特性，可以用一个框架把它们统一起来。<br>具体的算法就不细说了。</p><h2 id="参考文献">参考文献</h2><p>1.《reinforcement learning an introduction》第二版<br>2.<a href="https://stats.stackexchange.com/questions/335396/why-dont-we-use-importance-sampling-for-one-step-q-learning" target="_blank" rel="noopener">https://stats.stackexchange.com/questions/335396/why-dont-we-use-importance-sampling-for-one-step-q-learning</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;n-step-bootstrapping&quot;&gt;n-step Bootstrapping&lt;/h2&gt;
&lt;p&gt;这章要介绍的n-step TD方法，将MC和one-step TD用一个统一的框架整合了起来。在一个任务中，两种方法可以无缝切换。n-step方法生成了一系列方法
      
    
    </summary>
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="n-steps" scheme="http://mxxhcm.github.io/tags/n-steps/"/>
    
  </entry>
  
  <entry>
    <title>policy gradient</title>
    <link href="http://mxxhcm.github.io/2019/07/16/gradient-method-deterministic-policy-gradient/"/>
    <id>http://mxxhcm.github.io/2019/07/16/gradient-method-deterministic-policy-gradient/</id>
    <published>2019-07-16T02:31:55.000Z</published>
    <updated>2019-10-07T07:58:09.953Z</updated>
    
    <content type="html"><![CDATA[<h2 id="简介">简介</h2><p>本文主要介绍了deterministic policy gradient算法。它属于policy gradient的一种，只不过是将stochastic policy改成了deterministic policy，和stochastic policy一样，deterministic policy也有相应的deterministic policy gradient theorem。<br>实际上deterministic policy gradient是action value function的gradient的期望，这让deterministic policy gradient比stochastic policy gradient要更efficiently。还介绍了保证足够的exploration的off-policy actor-critic算法学习determinitic target policy。</p><h2 id="stochastic-policy-gradient-vs-deterministic-policy-gradiet">stochastic policy gradient vs deterministic policy gradiet</h2><h3 id="sotchastic-policy-gradient">sotchastic policy gradient</h3><p>Policy gradient的basic idea是用参数化的probability distribution $\pi_\theta(a|s) = \mathbb{P}\left[a|s,\theta\right]$表示policy，在state $s$处根据$\theta$表示的policy $\pi$随机选择action $a$。Policy gradient通过对policy 进行采样，根据stochastic policy gradient theorem近似计算$J$相对于参数$\theta$的累计梯度然后调整policy的参数$\theta$朝着使$J$更大的方向移动。</p><h3 id="deterministic-policy-gradient">deterministic policy gradient</h3><p>用$a=\mu_\theta(s)$表示deterministic policy，根据stochastic policy gradient theorm，很容易想到是否有deterministic policy gardient theorem，即朝着使得cumulative reward更大的方向更新policy的参数。<a href="https://arxiv.org/pdf/1509.02971.pdf" target="_blank" rel="noopener">deterministic policy gradient</a>证明了deterministic policy gradient是存在的，可以使用action value function的梯度近似得到。在某些情况下，deterministic policy gradient还可以看成stochastic policy gradient的特殊情况。</p><h3 id="spg-vs-dpg">spg vs dpg</h3><p>spg和dpg的区别和联系</p><ol><li>spg和dpg的第一个显著区别就是积分的space是不同的。Spg中policy gradient是在action和state spaces上进行积分的，而dpg的policy gradient仅仅在state space上进行积分。因此，计算spg需要更多samples，尤其是action spaces维度很高的情况下。</li><li>为了充分explore整个state和action space，需要使用stochastic policy。而在使用deterministic policy时，为了确保能够持续的进行explore，就需要使用off-policy的算法了，behaviour policy使用stochastic policy进行采样，target policy是deterministic policy。作者使用deterministic policy gradient推导出了一个off-policy的actor-critic方法，使用可导的function approximators估计action values，然后利用这个function的梯度更新policy的参数，同时为了确保policy gradient没有bias，使用而来compatible function。</li></ol><h2 id="stochastic-policy-gradient-theorem">stochastic policy gradient theorem</h2><p>详细的介绍可以查看<a href="http://mxxhcm.github.io/2019/09/07/gradient-method-policy-gradient/">policy gradient</a>。spg的基本想法就是朝着$J$的梯度方向更新policy的参数。对$J(\pi_{\theta})$对$\theta$求导，得到：<br>\begin{align*}<br>\nabla_{\theta} J(\pi_{\theta})&amp;=\int_S\rho^{\pi} (s) \int_A\nabla_\theta\pi_\theta (a|s)Q^{\pi} (s,a) dads \tag{1}\\<br>&amp;=\mathbb{E}_{s\sim \rho^{\pi} , a\sim \pi_\theta}\left[\nabla_\theta \log\pi_\theta(a|s)Q^{\pi} (s,a)\right] \tag{2}<br>\end{align*}<br>这就是policy gradient theorem。从这个theorem中，我们得到了一个有用的信息，state distribution $\rho\pi(s)$取决于policy parameters。我们计算$J$关于$\theta$的梯度时，需要计算statedistribution，很难实现，policy gradient theorem消去了对$\rho^{\pi} $的依赖，具有很重要的实用价值。同时利用log derivative trick将performance gradient的估计变成了一个期望，可以通过sampling估计。这个方法中需要使用$Q^{\pi} (s,a)$，估计$Q$不同方法就是不同的算法。比如最简单的使用sample return $G_t$估计$Q^{\pi} (s, a)$，就是REINFORCE算法。</p><h2 id="stochastic-actor-critic-算法">stochastic actor-critic 算法</h2><p>Actor-critic是一个基于policy gradient theorem的结构，包含policy和value function两个部分。Actors通过stochastic gradient ascent调整stochastic policy $\pi_\theta(s)$的参数$\theta$；同时critic用一个action-value function $Q^w (s,a)$近似$Q^\pi (s,a)$, $w$是function approximation的参数。Critic一般使用policy evaluation方法进行学习，比如使用td和mc等估计action value function $Q^w (s,a)\approx Q^\pi (s,a)$。一般来说，使用$Q^w (s,a)$代替真实的$Q^\pi (s,a)$会引入bias，但是，如果function approximator是compatible，即满足以下两个条件，就会是无偏的：</p><ol><li>$Q^w (s,a) = \nabla_\theta \log\pi_\theta(a|s)^T w$</li><li>参数$w$最小化mse: $\epsilon^2 (w)=\mathbb{E}_{s\sim \rho^\pi ,a\sim \pi_\theta}\left[(Q^w (s, a)-Q^\pi (s,a))^2 \right]$，这样就没有bias了，即：<br>$$\nabla_\theta J(\pi_\theta)=\mathbb{E}_{s\sim \rho^\pi, a\sim \pi_\theta}\left[\nabla_\theta \log\pi_\theta(a|s)Q^w (s,a)\right] \tag{3}$$</li></ol><p>直观上来说，条件1说的是compatible function approximators是stochastic policy梯度$\nabla_\theta \log\pi_\theta(a|s)$的线性features，条件2需要满足$Q^w (s,a)$是$Q^\pi (s,a)$的linear regression soulution。在实际应用中，条件2会放宽，使用如TD之类policy evaluation算法更efficiently的估计value function。事实上，如果条件1和2都满足的话，整个算法等价于没有使用critic，和REINFORCE算法很像。</p><h2 id="off-policy-actor-critic">off-policy actor critic</h2><p>在off-policy设置中，performance objective通常改成target policy的value function在behaviour policy的state distribution上进行平均，用$\beta(a|s)$表示behaviour policy：<br>\begin{align*}<br>J_\beta(\pi_\theta) &amp;= \int_S\rho^\beta (s) V^\pi (s)ds\tag{4}\\<br>&amp;=\int_S\int_A\rho^\beta (s)\pi_\theta(a|s)Q^\pi (s,a)dads \tag{5}\\<br>\end{align*}<br>对其求导和近似，得到：<br>\begin{align*}<br>\nabla_\theta J_\beta(\pi_\theta) &amp;\approx \int_S\int_A\rho^\beta (s)\nabla_\theta\pi_\theta(a|s) Q^\pi (s,a)dads\tag{6} \\<br>&amp;=\mathbb{E}_{s\sim \rho\beta, a\sim \beta}\left[\frac{\pi_\theta(a|s)}{\beta_\theta(a|s)}\nabla_\theta \log\pi_\theta(a|s) Q^\pi (s,a) \right]\tag{7}<br>\end{align*}<br>这个近似去掉了一项：$\nabla_\theta Q^\pi (s,a)$，已经有人证明去掉这一项之后仍然会收敛到局部最优。Off-policy actor-critic算法使用behaviour policy $\beta$生成trajectories，critic off-policy的从那些trajectories中估计state value function $V^v (s)\approx v^\pi (s)$。actor使用sgd从这些trajectories中off policy的更新policy paramters $\theta$，同时使用TD-error估计$Q^\pi (s,a)$。actor和critic都是用importance sampling ratio $\frac{\pi_\theta(a|s)}{\beta_\theta(a|s)}$。</p><h2 id="action-value-gradients">action value gradients</h2><p>绝大多数的model-free rl算法都属于GPI，迭代的policy evaluation和policy improvement。在contious action spaces中，greedy policy improvement是不可行的，因为在每一步都需要计算一个全局最大值。一个替代的方法是让policy朝着$Q$的gradient方向移动，而不是全局最大化$Q$。具体而言，对每一个访问到的state $s$，policy parameters $\theta^{k+1} $的更新正比于$\nabla_{\theta} Q^{ {\mu}^k } (s, \mu_{\theta}(s) )$。每一个state给出一个不同的方向，可以使用state distribution $\rho^{\mu} (s)$求期望，对最终的方向进行平均：<br>$$\theta^{k+1} = \theta^k + \alpha \mathbb{E}_{s\sim \rho^{ {\mu}^k } }\left[\nabla_\theta Q^{ {\mu}^k } (s, \mu_{\theta}(s))\right] \tag{8}$$<br>通过使用chain rule，我们可以看到policy improvement可以被分解成action-value对于action的gradient和policy相对于policy parameters的gradient：<br>$$\theta_{k+1} = \theta^k + \alpha \mathbb{E}_{s\sim \rho^{ {\mu}^k } }\left[\nabla_{\theta}\mu_{\theta}(s)\nabla_a Q^{ {\mu}^k } (s,a)|_{a=\mu_\theta(s_0)}\right] \tag{9}$$<br>按照惯例来说，$\nabla_{\theta}\mu_{\theta}(s)$是一个jacobian matrix，每一列是policy的$dth$ action对$\theta$的gradient $\nabla_\theta\left[\mu_\theta(s)\right]_d$。如果改变了policy，state distribution　$\rho^{\mu} $也会改变。如果不考虑distribution的变化的话，这个方法是保证收敛的。不过幸运的是，已经有人证明了和sgd一样，有deterministic policy gradient theorem，不需要计算state distributiond的gradient即可更新参数。</p><h2 id="deterministic-policy-gradient-theorem">deterministic policy gradient theorem</h2><h3 id="新的术语定义">新的术语定义</h3><ul><li>$\rho_0(s)$：初始状态分布</li><li>$\rho^{\mu} (s\rightarrow s’, k)$：在policy $\mu$下从state $s$出发经过$k$步到达$s’$的概率。</li><li>$\rho^{\mu} (s’)$：带有折扣因子的状态分布，定义为：<br>$$\rho^{\mu} (s’) = \int_S \sum_{k=0}^{\infty} \gamma^{k} \rho_0(s) \rho^{\mu} (s\rightarrow s’, k)ds\tag{10}$$</li></ul><h3 id="证明">证明</h3><p>deterministic policy定义为：$\mu_\theta: S\rightarrow A, \theta \in \mathbb{R}^n $。定义performance objective $J(\mu_\theta) =\mathbb{E}\left[G_0|\mu\right]$，将performance objective写成expectation如下：<br>\begin{align*}<br>J(\mu_\theta) &amp; = \int_S\rho^\mu (s) G_0 ds\tag{11}\\<br>&amp;= \mathbb{E}_{s\sim \rho^\mu }\left[G_0\right] \tag{12}<br>\end{align*}<br>给出deterministic policy gradient theorem：<br>假设MDP满足以下条件，即$\nabla_\theta\mu_\theta(s)$和$\nabla_a Q^\mu (s,a)$存在，那么deterministic policy gradient存在，<br>\begin{align*}<br>\nabla_\theta J(\mu_\theta) &amp;= \int_S\rho^\mu (s) \nabla_\theta\mu_\theta(s)\nabla_aQ^\mu (s,a)|_{a=\mu_\theta(s)}ds\tag{13}\\<br>&amp;=\mathbb{E}_{s\sim \rho^\mu }\left[\nabla_\theta\mu_\theta(s)\nabla_aQ^\mu (s,a)|_{a=\mu_\theta(s)}\right] \tag{14}<br>\end{align*}<br>证明：<br>\begin{align*}<br>\nabla_{\theta}V^{\mu} (s) &amp; =  \nabla_{\theta} Q^{\mu} (s, \mu(s))\tag{15}\\<br>&amp; = \nabla_{\theta} \left( R(s,\mu(s)) + \int_S \gamma p(s’|s,\mu(s))V^{\mu} (s’) ds’ \right)\tag{16}\\<br>&amp; = \nabla_{\theta}\mu(s) \nabla_a R(s,a)|_{a=\mu(s)} + \nabla_{\theta}\int_S \gamma p(s’|s,\mu(s))V^{\mu} (s’) ds’\tag{17}\\<br>&amp; = \nabla_{\theta}\mu(s) \nabla_a R(s,a)|_{a=\mu(s)} + \int_S \gamma \left(p(s’|s,\mu(s))\nabla_{\theta} V^{\mu} (s’) + \nabla_{\theta}\mu(s)\nabla_a p(s’|s, a)|_{a=\mu(s)} V^{\mu} (s’) \right) ds’\tag{18}\\<br>&amp; = \nabla_{\theta}\mu(s) \nabla_a \left( R(s,a) + \nabla_{\theta}\mu(s)\nabla_a p(s’|s, a)|_{a=\mu(s)} V^{\mu} (s’)ds’ \right) |_{a=\mu(s)} + \int_S \gamma p(s’|s,\mu(s))\nabla_{\theta} V^{\mu} (s’)  ds’\tag{19}\\<br>&amp; = \nabla_{\theta}\mu(s) \nabla_a Q^{\mu}(s,a) |_{a=\mu(s)} + \int_S \gamma p(s\rightarrow s’, 1, \mu(s))\nabla_{\theta} V^{\mu} (s’)  ds’\tag{20}\\<br>&amp; = \nabla_{\theta}\mu(s) \nabla_a Q^{\mu}(s,a) |_{a=\mu(s)}\\<br>&amp;\qquad+ \int_S \gamma p(s\rightarrow s’, 1, \mu) \left(\nabla_{\theta}\mu(s’) \nabla_a Q^{\mu} (s’, a’) |_{a’ = \mu(s’)}  + \int_S \gamma p(s’ \rightarrow s^{’’}, 1, \mu(s’) ) \nabla_{\theta} V^{\mu} (s’’) ds^{’’} \right) ds’ \tag{21}\\<br>&amp; = \nabla_{\theta}\mu(s) \nabla_a Q^{\mu}(s,a) |_{a=\mu(s)}\\<br>&amp;\qquad + \int_S \gamma p(s\rightarrow s’, 1, \mu) \nabla_{\theta}\mu(s’) \nabla_a Q^{\mu} (s’, a’) |_{a’ = \mu(s’)}ds’ \\<br>&amp;\qquad + \int_S \gamma p(s\rightarrow s’, 1, \mu) \int_S \gamma p(s’ \rightarrow s^{’’}, 1, \mu(s’) )\nabla_{\theta} V^{\mu} (s’)ds^{’’} ds’ \tag{22}\\<br>&amp; = \nabla_{\theta}\mu(s) \nabla_a Q^{\mu}(s,a) |_{a=\mu(s)}\\<br>&amp;\qquad + \int_S \gamma p(s\rightarrow s’, 1, \mu) \nabla_{\theta}\mu(s’) \nabla_a Q^{\mu} (s’, a’) |_{a’ = \mu(s’)} ds’\\<br>&amp;\qquad + \int_S \gamma^2 p(s\rightarrow s’, 2, \mu)\nabla_{\theta} V^{\mu} (s’) ds’ \tag{23}\\<br>&amp; = \int_S \sum_{t=0}^{\infty} \gamma^t p(s\rightarrow s’, t, \mu)\nabla_{\theta} \mu(s’)\nabla_{a} Q^{\mu} (s’, a)|_{a=\mu(s’)} ds’ \tag{24}\\<br>\end{align*}<br>所以：<br>\begin{align*}<br>\nabla_{\theta} J(\mu) &amp; = \nabla_{\theta} \int_S \rho_0(s) V^{\mu} (s) ds \tag{25}\\<br>&amp; = \int_S \rho_0(s) \nabla_{\theta} V^{\mu} (s) ds \tag{26}\\<br>&amp; = \int_S\int_S \sum_{t=0}^{\infty} \gamma^t\rho_0(s) p(s\rightarrow s’, t, \mu)\nabla_{\theta} \mu(s’)\nabla_{a} Q^{\mu} (s’, a)|_{a=\mu(s’)} ds’ ds\tag{27}\\<br>&amp; = \int_S\rho^{\mu} (s) \nabla_{\theta} \mu(s)\nabla_{a} Q^{\mu} (s, a)|_{a=\mu(s)} ds\tag{28}\\<br>&amp; = \mathbb{E}_{s\sim \rho^\mu}\left[ \nabla_{\theta}\mu(s)\nabla_{a} Q^{\mu} (s, a)|_{a=\mu(s)} \right]\tag{30}\\<br>\end{align*}<br>公式$(25)$到公式$(26)$是因为$\rho_0(s_0)$和$\mu$无关。。</p><h2 id="spg的limit">spg的limit</h2><p>dpg实际上可以看成是spg的一个特殊情况。如果使用deterministic policy $\mu_\theta:S\rightarrow A$和variance parameter $\sigma$表示某些stochastic policy $\pi_{\mu_{\theta,\sigma}}$，比如$\sigma = 0$时，$\pi_{\mu_{\theta, 0}} \equiv \mu_\theta$，当$\sigma \rightarrow 0$时，stochastic policy gradient收敛于deterministic policy gradient。<br>考虑一个stochastic policy $\pi_{\mu_{\theta,\sigma}}$让$\pi_{\mu_{\theta,\sigma}}(s,a)=v_\sigma(\mu_\theta(s),a)$，其中$\sigma$是控制方差的参数，并且$v_\sigma$满足条件B.1，以及MDP满足条件A.1和A.2，那么<br>$$\lim_{\sigma\rightarrow 0}\nabla_\theta J(\pi_{\mu_{\theta, \sigma}}) = \nabla_\theta J(\mu_\theta) \tag{31} $$<br>其中左边的gradient是标准spg的gradient，右边是dpg的gradient。这就说明spg的很多方法同样也是适用于dpg的。</p><h2 id="deterministic-actor-critic">deterministic actor-critic</h2><p>接下来使用dpg theorem推导on-policy和off-policy的actor-critic算法。从最简单的on-policy update开始，使用Sarsa critic，然后介绍off-policy算法，使用Q-learning critic介绍核心思想。这些算法在实践中可能会有收敛问题，因为function approximator引入了biases，off-policy引入了不稳定性，可以使用compatiable function approximation的方法以及gradient td learning解决这些问题。</p><h3 id="on-policy-deterministic-actor-critic">on-policy deterministic actor-critic</h3><p>使用deterministic behaviour policy不能确保足够的exploration，会产生suboptimal solutions。但是我们还是要首先介绍on-policy actor-critic算法，它对于我们理解算法背后的核心思路很有用。尤其是在环境有噪音，即使使用deterministic behaviour policy也能够保证充足的exploration时。<br>和stochastic actor-critic一样。deterministic actor-critic也由actor和critic两部分组成，actor使用梯度下降调整$\mu(s)$的参数$\theta$，critic使用policy evaluation方法估计$Q^w (s,a) \approx Q^{\mu} (s, a)$指导actor的更新。比如critic使用Sarsa估计action value：<br>\begin{align*}<br>\delta_t &amp; = R_{t+1} + \gamma Q^w (s_{t+1}, a_{t+1}) - Q^w(s_t, a_t) \tag{32}\\<br>w_{t+1} &amp; = w_t + \alpha_w \delta_t \nabla_w Q^w (s_t,a_t)\tag{33}\\<br>\theta_{t+1} &amp; = \theta_t + \alpha_{\theta} \nabla_{\theta} \mu_{\theta}(s_t) \nabla_a Q^w(s_t, a_t)|_{a=\mu(s)} \tag{34}\\<br>\end{align*}</p><h3 id="off-policy-deterministic-actor-critic">off-policy deterministic actor-critic</h3><p>考虑off-policy actor-critic方法。目标函数是使用behaviour policy对target policy的value function求期望：<br>\begin{align*}<br>J_{\beta}(\mu) &amp; = \int_S \rho^{\beta}(s) V^{\mu} (s) ds\\<br>&amp; = \int_S \rho^{\beta} (s) Q^{\mu} (s, \mu(s,a) ) ds \tag{35}\\<br>\end{align*}<br>求梯度是：<br>\begin{align*}<br>\nabla_{\theta}J_{\beta}(\theta) &amp; \approx \int_S\rho^\beta (s) \nabla_{\theta}\mu(s)\nabla_{a} Q^{\mu} (s, a) ds \\<br>&amp; = \mathbb{E}_{s\sim \rho^\beta }\left[ \nabla_{\theta}\mu(s)\nabla_{a} Q^{\mu} (s, a)|_{a=\mu(s)} \right]\tag{36}\\<br>\end{align*}<br>critic使用Q-learning更新action value function：<br>\begin{align*}<br>\delta_t &amp; = R_{t+1} + \gamma Q^w (s_{t+1}, \mu_{\theta}(s_{t+1})) - Q^w(s_t, a_t) \tag{37}\\<br>w_{t+1} &amp; = w_t + \alpha_w \delta_t \nabla_w Q^w (s_t,a_t)\tag{38}\\<br>\theta_{t+1} &amp; = \theta_t + \alpha_{\theta} \nabla_{\theta} \mu_{\theta}(s_t) \nabla_a Q^w(s_t, a_t)|_{a=\mu(s)} \tag{39}\\<br>\end{align*}<br>在stochastic off-policy actor-critic中，actor和critic都使用了importance sampling，而在deterministic off policy actor-critic中，deterministic去掉了对actions的积分，所以避免了actor的importance sampling，使用one step Q-learning去掉了critic中的importance sampling。</p><h2 id="总结">总结</h2><p>Deterministic policy gradient比stochastic policy gradient要好，尤其是high dimensional tasks上。此外，dpg不需要消耗更多的计算资源。还有就是对于一些应用，有可导的policy，但是没有办法加noise，这种情况下dpg更合适。<br>目标函数：</p><h3 id="stochastic-policy-gradient-theorem-v2">stochastic policy gradient theorem:</h3><p>具体证明可以见<a href="http://mxxhcm.github.io/2019/09/07/gradient-method-policy-gradient/">policy gradient</a>。<br>\begin{align*}<br>J(\pi_\theta) &amp; = \int_S\rho^{\pi} (s)\int_A\pi(s,a) R(s,a) da ds \tag{40}\\<br>&amp; = \mathbb{E}_{s\sim \rho^{\pi} , a\sim \pi_{\theta}} \left [R(s,a) \right]\tag{41}\\<br>\end{align*}</p><p>\begin{align*}<br>\nabla_{\theta}J(\pi_\theta) &amp; = \int_S\rho^{\pi} (s)\int_A \nabla\pi(s,a) Q^{\pi} (s,a) da ds\tag{42}\\<br>&amp; = \mathbb{E}_{s\sim \rho^{\pi} , a\sim \pi_{\theta}} \left [\log \nabla_{\theta} \pi(a|s)Q^{\pi} (s,a) \right]\tag{43}\\<br>\end{align*}</p><h3 id="stochastic-off-policy-actor-crtic">stochastic off-policy actor-crtic</h3><p>具体证明可以参考<a href="https://arxiv.org/pdf/1205.4839.pdf" target="_blank" rel="noopener">Linear off-policy actor-critic</a>。<br>\begin{align*}<br>J(\pi_\theta) &amp; = \int_S\rho^{\beta}(s) V^{\pi} (s) ds\tag{44}\\<br>&amp; = \int_S \int_A\rho^{\beta}(s) \pi(a|s) Q^{\pi} (s, a) ds\tag{45}\\<br>\end{align*}</p><p>\begin{align*}<br>\nabla_{\theta}J(\pi_\theta) &amp; \approx \int_S\int_A\rho^{\pi} (s) \nabla_{\theta} \pi_{\theta}(s,a) Q^{\pi} (s,a) da ds\tag{46}\\<br>&amp; = \mathbb{E}_{s\sim \rho^{\beta}, a\sim \beta} \left[ \frac{\pi_{\theta}(a|s) }{\beta_{\theta}(a|s) } \nabla_{\theta}\log \pi_{\theta}(s,a) Q^{\pi} (s,a) \right]\tag{47}\\<br>\end{align*}</p><h3 id="deterministic-policy-gradient-theorem-v2">deterministic policy gradient theorem:</h3><p>具体证明见本文上半部分。但是我还有一些疑问，在stochastic policy gradient theorem中，$p(s’, r|s, a)$是$a$的函数，$a = \pi(s)$，所以$p(s’,r|s, a)$不应该也是$\theta$的函数吗？<br>\begin{align*}<br>J(\mu_{\theta}) &amp; = \int_S\rho^{\mu} (s) R(s, \mu_{\theta}(s)) da ds\tag{48}\\<br>&amp; = \mathbb{E}_{s\sim \rho^{\mu} } \left[R(s, \mu_{\theta}(s) \right]\tag{49}\\<br>\end{align*}</p><p>\begin{align*}<br>\nabla_{\theta} J(\mu_\theta) &amp; = \int_S\rho^{\mu} (s)\nabla_{\theta}\mu_{\theta}(s) \nabla_a Q^{\mu} (s, a)|_{\mu_{\theta}(s)} da ds\tag{50}\\<br>&amp; = \mathbb{E}_{s\sim \rho^{\mu} (s)} \left[ \nabla_{\theta}\mu_{\theta}(s) \nabla_a Q^{\mu} (s, a)|_{\mu_{\theta}(s)} \right]\tag{51}\\<br>\end{align*}</p><h3 id="deterministic-off-policy-actor-critic">deterministic off-policy actor-critic</h3><p>具体证明可以参考<a href="https://arxiv.org/pdf/1205.4839.pdf" target="_blank" rel="noopener">Linear off-policy actor-critic</a>。<br>\begin{align*}<br>J_{\beta}(\mu_\theta) &amp; = \int_S\rho^{\beta} (s) V^{\mu} (s) ds\tag{52}\\<br>&amp; = \int_S\rho^{\beta} (s) Q^{\mu} (s, \mu_{\theta}(s)) ds\tag{53}\\<br>\end{align*}<br>\begin{align*}<br>\nabla_{\theta} J_{\beta} (\mu_\theta) &amp; \approx \int_S\rho^{\beta} (s)\nabla_{\theta}\mu_{\theta}(a|s) \nabla_a Q^{\mu} (s, a) ds\tag{54}\\<br>&amp; = \mathbb{E}_{s\sim \rho^{\beta}}\left[\nabla_{\theta}\mu_{\theta}(a|s) \nabla_a Q^{\mu} (s, a) \right]\tag{55}\\<br>\end{align*}</p><h2 id="参考文献">参考文献</h2><p>Policy Gradient<br>1.<a href="https://arxiv.org/pdf/1509.02971.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1509.02971.pdf</a><br>2.<a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Publications_files/deterministic-policy-gradients.pdf" target="_blank" rel="noopener">http://www0.cs.ucl.ac.uk/staff/d.silver/web/Publications_files/deterministic-policy-gradients.pdf</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;简介&quot;&gt;简介&lt;/h2&gt;
&lt;p&gt;本文主要介绍了deterministic policy gradient算法。它属于policy gradient的一种，只不过是将stochastic policy改成了deterministic policy，和stochasti
      
    
    </summary>
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="http://mxxhcm.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="policy gradient" scheme="http://mxxhcm.github.io/tags/policy-gradient/"/>
    
      <category term="pg" scheme="http://mxxhcm.github.io/tags/pg/"/>
    
      <category term="dpg" scheme="http://mxxhcm.github.io/tags/dpg/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow distributed tensorflow</title>
    <link href="http://mxxhcm.github.io/2019/07/13/tensorflow-distributed-tensorflow/"/>
    <id>http://mxxhcm.github.io/2019/07/13/tensorflow-distributed-tensorflow/</id>
    <published>2019-07-13T13:11:06.000Z</published>
    <updated>2019-07-13T14:01:56.649Z</updated>
    
    <content type="html"><![CDATA[<h2 id="distributed-tensorflow">Distributed Tensorflow</h2><p>这篇文章主要介绍如何创建cluster of tensorflow serves，并且将一个computation graph分发到这个cluster上。</p><h2 id="cluster和server">Cluster和Server</h2><h3 id="简介">简介</h3><p>tensorflow中，一个cluster是一系列参与tensorflow graph distriuted execution的tasks。每个task和一个tensorflow server相关联，这个server可能包含一个&quot;master&quot;用来创建session，或者&quot;worker&quot;用来执行图中的op。一个cluster可以被分成更多jobs，每一个job包含一个或者更多个tasks。<br>为了创建一个cluster，需要给每一个task指定一个server。每一个task都运行在不同的devices上。在每一个task上，做以下事情：</p><ol><li>创建一个tf.train.ClusterSpec描述这个cluster的所有tasks，这对于所有的task都是一样的。</li><li>创建一个tf.train.Server，需要的参数是tr.train.ClusterSpec，识别local task使用job name和task index。</li></ol><h3 id="创建一个tf-train-clusterspec">创建一个tf.train.ClusterSpec</h3><p>下面的表格中给出了两个cluster的示例。传入的参数是一个dictionary，key是job的name，value是该job的所有可用devices。第二列对应的task的scope name。</p><table><thead><tr><th>tf.train.ClusterSpec construction</th><th>Available tasks</th></tr></thead><tbody><tr><td>tf.train.ClusterSpec({“local”: [“localhost:2222”, “localhost:2223”]})</td><td>/job:local/task:0/job:local/task:1</td></tr><tr><td>tf.train.ClusterSpec({ “worker”: [ “<a href="http://worker0.example.com:2222" target="_blank" rel="noopener">worker0.example.com:2222</a>”, “<a href="http://worker1.example.com:2222" target="_blank" rel="noopener">worker1.example.com:2222</a>”, “<a href="http://worker2.example.com:2222" target="_blank" rel="noopener">worker2.example.com:2222</a>”], “ps”: [“<a href="http://ps0.example.com:2222" target="_blank" rel="noopener">ps0.example.com:2222</a>”, “<a href="http://ps1.example.com:2222" target="_blank" rel="noopener">ps1.example.com:2222</a>”]})</td><td>/job:worker/task:0    /job:worker/task:1  /job:worker/task:2  /job:ps/task:0  /job:ps/task:1</td></tr></tbody></table><h3 id="为每一个task创建一个tf-train-server-instance">为每一个task创建一个tf.train.Server instance</h3><p>tf.train.Server对象包含local devices的集合，和其他tasks的connections</p><h2 id="参考文献">参考文献</h2><p>1.<a href="https://github.com/tensorflow/examples/blob/master/community/en/docs/deploy/distributed.md" target="_blank" rel="noopener">https://github.com/tensorflow/examples/blob/master/community/en/docs/deploy/distributed.md</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;distributed-tensorflow&quot;&gt;Distributed Tensorflow&lt;/h2&gt;
&lt;p&gt;这篇文章主要介绍如何创建cluster of tensorflow serves，并且将一个computation graph分发到这个cluster上。
      
    
    </summary>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/categories/tensorflow/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/tags/tensorflow/"/>
    
      <category term="distributed tensorflow" scheme="http://mxxhcm.github.io/tags/distributed-tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tensosrflow Coordinator</title>
    <link href="http://mxxhcm.github.io/2019/07/13/tensorflow-Coordinator/"/>
    <id>http://mxxhcm.github.io/2019/07/13/tensorflow-Coordinator/</id>
    <published>2019-07-13T12:17:13.000Z</published>
    <updated>2019-07-13T12:29:31.073Z</updated>
    
    <content type="html"><![CDATA[<h2 id="coordinator">Coordinator</h2><h3 id="简介">简介</h3><p>这个类用来协调多个thread同时工作，同时停止。常用的方法有：</p><ul><li>should_stop()：如果满足thread停止条件的话，返回True</li><li>request_stop()：请求thread停止，调用该方法后，should_stop()返回True</li><li>join(<list of threads>)：等待所有的threads停止。</list></li></ul><h3 id="理解">理解</h3><p>其实我觉得这个类的作用好像没有那么大，或者我没有用到这种场景。反正就是满足thread的终止条件时，调用request_stop()函数，让should_stop()返回True。</p><h2 id="代码示例">代码示例</h2><p><a href="https://github.com/mxxhcm/code/blob/master/tf/ops/tf_train_Coordinator.py" target="_blank" rel="noopener">代码地址</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"></span><br><span class="line">n = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add</span><span class="params">(index)</span>:</span></span><br><span class="line">    <span class="keyword">global</span> n</span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">not</span> coord.should_stop():</span><br><span class="line">    <span class="comment">#while True:</span></span><br><span class="line">        <span class="keyword">if</span> n &gt; <span class="number">10</span>:</span><br><span class="line">            print(index, <span class="string">" done"</span>)</span><br><span class="line">            coord.request_stop()</span><br><span class="line">            <span class="comment">#break</span></span><br><span class="line">        print(<span class="string">"before A, thread "</span>, index, n)</span><br><span class="line">        n += <span class="number">1</span></span><br><span class="line">        print(<span class="string">"after A, thread "</span>, index, n)</span><br><span class="line">        time.sleep(random.random())</span><br><span class="line">        print(<span class="string">"before B, thread "</span>, index, n)</span><br><span class="line">        n += <span class="number">1</span></span><br><span class="line">        print(<span class="string">"after B, thread "</span>, index, n)</span><br><span class="line">    </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">        coord = tf.train.Coordinator()</span><br><span class="line"></span><br><span class="line">        jobs = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">            jobs.append(threading.Thread(target=add, args=(i,)))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> jobs:</span><br><span class="line">            j.start()</span><br><span class="line"></span><br><span class="line">        coord.join(jobs)</span><br><span class="line">        print(<span class="string">"Hello World!"</span>)</span><br></pre></td></tr></table></figure><h2 id="参考文献">参考文献</h2><p>1.<a href="https://wiki.jikexueyuan.com/project/tensorflow-zh/how_tos/threading_and_queues.html" target="_blank" rel="noopener">https://wiki.jikexueyuan.com/project/tensorflow-zh/how_tos/threading_and_queues.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;coordinator&quot;&gt;Coordinator&lt;/h2&gt;
&lt;h3 id=&quot;简介&quot;&gt;简介&lt;/h3&gt;
&lt;p&gt;这个类用来协调多个thread同时工作，同时停止。常用的方法有：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;should_stop()：如果满足thread停止条件的话，返回
      
    
    </summary>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/categories/tensorflow/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/tags/tensorflow/"/>
    
      <category term="Coortinator" scheme="http://mxxhcm.github.io/tags/Coortinator/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow one_hot</title>
    <link href="http://mxxhcm.github.io/2019/07/13/tensorflow-one-hot/"/>
    <id>http://mxxhcm.github.io/2019/07/13/tensorflow-one-hot/</id>
    <published>2019-07-13T04:08:55.000Z</published>
    <updated>2019-10-11T05:30:51.354Z</updated>
    
    <content type="html"><![CDATA[<h2 id="tf-one-hot">tf.one_hot</h2><h3 id="一句话介绍">一句话介绍</h3><p>返回一个one-hot tensor</p><h3 id="api">API</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">tf.one_hot(</span><br><span class="line">    indices,    <span class="comment"># 每一个one-hot向量不为0的维度</span></span><br><span class="line">    depth,  <span class="comment"># 指定每一个one-hot向量的维度</span></span><br><span class="line">    on_value=<span class="literal">None</span>,  <span class="comment"># indices上取该值</span></span><br><span class="line">    off_value=<span class="literal">None</span>,     <span class="comment">#其他地方取该值</span></span><br><span class="line">    axis=<span class="literal">None</span>,</span><br><span class="line">    dtype=<span class="literal">None</span>,</span><br><span class="line">    name=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="代码示例">代码示例</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">indices = [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line">depth = <span class="number">5</span></span><br><span class="line"></span><br><span class="line">result = tf.one_hot(indices, depth)</span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line">print(sess.run(result))</span><br><span class="line"><span class="comment"># [[0. 1. 0. 0. 0.]</span></span><br><span class="line"><span class="comment">#  [0. 1. 0. 0. 0.]</span></span><br><span class="line"><span class="comment">#  [0. 1. 0. 0. 0.]]</span></span><br></pre></td></tr></table></figure><h2 id="参考文献">参考文献</h2><p>1.<a href="https://www.tensorflow.org/api_docs/python/tf/one_hot" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/one_hot</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;tf-one-hot&quot;&gt;tf.one_hot&lt;/h2&gt;
&lt;h3 id=&quot;一句话介绍&quot;&gt;一句话介绍&lt;/h3&gt;
&lt;p&gt;返回一个one-hot tensor&lt;/p&gt;
&lt;h3 id=&quot;api&quot;&gt;API&lt;/h3&gt;
&lt;figure class=&quot;highlight pytho
      
    
    </summary>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/categories/tensorflow/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/tags/tensorflow/"/>
    
      <category term="one-hot" scheme="http://mxxhcm.github.io/tags/one-hot/"/>
    
  </entry>
  
  <entry>
    <title>python slice</title>
    <link href="http://mxxhcm.github.io/2019/07/13/python-slice/"/>
    <id>http://mxxhcm.github.io/2019/07/13/python-slice/</id>
    <published>2019-07-13T02:30:16.000Z</published>
    <updated>2019-07-13T02:43:00.402Z</updated>
    
    <content type="html"><![CDATA[<h2 id="python-slice-index"><a href="#python-slice-index" class="headerlink" title="python slice index"></a>python slice index</h2><p> +—-+—-+—-+—-+—-+—-+<br> | P | y | t | h | o | n |<br> +—-+—-+—-+—-+—-+—-+<br> 0   1   2   3   4   5   6<br>-6  -5  -4  -3  -2  -1</p><h2 id="start和stop"><a href="#start和stop" class="headerlink" title="start和stop"></a>start和stop</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a[start:stop]   # 从start到stop-1的所有items</span><br><span class="line">a[start:]   # 从start到array结尾的所有items</span><br><span class="line">a[:stop]   # 从开始到stop-1的所有items</span><br><span class="line">a[:]   # 整个array的copy</span><br></pre></td></tr></table></figure><h2 id="step"><a href="#step" class="headerlink" title="step"></a>step</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a[start:stop:step]   # 从start，每次加上step，不超过stop，step默认为1</span><br></pre></td></tr></table></figure><h2 id="负的start和stop"><a href="#负的start和stop" class="headerlink" title="负的start和stop"></a>负的start和stop</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a[-1]   # array的最后一个item</span><br><span class="line">a[-2:]   # array的最后两个items</span><br><span class="line">a[:-2]   # 从开始到倒数第三个的所有items</span><br></pre></td></tr></table></figure><h2 id="负的step"><a href="#负的step" class="headerlink" title="负的step"></a>负的step</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a[::-1]   # 所有元素，逆序</span><br><span class="line">a[1::-1]    # 前两个元素，逆序</span><br><span class="line">a[:-3:-1]   # 后两个元素，逆序</span><br><span class="line">a[-3::-1]   # 除了最后两个元素，逆序</span><br></pre></td></tr></table></figure><p>这里加一些自己的理解，其实就是倒着数而已，包含第一个:前面的，不包含两个:之间的。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>1.<a href="https://stackoverflow.com/a/509297/8939281" target="_blank" rel="noopener">https://stackoverflow.com/a/509297/8939281</a><br>2.<a href="https://stackoverflow.com/a/509295/8939281" target="_blank" rel="noopener">https://stackoverflow.com/a/509295/8939281</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;python-slice-index&quot;&gt;&lt;a href=&quot;#python-slice-index&quot; class=&quot;headerlink&quot; title=&quot;python slice index&quot;&gt;&lt;/a&gt;python slice index&lt;/h2&gt;&lt;p&gt; +—-+—
      
    
    </summary>
    
      <category term="python" scheme="http://mxxhcm.github.io/categories/python/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="slice" scheme="http://mxxhcm.github.io/tags/slice/"/>
    
  </entry>
  
  <entry>
    <title>linux init系统</title>
    <link href="http://mxxhcm.github.io/2019/06/23/linux-init%E7%B3%BB%E7%BB%9F/"/>
    <id>http://mxxhcm.github.io/2019/06/23/linux-init系统/</id>
    <published>2019-06-23T08:35:02.000Z</published>
    <updated>2019-06-23T12:41:34.401Z</updated>
    
    <content type="html"><![CDATA[<h2 id="linux-init系统">Linux init系统</h2><p>这里会介绍下面三种init系统</p><ul><li>SysVInit(initd)</li><li>Upstart</li><li>SystemD</li></ul><p>Linux的启动从BIOS开始，bootloader载入内核，进行内核初始化。内核初始化的最后一步是启动pid为$1$的init进程。这个进程是系统的第一个进程，它负责产生其他所有用户进程。<br>Init系统能够定义、管理和控制init进程的行为。它负责组织和运行许多独立的或相关的始化job(因此被称为init系统)，从而让计算机系统进入某种用户规定的run level。<br>大多数Linux发行版的init 系统是和 System V 相兼容的，称为 SysVInit。Ubuntu和RHEL采用upstart替代了SysVInit。而 Fedora 从版本15开始使用SystemD的新init系统。ubuntu-16.10之后开始不再使用SysVInt管理系统，改用SystemD。</p><h2 id="sysvinit-initd">SysVInit(initd)</h2><p>SysVInit的最大缺点是主要依赖于 Shell 脚本，启动太慢。</p><h3 id="runlevel">runlevel</h3><p>SysVInit检查/etc/inittab文件中是否含有’initdefault’ 项，然后进入默认run level。如果没有默认的run level，用户手动决定进入哪个run level。SysVInit通常会有8种run-level，0到6和S或者s。但 0，1，6 run level的操作是公认的： 0代表关机, 1代表单用户模式, 6代表重启，其他run level跟发行版有关。/etc/inittab 文件中还定义了各种run level需要执行的初始化工作。</p><h3 id="sysvinit初始化顺序">SysVInit初始化顺序</h3><p>SysVInit用脚本，文件命名规则和软链接来实现不同的runlevel。首先，SysVInit 需要读取/etc/inittab 文件，获得以下配置信息：</p><ul><li>系统需要进入的 runlevel</li><li>捕获组合键的定义</li><li>定义电源 fail/restore 脚本</li><li>启动 getty 和虚拟控制台</li></ul><p>得到配置信息后，SysVInit 顺序地执行以下初始化步骤将系统初始化为相应的runlevel X。</p><ul><li>/etc/rc.d/rc.sysinit</li><li>/etc/rc.d/rc 和/etc/rc.d/rcX.d/ (X 代表运行级别 0-6)</li><li>/etc/rc.d/rc.local</li><li>X Display Manager（如果需要的话）</li></ul><h4 id="rc-sysinit初始化">rc.sysinit初始化</h4><p>首先运行rc.sysinit完成以下任务。</p><ul><li>激活 udev 和 selinux</li><li>设置定义在/etc/sysctl.conf 中的内核参数</li><li>设置系统时钟</li><li>加载 keymaps</li><li>使能交换分区</li><li>设置主机名(hostname)</li><li>根分区检查和 remount</li><li>激活 RAID 和 LVM 设备</li><li>开启磁盘配额</li><li>检查并挂载所有文件系统</li><li>清除过期的 locks 和 PID 文件</li></ul><h4 id="etc-rd-c-rc初始化">/etc/rd.c/rc初始化</h4><p>接下来运行/etc/rc.d/rc脚本。根据不同的runlevel，rc脚本打开对应该runlevel的rcX.d目录，以S开头的脚本是启动时运行的脚本，S后面的数字定义了这些脚本的执行顺序。</p><h4 id="etc-rc-d-rc-local初始化">/etc/rc.d/rc.local初始化</h4><p>rc.local是自定义脚本存放目录。</p><h3 id="sysvinit关闭系统">SysVInit关闭系统</h3><p>关闭顺序的控制也是依靠/etc/rc.d/rcX.d/目录下所有脚本的命名规则来控制的，所有以K开头的脚本都将在关闭系统时调用，K后的数字规定了执行顺序。这些脚本负责安全地停止service或者其他的关闭job。</p><h3 id="sysvinit工具">SysVInit工具</h3><p>SysVInit包含了一系列启动，运行和关闭所有其他程序的工具和命令。</p><ul><li>halt  停止系统。</li><li>init  SysVInit本身的init进程，pid=1，是所有用户进程的父进程。最主要的作用是在启动过程中使用/etc/inittab文件创建所有其他初始化进程。</li><li>killall5  SystemV的killall 命令。向除自己的会话(session)进程之外的其它进程发出信号，所以不能杀死当前使用的 shell。</li><li>last  回溯/var/log/wtmp 文件(或者-f 选项指定的文件)，显示自从这个文件建立以来，所有用户的登录情况。</li><li>lastb 作用和 last 差不多，默认情况下使用/var/log/btmp 文件，显示所有失败登录企图。</li><li>mesg  控制其它用户对用户终端的访问。</li><li>pidof 找出程序的进程识别号(pid)，输出到标准输出设备。</li><li>poweroff  等于 shutdown -h –p，或者 telinit 0。关闭系统并切断电源。</li><li>reboot    等于 shutdown –r 或者 telinit 6。重启系统。</li><li>runlevel  读取系统的登录记录文件(一般是/var/run/utmp)把以前和当前的run level输出到标准输出设备。</li><li>shutdown  以一种安全的方式终止系统，所有正在登录的用户都会收到系统将要终止通知，并且不准新的登录。</li><li>sulogin   当系统进入单用户模式时，被init调用。当接收到启动加载程序传递的-b 选项时，init 也会调用 sulogin。</li><li>telinit   实际是init的一个连接，用来向init传送单字符参数和信号。</li><li>utmpdump  以一种用户友好的格式向标准输出设备显示/var/run/utmp 文件的内容。</li><li>wall  向所有有信息权限的登录用户发送消息。</li></ul><h2 id="upstart">Upstart</h2><p>Ubuntu使用upstart init系统，没有/etc/inittab文件。Upstart解决了热插拔以及网络共享盘的挂载问题。在/etc/fstab 中，可以指定系统自动挂载一个网络盘，比如 NFS等，SysVInit 分析/etc/fstab 挂载文件系统这个步骤是在网络启动之前。可是如果网络没有启动，NFS或者iSCSI服务都不可访问，当然也无法进行挂载操作。SysVInit采用netdev的方式来解决这个问题，即/etc/fstab发现netdev属性挂载点的时候，不尝试挂载它，在网络初始化之后，有专门的netfs service来挂载所有这些网络盘。<br>UpStart 解决了之前提到的 SysVInit 的缺点。采用event驱动模型，UpStart 可以：</p><ul><li>更快地启动系统。采用event驱动机制加快了系统启动时间。SysVInit 运行时是同步阻塞的。一个脚本运行的时候，后续脚本必须等待。这意味着所有的初始化步骤都是串行执行的，而实际上很多service彼此并不相关，完全可以并行启动，从而减小系统的启动时间。</li><li>当新硬件被发现时动态启动相应的service，Ubuntu 开发了基于event机制的UpStart，比如 U 盘插入 USB 接口后，udev得到内核通知，发现该设备，这是一个新的event。UpStart感知到该event之后触发相应的等待任务，比如处理/etc/fstab中新的挂载点。采用这种event驱动的模式，upstart 完美地解决了即插即用设备带来的新问题。</li><li>硬件被拔除时动态停止相应的service</li></ul><h3 id="upstart-job和event">Upstart job和event</h3><p>Job是一个job unit，用来完成一件工作，比如启动一个后台service，或者运行一个配置命令。每个 Job 都等待一个或多个event，一旦event发生，upstart 就触发该job完成相应的工作。</p><h4 id="job">Job</h4><p>Job是一个job的unit，一个task或者一个service。可以理解为SysVInit中的一个service脚本。有三种类型的job：</p><ul><li>task job，task job 代表在一定时间内会执行完毕的任务，比如删除一个文件；</li><li>service job，service job 代表后台service进程，比如 apache httpd。这里进程一般不会退出，一旦开始运行就成为一个后台deamon，由 init 进程管理，如果这类进程退出，由 init 进程重新启动，它们只能由 init 进程发送信号停止。它们的停止一般也是由于所依赖的停止event而触发的，不过 upstart 也提供命令行工具，让管理人员手动停止某个service；</li><li>abstract job，abstract job 仅由 upstart 内部使用，仅对理解 upstart 内部机理有所帮助。我们不用关心它。</li></ul><h5 id="system-job-and-session-job">system job and session job</h5><p>还可以根据Upstart初始化的范围对job进行分类。系统的初始化任务叫做system job，比如挂载文件系统的任务就是一个system job；用户会话的初始化service就叫做 session job。</p><h5 id="job-生命周期">Job 生命周期</h5><p>Upstart为每个job都维护一个生命周期。一般来说，job有开始，运行和结束。以及其他更精细的状态，比如开始有开始之前(pre-start)，即将开始(starting)和已经开始了(started)几种不同的状态，这样可以更加精确地描述job的当前状态。详细的状态如下表所示：</p><table><thead><tr><th>状态名</th><th>含义</th></tr></thead><tbody><tr><td>Waiting</td><td>初始状态</td></tr><tr><td>Starting</td><td>Job 即将开始</td></tr><tr><td>pre-start</td><td>执行 pre-start 段，即任务开始前应该完成的job</td></tr><tr><td>Spawned</td><td>准备执行 script 或者 exec 段</td></tr><tr><td>post-start</td><td>执行 post-start 动作</td></tr><tr><td>Running</td><td>interim state set after post-start section processed denoting job is running (But it may have no associated PID!)</td></tr><tr><td>pre-stop</td><td>执行 pre-stop 段</td></tr><tr><td>Stopping</td><td>interim state set after pre-stop section processed</td></tr><tr><td>Killed</td><td>任务即将被停止</td></tr><tr><td>post-stop</td><td>执行 post-stop 段</td></tr></tbody></table><p>当job的状态即将发生变化的时候，init 进程会发出相应的event（event），如下图是job的状态机。其中只有四个状态Starting，Started，Stopping，Stopped会引起init进程发送相应的event，而其它的状态变化不会发出event。<br><img src="/2019/06/23/linux-init系统/" alt></p><h4 id="event">Event</h4><p>Event在upstart中以通知消息的形式具体存在。一旦某个event发生了，Upstart 就向整个系统发送一个消息。Event 可以分为三类: signal，methods 或者 hooks。</p><ul><li>Signals，Signal event是非阻塞的，异步的。发送一个信号之后控制权立即返回。</li><li>Methods，Methods event是阻塞的，同步的。</li><li>Hooks，Hooks event是阻塞的，同步的。它介于 Signals 和 Methods 之间，调用发出 Hooks event的进程必须等待event完成才可以得到控制权，但不检查event是否成功。</li></ul><h5 id="event示例">Event示例</h5><ol><li>系统上电启动，init 进程会发送&quot;start&quot;event</li><li>根文件系统可写时，相应 job 会发送文件系统就绪的event</li><li>一个块设备被发现并初始化完成，发送相应的event</li><li>某个文件系统被挂载，发送相应的event</li><li>类似 atd 和 cron，可以在某个时间点，或者周期的时间点发送event</li><li>另外一个 job 开始或结束时，发送相应的event</li><li>一个磁盘文件被修改时，可以发出相应的event</li><li>一个网络设备被发现时，可以发出相应的event</li><li>缺省路由被添加或删除时，可以发出相应的event</li></ol><p>不同的 Linux 发行版对 upstart 有不同的定制和实现，实现和支持的event也有所不同，可以用man 7 upstart-events来查看event列表。</p><h4 id="job-和-event-的相互协作">Job 和 Event 的相互协作</h4><p>Upstart是由event触发job运行的一个系统，每一个程序的运行都由其依赖的event发生而触发的。系统初始化时，init进程开始运行，init进程自身会发出不同的event，这些event会触发一些job运行。每个job运行过程中会释放不同的event，这些event又将触发新的job运行。如此反复，直到整个系统正常运行起来。</p><h4 id="job配置文件">job配置文件</h4><p>每一个Job都是由一个job配置文件（Job Configuration File）定义的。job配置文件存放在/etc/init下面，是以.conf 作为文件后缀的文件。</p><h5 id="示例">示例</h5><p>~$:cat /etc/init/anacron.conf</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># anacron - anac(h)ronistic cron</span><br><span class="line">#</span><br><span class="line"># anacron executes commands at specific periods, but does not assume that</span><br><span class="line"># the machine is running continuously</span><br><span class="line"></span><br><span class="line">description&quot;anac(h)ronistic cron&quot;</span><br><span class="line"></span><br><span class="line">start on runlevel [2345]</span><br><span class="line">stop on runlevel [!2345]</span><br><span class="line"></span><br><span class="line">expect fork</span><br><span class="line">normal exit 0</span><br><span class="line"></span><br><span class="line">exec anacron -s</span><br></pre></td></tr></table></figure><h5 id="常见的sec">常见的sec</h5><p><strong>“expect” Stanza</strong><br>为了启动，停止，重启和查询某个系统service。Upstart 需要跟踪该service所对应的进程。部分service为了将自己变成daemon，会采用fork调用， UpStart必须采用fork出来的进程号作为service的 PID。但是，UpStart 本身无法判断service进程是否fork了，所以需要指定expect告诉 UpStart 进程是否fork了。&quot;expect fork&quot;表示进程只会 fork 一次；&quot;expect daemonize&quot;表示进程会 fork 两次。</p><p><strong>“exec” Stanza 和&quot;script&quot; Stanza</strong><br>&quot;exec&quot;关键字配置job需要运行的命令，&quot;script&quot;关键字定义需要运行的脚本。</p><h5 id="示例-v2">示例</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># mountall.conf</span><br><span class="line">description “Mount filesystems on boot”</span><br><span class="line">start on startup</span><br><span class="line">stop on starting rcS</span><br><span class="line">...</span><br><span class="line">script</span><br><span class="line">  . /etc/default/rcS</span><br><span class="line">  [ -f /forcefsck ] &amp;&amp; force_fsck=”--force-fsck”</span><br><span class="line">  [ “$FSCKFIX”=”yes” ] &amp;&amp; fsck_fix=”--fsck-fix”</span><br><span class="line">    </span><br><span class="line">  ...</span><br><span class="line">   </span><br><span class="line">  exec mountall –daemon $force_fsck $fsck_fix</span><br><span class="line">end script</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>该job在系统启动时运行，负责挂载所有的文件系统。该job需要执行复杂的脚本，由&quot;script&quot;关键字定义；在脚本中，使用了 exec 来执行 mountall 命令。</p><p><strong>“start on” Stanza 和&quot;stop on&quot; Stanza</strong><br>&quot;start on&quot;定义了触发job的所有event。&quot;start on&quot;的语法很简单，如下所示：</p><p>start on EVENT [[KEY=]VALUE]… [and|or…]<br>EVENT 表示event的名字，可以在 start on 中指定多个event，表示该job的开始需要依赖多个event发生。多个event之间可以用 and 或者 or 组合，&quot;表示全部都必须发生&quot;或者&quot;其中之一发生即可&quot;等不同的依赖条件。除了event发生之外，job的启动还可以依赖特定的条件，因此在 start on 的 EVENT 之后，可以用 KEY=VALUE 来表示额外的条件，一般是某个环境变量(KEY)和特定值(VALUE)进行比较。如果只有一个变量，或者变量的顺序已知，则 KEY 可以省略。<br>&quot;stop on&quot;定义job在什么情况下需要停止。</p><h5 id="示例-v3">示例</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#dbus.conf</span><br><span class="line">description     “D-Bus system message bus”</span><br><span class="line"> </span><br><span class="line">start on local-filesystems</span><br><span class="line">stop on deconfiguring-networking</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>D-Bus 是一个系统消息service，上面的配置文件表明当系统发出 local-filesystems event时启动 D-Bus；当系统发出 deconfiguring-networking event时，停止 D-Bus service。</p><h3 id="session初始化">Session初始化</h3><p>Session 就是一个用户会话，即用户从远程或者本地登入系统开始job，直到用户退出，整个过程构成一个会话。用户往往会为自己的会话做一个定制，如添加特定的命令别名等等。这些job都属于对特定会话的初始化操作，被称为 Session Init。在字符模式下，会话初始化相对简单。用户登录后只能启动一个 Shell，通过 shell 命令使用系统。各种 shell 程序都支持一个自动运行的启动脚本，比如~/.bashrc。用户在这些脚本中加入需要运行的定制化命令。在图形界面下，用户登录后看到的并不是一个 shell 提示符，而是一个桌面。一个完整的桌面环境包括 window manager，panel以及其它一些定义在/usr/share/gnome-session/sessions/下面的基本组件，此外还有一些辅助的应用程序，，比如 system monitors，panel applets，NetworkManager，Bluetooth，printers 等。当用户登录之后，这些组件都需要被初始化。目前启动各种图形组件和应用的job由 gnome-session 完成。过程如下：<br>以 Ubuntu 为例，当用户登录 Ubuntu 图形界面后，显示管理器(Display Manager)lightDM 启动 Xsession。Xsession 接着启动 gnome-session，gnome-session 负责其它的初始化job，然后就开始了一个 desktop session。如下所示，是传统的desktop session 启动过程：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">init</span><br><span class="line"> |- lightdm</span><br><span class="line"> |   |- Xorg</span><br><span class="line"> |   |- lightdm ---session-child</span><br><span class="line"> |        |- gnome-session --session=ubuntu</span><br><span class="line"> |             |- compiz</span><br><span class="line"> |             |- gwibber</span><br><span class="line"> |             |- nautilus</span><br><span class="line"> |             |- nm-applet</span><br><span class="line"> |             :</span><br><span class="line"> |             :</span><br><span class="line"> |</span><br><span class="line"> |- dbus-daemon --session</span><br><span class="line"> |</span><br><span class="line"> :</span><br><span class="line"> :</span><br></pre></td></tr></table></figure><p>但是事实上一些应用和组件并不需要在会话初始化过程中启动，而是需要在需要它们的时候才启动。比如 Network Manager，一天之内用户很少切换网络设备，所以大部分时间 Network Manager service仅仅是在浪费系统资源。UpStart的基于event的按需启动的模式就可以很好地解决这些问题。下面给出了采用UpStart之后的会话初始化过程：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">init</span><br><span class="line"> |- lightdm</span><br><span class="line"> |   |- Xorg</span><br><span class="line"> |   |- lightdm ---session-child</span><br><span class="line"> |        |- session-init # &lt;-- upstart running as normal user</span><br><span class="line"> |             |- dbus-daemon --session</span><br><span class="line"> |             |- gnome-session --session=ubuntu</span><br><span class="line"> |             |- compiz</span><br><span class="line"> |             |- gwibber</span><br><span class="line"> |             |- nautilus</span><br><span class="line"> |             |- nm-applet</span><br><span class="line"> |             :</span><br><span class="line"> |             :</span><br><span class="line"> :</span><br><span class="line"> :</span><br></pre></td></tr></table></figure><h3 id="upstart-v2">UpStart</h3><h4 id="upstart系统中的run-level">Upstart系统中的run level</h4><p>Upstart 的运作完全是基于job和event的。Job的状态变化和运行会引起event，进而触发其它job和event。而SysVInit是基于运行级别的，因为历史的原因，Linux 上的多数软件还是采用传统的 SysVInit 脚本启动方式，所以UpStart还是必须模拟老的SysVInit的run level，以便和多数现有软件兼容。</p><h4 id="系统启动过程">系统启动过程</h4><p>下图描述了 UpStart 的启动过程。<br><img src="/2019/06/23/linux-init系统/startup.png" alt="upstart startup"><br>系统上电后运行GRUB载入内核。内核执行硬件初始化和内核自身初始化。在内核初始化的最后，内核将启动pid为1的Upstart init 进程。Upstart 进程执行一些自身的初始化job后，立即发出&quot;startup&quot; event。上图中用红色方框加红色箭头表示event，可以在左上方看到&quot;startup&quot;event。<br>所有依赖于&quot;startup&quot;event的job被触发，其中最重要的是mountall。mountall jog负责挂载系统中需要使用的文件系统，完成相应job后，mountall任务会发出以下event：local-filesystem，virtual-filesystem，all-swaps，<br>其中 virtual-filesystem event触发 udev 任务开始job。任务 udev 触发 upstart-udev-bridge 的job。Upstart-udev-bridge 会发出 net-device-up IFACE=lo event，表示本地回环 IP 网络已经准备就绪。同时，任务 mountall 继续执行，最终会发出 filesystem event。此时，任务 rc-sysinit 会被触发，因为 rc-sysinit 的 start on条件如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start on filesystem and net-device-up IFACE=lo</span><br></pre></td></tr></table></figure><p>任务rc-sysinit调用telinit。Telinit任务会发出 runlevel event，触发执行/etc/init/rc.conf。rc.conf 执行/etc/rc$.d/目录下的所有脚本，和 SysVInit 非常类似。</p><h3 id="upstart注意的事项">Upstart注意的事项</h3><ol><li>fork次数，都通过fork 两次的技巧将自己变成后台service程序需要指定expect stanza。</li><li>fork后即可用，后台程序在完成第二次fork的时候，必须保证service已经可用。因为UpStart通过派生计数来决定service是否处于就绪状态。</li><li>遵守 SIGHUP 的要求，UpStart 会给daemon发送SIGHUP信号，此时，UpStart 希望该deamon做以下这些响应job：</li></ol><ul><li>完成所有必要的重新初始化job，比如重新读取配置文件。这是因为 UpStart 的命令&quot;initctl reload&quot;被设计为可以让service在不重启的情况下更新配置。</li><li>deamon必须继续使用现有的 PID，即收到 SIGHUP 时不能调用 fork。如果service必须在这里调用 fork，则等同于派生两次，参考上面的规则一的处理。这个规则保证了 UpStart 可以继续使用 PID 管理本service。</li></ul><ol start="4"><li>收到 SIGTEM 即 shutdown。</li></ol><ul><li>当收到 SIGTERM 信号后，UpStart 希望deamon进程立即干净地退出，释放所有资源。如果一个进程在收到 SIGTERM 信号后不退出，Upstart 将对其发送 SIGKILL 信号。</li></ul><ol start="5"><li>initctl list 来查看所有job的概况，用 initctl stop 停止一个正在运行的job；用 initctl start 开始一个job；还可以用 initctl status 来查看一个job的状态；initctl restart 重启一个job；initctl reload 可以让一个正在运行的service重新载入配置文件。这些命令和传统的 service 命令十分相似。<br>~$:initctl list</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">alsa-mixer-save stop/waiting</span><br><span class="line">avahi-daemon start/running, process 690</span><br><span class="line">mountall-net stop/waiting</span><br><span class="line">rc stop/waiting</span><br><span class="line">rsyslog start/running, process 482</span><br><span class="line">screen-cleanup stop/waiting</span><br><span class="line">tty4 start/running, process 859</span><br><span class="line">udev start/running, process 334</span><br><span class="line">upstart-udev-bridge start/running, process 304</span><br><span class="line">ureadahead-other stop/waiting</span><br></pre></td></tr></table></figure><p>第一列是job名，比如 rsyslog。第二列是job的目标；第三列是job的状态。<br>UpStart 还提供了一些快捷命令来简化 initctl。比如 reload，restart，start，stop 等等。启动一个service可以简单地调用<br>~$:start job<br>这和执行 initctl start job是一样的效果。<br>一些命令是为了兼容其它系统(主要是 SysVInit)，比如显示 runlevel 用/sbin/runlevel 命令：<br>~$:runlevel<br>&gt; N 2</p><p>在 Upstart 系统中，需要修改/etc/init/rc-sysinti.conf 中的 DEFAULT_RUNLEVEL 这个参数，以便修改默认启动运行级别。这一点和 SysVInit 的习惯有所不同，大家需要格外留意。</p><h2 id="systemd">SystemD</h2><p>SystemD和SysVInit 和 LSB init scripts 兼容，以及更快的启动速度，SystemD 提供了比 UpStart 更激进的并行启动能力。<br>为了减少系统启动时间，SystemD 的目标是尽可能启动更少的进程和尽可能将更多进程并行启动，同样地，UpStart 也试图实现这两个目标。UpStart 采用event驱动机制，当service需要的时候才通过event触发启动；不相干的service可以并行启动。下面的图形演示了 UpStart 相对于 SysVInit 在并发启动这个方面的改进：<br><img src="/2019/06/23/linux-init系统/SystemD.png" alt="systemd"><br>假设有 7 个不同的启动项目， 比如 JobA、Job B 等等。在 SysVInit 中，每一个启动项目都由一个独立的脚本负责，它们由 SysVInit 顺序地，串行地调用。因此总的启动时间为 T1+T2+T3+T4+T5+T6+T7。其中一些任务有依赖关系，比如 A,B,C,D。而Job E和F却和A,B,C,D无关。这种情况下，UpStart 能够并发地运行任务{E，F，(A,B,C,D)}，使得总的启动时间减少为 T1+T2+T3。但是在 UpStart 中，有依赖关系的service还是必须先后启动。<br>让我们例举一些例子， Avahi service需要 D-Bus 提供的功能，因此 Avahi 的启动依赖于 D-Bus，UpStart 中，Avahi 必须等到 D-Bus 启动就绪之后才开始启动。类似的，livirtd 和 X11 都需要 HAL service先启动，而所有这些service都需要 syslog service记录日志，因此它们都必须等待 syslog service先启动起来。然而 httpd 和他们都没有关系，因此 httpd 可以和 Avahi 等service并发启动。<br>SystemD 能够更进一步提高并发性，即便对于那些 UpStart 认为存在相互依赖而必须串行的service，比如 Avahi 和 D-Bus 也可以并发启动。从而实现如下图所示的并发启动过程：<br><img src="/2019/06/23/linux-init系统/SystemD_para.jpg" alt="systemD_para"><br>所有的任务都同时并发执行，总的启动时间被进一步降低为 T1。</p><ul><li>SystemD 提供按需启动能力。</li><li>SystemD采用Cgroup 特性跟踪和管理进程的生命周期。</li></ul><p><img src="/2019/06/23/linux-init系统/find_pid.jpg" alt="find_pid"><br>如果 UpStart 找错了，将 p1作为service进程的 Pid，那么停止service的时候，UpStart 会试图杀死 p1进程，而真正的 p1进程则继续执行。换句话说该service就失去控制了。还有更加特殊的情况。比如，一个 CGI 程序会fork两次，从而脱离了和 Apache 的父子关系。当 Apache 进程被停止后，该 CGI 程序还在继续运行。而我们希望service停止后，所有由它所启动的相关进程也被停止。为了处理这类问题，UpStart 通过 strace 来跟踪 fork、exit 等系统调用，但是这种方法很笨拙，且缺乏可扩展性。SystemD 则利用了 Linux 内核的特性即 CGroup 来完成跟踪的任务。当停止service时，通过查询 CGroup，SystemD 可以确保找到所有的相关进程，从而干净地停止service。CGroup 已经出现了很久，它主要用来实现系统资源配额管理。CGroup 提供了类似文件系统的接口，使用方便。当进程创建子进程时，子进程会继承父进程的 CGroup。因此无论service如何启动新的子进程，所有的这些相关进程都会属于同一个 CGroup，SystemD 只需要简单地遍历指定的 CGroup 即可正确地找到所有的相关进程，将它们一一停止即可。</p><ul><li>启动挂载点和自动挂载的管理。</li><li>实现事务性依赖关系管理。</li><li>能够对系统进行快照和恢复。</li><li>日志service。SystemD Journal 的优点如下：</li></ul><ol><li>简单性：代码少，依赖少，抽象开销最小。</li><li>零维护：日志是除错和监控系统的核心功能，因此它自己不能再产生问题。举例说，自动管理磁盘空间，避免由于日志的不断产生而将磁盘空间耗尽。</li><li>移植性：日志 文件应该在所有类型的 Linux 系统上可用，无论它使用的何种 CPU 或者字节序。</li><li>性能：添加和浏览 日志 非常快。</li><li>最小资源占用：日志 数据文件需要较小。</li><li>统一化：各种不同的日志存储技术应该统一起来，将所有的可记录event保存在同一个数据存储中。</li><li>扩展性：日志的适用范围很广，从嵌入式设备到超级计算机集群都可以满足需求。</li><li>安全性：日志 文件是可以验证的，让无法检测的修改不再可能。</li></ol><h3 id="unit">Unit</h3><p>一个service可以认为是一个unit；一个挂载点是一个unit；一个交换分区的配置是一个unit；等等，总共有12种unit</p><ul><li>service ：系统服务</li><li>socket ：进程间通信的socket</li><li>device ：硬件设备</li><li>mount ：文件系统的挂在</li><li>automount ：自动挂载点</li><li>swap：swap文件</li><li>target ：unit分组统一的控制。比如图形化模式的所有unit组合为一个target。 (例如：multi-user.target 相当于在传统使用 SysV 的系统中运行run level 5)</li><li>timer：定时器</li><li>snapshot ：快照，它保存了系统当前的运行状态。</li></ul><h3 id="依赖关系">依赖关系</h3><p>虽然 SystemD 将大量的启动job解除了依赖，使得它们可以并发启动。但还是存在有些任务，它们之间存在天生的依赖，不能用&quot;套接字激活&quot;(socket activation)、D-Bus activation 和 autofs 三大方法来解除依赖（三大方法详情见后续描述）。比如：挂载必须等待挂载点在文件系统中被创建；挂载也必须等待相应的物理设备就绪。为了解决这类依赖问题，SystemD 的配置单元之间可以彼此定义依赖关系。<br>SystemD 用配置单元定义文件中的关键字来描述配置单元之间的依赖关系。比如：unit A 依赖 unit B，可以在 unit B 的定义中用&quot;require A&quot;来表示。这样 SystemD 就会保证先启动 A 再启动 B。</p><h3 id="systemd-事务">SystemD 事务</h3><p>SystemD 能保证事务完整性。SystemD 的事务概念和数据库中的有所不同，主要是为了保证多个依赖的配置单元之间没有环形引用。比如 unit A、B、C，假如它们的依赖关系为:<br><img src="/2019/06/23/linux-init系统/recurrent_dependency.jpg" alt="dependency"><br>存在循环依赖，那么 SystemD 将无法启动任意一个service。此时 SystemD 将会尝试解决这个问题，因为配置单元之间的依赖关系有两种：required 是强依赖；want 则是弱依赖，SystemD 将去掉 wants 关键字指定的依赖看看是否能打破循环。如果无法修复，SystemD 会报错。<br>SystemD 能够自动检测和修复这类配置错误，极大地减轻了管理员的排错负担。</p><h3 id="target-和运行级别">Target 和运行级别</h3><p>SystemD 用目标（target）替代了运行级别的概念，提供了更大的灵活性，如您可以继承一个已有的目标，并添加其它service，来创建自己的目标。下表列举了 SystemD 下的目标和常见 runlevel 的对应关系：<br>SysVInit 运行级别|SystemD 目标|备注<br>0|runlevel0.target, poweroff.target|关闭系统。<br>1, s, single|runlevel1.target, rescue.target|单用户模式。<br>2, 4|runlevel2.target, runlevel4.target, multi-user.target|用户定义/域特定运行级别。默认等同于 3。<br>3|runlevel3.target, multi-user.target|多用户，非图形化。用户可以通过多个控制台或网络登录。<br>5|runlevel5.target, graphical.target|多用户，图形化。通常为所有运行级别 3 的service外加图形化登录。<br>6|runlevel6.target, reboot.target|重启<br>emergency|emergency.target|紧急Shell</p><h3 id="systemd-的并发启动原理">SystemD 的并发启动原理</h3><p>并发启动原理之一：解决 socket 依赖<br>并发启动原理之二：解决 D-Bus 依赖<br>并发启动原理之三：解决文件系统依赖</p><h3 id="systemd-的使用">SystemD 的使用</h3><ul><li>后台service进程代码不需要执行两次fork来实现后台deamon，只需要实现service本身的主循环即可。</li><li>不要调用 setsid()，交给 SystemD 处理</li><li>不再需要维护 pid 文件。</li><li>SystemD 提供了日志功能，service进程只需要输出到 stderr 即可，无需使用 syslog。</li><li>处理信号 SIGTERM，这个信号的唯一正确作用就是停止当前service，不要做其他的事情。</li><li>SIGHUP 信号的作用是重启service。</li><li>需要套接字的service，不要自己创建套接字，让 SystemD 传入套接字。</li><li>使用 sd_notify()函数通知 SystemD service自己的状态改变。一般地，当service初始化结束，进入service就绪状态时，可以调用它。</li><li>Unit 文件的编写，详细可点击查看<a href></a>。</li></ul><h3 id="systemctl-工具">systemctl 工具</h3><h3 id="systemctl-工具-v2">systemctl 工具</h3><p>~$:systemctl list-units     # 列出正在运行的 Unit<br>~$:systemctl list-units --all   # 列出所有Unit，包括没有找到配置文件的或者启动失败的<br>~$:systemctl list-units --all --state=inactive      # 列出所有没有运行的 Unit<br>~$:systemctl list-units --failed    # 列出所有加载失败的 Unit<br>~$:systemctl list-units --type=service  # 列出所有正在运行的、类型为 service 的 Unit<br>~$:systemctl list-unit-files    # 列出所有配置文件<br>~$:systemctl list-unit-files --type=service    # 列出指定类型的配置文件<br>~$:systemctl start foo.service# 用来启动一个service (并不会重启现有的)<br>~$:systemctl stop foo.service# 用来停止一个service (并不会重启现有的)。<br>~$:systemctl restart foo.service# 用来停止并启动一个service。<br>~$:systemctl reload foo.service    # 当支持时，重新装载配置文件而不中断等待操作。<br>~$:systemctl condrestart foo.service    # 如果service正在运行那么重启它。<br>~$:systemctl status foo.service    # 汇报service是否正在运行。<br>~$:systemctl list-unit-files --type=service     # 用来列出可以启动或停止的service列表。<br>~$:systemctl enable foo.service     # 在下次启动时或满足其他触发条件时设置service为启用<br>~$:systemctl disable foo.service# 在下次启动时或满足其他触发条件时设置service为禁用<br>~$:systemctl is-enabled foo.service     # 用来检查一个service在当前环境下被配置为启用还是禁用。<br>~$:systemctl list-unit-files --type=service     # 输出在各个运行级别下service的启用和禁用情况<br>~$:systemctl daemon-reload    # 创建新service文件或者变更设置时使用。<br>~$:systemctl isolate multi-user.target (OR systemctl isolate runlevel3.target OR telinit 3)    # 改变至多用户运行级别<br>~$:ls /etc/SystemD/system/*.wants/foo.service      # 用来列出该service在哪些运行级别下启用和禁用。<br>~$:systemctl reboot     # 重启机器<br>~$:systemctl poweroff   # 关机<br>~$:systemctl suspend    # 待机<br>~$:systemctl hibernate  # 休眠<br>~$:systemctl hybrid-sleep   # 混合休眠模式（同时休眠到硬盘并待机）</p><p>一般只有管理员才可以关机。正常情况下系统不应该允许 SSH 远程登录的用户执行关机命令。否则其他用户正在job，一个用户把系统关了就不好了。使用logind解决这个问题。logind 不是 pid-1 的 init 进程。它的作用和 UpStart 的 session init 类似，但功能要丰富很多，它能够管理几乎所有用户会话(session)相关的事情。logind 不仅是 ConsoleKit 的替代，它可以：</p><ul><li>维护，跟踪会话和用户登录情况。</li><li>Logind 也负责统计用户会话是否长时间没有操作，可以执行休眠/关机等相应操作。</li><li>为用户会话的所有进程创建 CGroup。这不仅方便统计所有用户会话的相关进程，也可以实现会话级别的系统资源控制。</li><li>负责电源管理的组合键处理，比如用户按下电源键，将系统切换至睡眠状态。</li><li>多席位(multi-seat) 管理。</li></ul><h2 id="sysvinit-upstart-systemd比较">SysVInit，Upstart，SystemD比较</h2><p>SysVInit比较简单。Service开发人员只需要编写启动和停止脚本，将 service 添加/删除到某个 runlevel 时，只需要执行一些创建/删除软连接文件的基本操作。<br>其次，SysVInit 的另一个优点是确定的执行顺序：脚本严格按照启动数字的大小顺序执行，一个执行完毕再执行下一个，这非常有益于错误排查。UpStart 和 SystemD 支持并发启动，导致没有人可以确定地了解具体的启动顺序，排错不易。但是串行地执行脚本导致 SysVInit 运行效率较慢。<br>可以看到，UpStart 的设计比 SysVInit 更加先进。<br>SystemD 的最大特点有两个：并发启动能力强，极大地提高了系统启动速度；用 CGroup 统计跟踪子进程，干净可靠。<br>此外，和其前任不同的地方在于，SystemD 已经不仅仅是一个初始化系统了。SystemD 出色地替代了 SysVInit 的所有功能，但它并未就此自满。因为 init 进程是系统所有进程的父进程这样的特殊性，SystemD 非常适合提供曾经由其他service提供的功能，比如定时任务 (以前由 crond 完成)；会话管理 (以前由 ConsoleKit/PolKit 等管理) 。</p><h2 id="参考文献">参考文献</h2><p>1.<a href="https://www.ibm.com/developerworks/cn/linux/1407_liuming_init1/index.html" target="_blank" rel="noopener">https://www.ibm.com/developerworks/cn/linux/1407_liuming_init1/index.html</a><br>2.<a href="https://www.ibm.com/developerworks/cn/linux/1407_liuming_init2/index.html?ca=drs-" target="_blank" rel="noopener">https://www.ibm.com/developerworks/cn/linux/1407_liuming_init2/index.html?ca=drs-</a><br>3.<a href="https://www.ibm.com/developerworks/cn/linux/1407_liuming_init3/index.html?ca=drs-" target="_blank" rel="noopener">https://www.ibm.com/developerworks/cn/linux/1407_liuming_init3/index.html?ca=drs-</a><br>4.<a href="http://www.ruanyifeng.com/blog/2016/03/systemd-tutorial-commands.html" target="_blank" rel="noopener">http://www.ruanyifeng.com/blog/2016/03/systemd-tutorial-commands.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;linux-init系统&quot;&gt;Linux init系统&lt;/h2&gt;
&lt;p&gt;这里会介绍下面三种init系统&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;SysVInit(initd)&lt;/li&gt;
&lt;li&gt;Upstart&lt;/li&gt;
&lt;li&gt;SystemD&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Linu
      
    
    </summary>
    
      <category term="linux" scheme="http://mxxhcm.github.io/categories/linux/"/>
    
    
      <category term="linux" scheme="http://mxxhcm.github.io/tags/linux/"/>
    
      <category term="init" scheme="http://mxxhcm.github.io/tags/init/"/>
    
      <category term="Upstart" scheme="http://mxxhcm.github.io/tags/Upstart/"/>
    
      <category term="SystemD" scheme="http://mxxhcm.github.io/tags/SystemD/"/>
    
      <category term="SysVInit" scheme="http://mxxhcm.github.io/tags/SysVInit/"/>
    
  </entry>
  
  <entry>
    <title>linux screen capture</title>
    <link href="http://mxxhcm.github.io/2019/06/22/linux-screen-capture/"/>
    <id>http://mxxhcm.github.io/2019/06/22/linux-screen-capture/</id>
    <published>2019-06-22T13:48:27.000Z</published>
    <updated>2019-06-22T15:04:49.104Z</updated>
    
    <content type="html"><![CDATA[<h2 id="常用的截屏工具">常用的截屏工具</h2><ul><li>screenshot</li><li>scrot</li></ul><p>screentshot是系统自带的截屏工具，可以部分截取，直接按win键搜索可运行。<br>scrot需要安装，这个工具需要从终端运行，截图默认保存在当前目录。</p><h2 id="screenshot">screenshot</h2><h2 id="scrot">scrot</h2><p>scrot [options] [filename]</p><h3 id="参数介绍">参数介绍</h3><p>scrot [-vusbcm]<br>不加参数，直接截取整个screen<br>-v  查看scort版本<br>-u  截取当前鼠标焦点所在window<br>-s  执行命令之后，点击鼠标截取相对应的window。<br>-b  包含当前window的边界<br>–delay [NUM]   延长NUM秒之后截图<br>-c  设置delay的时候显示数字延迟<br>–quliaty [NUM] 指定screenshot image的质量，从$1-100$<br>–thumb [NUM]   缩略图，是原始大小的百分之多少。。。</p><h2 id="参考文献">参考文献</h2><p>1.<a href="https://www.howtoforge.com/tutorial/how-to-take-screenshots-in-linux-with-scrot/" target="_blank" rel="noopener">https://www.howtoforge.com/tutorial/how-to-take-screenshots-in-linux-with-scrot/</a><br>2.<a href="http://awesomescreenshot.com/" target="_blank" rel="noopener">http://awesomescreenshot.com/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;常用的截屏工具&quot;&gt;常用的截屏工具&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;screenshot&lt;/li&gt;
&lt;li&gt;scrot&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;screentshot是系统自带的截屏工具，可以部分截取，直接按win键搜索可运行。&lt;br&gt;
scrot需要安装，这个工具
      
    
    </summary>
    
      <category term="linux" scheme="http://mxxhcm.github.io/categories/linux/"/>
    
    
      <category term="linux" scheme="http://mxxhcm.github.io/tags/linux/"/>
    
      <category term="scrot" scheme="http://mxxhcm.github.io/tags/scrot/"/>
    
      <category term="screenshot" scheme="http://mxxhcm.github.io/tags/screenshot/"/>
    
  </entry>
  
  <entry>
    <title>python 指定长度list初始化</title>
    <link href="http://mxxhcm.github.io/2019/06/18/python-%E6%8C%87%E5%AE%9A%E9%95%BF%E5%BA%A6list%E5%88%9D%E5%A7%8B%E5%8C%96/"/>
    <id>http://mxxhcm.github.io/2019/06/18/python-指定长度list初始化/</id>
    <published>2019-06-18T07:26:32.000Z</published>
    <updated>2019-06-18T07:35:43.725Z</updated>
    
    <content type="html"><![CDATA[<h2 id="指定长度list初始化"><a href="#指定长度list初始化" class="headerlink" title="指定长度list初始化"></a>指定长度list初始化</h2><p>想要找到初始化指定长度list最快的方法。<br>方法一：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">length = <span class="number">10</span></span><br><span class="line">array = [[]] * length</span><br></pre></td></tr></table></figure></p><p>方法二<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">length = <span class="number">10</span></span><br><span class="line">array = [[] <span class="keyword">for</span> _ <span class="keyword">in</span> range(length)]</span><br></pre></td></tr></table></figure></p><p>事实上，只有第二种方法是对的。第一种方法中，arrary中的10个[]都指向了同一个对象。。</p><h2 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">length = <span class="number">10</span></span><br><span class="line">v1 = [[]]*length</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(length):</span><br><span class="line">    v1[i].append(i)</span><br><span class="line">print(v1)</span><br><span class="line"><span class="comment"># [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]]</span></span><br><span class="line"></span><br><span class="line">v2 = [[] <span class="keyword">for</span> _ <span class="keyword">in</span> range(length)]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(length):</span><br><span class="line">    v2[i].append(i)</span><br><span class="line"></span><br><span class="line">print(v2)</span><br><span class="line"><span class="comment"># [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9]]</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;指定长度list初始化&quot;&gt;&lt;a href=&quot;#指定长度list初始化&quot; class=&quot;headerlink&quot; title=&quot;指定长度list初始化&quot;&gt;&lt;/a&gt;指定长度list初始化&lt;/h2&gt;&lt;p&gt;想要找到初始化指定长度list最快的方法。&lt;br&gt;方法一：&lt;br&gt;&lt;
      
    
    </summary>
    
      <category term="python" scheme="http://mxxhcm.github.io/categories/python/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="list" scheme="http://mxxhcm.github.io/tags/list/"/>
    
  </entry>
  
  <entry>
    <title>linux pipe and pipe command</title>
    <link href="http://mxxhcm.github.io/2019/06/17/linux-pipe-and-pipe-command/"/>
    <id>http://mxxhcm.github.io/2019/06/17/linux-pipe-and-pipe-command/</id>
    <published>2019-06-17T11:59:30.000Z</published>
    <updated>2019-06-26T09:09:55.226Z</updated>
    
    <content type="html"><![CDATA[<h2 id="pipe">PIPE</h2><p>管道命令仅会处理stdout并不会处理stderrout，管道命令必须要能接受前一个命令传回来的数据成为stdinput</p><h2 id="head-tail">head tail</h2><h2 id="cut">cut</h2><h3 id="参数说明">参数说明</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cut -d &apos;分隔字符&apos; -f (fields)fields为数字</span><br><span class="line">cut -c 字符范围</span><br></pre></td></tr></table></figure><h3 id="示例">示例</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cut -d &apos;:&apos; -f 2,3</span><br><span class="line">echo $PATH | cut -d &apos;:&apos; -f 2,4</span><br><span class="line">cut -c 20-30</span><br><span class="line">export | cut -c 12-</span><br><span class="line">不过cut 对于多个空格当做分隔字符的处理做的不够好</span><br></pre></td></tr></table></figure><h2 id="grep">grep</h2><h3 id="参数说明-v2">参数说明</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">grep [-aincv] [--color=auto] &apos;关键字&apos; filename</span><br><span class="line">-a 将binary文件以text的方式查找数据</span><br><span class="line">-i 忽略大小写</span><br><span class="line">-c 计算查找到的字符串的个数</span><br><span class="line">-n 顺便输出行号</span><br><span class="line">-v 反向选择</span><br><span class="line">grep -n  &apos;^$&apos; regular_express</span><br></pre></td></tr></table></figure><h3 id="示例-v2">示例</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">grep &apos;^the&apos; file</span><br><span class="line">grep &apos;[^[:lower:]]&apos; file</span><br><span class="line">grep &apos;\.$&apos; file</span><br><span class="line">grep &apos;^[^a-zA-Z]&apos; file</span><br><span class="line">grep &apos;go\&#123;2,3\&#125;g&apos; file</span><br><span class="line">对比</span><br><span class="line">ls -l /etc/a*</span><br><span class="line">grep -n &apos;^a.*/&apos;</span><br></pre></td></tr></table></figure><h2 id="dmesg">dmesg</h2><p>dmesg 查看内核信息<br>dmesg | grep -n A3 B2 ‘eth’<br>A --after  B --before</p><h2 id="last">last</h2><p>last | grep ‘mxx’ | cut -d ‘’ -f 1</p><h2 id="sort-wc-uniq">sort,wc,uniq</h2><h3 id="参数说明-v3">参数说明</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">sort 排序</span><br><span class="line">[-fbMnrutk] [file]</span><br><span class="line">-f 不区分大小写</span><br><span class="line">-u uniq</span><br><span class="line">-t 分隔符</span><br><span class="line">-k 以第几个字段进行排序</span><br><span class="line">-n 以数字进行排序(默认是以字母)</span><br><span class="line">-m 反向排序</span><br></pre></td></tr></table></figure><h3 id="示例-v3">示例</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sort -t &apos;:&apos; -k 3 -n /etc/passwd</span><br><span class="line">cat /etc/passwd | sort -t &apos;:&apos; -k 3 -n</span><br></pre></td></tr></table></figure><h2 id="uniq">uniq</h2><p>uniq 去重<br>[-il] [file]<br>-i 不区分大小写<br>-c 进行计数</p><pre><code>last | cut -d ' ' -f 1 | sort | uniq首先需要排序　才能去重</code></pre><p>last | cut -d ’ ’ -f 1 | sort | uniq -c</p><h2 id="tee">tee</h2><p>tee双重重定向将一份数据可以同时传到文件内以及屏幕中<br>last | tee last.list | sort</p><h2 id="tr">tr</h2><p>tr 删除一段文字或者对文字内容进行替换(如删除dos中的换行符^M)<br>[-ds]<br>-d 删除信息中的某个字串<br>-s 替换重复字符</p><pre><code>last | tr '[a-z]'  '[A-Z]'echo $PATH -d ':/'cat /root/passwd | tr -d '\r' &gt; passwd.linux</code></pre><h2 id="col">col</h2><p>col 简单处理<br>[-xb]<br>-x 将tab键换成空格键</p><pre><code>cat  manpath.config | col -x | cat -A | more</code></pre><h2 id="join">join</h2><p>join 将两个文件中具有相同数据的一行相加<br>join [-ti12] file1 file2<br>-i 大小写忽视<br>join -t ‘:’ passwd shadow<br>join -t ‘:’ -1 4 passwd -2 3 group</p><h2 id="paste">paste</h2><p>paste直接将两行粘在一起，默认并以tab键分开<br>-d后面可以加分隔字符默认以tab分隔<br>-表示来自standard input的数据的意思<br>paste shadow passwd<br>cat shadow | paste passwd - | head -n 3</p><h2 id="expand">expand</h2><p>expand将tab键换成空格默认是8个空格<br>-t 参数可以自行设定空格数<br>nl file | expand -t 6 - | cat -A</p><h2 id="split">split</h2><p>split [-bl] file PREFIX<br>-b后面加文件欲切割成的文件大小<br>-l以行数来切割<br>split -b 1M /etc/termcap termcap<br>ls -l termcap*</p><pre><code>cat termcap* &gt;&gt; termcapbackls -l / | spilt -l 10 -lsrootwc -l lsroot</code></pre><h2 id="xargs">xargs</h2><p>xargs产生某个命令的参数<br>[-pne0]<br>-p 执行每个命令询问用户<br>-e 是EOF的意思，后面可接一个字符，当xargs遇到这个字符，便会停止操作<br>-n 后面接次数，每次command命令执行时，要使用几个参数<br>-用来代替stdout以及stdin<br>tar -cvf - /home | tar xvf -</p><h2 id="sed-工具">sed 工具</h2><p>[-in]<br>-i直接修改文件内容<br>-n静默<br>-e 直接在shell下编辑<br>-c replace<br>-a append<br>-p print</p><p>nl file | sed '2,3d’<br>nl file | sed '$a add a test’<br>nl file | sed -n '5,7p’<br>nl file | sed '2,5c jkadfk<br>&gt;fdasf<br>&gt;asfddf '<br>nl file | sed ‘s/s_place/s_replace/g’<br>nl file | sed ‘/^$/d’</p><h2 id="egrep-扩展正则表达式">egrep 扩展正则表达式</h2><p>egrep -n ‘<sup>$|</sup>#’ file<br>egrep -n ‘go?d’ file　0个或者一个?之前的字符<br>egrep -n ‘go+d’ file　一个及以上+之前的字符<br>grep -n 'go<em>d’ file　0个或者0个以上</em>之前的字符</p><h2 id="printf-格式化打印">printf 格式化打印</h2><p>printf ‘%s\t %s\t %s\t \n’ $(cat file)<br>printf ‘%10s %5i %5i \n’ $(cat file)</p><h2 id="awk">awk</h2><p>last | awk '{print $1 “\t” S3 “\t” $4 NF NR}'<br>cat /etc/passwd | awk ‘BEGIN {FS=&quot;:&quot;} $3 &lt; 10 {print $1 “\t” $3 }’</p><p>cat /etc/passwd | awk ‘NR==1{printf &quot;%10s %10s %10s %10s “,$1,$2,$3,“total”}<br>NR&gt;=2{total=$2+$3;<br>printf “%10s %10d %10d %## f”,$1,$2,$3,total”}’</p><h2 id="参考文献">参考文献</h2><ol><li>《鸟哥的LINUX私房菜》</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;pipe&quot;&gt;PIPE&lt;/h2&gt;
&lt;p&gt;管道命令仅会处理stdout并不会处理stderrout，管道命令必须要能接受前一个命令传回来的数据成为stdinput&lt;/p&gt;
&lt;h2 id=&quot;head-tail&quot;&gt;head tail&lt;/h2&gt;
&lt;h2 id=&quot;cut&quot;&gt;c
      
    
    </summary>
    
      <category term="linux" scheme="http://mxxhcm.github.io/categories/linux/"/>
    
    
      <category term="linux" scheme="http://mxxhcm.github.io/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>linux cp, scp vs rsync</title>
    <link href="http://mxxhcm.github.io/2019/06/16/linux-cp-scp-vs-rsync/"/>
    <id>http://mxxhcm.github.io/2019/06/16/linux-cp-scp-vs-rsync/</id>
    <published>2019-06-16T08:29:17.000Z</published>
    <updated>2019-08-24T11:26:25.269Z</updated>
    
    <content type="html"><![CDATA[<h2 id="cp">cp</h2><h3 id="参数介绍">参数介绍</h3><h3 id="示例">示例</h3><h2 id="scp">scp</h2><h3 id="参数介绍-v2">参数介绍</h3><h3 id="示例-v2">示例</h3><h2 id="rsync">rsync</h2><p>rsync相对于scp有以下优点：<br>1.支持断点续传<br>2.支持ssh<br>3.可分块传输<br>~$:rsync options source destination</p><h3 id="参数介绍-v3">参数介绍</h3><p>rsync [-zvra] source destination<br>-z  传输前进行压缩<br>-v  显示详细信息<br>-r  递归拷贝<br>-a  保留时间戳，owner,group<br>-e ssh  使用ssh<br>–partial    单个文件的断点续传<br>–progress  显示同步进度<br>-P等于–paritial和–progress一同使用<br>–include   只同步某些目录<br>–exclude   不同步某些目录</p><h3 id="示例-v3">示例</h3><p>~$:rsync -avc --exclude=**<strong>pycache</strong>  --exclude=**data --exclude=**tmp --exclude=**result_pictures experimental/ ~/<br>~$:rsync -rP source_dir target_dir</p><h2 id="参考文献">参考文献</h2><p>1.<a href="https://stackoverflow.com/questions/4585929/how-to-use-cp-command-to-exclude-a-specific-directory" target="_blank" rel="noopener">https://stackoverflow.com/questions/4585929/how-to-use-cp-command-to-exclude-a-specific-directory</a><br>2.<a href="https://www.cnblogs.com/bangerlee/archive/2013/04/07/3003243.html" target="_blank" rel="noopener">https://www.cnblogs.com/bangerlee/archive/2013/04/07/3003243.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;cp&quot;&gt;cp&lt;/h2&gt;
&lt;h3 id=&quot;参数介绍&quot;&gt;参数介绍&lt;/h3&gt;
&lt;h3 id=&quot;示例&quot;&gt;示例&lt;/h3&gt;
&lt;h2 id=&quot;scp&quot;&gt;scp&lt;/h2&gt;
&lt;h3 id=&quot;参数介绍-v2&quot;&gt;参数介绍&lt;/h3&gt;
&lt;h3 id=&quot;示例-v2&quot;&gt;示例&lt;/h3&gt;
&lt;h2 
      
    
    </summary>
    
      <category term="linux" scheme="http://mxxhcm.github.io/categories/linux/"/>
    
    
      <category term="linux" scheme="http://mxxhcm.github.io/tags/linux/"/>
    
      <category term="cp" scheme="http://mxxhcm.github.io/tags/cp/"/>
    
      <category term="scp" scheme="http://mxxhcm.github.io/tags/scp/"/>
    
      <category term="rsync" scheme="http://mxxhcm.github.io/tags/rsync/"/>
    
  </entry>
  
  <entry>
    <title>linux ssh tunnel内网穿透</title>
    <link href="http://mxxhcm.github.io/2019/06/10/linux-ssh-tunnel%E5%86%85%E7%BD%91%E7%A9%BF%E9%80%8F/"/>
    <id>http://mxxhcm.github.io/2019/06/10/linux-ssh-tunnel内网穿透/</id>
    <published>2019-06-10T13:33:16.000Z</published>
    <updated>2019-06-30T13:12:34.008Z</updated>
    
    <content type="html"><![CDATA[<h2 id="ssh-命令介绍">ssh 命令介绍</h2><p>ssh是一种安全传输协议，此外还有tunnel转发功能，可以用来内网渗透。</p><h3 id="参数介绍">参数介绍</h3><p>-L port:host:hostport，访问本机的port端口就相当于访问host的hostport端口。<br>将本机的某个端口转发到远端指定机器的指定端口。本机上分了一个socket监听port端口，一旦该端口有了连接，就通过一个ssh转发出去。<br>-R port:host:hostport，将远程主机的某个端口转发到指定的本地机器的指定端口。远程主机上分了一个socket监听port端口，一旦该端口有了连接，就通过一个ssh转发到指定的本地机器的指定端口。<br>-N 不指定脚本或者命令<br>-f 后台认证，需要和-N连用</p><p>-L和-R的区别，-L是ssh隧道，-R是ssh反向隧道。</p><h2 id="示例">示例</h2><h3 id="翻墙ssh-l">翻墙ssh -L</h3><p>执行以下命令的本机(localhost)通过中间服务器(45.32.22.289)访问被屏蔽的网站(google)。<br>~$:ssh -L 1234:google_ip:80 root@45.32.22.289<br>拿这个举个例子，可能不是很恰当，但是有助于理解。我自己的机器(A)是不能访问google©的，但是我有一台vps(B)，地址为45.32.22.289是可以访问google的，可以通过ssh隧道将A的端口(1234)通过B映射到C的端口(80)。</p><h3 id="本机访问局域网的tensorboard-server">本机访问局域网的tensorboard server</h3><h4 id="本机设置">本机设置</h4><p>~$:ssh -L 12345:10.1.114.50:6006 <a href="mailto:mxxmhh@127.0.0.1" target="_blank" rel="noopener">mxxmhh@127.0.0.1</a><br>将本机的12345端口映射到10.1.114.50的6006端口，中间服务器使用的是本机。<br>或者可以使用10.1.114.50作为中间服务器。<br>~$:ssh -L 12345:10.1.114.50:6006 <a href="mailto:liuchi@10.1.114.50" target="_blank" rel="noopener">liuchi@10.1.114.50</a><br>或者可以使用如下方法：<br>~$:ssh -L 12345:127.0.0.1:6006 <a href="mailto:liuchi@10.1.114.50" target="_blank" rel="noopener">liuchi@10.1.114.50</a><br>从这个方法中，可以看出127.0.0.1这个ip是中间服务器可以访问的ip。<br>以上三种方法中，-L后的端口号12345可以随意设置，只要不冲突即可。</p><h4 id="服务端设置">服务端设置</h4><p>然后在服务端运行以下命令：<br>~$:tensorboard --logdir logdir -port 6006<br>这个端口号也是可以任意设置的，不冲突即可。</p><h4 id="运行">运行</h4><p>然后在本机访问<br><a href="https://127.0.0.1:12345" target="_blank" rel="noopener">https://127.0.0.1:12345</a>即可。</p><h3 id="内网穿透ssh-r">内网穿透ssh -R</h3><p>外网A(123.123.123.123)访问处于内网B的(127.0.0.1)的机器。<br>~$:ssh -N -f -R 2222:127.0.0.1:22 <a href="mailto:root@123.123.123.123" target="_blank" rel="noopener">root@123.123.123.123</a></p><p>可以在外网机器A(123.123.123.123)上通过如下命令访问(-R)指定的内网机器B：<br>~$:ssh -p 2222 userB@localhost</p><h3 id="报错">报错</h3><p>Host key verification failed<br>直接把/home/username/.ssh/known_hosts中相应的给删了。</p><h3 id="内网访问内网-挖洞">内网访问内网（挖洞）</h3><p>A是家里的内网（无公网IP）上机器(196.)，B是VPS（有公网IP）(45.32.)，C是公司内网（无公网IP）机器(10.)。<br>要在家里的内网访问公司的内网，即A访问C。在C上建立ssh反向隧道：<br>~$:ssh -N -f -R 2222:127.0.0.1:22 userB@B.ip<br>在A上访问：<br>~$:ssh -p  2222 userC@B.ip</p><h2 id="参考文献">参考文献</h2><p>1.<a href="https://blog.trackets.com/2014/05/17/ssh-tunnel-local-and-remote-port-forwarding-explained-with-examples.html" target="_blank" rel="noopener">https://blog.trackets.com/2014/05/17/ssh-tunnel-local-and-remote-port-forwarding-explained-with-examples.html</a><br>2.<a href="https://blog.creke.net/722.html" target="_blank" rel="noopener">https://blog.creke.net/722.html</a><br>3.<a href="http://arondight.me/2016/02/17/%E4%BD%BF%E7%94%A8SSH%E5%8F%8D%E5%90%91%E9%9A%A7%E9%81%93%E8%BF%9B%E8%A1%8C%E5%86%85%E7%BD%91%E7%A9%BF%E9%80%8F/" target="_blank" rel="noopener">http://arondight.me/2016/02/17/使用SSH反向隧道进行内网穿透/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;ssh-命令介绍&quot;&gt;ssh 命令介绍&lt;/h2&gt;
&lt;p&gt;ssh是一种安全传输协议，此外还有tunnel转发功能，可以用来内网渗透。&lt;/p&gt;
&lt;h3 id=&quot;参数介绍&quot;&gt;参数介绍&lt;/h3&gt;
&lt;p&gt;-L port:host:hostport，访问本机的port端口就相当
      
    
    </summary>
    
      <category term="linux" scheme="http://mxxhcm.github.io/categories/linux/"/>
    
    
      <category term="linux" scheme="http://mxxhcm.github.io/tags/linux/"/>
    
      <category term="ssh" scheme="http://mxxhcm.github.io/tags/ssh/"/>
    
  </entry>
  
  <entry>
    <title>pytorch nn.Conv2d vs nn.functional.conv2d</title>
    <link href="http://mxxhcm.github.io/2019/06/07/pytorch-nn-Conv2d-vs-nn-functional-conv2d/"/>
    <id>http://mxxhcm.github.io/2019/06/07/pytorch-nn-Conv2d-vs-nn-functional-conv2d/</id>
    <published>2019-06-07T04:45:21.000Z</published>
    <updated>2019-06-07T05:09:47.408Z</updated>
    
    <content type="html"><![CDATA[<h2 id="nn-conv2d-vs-nn-functional-conv2d">nn.Conv2d vs nn.functional.conv2d</h2><ul><li>nn.functional包中是函数接口，是从输入到输出的一个变换，内部没有Variable，不能够构成一个layer；nn包中是nn.functional函数对应的类封装，nn中的类可能有Variable（如Conv2d)，也可能没有（如Dropout，Maxpooling）</li><li>nn中的类一般是nn.Module的子类，继承了nn.Module的方法和属性。</li><li>nn中的类需要传入参数实例化，然后用函数调用的方法调用实例化对象传入数据。而nn.functional是函数，不需要实例化可以直接调用，需要同时传入filters的weights和biases。</li></ul><h2 id="参考文献">参考文献</h2><p>1.<a href="https://www.zhihu.com/question/66782101/answer/579393790" target="_blank" rel="noopener">https://www.zhihu.com/question/66782101/answer/579393790</a><br>2.<a href="https://www.zhihu.com/question/66782101/answer/246460048" target="_blank" rel="noopener">https://www.zhihu.com/question/66782101/answer/246460048</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;nn-conv2d-vs-nn-functional-conv2d&quot;&gt;nn.Conv2d vs nn.functional.conv2d&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;nn.functional包中是函数接口，是从输入到输出的一个变换，内部没有Variable，不能
      
    
    </summary>
    
      <category term="pytorch" scheme="http://mxxhcm.github.io/categories/pytorch/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="pytorch" scheme="http://mxxhcm.github.io/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>python json</title>
    <link href="http://mxxhcm.github.io/2019/06/06/python-json/"/>
    <id>http://mxxhcm.github.io/2019/06/06/python-json/</id>
    <published>2019-06-06T07:58:02.000Z</published>
    <updated>2019-06-06T08:16:11.539Z</updated>
    
    <content type="html"><![CDATA[<h2 id="什么是json"><a href="#什么是json" class="headerlink" title="什么是json"></a>什么是json</h2><p>是一种文件格式<br>json object和python的字典差不多。<br>json在python中可以以字符串形式读入。</p><h2 id="python读取json"><a href="#python读取json" class="headerlink" title="python读取json"></a>python读取json</h2><h3 id="json-loads"><a href="#json-loads" class="headerlink" title="json.loads()"></a>json.loads()</h3><p>json.loads()从内存中读取。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import json</span><br><span class="line"></span><br><span class="line">person = &apos;&#123;&quot;name&quot;: &quot;Bob&quot;, &quot;languages&quot;: [&quot;English&quot;, &quot;Fench&quot;]&#125;&apos;</span><br><span class="line">person_dict = json.loads(person)</span><br><span class="line"></span><br><span class="line"># Output: &#123;&apos;name&apos;: &apos;Bob&apos;, &apos;languages&apos;: [&apos;English&apos;, &apos;Fench&apos;]&#125;</span><br><span class="line">print( person_dict)</span><br><span class="line"></span><br><span class="line"># Output: [&apos;English&apos;, &apos;French&apos;]</span><br><span class="line">print(person_dict[&apos;languages&apos;])</span><br></pre></td></tr></table></figure></p><h3 id="json-load"><a href="#json-load" class="headerlink" title="json.load()"></a>json.load()</h3><p>json.load()从文件对象中读取。<br>假设有名为person.json的json文件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">&quot;name&quot;: &quot;Bob&quot;, </span><br><span class="line">&quot;languages&quot;: [&quot;English&quot;, &quot;Fench&quot;]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>直接从文件对象中读取<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'person.json'</span>) <span class="keyword">as</span> f:</span><br><span class="line">  data = json.load(f)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Output: &#123;'name': 'Bob', 'languages': ['English', 'Fench']&#125;</span></span><br><span class="line">print(data)</span><br></pre></td></tr></table></figure></p><h2 id="python写json文件"><a href="#python写json文件" class="headerlink" title="python写json文件"></a>python写json文件</h2><h3 id="json-dumps"><a href="#json-dumps" class="headerlink" title="json.dumps()"></a>json.dumps()</h3><p>json.dumps()将字典转化为JSON字符串<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line">person_dict = &#123;<span class="string">'name'</span>: <span class="string">'Bob'</span>,</span><br><span class="line"><span class="string">'age'</span>: <span class="number">12</span>,</span><br><span class="line"><span class="string">'children'</span>: <span class="literal">None</span></span><br><span class="line">&#125;</span><br><span class="line">person_json = json.dumps(person_dict)</span><br><span class="line"></span><br><span class="line">print(person_json)</span><br></pre></td></tr></table></figure></p><h3 id="json-dump"><a href="#json-dump" class="headerlink" title="json.dump()"></a>json.dump()</h3><p>json.dump()直接将字典写入文件对象<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line">person_dict = &#123;<span class="string">"name"</span>: <span class="string">"Bob"</span>,</span><br><span class="line"><span class="string">"languages"</span>: [<span class="string">"English"</span>, <span class="string">"Fench"</span>],</span><br><span class="line"><span class="string">"married"</span>: <span class="literal">True</span>,</span><br><span class="line"><span class="string">"age"</span>: <span class="number">32</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'person.txt'</span>, <span class="string">'w'</span>) <span class="keyword">as</span> json_file:</span><br><span class="line">  json.dump(person_dict, json_file)</span><br></pre></td></tr></table></figure></p><h2 id="pretty-JSON"><a href="#pretty-JSON" class="headerlink" title="pretty JSON"></a>pretty JSON</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line">person_string = <span class="string">'&#123;"name": "Bob", "languages": "English", "numbers": [2, 1.6, null]&#125;'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Getting dictionary</span></span><br><span class="line">person_dict = json.loads(person_string)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Pretty Printing JSON string back</span></span><br><span class="line">print(json.dumps(person_dict, indent = <span class="number">4</span>, sort_keys=<span class="literal">True</span>))</span><br></pre></td></tr></table></figure><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>1.<a href="https://www.programiz.com/python-programming/json" target="_blank" rel="noopener">https://www.programiz.com/python-programming/json</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;什么是json&quot;&gt;&lt;a href=&quot;#什么是json&quot; class=&quot;headerlink&quot; title=&quot;什么是json&quot;&gt;&lt;/a&gt;什么是json&lt;/h2&gt;&lt;p&gt;是一种文件格式&lt;br&gt;json object和python的字典差不多。&lt;br&gt;json在pytho
      
    
    </summary>
    
      <category term="python" scheme="http://mxxhcm.github.io/categories/python/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="json" scheme="http://mxxhcm.github.io/tags/json/"/>
    
  </entry>
  
  <entry>
    <title>linux process ps kill top pstree nice</title>
    <link href="http://mxxhcm.github.io/2019/06/03/linux-process-ps-kill-top-pstree-nice/"/>
    <id>http://mxxhcm.github.io/2019/06/03/linux-process-ps-kill-top-pstree-nice/</id>
    <published>2019-06-03T13:30:04.000Z</published>
    <updated>2019-07-09T02:08:45.424Z</updated>
    
    <content type="html"><![CDATA[<h2 id="概述">概述</h2><p>这一节介绍和process相关的命令，包含ps,top,kill, pstree, nice, fuser, lsof, pidof, /proc/等</p><h2 id="ps查看进程">ps查看进程</h2><h3 id="参数介绍">参数介绍</h3><p>ps [-Aauf] [xlj]<br>-A 所有的进程全部显示出来<br>a 现行终端机下所有程序，包含其他用户<br>u 有效用户相关的进程，主要以用户为主的格式来区分<br>f 用ASCII字符显示树状结构，表达进程间的关系<br>x　通常与a这个参数一块使用，显示所有程序，不以终端机来区分<br>l　较长，较详细的将该PID的信息列出<br>j　工作的格式</p><h3 id="示例">示例</h3><p>~$:ps aux　查看系统所有的进程数据<br>~$:ps -lA　查看所有系统的数据<br>~$:ps axjf　连同部分进程树状态<br>~$:ps aux | grep ‘sslocal’ #查看sslocal程序是否运行<br>~$:ps ax # 显示当前系统进程的列表<br>~$:ps aux #显示当前系统进程详细列表以及进程用户<br>~$:ps -A  #列出进程号<br>~$:ps aux |grep 2222’|grep -v grep  # 找出所有包含2222的进程，grep -v 过滤掉含有grep字符的行</p><h3 id="aux-查看系统所有进程">aux 查看系统所有进程</h3><p>~$:ps aux     # 使用BSD格式显示进程<br>输出<br>USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND<br>USER<br>PID<br>%CPU    该进程使用掉的CPU资源百分比<br>%MEM    该进程占用物理内存百分比<br>VSZ 该进程占用虚拟内存量<br>RSS 该进程占用固定内存量(KB)<br>TTY pts/0　表示由网络连接进主机的进程<br>STAT    进程状态<br>START   该进程被触发的时间<br>TIME    CPU时间<br>COMMAND     该进程实际命令</p><p>僵尸进程(<defunct>)</defunct></p><h3 id="ps-ef">ps -ef</h3><p>~\$:ps -ef  # 使用标准格式显示进程<br>输出<br>UID        PID  PPID  C STIME TTY          TIME CMD<br>UID 用户名<br>PID 进程ID<br>PPID    父进程ID<br>C   CPU占用百分比<br>STIME   进程启动到现在的时间<br>TTY     在哪个终端上运行，ps/0表示网络连接<br>TIME<br>CMD     命令的名称和参数</p><h3 id="l仅查看自己相关的bash进程">-l仅查看自己相关的bash进程</h3><p>~\$:ps -l #仅查看自己相关的bash进程<br>输出<br>F S UID PID PPID C PR NI ADDZ SZ WCHAN TTY TIME CMD<br>F  说明进程权限<br>S　进程状态STAT<br>R(running)　S(sleep)　D(不可被唤醒的睡眠状态,通常是IO的进程)　T(stop)　Z(zombie僵尸状态)进程已终止，但无法被删除到到内存外,PCB还在，但是其他资源全部被收回，是由父进程负责收回资源。<br>UID/PID/PPID<br>C CPU使用率<br>PR/NI  Priority/Nice的缩写，此进程被CPU执行的优先级<br>ADDR/SZ/WCHAN都与内存有关，ADDR是kernel function ,指出该进程在内存的哪个部分，如果是个running的过程，显示-;SZ代表用掉多少内存;WCHAN表示目前进程是否运行，-表示正在运行<br>TTY 使用的终端接口<br>TIME    使用掉的CPU时间，而不是系统时间<br>CMD command缩写,造成此进程被触发的命令</p><h2 id="top-动态查看进程的变化">top 动态查看进程的变化</h2><h3 id="参数介绍-v2">参数介绍</h3><p>top [-d 数字] | top [-bnp]<br>-d　整个进程界面更新的秒数<br>-b  以批次的方式执行top，<br>-n  与-b搭配，意义是，需要进行几次top的输出结果<br>-p  指定某个PID来查看<br>在top执行中可以使用的命令<br>?查询所有的命令<br>P 按CPU使用率排序<br>M Mem排序<br>N PID排序<br>T CPU累计时间排序<br>k 给某个PID一个信号<br>r 给某个PID重新制定一个nice值<br>q 离开top软件</p><h3 id="示例-v2">示例</h3><p>~\$:top -d 2 #每两秒钟刷新一次top，默认为5s<br>~\$:top -b -n 2 &gt; ~/tmp/top.out</p><h3 id="top输出内容">top输出内容</h3><p>第一行top<br>目前时间　<br>开机到现在时间　up n days , hh:mm<br>登陆的用户<br>系统在1,5,15分钟时的负载，batch工作方式负载小于0.8即为这个负载，代表的是1,5,15分钟，系统平均要负责多少个程序。越小说明系统越闲<br>第二行task<br>目前进程总量，分别有多少个处于什么状态，不能有处于zombie的进程，<br>第三行%cpu<br>wa代表的是I/Owait，系统变慢都是由于I/O产生问题较多<br>第四五行内存和swap使用情况，swap被使用的应该尽量少，否则说明物理内存实在不足。<br>第六行<br>PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND<br>PID　每个进程ID<br>USER　该进程所属用户<br>PR　Priority 进程的优先级顺序，越小越早被执行<br>NI　Nice 与Priority有关，越小越早被执行<br>VIRT<br>RES<br>SHR<br>S   STAT<br>%CPU　CPU使用率<br>%MEM　内存使用率<br>TIME+  CPU使用时间的累加<br>COMMAND</p><h2 id="pstree-查看进程树">pstree 查看进程树</h2><h3 id="参数介绍-v3">参数介绍</h3><p>pstree [-A|-U] [-up]<br>-A  各进程树直接以ASCII字符连接<br>-U  各进程树之间以utf-8字符连接<br>-u  显示进程所属账号名<br>-p  显示pid</p><h3 id="示例-v3">示例</h3><p>~\$:pstree -Aup</p><h2 id="kill管理进程">kill管理进程</h2><h3 id="参数介绍-v4">参数介绍</h3><p>kill -signals %jobnumber 杀掉某个job<br>-l  列出所有signal<br>-1  重新读取一次参数的配置文件<br>-2  与ctrl+c　一样<br>-9  强制删除一个job，非正常状态<br>-15 让一个job正常结束</p><h3 id="查看signal种类">查看signal种类</h3><p>~\$:man 7 signal</p><h3 id="kill示例">kill示例</h3><p>kill -signal %jobnum<br>kill -signal pid<br>这两种情况是不同的，第一种是job，第二种是pid,不能弄混</p><h3 id="killall将系统中所有以某个命令启动的服务全部删除">killall将系统中所有以某个命令启动的服务全部删除</h3><p>killall [-iIe]  用来删除某个服务<br>-i iteractive<br>-e exact<br>-I 忽略大小写</p><h3 id="killall示例">killall示例</h3><p>~\$:killall utserver<br>~\$:killall -1 syslogd<br>~\$:killall -9 httpd<br>~\$:killall -i -9 bash</p><h2 id="nice管理进程优先级">nice管理进程优先级</h2><p>PRI(priority)与NI(nice)<br>PRI值是由内核动态调整的，用户无法直接调整PRI值<br>PRI(new)=PRI(old)+nice</p><p>nice值虽然可以影响PRI，但是并不是说原来PRI为50,nice为5,就会让PRI变为55,<br>这是需要经过系统分析之后决定的</p><p>nice值<br>a.可调整范围为-20~19<br>b.root可随意调整任何人的nice值-20~19间的任意一个值<br>c.一般用户仅可以调整自己nice值，且范围在0~19<br>d.一般用户仅可将nice值调高，而无法降低<br>e.调整nice值的方法<br>新执行的命令手动设置nice值<br>nice -n [number] command</p><h3 id="示例-v4">示例</h3><p>~\$:nice -n -5 vim &amp;</p><h3 id="已存在的进程调整nice值">已存在的进程调整nice值</h3><p>renice [number] command<br>~\$:ps -l | grep ‘\*bash\$’<br>~\$:renice 10 \$(ps -l|grep ‘bash\$’ | awk ‘{print \$4}’)</p><h2 id="fuser找到使用某文件的程序">fuser找到使用某文件的程序</h2><h3 id="参数介绍-v5">参数介绍</h3><p>fuser [-muv] [-k [i] [-signal]] name<br>-m  后面接的文件名会主动提到文件顶层<br>-u  user<br>-v  verbose<br>-k  SIGKILL<br>-i  询问用户，与-k搭配<br>-signal  -1,-15等，默认为-9</p><h3 id="示例-v5">示例</h3><p>~\$:mount -o loop ubuntu.iso /mnt/iso<br>~\$:cd /mnt/iso<br>~\$:umount /mnt/iso<br>error<br>~\$:fuser -muv   /mnt/iso<br>…<br>~\$:cd<br>~\$:umount /mnt/iso</p><p>~\$:fuser -muv /proc<br>~\$:fuser -ki /bin/bash</p><h2 id="lsof找到被进程打开的文件">lsof找到被进程打开的文件</h2><h3 id="参数介绍-v6">参数介绍</h3><p>lsof [-uaU] [+d]<br>-a 相当于and连接符<br>-u 某个用户的相关进程打开的文件<br>-U Unix like 的socket文件类型<br>+d 某个目录下被打开的文件</p><h3 id="示例-v6">示例</h3><p>~\$:lsof +d ~/Desktop<br>~\$:lsof -u mxx | grep ‘bash’<br>~\$:lsof -u mxx -a -U</p><h2 id="pidof找出某个正在进行的进程的pid">pidof找出某个正在进行的进程的pid</h2><h3 id="参数介绍-v7">参数介绍</h3><p>pidof [-sx] program_name<br>-s 仅列出一个pid而不列出所有的pid<br>-x 列出该进程可能的ppid的pid</p><h2 id="获取进程id">获取进程id</h2><p>ps -A |grep “command” | awk '{print $1}'<br>pidof 'command’<br>pgrep ‘command’</p><h2 id="proc-文件">/proc/\* 文件</h2><p>/proc/cmdline   加载kernel执行的参数<br>/proc/cpuinfo   CPU相关信息<br>/proc/devices   主要设备代号　与mknod相关<br>/proc/filesystems   目前已加载的文件系统<br>/proc/interrupts    系统上IRQ分配状态<br>/proc/ioports   目前系统上每个设备配置的I/O地址<br>/proc/loadavg   top/uptime显示的负载<br>/proc/kcore 内存大写<br>/proc/meminfo   free<br>/proc/modules   目前系统已加载的模块列表，可想成驱动程序<br>/proc/mounts　mount<br>/proc/swaps  系统加载的内存使用的分区记录<br>/proc/partitions　fdisk -l<br>/proc/pci　PCI总线上每个设备的详细情况　lspci<br>/proc/uptime　uptime<br>/proc/version　内核版本 uname -a<br>/proc/bus/\*　总线设备，USB设备</p><h2 id="具有suid-sgid的程序">具有SUID，SGID的程序</h2><p>如passwd，当触发passwd之后，会取得一个新的进程与PID,该PID产生时通过SUID给予该PID特殊的权限设置。<br>在一个bash中执行passwd会衍生出一个passwd进程，而且权限为root<br>~\$:passwd &amp;<br>~\$:pstree -up找到该进程</p><h2 id="参考文献">参考文献</h2><p>1.《鸟哥的LINUX私房菜》基础篇</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;概述&quot;&gt;概述&lt;/h2&gt;
&lt;p&gt;这一节介绍和process相关的命令，包含ps,top,kill, pstree, nice, fuser, lsof, pidof, /proc/等&lt;/p&gt;
&lt;h2 id=&quot;ps查看进程&quot;&gt;ps查看进程&lt;/h2&gt;
&lt;h3 id=&quot;参
      
    
    </summary>
    
      <category term="linux" scheme="http://mxxhcm.github.io/categories/linux/"/>
    
    
      <category term="linux" scheme="http://mxxhcm.github.io/tags/linux/"/>
    
      <category term="ps" scheme="http://mxxhcm.github.io/tags/ps/"/>
    
      <category term="top" scheme="http://mxxhcm.github.io/tags/top/"/>
    
      <category term="pstree" scheme="http://mxxhcm.github.io/tags/pstree/"/>
    
      <category term="process" scheme="http://mxxhcm.github.io/tags/process/"/>
    
      <category term="kill" scheme="http://mxxhcm.github.io/tags/kill/"/>
    
      <category term="nice" scheme="http://mxxhcm.github.io/tags/nice/"/>
    
  </entry>
  
  <entry>
    <title>linux 系统资源查看</title>
    <link href="http://mxxhcm.github.io/2019/06/03/linux-%E7%B3%BB%E7%BB%9F%E8%B5%84%E6%BA%90%E6%9F%A5%E7%9C%8B/"/>
    <id>http://mxxhcm.github.io/2019/06/03/linux-系统资源查看/</id>
    <published>2019-06-03T13:20:52.000Z</published>
    <updated>2019-06-24T13:37:59.615Z</updated>
    
    <content type="html"><![CDATA[<h2 id="查看内存">查看内存</h2><p>free [-bkmg] -t<br>-t  输出结果中显示free和swap加在一起的总量<br>free -h</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">~$:sudo dmidecode -t memory |grep "Number Of Devices" |awk '&#123;print $NF&#125;'    # 卡槽数量</span><br><span class="line">~$:sudo dmidecode -t memory |grep -A16 "Memory Device$" |grep 'Size:.*MB' |wc -l    # 内存数量</span><br><span class="line">~$:sudo dmidecode -t memory |grep -A16 "Memory Device$" |grep "Type:"  # 内存支持类型</span><br><span class="line">~$:sudo dmidecode -t memory |grep -A16 "Memory Device$" |grep "Speed:" # 每个内存频率</span><br><span class="line">~$:sudo dmidecode -t memory |grep -A16 "Memory Device$" |grep "Size:"  # 每个内存大小</span><br></pre></td></tr></table></figure><h2 id="查看磁盘">查看磁盘</h2><p>df -h 查看磁盘占用情况<br>du -h --max-depth=2 统计当前目录下深度为2的文件和目录大小</p><h2 id="查看cpu占用">查看cpu占用</h2><p>top<br><a href="https://mxxhcm.github.io//2019/05/07/linux-cpu%E4%BF%A1%E6%81%AF%E6%9F%A5%E7%9C%8B/">点击查看cpu信息命令</a></p><h2 id="查看系统与内核相关信息">查看系统与内核相关信息</h2><p>uname [-asrmpi]<br>-a 所有系统相关的信息，<br>-s 系统内核名称<br>-r 内核版本<br>-m 本系统硬件名称<br>-p CPU类型，与-m类似，显示的是CPU类型<br>-i 硬件平台</p><h2 id="查看系统启动时间与工作负载">查看系统启动时间与工作负载</h2><p>uptime</p><h2 id="查看网络">查看网络</h2><p>netstat [-atunlp]<br>-a 将目前系统上所有连接监听，socket列出来<br>-t 列出tcp网络数据包的数据<br>-u 列出udp网络数据包的数据<br>-n 不列出进程的服务名称，以端口号来显示<br>-l 列出目前正在网络监听(listen)的服务<br>-p 列出该网络服务的进程pid<br>~$:netstat -tnlp　　#找出目前系统上已经在监听的网络连接及其PID</p><h2 id="分析内核产生的信息">分析内核产生的信息</h2><p>dmesg | more<br>dmesg | grep -i 'sd’<br>dmesg | grep -i ‘eth’</p><h2 id="动态分析系统资源使用情况">动态分析系统资源使用情况</h2><p>man vmstat 查看各字段含义</p><p>vmstat      动态查看系统资源<br>vmstat [-a]     display active and inactive memory<br>vmstat [-d] report sisk statistics<br>vmstat [-p 分区]　Detailed statistics about partition<br>vmstat [-S 单位]  B K M G<br>vmstat [-f]  displays the num of forks since boot<br>~$:vmstat<br>~$:vmstat -a<br>~$:vmstat -p /dev/sda1<br>~$:vmstat -S M<br>~$:vmstat -f</p><h2 id="参考文献">参考文献</h2><p>1.《鸟哥的LINUX私房菜》</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;查看内存&quot;&gt;查看内存&lt;/h2&gt;
&lt;p&gt;free [-bkmg] -t&lt;br&gt;
-t  输出结果中显示free和swap加在一起的总量&lt;br&gt;
free -h&lt;/p&gt;
&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td cl
      
    
    </summary>
    
      <category term="linux" scheme="http://mxxhcm.github.io/categories/linux/"/>
    
    
      <category term="linux" scheme="http://mxxhcm.github.io/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>linux python调用shell脚本并将输出重定向到文件</title>
    <link href="http://mxxhcm.github.io/2019/06/03/linux-python%E8%B0%83%E7%94%A8shell%E8%84%9A%E6%9C%AC%E5%B9%B6%E5%B0%86%E8%BE%93%E5%87%BA%E9%87%8D%E5%AE%9A%E5%90%91%E5%88%B0%E6%96%87%E4%BB%B6/"/>
    <id>http://mxxhcm.github.io/2019/06/03/linux-python调用shell脚本并将输出重定向到文件/</id>
    <published>2019-06-03T12:11:38.000Z</published>
    <updated>2019-06-26T03:32:08.128Z</updated>
    
    <content type="html"><![CDATA[<h2 id="python执行shell脚本并且重定向输出到文件">python执行shell脚本并且重定向输出到文件</h2><p>目的：有一些shell脚本的参数需要调整，在shell中处理有些麻烦，就用python控制参数，然后调用shell，问题就是如何将shell脚本的输出进行重定向。最开始我想直接用python调用终端中shell重定向的语法，我用的是os.system(command)，command包含重定向的命令，在实践中证明是不可行的。为什么？？？留待解决。</p><p>找到的解决方案是调用subprocess包，将要执行的命令存入一个list，将这个list当做参数传入，获得返回值，进行文件读写。这里拿ls命令举个例子，注意一点是，包含重定向语句的ls命令使用os.system()也能重定向成功，我使用的一个命令就不行了。</p><h3 id="call-0">call <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">subprocess.call([<span class="string">'ls'</span>, <span class="string">'-l'</span>, <span class="string">'.'</span>]) <span class="comment"># 直接将程序执行结果输出，没有返回值。</span></span><br></pre></td></tr></table></figure></h3><h3 id="popen">Popen</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">results = subprocess.Popen([<span class="string">'ls'</span>, <span class="string">'-l'</span>, <span class="string">'.'</span>], </span><br><span class="line">            stdout=subprocess.PIPE, </span><br><span class="line">            stderr=subprocess.STDOUT)</span><br><span class="line">stdout, stderr = results.communicate() </span><br><span class="line">res = stdout.decode(<span class="string">'utf-8'</span>) <span class="comment"># 利用res将结果输出到文件</span></span><br></pre></td></tr></table></figure><h3 id="run">run</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">result = subprocess.run([<span class="string">'ls'</span>, <span class="string">'-l'</span>], stdout=subprocess.PIPE) </span><br><span class="line">res = result.stdout.decode(<span class="string">'utf-8'</span>)  <span class="comment"># 利用res将结果输出到文件</span></span><br></pre></td></tr></table></figure><h2 id="参考文献">参考文献</h2><p>1.<a href="https://stackoverflow.com/questions/4760215/running-shell-command-and-capturing-the-output" target="_blank" rel="noopener">https://stackoverflow.com/questions/4760215/running-shell-command-and-capturing-the-output</a><br>2.<a href="https://linuxhandbook.com/execute-shell-command-python/" target="_blank" rel="noopener">https://linuxhandbook.com/execute-shell-command-python/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;python执行shell脚本并且重定向输出到文件&quot;&gt;python执行shell脚本并且重定向输出到文件&lt;/h2&gt;
&lt;p&gt;目的：有一些shell脚本的参数需要调整，在shell中处理有些麻烦，就用python控制参数，然后调用shell，问题就是如何将shell脚
      
    
    </summary>
    
      <category term="linux" scheme="http://mxxhcm.github.io/categories/linux/"/>
    
      <category term="python" scheme="http://mxxhcm.github.io/categories/python/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="工具" scheme="http://mxxhcm.github.io/tags/%E5%B7%A5%E5%85%B7/"/>
    
      <category term="linux" scheme="http://mxxhcm.github.io/tags/linux/"/>
    
      <category term="常见问题" scheme="http://mxxhcm.github.io/tags/%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/"/>
    
      <category term="shell" scheme="http://mxxhcm.github.io/tags/shell/"/>
    
  </entry>
  
  <entry>
    <title>linux custom configure file</title>
    <link href="http://mxxhcm.github.io/2019/06/03/linux-custom-configure-file/"/>
    <id>http://mxxhcm.github.io/2019/06/03/linux-custom-configure-file/</id>
    <published>2019-06-03T02:40:12.000Z</published>
    <updated>2019-07-03T06:57:20.520Z</updated>
    
    <content type="html"><![CDATA[<h2 id="bashrc自定义配置">bashrc自定义配置</h2><p>~/.bashrc文件详细内容，<a href="https://github.com/mxxhcm/code/blob/master/shell/bashrc" target="_blank" rel="noopener">代码地址</a></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> 使用alias给常用文件夹起别名</span><br><span class="line">alias post-dir='/home/mxxmhh/mxxhcm/blog/source/_posts'</span><br><span class="line">alias code-dir='/home/mxxmhh/mxxhcm/code/'</span><br><span class="line">alias matplotlib-dir='/home/mxxmhh/mxxhcm/code/tools/matplotlib/'</span><br><span class="line">alias numpy-dir='/home/mxxmhh/mxxhcm/code/tools/numpy/'</span><br><span class="line">alias shell-dir='/home/mxxmhh/mxxhcm/code/shell'</span><br><span class="line">alias github-dir='/home/mxxmhh/github/'</span><br><span class="line">alias torch-dir='/home/mxxmhh/mxxhcm/code/pytorch'</span><br><span class="line">alias tf-dir='/home/mxxmhh/mxxhcm/code/tf'</span><br><span class="line">alias rl-dir='/home/mxxmhh/mxxhcm/code/rl'</span><br><span class="line">alias ops-dir='/home/mxxmhh/mxxhcm/code/tf/ops'</span><br><span class="line">alias paper-dir='/home/mxxmhh/mxxhcm/papers'</span><br><span class="line">alias tf-dir='/home/mxxmhh/mxxhcm/code/tf'</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 使用alias直接cd到常用文件夹</span><br><span class="line">alias post='cd /home/mxxmhh/mxxhcm/blog/source/_posts'</span><br><span class="line">alias code='cd /home/mxxmhh/mxxhcm/code/'</span><br><span class="line">alias matplotlib='cd /home/mxxmhh/mxxhcm/code/tools/matplotlib/'</span><br><span class="line">alias numpy='cd /home/mxxmhh/mxxhcm/code/tools/numpy/'</span><br><span class="line">alias shell='cd /home/mxxmhh/mxxhcm/code/shell'</span><br><span class="line">alias github='cd /home/mxxmhh/github/'</span><br><span class="line">alias torch='cd /home/mxxmhh/mxxhcm/code/pytorch'</span><br><span class="line">alias tf='cd /home/mxxmhh/mxxhcm/code/tf'</span><br><span class="line">alias rl='cd /home/mxxmhh/mxxhcm/code/rl'</span><br><span class="line">alias ops='cd /home/mxxmhh/mxxhcm/code/tf/ops'</span><br><span class="line">alias paper='cd /home/mxxmhh/mxxhcm/papers'</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> hexo博客相关命令</span><br><span class="line">alias update='hexo g -d'</span><br><span class="line">alias n='hexo n '</span><br><span class="line">alias new='hexo n '</span><br><span class="line"><span class="meta">#</span> git相关命令</span><br><span class="line">alias status='git status'</span><br><span class="line">alias add='git add .'</span><br><span class="line">alias remove='git rm'</span><br><span class="line">alias commit='git commit -m '</span><br><span class="line">alias branch='git branch'</span><br><span class="line">alias check='git checkout '</span><br><span class="line">alias push-master='git push origin master'</span><br><span class="line">alias pull-master='git pull origin master'</span><br><span class="line">alias push-hexo='git push origin hexo'</span><br><span class="line">alias pull-hexo='git pull origin hexo'</span><br><span class="line">alias utorrent='utserver -settingspath /opt/utorrent/'</span><br><span class="line"><span class="meta">#</span> shadowsocks local运行</span><br><span class="line">alias ssr='nohup sslocal -c /etc/shadowsocks_v6.json &lt;/dev/null &amp;&gt;&gt;~/.log/ss-local.log &amp; '</span><br><span class="line">alias ssr4='nohup sslocal -c /etc/shadowsocks_v4.json &lt;/dev/null &amp;&gt;&gt;~/.log/ss-local.log &amp; '</span><br><span class="line">alias ssr5='nohup sslocal -c /etc/shadowsocks_v6.json &lt;/dev/null &amp;&gt;&gt;~/.log/ss-local.log &amp; '</span><br><span class="line">alias ssr6='nohup sslocal -c /etc/shadowsocks_v6.json &lt;/dev/null &amp;&gt;&gt;~/.log/ss-local.log &amp; '</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 获得sslocal进程id</span><br><span class="line">function sspid()&#123;</span><br><span class="line">ps aux |grep 'sslocal'</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> kill sslocal进程</span><br><span class="line">function killss()&#123;</span><br><span class="line">pid=`ps -A | grep 'sslocal' |awk '&#123;print $1&#125;'`</span><br><span class="line">echo $pid</span><br><span class="line">kill -9 $pid</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 失效了</span><br><span class="line">function anaconda_on()&#123;</span><br><span class="line">export PATH=/home/mxxmhh/anaconda3/bin:$PATH</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 连接到我的vultr账户</span><br><span class="line">function vultr()&#123;</span><br><span class="line">ssh root@2001:19f0:7001:20f8:5400:01ff:fee6:aff6</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 定义函数实现hexo博客的add, commit, push整个流程</span><br><span class="line">function deploy-upload-hexo()&#123;</span><br><span class="line">git add .</span><br><span class="line">git commit -m "update blog"</span><br><span class="line">git push origin hexo</span><br><span class="line">hexo g -d</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 定义函数实现普通repo的add, commit, push整个流程</span><br><span class="line">function upload-master()&#123;</span><br><span class="line">git add .</span><br><span class="line">git commit -m "update code"</span><br><span class="line">git push origin master</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 获得当前folder的大小</span><br><span class="line">function folder-size()&#123;</span><br><span class="line">dir=`pwd`</span><br><span class="line">du -h --max-depth=1</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 更新环境变量</span><br><span class="line"><span class="meta">#</span> PATH</span><br><span class="line">export PATH=/home/mxxmhh/anaconda3/bin:$PATH</span><br><span class="line">export PATH=/usr/local/cuda/bin$&#123;PATH:+:$&#123;PATH&#125;&#125;</span><br><span class="line"><span class="meta">#</span> CUDA PATH</span><br><span class="line">export LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64/:$&#123;LD_LIBRARY_PATH:+:$&#123;LD_LIBRARY_PATH&#125;&#125;</span><br><span class="line">export CUDA_HOME=/usr/local/cuda</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> terminal走privoxy设置</span><br><span class="line">export https_proxy="http://127.0.0.1:8118"</span><br><span class="line">export http_proxy="http://127.0.0.1:8118"</span><br><span class="line">export HTTP_PROXY="http://127.0.0.1:8118"</span><br><span class="line">export HTTPS_PROXY="http://127.0.0.1:8118"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 关闭terminal走privoxy设置</span><br><span class="line">function proxyv6_off()&#123;</span><br><span class="line">unset https_proxy</span><br><span class="line">unset http_proxy</span><br><span class="line">unset HTTP_PROXY</span><br><span class="line">unset HTTPS_PROXY</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="bash-aliases">.bash_aliases</h2><p>在.bashrc文件中会发现下面几行：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">if [ -f ~/.bash_aliases ]; then</span><br><span class="line">    . ~/.bash_aliases</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><p>即.bashrc包含.bash_aliases文件。<br>.bash_aliases放置常用的命令别名。</p><h2 id="vim自定义配置">vim自定义配置</h2><p>关于vim详细介绍，可以查看<a href="https://mxxhcm.github.io/2019/05/07/linux-vim/">linux-vim</a><br>vimrc文件如下，<a href="https://github.com/mxxhcm/code/blob/master/shell/vimrc" target="_blank" rel="noopener">代码地址</a></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> 使用四个空格代替tab键</span><br><span class="line">set expandtab</span><br><span class="line">set tabstop=4</span><br><span class="line">set shiftwidth=4</span><br><span class="line">set softtabstop=4</span><br><span class="line"></span><br><span class="line">" 打开文件类型检测</span><br><span class="line">filetype on</span><br><span class="line">" 根据不同的文件类型加载插件</span><br><span class="line">filetype plugin on</span><br><span class="line">set ignorecase</span><br><span class="line"></span><br><span class="line">" 定义前缀键</span><br><span class="line">" let mappleader=";"</span><br><span class="line"></span><br><span class="line">" 设置ctrl+a全选，ctrl+c复制，;y粘贴</span><br><span class="line">vnoremap ; "+</span><br><span class="line">nnoremap ; "+</span><br><span class="line">nmap ;p "+p</span><br><span class="line">nnoremap &lt;C-C&gt; "+y</span><br><span class="line">vnoremap &lt;C-C&gt; "+y</span><br><span class="line">nnoremap &lt;C-A&gt; ggVG</span><br><span class="line">vnoremap &lt;C-A&gt; ggVG</span><br><span class="line"></span><br><span class="line">" 删除#号开头</span><br><span class="line">nnoremap ;d3 :g/^#.*$/d&lt;CR&gt;</span><br><span class="line">nnoremap ;d# :g/^#.*$/d&lt;CR&gt;</span><br><span class="line">" 删除空行</span><br><span class="line">nnoremap ;ds :g/^\s*$/d&lt;CR&gt;</span><br><span class="line">" 删除以tab开头的tab</span><br><span class="line">nnoremap ;rt :0,$s/^\t//g&lt;CR&gt;</span><br><span class="line">" 用\^代替^</span><br><span class="line">nnoremap ;r6 :0,$s/\^/\\^/g&lt;CR&gt;</span><br><span class="line">nnoremap ;r^ :0,$s/\^/\\^/g&lt;CR&gt;</span><br><span class="line">" 用\\\\代替\\</span><br><span class="line">nnoremap ;r/ :0,$s/\\\\/\\\\\\\\/g&lt;CR&gt;</span><br><span class="line">nnoremap ;r? :0,$s/\\\\/\\\\\\\\/g&lt;CR&gt;</span><br><span class="line"></span><br><span class="line">" 给选中行加注释</span><br><span class="line">" cnoremap &lt;C-#&gt; s/^/# /g&lt;CR&gt;</span><br><span class="line">nmap ;ic :s/^/# /g&lt;CR&gt;</span><br><span class="line">vmap ;ic :s/^/# /g&lt;CR&gt;</span><br><span class="line">nmap ;dc :s/^# //g&lt;CR&gt;</span><br><span class="line">vmap ;dc :s/^# //g&lt;CR&gt;</span><br><span class="line">"vmap &lt;C-#&gt; :s/^/#/g&lt;CR&gt;</span><br><span class="line">"nmap &lt;C-#&gt; :s/^/#/g&lt;CR&gt;</span><br><span class="line"></span><br><span class="line">""" 状态栏设置</span><br><span class="line">" 总是显示状态栏</span><br><span class="line">set laststatus=2</span><br><span class="line">" 状态信息</span><br><span class="line">set statusline=%f%m%r%h%w\ %=#%n\ [%&#123;&amp;fileformat&#125;:%&#123;(&amp;fenc==\"\"?&amp;enc:&amp;fenc).((exists(\"\+bomb\")\ &amp;&amp;\ &amp;bomb)?\"\+B\":\"\").\"\"&#125;:%&#123;strlen(&amp;ft)?&amp;ft:'**'&#125;]\ [%c,%l/%L]\ %p%%</span><br><span class="line"></span><br><span class="line">"""光标设置</span><br><span class="line">" 设置显示光标当前位置</span><br><span class="line">set ruler</span><br><span class="line"></span><br><span class="line">" 开启行号显示</span><br><span class="line">set number</span><br><span class="line">" 高亮显示当前行/列</span><br><span class="line">set cursorline</span><br><span class="line">" set cursorcolumn</span><br><span class="line">" 高亮显示搜索结果</span><br><span class="line">set hlsearch</span><br><span class="line">" 显示文件名</span><br><span class="line"></span><br><span class="line">" 开启语法高亮</span><br><span class="line">syntax enable</span><br><span class="line">" 允许用指定语法高亮配色方案替换默认方案</span><br><span class="line">syntax on</span><br><span class="line">" 将制表符扩展为空格</span><br><span class="line">" 设置编辑时制表符占用空格数</span><br><span class="line">" 设置格式化时制表符占用空格数</span><br><span class="line">" 让 vim 把连续数量的空格视为一个制表符</span><br><span class="line">set autoindent</span><br></pre></td></tr></table></figure><h2 id="参考文献">参考文献</h2><p>1.《鸟哥的LINUX私房菜》基础篇<br>2.<a href="https://github.com/yangyangwithgnu/use_vim_as_ide" target="_blank" rel="noopener">https://github.com/yangyangwithgnu/use_vim_as_ide</a><br>3.<a href="https://vi.stackexchange.com/questions/9028/what-is-the-command-for-select-all-in-vim-and-vsvim/9029" target="_blank" rel="noopener">https://vi.stackexchange.com/questions/9028/what-is-the-command-for-select-all-in-vim-and-vsvim/9029</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;bashrc自定义配置&quot;&gt;bashrc自定义配置&lt;/h2&gt;
&lt;p&gt;~/.bashrc文件详细内容，&lt;a href=&quot;https://github.com/mxxhcm/code/blob/master/shell/bashrc&quot; target=&quot;_blank&quot; r
      
    
    </summary>
    
      <category term="linux" scheme="http://mxxhcm.github.io/categories/linux/"/>
    
    
      <category term="linux" scheme="http://mxxhcm.github.io/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>linux stdin stdout和stderr</title>
    <link href="http://mxxhcm.github.io/2019/06/03/linux-stdin-stdout%E5%92%8Cstderr/"/>
    <id>http://mxxhcm.github.io/2019/06/03/linux-stdin-stdout和stderr/</id>
    <published>2019-06-02T16:09:27.000Z</published>
    <updated>2019-07-01T15:02:07.886Z</updated>
    
    <content type="html"><![CDATA[<h2 id="stdin-stdout-stderr">stdin, stdout, stderr</h2><p>standard input(file handle 0)：标准输入，process利用这个file handle从用户读取数据<br>standard output(file handle 1)：标准输出，process向这个file handle写normal infor，会将输出输出到屏幕<br>standard error(file handle 2)：标准错误，process向这个file hanle写error infor</p><h2 id="数据流重定向">数据流重定向</h2><blockquote><p>, &gt;&gt;, &lt;, &lt;&lt;</p></blockquote><h3 id="输出重定向">输出重定向</h3><blockquote><p>是将std output进行重定向，&gt;&gt;是将std output进行追加。如果要将std error重定向或者追加，需要使用2表示表示std error。<br>~$:ls -ld /etc/ &gt;~/etc_dir.txt</p></blockquote><h4 id="std-output-重定向">std output 重定向</h4><p>~$:find / -name ~/.bashrc &gt;find_result</p><h4 id="std-errort重定向">std errort重定向</h4><p>~$:find / -name ~/.bashrc &gt;fing_right 2&gt; find_error<br>~$:find / -name ~/.bashrc &gt; find_result 2&gt;/dev/null<br>~$:find / -name ~/.bashrc &gt;find_result 2&gt;&amp;1</p><h3 id="输入重定向">输入重定向</h3><h4 id="从键盘读入">&lt;从键盘读入</h4><p>~$:cat &gt; catfile<br>&gt;<br>&gt;<br>&gt; ctrl + D</p><h4 id="从文件中读入数据">从文件中读入数据</h4><p>cat &gt; catfile &lt; ~/.bashrc</p><h4 id="重定义输入结束符">重定义输入结束符</h4><p>cat &gt; catfile &lt;&lt; “eof”</p><h2 id="示例">示例</h2><h3 id="示例1">示例1</h3><p>~$:my_command &lt;inputfile 2&gt;errorfile | grep XYZ<br>执行my_command，<br>打开inputfile文件作为标准输入，<br>打开errorfile文件作为标准错误，<br>|重定向上述命令的结果到grep 命令</p><h3 id="示例2">示例2</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">echo "hello"</span><br><span class="line"><span class="meta">#</span> 是将"hello"给了echo的stdin的简写</span><br><span class="line">echo &lt; text.txt</span><br><span class="line"><span class="meta">#</span> 将text.txt给echo的stdin</span><br><span class="line">ps | grep 1000</span><br><span class="line"><span class="meta">#</span> |将ps的stdout给了grep的stdin</span><br><span class="line">ls . &gt; current_dir_list.txt</span><br><span class="line"><span class="meta">#</span> &gt;将ls .的stdout输出到相应文件</span><br><span class="line">ls. &gt;&gt; current_dir_list.txt</span><br><span class="line"><span class="meta">#</span> &gt;&gt;是追加，&gt;不是</span><br></pre></td></tr></table></figure><h2 id="2-1和">2&gt;&amp;1和&amp;&gt;</h2><p>2&gt;&amp;1是将stderr重定向到stdout，比如&quot;command &gt;/dev/null 2&gt;&amp;1&quot;是先将comand的stdout重定向到/dev/null中，然后将stderr重定向到stdout，因为stdout已经指向了/dev/null，所以stderr就重定向到了/dev/null。&quot;command &gt;out.txt 2&gt;&amp;1&quot;是先将stdout重定向到out.txt，然后将stderr重定向到stdout，也就是out.txt。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">command &gt; output_error.txt 2&gt;&amp;1，</span><br><span class="line">#可以将2&gt;&amp;1看成将stderr重定向到stdout，如果写成2&gt;1的话看起来像是将stderr重定向到一个名为$1$的文件。在redirece的上下文中，&amp;可以看成file descriptor的意思。为什么不写成&amp;2&gt;&amp;1，这会被解析成&amp; 和2&amp;1，第一个&amp;会被解析成后台运行，然后剩下的就是2&gt;&amp;1了。</span><br><span class="line"># 将command的stdout和stderr都输出到该文件</span><br><span class="line">command &amp;&gt; /dev/null</span><br><span class="line"># 将command的stderr和stdout输出到/dev/null，将会什么也不输出</span><br></pre></td></tr></table></figure><p>&amp;&gt;是bash的扩展，而2&gt;&amp;1是standard Bourne/POSIX shell。</p><h2 id="一个神奇的命令-dev-null">一个神奇的命令&lt;/dev/null &amp;&gt;</h2><p>/dev/null是一个神奇的文件，它代表null device，会抛弃所有写入它的数据，但是会report写操作成功了，并且它不会向读取它的任何process提供任何数据，也就是向读取它的程序发送一个EOF。<br>所以&lt;/dev/null会发送一个EOF到stdin，&amp;就是将程序放在后台运行，&amp;的详细介绍可见<a href>linux process章节</a><br>这时候如果再加一个重定向命令，就是将命令的输出重定向到某个文件中而不在stdout中显示。</p><h2 id="示例-v2">示例</h2><p>~$:nohup sslocal -c /etc/shadowsocks_v6.json &lt;/dev/null &amp;&gt;&gt;~/.log/ss-local.log &amp;  # 后台运行sslocal<br>~$:nohup /home/mxxmhh/anaconda3/bin/python <a href="http://main.py" target="_blank" rel="noopener">main.py</a> &gt;log_name 2&gt;&amp;1 &amp;</p><h2 id="参考文献">参考文献</h2><p>1.<a href="https://unix.stackexchange.com/questions/27955/the-usage-of-dev-null-in-the-command-line" target="_blank" rel="noopener">https://unix.stackexchange.com/questions/27955/the-usage-of-dev-null-in-the-command-line</a><br>2.<a href="https://stackoverflow.com/questions/3385201/confused-about-stdin-stdout-and-stderr" target="_blank" rel="noopener">https://stackoverflow.com/questions/3385201/confused-about-stdin-stdout-and-stderr</a><br>3.<a href="https://stackoverflow.com/questions/818255/in-the-shell-what-does-21-mean" target="_blank" rel="noopener">https://stackoverflow.com/questions/818255/in-the-shell-what-does-21-mean</a><br>4.<a href="https://askubuntu.com/questions/635065/what-is-the-differences-between-and-21" target="_blank" rel="noopener">https://askubuntu.com/questions/635065/what-is-the-differences-between-and-21</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;stdin-stdout-stderr&quot;&gt;stdin, stdout, stderr&lt;/h2&gt;
&lt;p&gt;standard input(file handle 0)：标准输入，process利用这个file handle从用户读取数据&lt;br&gt;
standard out
      
    
    </summary>
    
      <category term="linux" scheme="http://mxxhcm.github.io/categories/linux/"/>
    
    
      <category term="linux" scheme="http://mxxhcm.github.io/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>reinforcement learning an introduction 第6章笔记</title>
    <link href="http://mxxhcm.github.io/2019/06/02/reinforcement-learning-an-introduction-%E7%AC%AC6%E7%AB%A0%E7%AC%94%E8%AE%B0/"/>
    <id>http://mxxhcm.github.io/2019/06/02/reinforcement-learning-an-introduction-第6章笔记/</id>
    <published>2019-06-02T06:31:49.000Z</published>
    <updated>2019-10-07T02:53:38.308Z</updated>
    
    <content type="html"><![CDATA[<h2 id="td-learning">TD Learning</h2><p>TD方法是DP和MC方法的结合，像MC一样，TD可以不需要model直接从experience中学习，像DP一样，TD是bootstrap的方法。<br>本章的结构和之前一样，首先研究policy evaluation或者叫prediction问题，即给定一个policy $\pi$，估计$v_{\pi}$；然后研究control问题。DP,TD,MC方法都是使用GPI方法解control问题，不同点在于prediction问题的解法。<br>为什么叫TD？<br>因为TD更新是基于不同时间上两个estimate的估计进行的。</p><h2 id="td-prediction">TD prediction</h2><p>TD和MC都是利用采样获得的experience求解prediction问题。给定policy $\pi$下的一个experience，TD和MC方法使用该experience中出现的non-terminal state $S_t$估计$v_{\pi}$的$V$。他们的不同之处在于MC需要等到整个experience的return知道以后，把这个return当做$V(S_t)$的target，every visit MC方法的更新规则如下：<br>$$V(S_t) = V(S_t) + \alpha \left[G_t - V(S_t)\right]\tag{1}$$<br>其中$G_t$是从时刻$t$到这个episode结束的return，$\alpha$是一个常数的步长，这个方法叫做$constant-\alpha$ MC。MC方法必须等到一个episode结束，才能进行更新，因为只有这个时候$G_t$才知道。为了更方便的训练，就有了TD方法。TD方法做的改进是使用$t+1$时刻state $V(S_{t+1})$的估计值和reward $R_{t+1}$的和作为target：<br>$$V(S_t) = V(S_t) + \alpha \left[R_{t+1}+\gamma V(S_{t+1}) - V(S_t)\right]\tag{2}$$<br>如果V在变的话，是不是应该是下面的公式？？<br>$$V_{t+1}(S_t) = V_t(S_t) + \alpha \left[R_{t+1}+\gamma V_t(S_{t+1}) - V_t(S_t)\right]$$<br>即只要有了到$S_{t+1}$的transition并且接收到了reward $R_{t+1}$就可以进行上述更新。MC方法的target是$G_t$，而TD方法的target是$\gamma V(S_{t+1} + R_{t+1})$，这种TD方法叫做$TD-0$或者$one\ step\ TD$，它是$TD(\lambda)$和$n-step\ TD$的一种特殊情况。</p><h3 id="算法">算法</h3><p>下面是$TD(0)$的完整算法：<br>算法1 Tabular TD(0) for $V$<br>输入： 待评估的policy $\pi$<br>算法参数：步长$\alpha \in (0,1]$<br>初始化： $V(s), \forall s\in S^{+}，V(terminal) = 0$<br><strong>Loop</strong> for each episode<br>$\qquad$初始化$S$<br>$\qquad A\leftarrow \pi(a|S)$<br>$\qquad$<strong>Loop</strong> for each step of episode<br>$\qquad\qquad$执行action $A$，得到$S’$和$R$<br>$\qquad\qquad V(S) = V(S) + \alpha \left[R + \gamma V(S’) - V(S)\right]$<br>$\qquad\qquad$$S\leftarrow S’$<br>$\qquad$<strong>Until</strong> $S$ 是terminal state</p><p>$TD(0)$是bootstrap方法，因为它基于其他state的估计value进行更新。从第三章中我们知道：<br>\begin{align*}<br>v_{\pi}(s) &amp; = \mathbb{E}_{\pi}\left[G_t\right]\tag{3}\\<br>&amp; = \mathbb{E}_{\pi}\left[R_{t+1}+\gamma G_{t+1}| S_t = s\right]\tag{4}\\<br>&amp; = \mathbb{E}_{\pi}\left[R_{t+1}+\gamma v_{\pi}(S_{t+1})|S_t = s\right]\tag{5}\\<br>\end{align*}<br>MC使用式子$(3)$的estimate作为target，而DP使用式子$(5)$的estimate作为target。MC方法用一个sample的return代替式子$(3)$中真实的未知expected return $G_t$；DP是用$V(S_{t+1})$作为$v_{\pi}(S_{t+1})$的一个估计，因为$v_{\pi}(S_{t+1})$的真实值是不知道的。TD结合了MC的采样以及DP的bootstrap，它对式子$(4)$的tranisition进行sample，同时使用$v_{\pi}$的估计值$V$进行计算。<br><img src="/2019/06/02/reinforcement-learning-an-introduction-第6章笔记/backup_td.png" alt="backup_TD"><br>TD的backup图如图所示。TD和MC updates被称为sample updates，因为这两个算法的更新都牵涉到采样一个sample successor state，使用这个state的value和它后继的这条路上的reward计算一个backed-up value，然后根据这个值更新该state的value。sample updates和DP之类的expected updates的不同在于，sample updates使用一个sample successor进行更新，expected updates使用所有可能的successors distribution进行更新。<br>$R_{t+1} + \gamma V(S_{t+1}) - V(S_t)$可以看成一种error，衡量了$S_t$当前的estimated value $V(S_t)$和一个更好的estimated value之间的差异$R_{t+1} +\gamma V(S_{t+1})$，我们把它叫做$TD-error$，用$\delta_t$表示。$\delta_t$是$t$时刻的$TD-error$，在$t+1$时刻可用，用公式表示是：<br>$$\delta_t = R_{t+1} + \gamma V(S_{t+1}) - V(S_t) \tag{6}$$<br>如果$V$在一个episode中改变的话，那么上述公式是不是应该写成：<br>$$\delta_t = R_{t+1} + \gamma V_t(S_{t+1}) - V_t(S_t)$$<br>应该在$t$时刻，计算的TD error是用来更新$t+1$时刻的value的。如果$V$在一个episdoe中不变的话，就像MC方法一样，那么MC error可以写成TD errors的和。<br>\begin{align*}<br>G_t - V(S_t) &amp; = R_{t+1} + \gamma G_{t+1} - V(S_t) + \gamma V(S_{t+1}) - \gamma V(S_{t+1})\\<br>&amp; = R_{t+1} + \gamma V(S_{t+1}) - V(S_t) + \gamma G_{t+1} - \gamma V(S_{t+1})\\<br>&amp; = \delta_t + \gamma G_{t+1} - \gamma V(S_{t+1})\\<br>&amp; = \delta_t + \gamma(G_{t+1} - V(S_{t+1}))\\<br>&amp; = \delta_t + \gamma\delta_{t+1} + \gamma^2(G_{t+2} - V(S_{t+2}))\\<br>&amp; = \delta_t + \gamma\delta_{t+1} + \gamma^2\delta_{t+2} + \cdots + \gamma^{T-t-1}\delta_{T-1} + \gamma^{T-t}(G_T-V(S_T))\\<br>&amp; = \delta_t + \gamma\delta_{t+1} + \gamma^2\delta_{t+2} + \cdots + \gamma^{T-t-1}\delta_{T-1} + \gamma^{T-t}(0-0)\\<br>&amp; = \sum_{k=t}^{T-1} \gamma^{k-t}\delta_k \tag{7}\\<br>\end{align*}<br>如果$V$在一个episode中改变了的话，像$TD(0)$一样，这个公式就不精确成立了，如果$\alpha$足够小的话，还是近似成立的。<br>$$\delta_t = R_{t+1} + \gamma V_t(S_{t+1}) - V_t(S_t)$$<br>\begin{align*}<br>V_{t+1}(S_t) &amp;= V_t(S_t) + \alpha \left[R_{t+1}+\gamma V_t(S_{t+1}) - V_t(S_t)\right]\\<br>&amp;= V_t(S_t) + \alpha \delta_t<br>\end{align*}</p><p>\begin{align*}<br>G_t - V_t(S_t) &amp; = R_{t+1} + \gamma G_{t+1} - V_t(S_t) + \gamma V_{t+1}(S_{t}) - \gamma V_{t+1}(S_{t})\\<br>&amp; = R_{t+1} + \gamma V_{t+1}(S_{t}) - V_t(S_t) + \gamma G_{t+1}- \gamma V_{t+1}(S_{t})\\<br>&amp; = R_{t+1} + \gamma (V_t(S_t) + \alpha \delta_t) - V_t(S_t) + \gamma G_{t+1}- \gamma V_{t+1}(S_{t})\\<br>&amp; = R_{t+1} + \gamma V_t(S_t) - V_t(S_t) + \gamma \alpha \delta_t + \gamma G_{t+1}- \gamma V_{t+1}(S_{t})\\<br>\end{align*}<br>然而上面是错误的，因为$\delta_t$需要的是$V_t(S_{t+1})$<br>\begin{align*}<br>G_t - V_t(S_t) &amp; = R_{t+1} + \gamma G_{t+1} - V_t(S_t) + \gamma V_{t+1}(S_{t+1}) - \gamma V_{t+1}(S_{t+1})\\<br>&amp; = R_{t+1} + \gamma V_{t+1}(S_{t+1}) - V_t(S_t) + \gamma G_{t+1}- \gamma V_{t+1}(S_{t+1})\\<br>\end{align*}<br>\begin{align*}<br>\delta_t &amp;= R_{t+1} + \gamma V_t(S_{t+1}) - V_t(S_t)\\<br>\delta_{t+1} &amp;= R_{t+2} + \gamma V_{t+1}(S_{t+2}) - V_{t+1}(S_{t+1})\\<br>\delta_{t+2} &amp;= R_{t+3} + \gamma V_{t+2}(S_{t+3}) - V_{t+2}(S_{t+2})\\<br>\delta_{t+3} &amp;= R_{t+4} + \gamma V_{t+3}(S_{t+4}) - V_{t+3}(S_{t+3})\\<br>\end{align*}</p><p>\begin{align*}<br>&amp;\delta_t+\delta_{t+1}+\delta_{t+2}+\delta_{t+3}\\<br>= &amp;R_{t+1} + \gamma V_t(S_{t+1}) - V_t(S_t)\\<br>+&amp;R_{t+2} + \gamma V_{t+1}(S_{t+2}) - V_{t+1}(S_{t+1})\\<br>+&amp;R_{t+3} + \gamma V_{t+2}(S_{t+3}) - V_{t+2}(S_{t+2})\\<br>+&amp;R_{t+4} + \gamma V_{t+3}(S_{t+4}) - V_{t+3}(S_{t+3})\\<br>\end{align*}<br>OK。。。还是没有算出来。。</p><h3 id="td例子">TD例子</h3><p>TD的一个例子。每天下班的时候，你会估计需要多久能到家。你回家的事件和星期，天气等相关。在周五的晚上6点，下班之后，你估计需要30分钟到家。到车旁边是$6:05$，而且天快下雨了。下雨的时候会有些堵车，所以估计从现在开始大概还需要$35$分钟才能到家。十五分钟后，下了高速，这个时候你估计总共的时间是$35$分钟（包括到达车里的$5$分钟）。然后就遇到了堵车，真正到达家里的街道是$6:40$，三分钟后到家了。</p><table><thead><tr><th style="text-align:center">State</th><th style="text-align:center">Elapsed Time</th><th style="text-align:center">Predicted Time to Go</th><th style="text-align:center">Predicted Total Time</th></tr></thead><tbody><tr><td style="text-align:center">leaveing office</td><td style="text-align:center">0</td><td style="text-align:center">30</td><td style="text-align:center">30</td></tr><tr><td style="text-align:center">reach car</td><td style="text-align:center">5</td><td style="text-align:center">35</td><td style="text-align:center">40</td></tr><tr><td style="text-align:center">下高速</td><td style="text-align:center">20</td><td style="text-align:center">15</td><td style="text-align:center">35</td></tr><tr><td style="text-align:center">堵车</td><td style="text-align:center">30</td><td style="text-align:center">10</td><td style="text-align:center">40</td></tr><tr><td style="text-align:center">门口的街道</td><td style="text-align:center">40</td><td style="text-align:center">3</td><td style="text-align:center">43</td></tr><tr><td style="text-align:center">到家</td><td style="text-align:center">43</td><td style="text-align:center">0</td><td style="text-align:center">43</td></tr></tbody></table><p>rewards是每一个journey leg的elapsed times，这里我们研究的是evaluation问题，所以可以直接使用elapsed time，如果是control问题，要在elapsed times前加负号。state value是expected time。上面的第一列数值是reward，第二列是当前state的value估计值。<br>如果使用$\alpha = 1$的TD和MC方法。对于MC方法，对于$S_t$的所有state，都有：<br>\begin{align*}<br>V(S_t) &amp;= V(S_t) + (G_t - V(S_t))\\<br>&amp; = G_t \\<br>&amp; = 43<br>\end{align*}<br>对于TD方法，让$\gamma=1$，有：<br>\begin{align*}<br>V(S_t) &amp;= V_t(S_t) + \alpha (R_{t+1} +  \gamma V_t(S_{t+1}) - V(S_t))\\<br>&amp;= R_{t+1} + V_t(S_{t+1})<br>\end{align*}</p><h3 id="td-prediction的好处">TD Prediction的好处</h3><p>TD是bootstrap方法，相对于MC和DP来说，TD的好处有以下几个：</p><ol><li>相对于DP，不需要environment, reward model以及next-state probability distribution。</li><li>相对于MC，TD是online，incremental的。MC需要等到一个episode结束，而TD只需要等一个时间步（本节介绍的TD0）。</li><li>TD在table-base case可以为证明收敛，而general linear function不一定收敛。</li></ol><p>但是具体TD好还是MC好，目前还没有明确的数学上的理论证明。而实践上表明，TD往往要比constant $\alpha$ MC算法收敛的快。</p><h2 id="td-0-的优势">TD(0)的优势</h2><p>如果我们只有很少的experience的话，比如有$10$个episodes，或者有$100$个timesteps。这种情况下，我们会重复的使用这些这些experience进行训练直到算法收敛。具体方法是，给定一个approximate value function $V$，在每一个只要不是terminal state的时间$t$处，计算MC和TD增量，最后使用所有增量之和只更新value function一次。举个例子好了，假如我们有三个episdoes，<br>A,B,C<br>B,A<br>A,A<br>更新的方法是，$V(A) = V(A) + \alpha(G_1 - V(A) + G_2 - V(A) + G_{31} - V(A) + G_{32} -V(A))$<br>这种方法叫做batch updating，因为只有在一个batch完全处理完之后才进行更新，其实这个和DP挺像的，只不过DP直接利用的是environment dynamic，而我们使用的是样本。<br>在batch updating中，TD(0)一定会收敛到一个与$\alpha$无关的结果，只要$\alpha$足够下即可，同理batch constant $\alpha$ MC算法同样条件下也会收敛到一个确定的结果，只不过和batch TD结果不同而已。Normal updating的方法并没有朝着整个batch increments的方法移动，但是大概方向差不多。其实就是一个把整个batch的所有experience的increment加起来一起更新，一个是每一个experience更新一次，就这么点区别。<br>具体来说，batch TD和batch MC哪个更好一些呢？这就牵扯到他们的原理了。Batch MC的目标是最小化training set上的mse，而batch TD的目标是寻找Markov process的最大似然估计。一般来说，maximum likeliood estimate是进行参数估计的。在这里的话，TD使用mle从已有episodes中生成markov process模型的参数：从$i$到$j$的transition probatility是观测到从$i$到$j$的transition所占的百分比，对应的expected reward是观测到的rewards的均值。给出了这个model之后，如果这个模型是exactly correct的话，那么我们就可以准确的计算出value function的estimate，这个成为certainty-equivalence estimate，因为它相当于假设markov process的model是一致的，而不是approximated，一般来说，batch TD(0)收敛到cetainty-equivalence estimate。<br>从而，我们可以简单的解释以下为什么batch TD比batch MC收敛的快。因为batch TD计算的是真实的cetainty-equivalence estimate。同样的，对于non batch的TD和MC来说，虽然TD没有使用cetainty-equivalence，但是它们大概在向那个方向移动。<br>尽管cetrinty-equivalence是最优解，但是，但是，但是，cost太大了，如果有$n$个states，计算mle需要$n^2$的空间，计算value function时候，需要$n^3$的计算步数。当states太多的话，实际上并不可行，还是老老实实的使用TD把，只会用不超过$n$的空间。。</p><h2 id="td具体算法介绍">TD具体算法介绍</h2><h3 id="sarsa">Sarsa</h3><h4 id="介绍">介绍</h4><p>Sarsa是一个on-policy的 TD control算法。按照GPI的思路来，先进行policy evaluation，在进行policy improvement。首先解prediction问题，按照以下action value的$TD(0)$公式估计当前policy $\pi$下，所有action和state的$q$值$q_{\pi}(s,a)$：<br>$$Q(S_t,A_T) \leftarrow Q(S_t,A_t) + \alpha \left[R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) -Q(S_t,A_t)\right] \tag{8}$$<br>当$S_{t+1} = 0$时，$Q(S_{t+1}, A_{t+1})=0$，相应的backup diagram如下图所示。<br><img src="/2019/06/02/reinforcement-learning-an-introduction-第6章笔记/ff.png" alt="f"><br>第二步解control问题，在on-policy的算法中，不断的估计behaviour policy $\pi$的$q_{\pi}$，同时改变$\pi$朝着$q_{\pi}$更大的方向移动。Sarsa算法中，behaviour policy和target policy是一样的，在不断的改变。完整的算法如下：<br><em><em>Sarsa算法(on-policy control) 估计$Q\approx q_</em>$</em>*<br>对于所有$s\in S^{+}, a\in A(s)$，随机初始化$Q(s,a)$，$Q(terminal, \cdot) = 0$<br>Loop for each episode<br>$\qquad$ 获得初始状态$S$<br>$\qquad$ 使用policy（如$\epsilon$-greedy算法）根据state $S$选择当前动作$A$<br>$\qquad$ Loop for each step of episode<br>$\qquad\qquad$ 采取action，得到R和S’<br>$\qquad\qquad$ 使用policy（和上面的policy一样）根据S’选择A’<br>$\qquad\qquad Q(S,A) \leftarrow Q(S,A) + \alpha \left[R+ \gamma Q(S’,A’) - Q(S,A)\right]$<br>$\qquad\qquad S\leftarrow S’, A\leftarrow A’$<br>$\qquad$ until $S$是terminal</p><h4 id="示例">示例</h4><h3 id="q-learning">Q-learning</h3><p>$$Q(S_t,A_T) \leftarrow Q(S_t,A_t) + \alpha \left[R_{t+1} + \gamma max Q(S_{t+1}, A_{t+1}) -Q(S_t,A_t)\right]\tag{9}$$<br>这一节介绍的是off-policy的TD contrl算法，Q-learning。对于off-policy算法来说，behaviour policy用来选择action，target policy是要评估的算法。在Q-learning算法中，直接学习的就是target policy的optimal action value function $q_{*}$，和behaviour policy无关。完整的Q-learning算法如下：<br><strong>Q-learning算法(off-policy control) 估计$\pi \approx \pi_{*}$</strong><br>对于所有$s\in S^{+}, a\in A(s)$，随机初始化$Q(s,a)$，$Q(terminal, \cdot) = 0$<br>Loop for each episode<br>$\qquad$ 获得初始状态$S$<br>$\qquad$ Loop for each step of episode<br>$\qquad\qquad$ 使用behaviour policy（如$\epsilon$-greedy算法）根据state $S$选择当前动作$A$<br>$\qquad\qquad$ 执行action $A$，得到$R$和$S’$<br>$\qquad\qquad Q(S,A) \leftarrow Q(S,A) + \alpha \left[R+ \gamma max Q(S’,A’) - Q(S,A)\right]$<br>$\qquad\qquad S\leftarrow S’$<br>$\qquad$ until $S$是terminal</p><h3 id="expected-sarsa">Expected Sarsa</h3><p>Q-learning对所有next state-action pairs取了max操作。如果不是取max，而是取期望呢？<br>\begin{align*}<br>Q(S_t,A_T) &amp; \leftarrow Q(S_t,A_t) + \alpha \left[R_{t+1} + \gamma \mathbb{E}_{\pi}\left[ Q(S_{t+1}, A_{t+1})| S_{t+1} \right] -Q(S_t,A_t)\right]\\<br>&amp;\leftarrow Q(S_t,A_t) + \alpha \left[R_{t+1} + \gamma \sum_a\pi(a|S_{t+1})Q -Q(S_t,A_t)\right]\tag{10}<br>\end{align*}<br>其他的和Q-learning保持一致。给定next state $S_{t+1}$，算法在expectation上和sarsa移动的方向一样，所以被称为expected sarsa。这个算法可以是on-policy，但是通常它是是off-policy的。比如，on-policy的话，policy使用$\epsilon$ greedy算法，off-policy的话，behaviour policy使用stochastic policy，而target policy使用greedy算法，这其实就是Q-learning算法了。所以，Expected Sarsa实际上是对Q-learning的一个归纳，同时又有对Sarsa的改进。<br>Q-learning和Expected Sarsa的backup diagram如下所示：<br><img src="/2019/06/02/reinforcement-learning-an-introduction-第6章笔记/q_learning_and_expected_Sarsa_backup_diagram.png" alt="q_learning_and_expected_Sarsa_backup_diagram"></p><h3 id="sarsa-vs-q-learning-vs-expected-sarsa">Sarsa vs Q-learning vs Expected Sarsa</h3><p>Sarsa是on-policy的，behaviour policy和target policy一直在变（$\epsilon$在变），但是behaviour policy和target policy一直都是一样的。<br>Q-learning是off-policy的，target policy和behaviour policy一直都不变（可能$\epsilon$会变，但是这个不是Q-learning的重点），behaviour policy保证exploration，target policy是greedy算法。<br>Q-learning是off-policy算法，那么又为什么one-step Q-learning不需要importance sampling？Importance sampling的作用是为了使用policy $b$下观察到的rewards估计policy $\pi$下的expected rewards。尽管在Q-learning中，behaviour policy和target policy不同，behaviour policy仅仅用来采样$s_t, a_t, R_{t+1}$，在更新$Q$值时，使用target policy(epsilon policy)生成的实际上是$a’ = \max_a Q(s_{t+1}, a)$。Target policy和behaviour policy不同的实际上是$a_t$，但是在更新$Q$值时，用的也是$a_t$。也就是说使用behaviour policy选择的是$a_t$，接下来使用target policy选择执行$a_t$后的新action $a_{t+1}$。<br>Q(0)和Expected Sarsa(0)都没有使用importance sampling，因为在$Q(s,a)$中，action $a$已经被选择了，用哪个policy选择的是无关紧要的，TD error可以使用$Q(s’,*)$上的boostrap进行计算，而不需要behaviour policy。</p><h2 id="maximization-bias和double-learning">Maximization Bias和Double Learning</h2><p>目前介绍的所有control算法，都涉及到target polices的maximization操作。Q-learning中有greedy target policy，Sarsa的policy通常是$\epsilon$ greedy，也会牵扯到maximization。Max操作会引入一个问题，加入某一个state，它的许多action对应的$q(s,a)=0$，然后它的估计值$Q(s,a)$是不确定的，可能比$0$大，可能比$0$小，还可能就是$0$。如果使用max $Q(s,a)$的话，得到的值一定是大于等于$0$的，显然有一个positive bias，这就叫做maximization bias。</p><h3 id="maximization-bias例子">Maximization Bias例子</h3><p>给出如下的一个例子：<br><img src="/2019/06/02/reinforcement-learning-an-introduction-第6章笔记/example_6_7.png" alt="example_6_7"><br>这个MDP有四个state，A,B,C,D，C和D是terminal state，A总是start state，并且有left和right两个action，right action转换到C，reward是0,left action转换到B，reward是$0$，B有很多个actions，都是转换到$D$，但是rewards是不同，reward服从一个均值为$-0.5$，方差为$1.0$的正态分布。所以reward的期望是负的，$-0.5$。这就意味着在大量实验中，reward的均值往往是小于$0$的。<br>基于这个假设，在A处总是选择left action是很蠢的，但是因为其中有一些reward是positive，如果使用max操作的话，整个policy会倾向于选择left action，这就造成了在一些episodes中，reward是正的，但是如果在long run中，reward的期望就是负的。</p><h3 id="maximizaiton-bias出现的直观解释">Maximizaiton Bias出现的直观解释</h3><p>那么为什么会出现这种问题呢？<br>用$X1$和$X2$表示reward的两组样本数据。如下所示：<br><img src="/2019/06/02/reinforcement-learning-an-introduction-第6章笔记/maximization_bias.png" alt="maximization_bias"><br>在$X1$这组样本中，样本均值是$-0.43$，X2样本均值是$-0.36$。在增量式计算样本均值$\mu$时，得到的最大样本均值的期望是$0.09$，而实际上计算出来的期望的最大值$\mathbb{E}(X)$是$-0.36$。要使用$\mathbb{E} \left[max\ (\mu)\right]$估计$max\ \mathbb{E}(X)$，显然它们的差距有点大，$max(\mu)$是$max E(X)$的有偏估计。也就是说使用$max Q(s’,a’)$更新$Q(s,a)$时，$Q(s,a)$并没有朝着它的期望$-0.5$移动。估计这只是一个直观的解释，严格的证明可以从论文中找。</p><h3 id="如何解决maximization-bias问题">如何解决Maximization Bias问题</h3><p>那么怎么解决这个问题呢，就是同时学习两个$Q$函数$Q_1, Q_2$，这两个$Q$函数的地位是一样的，每次随机选择一个选择action，然后更新另一个。证明的话，Van Hasselt证明了$\mathbb{E}(Q_2(s’,a*)\le max\ Q_1(s’,a*)$，也就是说$Q_1(s,a)$不再使用它自己的max value进行更新了。<br>下面是$Q$-learning和Double $Q$-learning在训练过程中在A处选择left的统计：<br><img src="/2019/06/02/reinforcement-learning-an-introduction-第6章笔记/q_learning_vs_double_q_learning.png" alt="q_learning_vs_double_q_learning"><br>可以看出来，Double $Q$-learning要比$Q$-learning收敛的快和好。<br>当然，Sarsa和Expected Sarsa也有maximization bias问题，然后有对应的double版本，Double Sarsa和Double Expected Sarsa。</p><h2 id="afterstates">Afterstates</h2><p>之前介绍了state value function和action value function。这里介绍一个afterstate value function，afterstate value function就是在某个state采取了某个action之后再进行评估，一开始我想这步就是action value function。事实上不是的，action value function估计的是$Q(s,a)$，重点是state和action这些pair，对于afterstate value来说，可能有很多个state和action都能到同一个next state，这个时候它们的作用是一样的，因为我们估计的是next state的value。<br>象棋就是一个这样的例子。。这里只是介绍一下，还有很多各种各样特殊的形式，它们可以用来解决各种各样的特殊问题。具体可以自己多了解一下。</p><h2 id="总结">总结</h2><p>这一章主要介绍了最简单的一种TD方法，one-step，tabular以及model-free。接下来的两章会介绍一些n-step的TD方法，可以和MC方法联系起来，以及包含一个模型的方法，和DP联系起来。在第二部分的时候，会将tabular的TD扩展到function approximation的形式，和deep learning以及artificial neural networks联系起来。</p><h2 id="参考文献">参考文献</h2><p>1.《reinforcement learning an introduction》第二版<br>2.<a href="https://stats.stackexchange.com/a/297892" target="_blank" rel="noopener">https://stats.stackexchange.com/a/297892</a><br>3.<a href="https://towardsdatascience.com/double-q-learning-the-easy-way-a924c4085ec3" target="_blank" rel="noopener">https://towardsdatascience.com/double-q-learning-the-easy-way-a924c4085ec3</a><br>4.<a href="https://stats.stackexchange.com/a/347090/254953" target="_blank" rel="noopener">https://stats.stackexchange.com/a/347090/254953</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;td-learning&quot;&gt;TD Learning&lt;/h2&gt;
&lt;p&gt;TD方法是DP和MC方法的结合，像MC一样，TD可以不需要model直接从experience中学习，像DP一样，TD是bootstrap的方法。&lt;br&gt;
本章的结构和之前一样，首先研究policy
      
    
    </summary>
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>reinforcment learning terms</title>
    <link href="http://mxxhcm.github.io/2019/05/29/reinforcement-learning-terms/"/>
    <id>http://mxxhcm.github.io/2019/05/29/reinforcement-learning-terms/</id>
    <published>2019-05-29T09:59:28.000Z</published>
    <updated>2019-09-28T03:55:24.123Z</updated>
    
    <content type="html"><![CDATA[<h2 id="术语定义">术语定义</h2><p>更多介绍可以点击查看<a href="https://mxxhcm.github.io/2018/12/21/reinforcement-learning-an-introduction-%E7%AC%AC3%E7%AB%A0%E7%AC%94%E8%AE%B0/">reinforcement learning an introduction 第三章</a></p><h3 id="状态集合">状态集合</h3><p>$\mathcal{S}$是有限states set，包含所有state的可能取值</p><h3 id="动作集合">动作集合</h3><p>$\mathcal{A}$是有限actions set，包含所有action的可能取值</p><h3 id="转换概率矩阵或者状态转换函数">转换概率矩阵或者状态转换函数</h3><p>$P:\mathcal{S}\times \mathcal{A}\times \mathcal{S} \rightarrow \mathbb{R}$是transition probability distribution，或者写成$p(s_{t+1}|s_t,a_t)$</p><h3 id="奖励函数">奖励函数</h3><p>$R:\mathcal{S}\times \mathcal{A}\rightarrow \mathbb{R}$是reward function</p><h3 id="折扣因子">折扣因子</h3><p>$\gamma \in (0, 1)$</p><h3 id="初始状态分布">初始状态分布</h3><p>$\rho_0$是初始状态$s_0$服从的distribution，$s_0\sim \rho_0$</p><h3 id="带折扣因子的mdp">带折扣因子的MDP</h3><p>定义为tuple $\left(\mathcal{S},\mathcal{A},P,R,\rho_0, \gamma\right)$</p><h3 id="随机策略">随机策略</h3><p>选择action，stochastic policy表示为：$\pi_\theta: \mathcal{S}\rightarrow P(\mathcal{A})$，其中$P(\mathcal{A})$是选择$\mathcal{A}$中每个action的概率，$\theta$表示policy的参数，$\pi_\theta(a_t|s_t)$是在$s_t$处取action $a_t$的概率</p><h3 id="accumulated-reward">Accumulated Reward</h3><h4 id="期望折扣回报">期望折扣回报</h4><p>定义<br>$$G_t =\mathbb{E} \left[\sum_{k=t}^{\infty} \gamma^{k-t} R_{k+1}\right] \tag{1}$$<br>为expected discounted returns，表示从$t$时刻开始的expected discounted return；</p><h4 id="状态值函数">状态值函数</h4><p>state value function的定义是从$t$时刻的$s_t$开始的累计期望折扣奖励：<br>$$V^{\pi} (s_t) = \mathbb{E}_{a_{t}, s_{t+1},\cdots\sim \pi}\left[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \right] \tag{2}$$<br>或者有时候也定义成从$t=0$开始的expected return：<br>$$V^{\pi} (s_0) = \mathbb{E}_{\pi}\left[G_0|S_0=s_0;\pi\right]=\mathbb{E}_{\pi}\left[\sum_{t=0}^{\infty} \gamma^t R_{t+1}|S_0=s_0;\pi \right] \tag{3}$$</p><h4 id="动作值函数">动作值函数</h4><p>action value function定义为从$t$时刻的$s_t, a_t$开始的累计期望折扣奖励：<br>$$Q^{\pi} (s_t, a_t) = \mathbb{E}_{s_{t+1}, a_{t+1},\cdots\sim\pi}\left[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \right] \tag{4}$$<br>或者有时候也定义为从$t=0$开始的return的期望：<br>$$Q^{\pi} (s_0, a_0) = \mathbb{E}_{\pi}\left[G_0|S_0=s_0,A_0=a_0;\pi\right]=\mathbb{E}_{\pi}\left[\sum_{t=0}^{\infty} \gamma^t R_{t+1}|S_0=s_0,A_0=a_0;\pi \right] \tag{5}$$</p><h4 id="优势函数">优势函数</h4><p>$$A^{\pi} (s,a) = Q^{\pi}(s,a) -V^{\pi} (s) \tag{6}$$<br>其中$a_t\sim \pi(a_t|s_t), s_{t+1}\sim P(s_{t+1}|s_t, a_t)$。$V^{\pi} (s)$可以看成状态$s$下所有$Q(s,a)$的期望，而$A^{\pi} (s,a)$可以看成当前的单个$Q(s,a)$是否要比$Q(s,a)$的期望要好，如果为正，说明这个$Q$比$Q$的期望要好，否则就不好。<br>优势函数的期望是$0$：<br>$$\mathbb{E}_{\pi}\left[A^{\pi}(s,a)\right] = \mathbb{E}_{\pi}\left[Q^{\pi}(s,a) - V^{\pi}(s)\right] = \mathbb{E}_{\pi}\left[Q^{\pi}(s,a)\right] -  \mathbb{E}_{\pi}\left[V^{\pi}(s)\right] = V^{\pi}(s) - V^{\pi}(s) = 0$$</p><h4 id="目标函数">目标函数</h4><p>Agents的目标是找到一个policy，最大化从state $s_0$开始的expected return：$J(\pi)=\mathbb{E}_{\pi} \left[G_0|\pi\right]$，或者写成：<br>$$\eta(\pi)= \mathbb{E}_{s_0, a_0, \cdots\sim \pi}\left[\sum_{t=0}^{\infty} \gamma^t R_{t+1}\right] \tag{7}$$<br>表示$t=0$时policy $\pi$的expected discounted return，其中$s_0\sim\rho_0(s_0)$, $a_t\sim\pi(a_t|s_t)$, $s_{t+1}\sim P(s_{t+1}|s_t,a_t)$。</p><h4 id="station-distribution">station distribution</h4><p>用$p(s_0\rightarrow s,t,\pi)$表示从$s_0$经过$t$个timesteps到$s$的概率，则<a href="https://zhuanlan.zhihu.com/p/60257706" target="_blank" rel="noopener">policy $\pi$下$s$服从的概率分布为</a>：<br>$$\rho^{\pi} (s) = \int_S \sum_{t=0}^{\infty} \gamma^{t} \rho_0(s_0)p(s_0\rightarrow s, t,\pi)ds_0 \tag{8}$$<br>其中$\rho_0(s_0)$是初始状态$s_0$服从的概率分布，当指定$s_0$时，可以看成$\rho_0(s_0) = 1$。$\rho^{\pi} (s)$可以理解为：<br>$$\rho^{\pi} (s) = P(s_0 = s) +\gamma P(s_1=s) + \gamma^2 P(s_2 = s)+\cdots \tag{9}$$<br>表示policy $\pi$下state $s$出现的概率。在每一个timestep $t$处，$s_t=s$都有一定概率发生的，也就是式子$9$。</p><h3 id="average-reward">Average Reward</h3><h4 id="目标函数-v2">目标函数</h4><p>定义average reward $\eta$为在state distribution $\rho^\pi $和policy $\pi_\theta$上的期望：<br>\begin{align*}<br>\eta(\pi) &amp;= \int_S \rho^{\pi} (s) \int_A \pi(s,a) R^{\pi}(s,a)dads\\<br>&amp;= \mathbb{E}_{s\sim \rho^{\pi} , a\sim \pi}\left[R^{\pi}(s,a)\right] \tag{10}\\<br>\end{align*}<br>其中$R(s,a) = \mathbb{E}\left[ r_{t+1}|s_t=s, a_t=a\right]$，是state action pair $(s,a)$的immediate reward的期望值。</p><h4 id="动作值函数-v2">动作值函数</h4><p>根据average reward，给出一种新的state-action value的定义方式：<br>$$Q^{\pi} (s,a) = \sum_{t=0}^{\infty} \mathbb{E}\left[R_t - \eta(\pi)|s_0=s,a_0=a,\pi\right], \forall s\in S, a\in A \tag{11}$$</p><h4 id="状态值函数-v2">状态值函数</h4><p>Value function定义还和原来一样，形式没有变，但是因为$Q$计算方法变了，所以$V$的值也变了：<br>$$V^{\pi} (s) = \mathbb{E}_{\pi(a’;s)}\left[Q^{\pi}(s,a’)\right] \tag{12}$$</p><h4 id="stationary-distribution">stationary distribution</h4><p>对于average reward来说，它的stationary distribution和accumulated reward有一些不同：<br>$$\rho^{\pi}(s) = \lim_{t\rightarrow \infty}Pr\left[s_t=s|s_0, \pi\right] \tag{13}$$<br>表示的是当MDP稳定之后，state $s$出现的概率。</p><h3 id="accumulated-reward和average-reward">Accumulated Reward和Average Reward</h3><p>这两种方式，accumulated reward需要加上折扣因子，而average reward不需要。我们常见的都是accumulated reward这种方式的动作值函数以及状态值函数。</p><h2 id="分类方式">分类方式</h2><h3 id="online-vs-offline">online vs offline</h3><p>online方法中训练数据一直在不断增加，基本上强化学习都是online的，而监督学习是offline的。</p><h3 id="on-policy-vs-off-policy">on-policy vs off-policy</h3><p>behaviour policy是采样的policy。<br>target policy是要evaluation的policy。<br>behaviour policy和target policy是不是相同的，相同的就是on-policy，不同的就是off-policy，带有replay buffer的都是off-policy的方法。</p><h2 id="bootstrap">bootstrap</h2><p>当前value的计算是否基于其他value的估计值。<br>常见的bootstrap算法有DP，TD-gamma<br>MC算法不是bootstrap算法。</p><h2 id="value-based-vs-policy-gradient-vs-actor-critic">value-based vs policy gradient vs actor-critic</h2><h3 id="value-based">value-based</h3><p>values-based方法主要有policy iteration和value iteration。policy iteration又分为policy evaluation和policy improvement。<br>给出一个任务，如果可以使用value-based。随机初始化一个policy，然后可以计算这个policy的value function，这就叫做policy evaluation，然后根据这个value function，可以对policy进行改进，这叫做policy improvement，可以证明policy一定会更好。policy evaluation和policy improvement交替迭代，在线性case下，收敛性是可以证明的，在non-linear情况下，就不一定了。<br>policy iteraion中，policy evaluation每一次都要进行收敛后才进行policy improvemetn，如果policy evalution只进行一次，然后就进行一次policy improvemetn的话，也就是policy evalution的粒度变小后，就是value iteration。</p><h3 id="policy-gradient">policy gradient</h3><p>value-based方法只适用于discrete action space，对于contionous action space的话，就无能为力了。这个时候就有了policy gradient，给出一个state，policy gradient给出一个policy直接计算出相应的action，然后给出一个衡量action好坏的指标，直接对policy的参数求导，最后收敛之后就求解出一个使用与contionous的policy</p><h3 id="actor-critic">actor-critic</h3><p>如果policy gradient的metrics选择使用value function，一般是aciton value function的话，我们把这个value function叫做critic，然后把policy叫做actor。通过value funciton Q对policy的参数求导进行优化。<br>critic跟policy没有关系，而critic指导actor的训练，通过链式法则实现。critic对a求偏导，a对actor的参数求偏导。</p><h2 id="参考文献">参考文献</h2><p>1.<a href="https://stats.stackexchange.com/questions/897/online-vs-offline-learning" target="_blank" rel="noopener">https://stats.stackexchange.com/questions/897/online-vs-offline-learning</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;术语定义&quot;&gt;术语定义&lt;/h2&gt;
&lt;p&gt;更多介绍可以点击查看&lt;a href=&quot;https://mxxhcm.github.io/2018/12/21/reinforcement-learning-an-introduction-%E7%AC%AC3%E7%AB%A0
      
    
    </summary>
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="policy gradient" scheme="http://mxxhcm.github.io/tags/policy-gradient/"/>
    
      <category term="on-policy" scheme="http://mxxhcm.github.io/tags/on-policy/"/>
    
      <category term="online" scheme="http://mxxhcm.github.io/tags/online/"/>
    
      <category term="offline" scheme="http://mxxhcm.github.io/tags/offline/"/>
    
      <category term="off-policy" scheme="http://mxxhcm.github.io/tags/off-policy/"/>
    
      <category term="bootstrap" scheme="http://mxxhcm.github.io/tags/bootstrap/"/>
    
      <category term="model-free" scheme="http://mxxhcm.github.io/tags/model-free/"/>
    
      <category term="model-based" scheme="http://mxxhcm.github.io/tags/model-based/"/>
    
      <category term="value based" scheme="http://mxxhcm.github.io/tags/value-based/"/>
    
      <category term="actor critic" scheme="http://mxxhcm.github.io/tags/actor-critic/"/>
    
      <category term="state space" scheme="http://mxxhcm.github.io/tags/state-space/"/>
    
      <category term="action space" scheme="http://mxxhcm.github.io/tags/action-space/"/>
    
      <category term="reward" scheme="http://mxxhcm.github.io/tags/reward/"/>
    
      <category term="return" scheme="http://mxxhcm.github.io/tags/return/"/>
    
      <category term="advantage" scheme="http://mxxhcm.github.io/tags/advantage/"/>
    
      <category term="state value" scheme="http://mxxhcm.github.io/tags/state-value/"/>
    
      <category term="action value" scheme="http://mxxhcm.github.io/tags/action-value/"/>
    
  </entry>
  
  <entry>
    <title>linux-ubuntu 18.04 gnome-shell自定义操作</title>
    <link href="http://mxxhcm.github.io/2019/05/22/linux-ubuntu-18-04-gnome-shell-%E8%87%AA%E5%AE%9A%E4%B9%89%E6%93%8D%E4%BD%9C/"/>
    <id>http://mxxhcm.github.io/2019/05/22/linux-ubuntu-18-04-gnome-shell-自定义操作/</id>
    <published>2019-05-22T13:22:29.000Z</published>
    <updated>2019-07-30T09:02:01.223Z</updated>
    
    <content type="html"><![CDATA[<h2 id="dock-配置">dock 配置</h2><h3 id="基本设置">基本设置</h3><p>打开Settings &gt;&gt; Dock，可以设置dock的位置和大小，以及自动隐藏。这些是ubuntu安装的默认配置。</p><h3 id="dconf安装">dconf安装</h3><p>为了更多的设置，需要安装dconf-editor，dconf相当于windows注册表的gnome，存储应用程序设置的gnome技术。<br>~\$:sudo apt install dconf-tools<br>按下Win键，搜索dconfig-editor，打开它。<br>找到org&gt;&gt;gnome&gt;&gt;shell&gt;&gt;extensions&gt;&gt;dash-to-dock，然后就可以修改相应的配置了。也可以在命令行中进行相应的设置，这里就不说了，可以查看参考文献尝试。</p><h2 id="ubuntu-18-04合上笔记本盖子后不挂起">ubuntu 18.04合上笔记本盖子后不挂起</h2><p>~\$:sudo apt install gnome-tweak-tool<br>~\$:gnome-tweaks<br>找到Power选项，设置Suspend when lapto lid is closed为OFF。[6]</p><h2 id="显示cpu和gpu温度">显示cpu和gpu温度</h2><h3 id="安装lm-sensors">安装lm-sensors</h3><p>~\$:sudo apt-get install lm-sensors<br>然后执行以下命令进行配置：<br>~\$:sudo sensors-detect<br>执行sensors命令获得各项硬件的温度<br>~\$:sensors</p><h2 id="安装gnome-shell">安装gnome-shell</h2><h3 id="安装gnome-tweak-tool">安装gnome tweak tool</h3><p>~\$:sudo apt install gnome-tweak-tool<br>~\$:gnome-shell --version<br>gnome tweak用来查看本地的gnome 插件。</p><h3 id="从ubuntu-仓库安装extensions">从ubuntu 仓库安装extensions</h3><p>ubuntu 提供了gnome-shell-extensions包，该包中有部分gnome扩展。然后可以使用gnome tweaks查看已经安装的程序。<br>~\$:sudo apt install gnome-shell-extensions</p><h3 id="在浏览器上安装gnome-shell-integration插件">在浏览器上安装gnome shell integration插件</h3><p>在firfox或者chrome上安装相应的gnome shell integration插件，直接google搜索安装就行了。<br>这个时候是不能添加插件的，因为还缺少一个东西，叫做native host connector<br>这种方法和从ubuntu仓库中装extension的不同之处是，ubuntu包中的扩展是固定的一部分，这中方法可以自定义安装。<br>安装完之后可以直接在浏览器的gnome shell integration插件上查看在浏览器上安装的gnome shell扩展，也可以使用gnome tweaks查看浏览器上安装的shell extensions。</p><h4 id="安装chrome-gnome-shell-native-host-connector">安装chrome-gnome-shell native host connector</h4><p>执行以下命令进行安装，chrome-gnome-shell并不是代表chrome浏览器的意思，用任何浏览器都要执行以下命令<br>~\$:sudo apt install chrome-gnome-shell<br>查看gnome shell版本<br>~\$:gnome-shell --version</p><h4 id="安装浏览器附加组件">安装浏览器附加组件</h4><h5 id="浏览器中安装">浏览器中安装</h5><p>直接打开gnome shell extensions图形化界面进行搜索安装</p><h5 id="命令行安装">命令行安装</h5><p>搜索<br>~\$:sudo apt search gnome-shell-extension<br>安装<br>~\$:sudo apt install gnome-shell-extension-package-name</p><h3 id="插件推荐">插件推荐</h3><ul><li>Coverflow Alt-Tab 按alt tab切换程序效果[7]</li></ul><h2 id="修改主题">修改主题</h2><p>下载系统主题文件，解压缩，放置在/usr/share/themes文件夹下。然后在tweaks中的Apperance选项修改。<br>下载鼠标和图标主题，放置在/usr/share/icons文件夹下。</p><h2 id="参考文献">参考文献</h2><p>1.<a href="https://linuxconfig.org/how-to-customize-dock-panel-on-ubuntu-18-04-bionic-beaver-linux" target="_blank" rel="noopener">https://linuxconfig.org/how-to-customize-dock-panel-on-ubuntu-18-04-bionic-beaver-linux</a><br>2.<a href="https://zhuanlan.zhihu.com/p/37852274" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/37852274</a><br>3.<a href="https://askubuntu.com/questions/15832/how-do-i-get-the-cpu-temperature" target="_blank" rel="noopener">https://askubuntu.com/questions/15832/how-do-i-get-the-cpu-temperature</a><br>4.<a href="https://linuxhint.com/install_gnome3_extensions_ubuntu_1804/" target="_blank" rel="noopener">https://linuxhint.com/install_gnome3_extensions_ubuntu_1804/</a><br>5.<a href="https://linux.cn/article-9447-1.html" target="_blank" rel="noopener">https://linux.cn/article-9447-1.html</a><br>6.<a href="https://askubuntu.com/a/1062401" target="_blank" rel="noopener">https://askubuntu.com/a/1062401</a><br>7.<a href="https://zhuanlan.zhihu.com/p/37852274" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/37852274</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;dock-配置&quot;&gt;dock 配置&lt;/h2&gt;
&lt;h3 id=&quot;基本设置&quot;&gt;基本设置&lt;/h3&gt;
&lt;p&gt;打开Settings &amp;gt;&amp;gt; Dock，可以设置dock的位置和大小，以及自动隐藏。这些是ubuntu安装的默认配置。&lt;/p&gt;
&lt;h3 id=&quot;dconf安
      
    
    </summary>
    
      <category term="linux" scheme="http://mxxhcm.github.io/categories/linux/"/>
    
    
      <category term="linux" scheme="http://mxxhcm.github.io/tags/linux/"/>
    
      <category term="ubuntu" scheme="http://mxxhcm.github.io/tags/ubuntu/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow contrib vs layers vs nn</title>
    <link href="http://mxxhcm.github.io/2019/05/18/tensorflow-nn-layers-contrib/"/>
    <id>http://mxxhcm.github.io/2019/05/18/tensorflow-nn-layers-contrib/</id>
    <published>2019-05-18T07:58:59.000Z</published>
    <updated>2019-07-25T12:39:48.109Z</updated>
    
    <content type="html"><![CDATA[<h2 id="tf-contrib">tf.contrib</h2><p>根据tensorflow官网的说法，tf.contrib模块中包含了易修改的测试代码，</p><blockquote><p>contrib module containing volatile or experimental code.</p></blockquote><p>当其中的某一个模块完成的时候，就会从contrib模块中移除。为了保持对历史版本的兼容性，可能这几个模块会存在同一个函数的不同实现。</p><h2 id="tf-nn-tf-layers和tf-contrib">tf.nn,tf.layers和tf.contrib</h2><p>tf.nn中是low-level的op<br>tf.layers是high-level的op<br>而tf.contrib中的是非正式版本的实现，在后续版本中可能会被弃用。</p><h2 id="tf-nn-conv2d-vs-tf-layers-conv2d">tf.nn.conv2d vs tf.layers.conv2d</h2><h3 id="api">API</h3><h4 id="tf-layer-conv2d">tf.layer.conv2d</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">tf.layers.conv2d(</span><br><span class="line">    inputs, </span><br><span class="line">    filters, </span><br><span class="line">    kernel_size, </span><br><span class="line">    strides=(<span class="number">1</span>, <span class="number">1</span>), </span><br><span class="line">    padding=<span class="string">'valid'</span>, </span><br><span class="line">    data_format=<span class="string">'channels_last'</span>, </span><br><span class="line">    dilation_rate=(<span class="number">1</span>, <span class="number">1</span>), </span><br><span class="line">    activation=<span class="literal">None</span>, </span><br><span class="line">    use_bias=<span class="literal">True</span>, </span><br><span class="line">    kernel_initializer=<span class="literal">None</span>, </span><br><span class="line">    bias_initializer=tf.zeros_initializer(), </span><br><span class="line">    kernel_regularizer=<span class="literal">None</span>, </span><br><span class="line">    bias_regularizer=<span class="literal">None</span>, </span><br><span class="line">    activity_regularizer=<span class="literal">None</span>, </span><br><span class="line">    trainable=<span class="literal">True</span>, </span><br><span class="line">    name=<span class="literal">None</span>, </span><br><span class="line">    reuse=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h4 id="tf-nn-conv2d">tf.nn.conv2d</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.conv2d(</span><br><span class="line">    input, </span><br><span class="line">    filter, </span><br><span class="line">    strides, </span><br><span class="line">    padding, </span><br><span class="line">    use_cudnn_on_gpu=<span class="literal">None</span>, </span><br><span class="line">    data_format=<span class="literal">None</span>, </span><br><span class="line">    name=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="nn-conv2d-vs-layers-conv2d">nn.conv2d vs layers.conv2d</h3><p>tf.nn.conv2d需要手动创建filter的tensor，传入filter的参数[kernel_height, kernel_width, in_channels, num_filters]。<br>tf.layer.conv2d需要传入filter的维度即可。</p><p>对于tf.nn.conv2d，<br>filter:和input的type一样，是一个4D的tensor，shape为[filter_height, filter_width, in_channels, out_channels]<br>对于tf.layers.conv2d，<br>filters:是整数，是需要多少个filters。</p><p>可以使用tf.nn.conv2d来加载一个pretrained model，使用tf.layers.conv2d从头开始训练一个model。</p><h3 id="用法">用法</h3><h4 id="tf-layers-conv2d">tf.layers.conv2d</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Convolution Layer with 32 filters and a kernel size of 5</span></span><br><span class="line">conv1 = tf.layers.conv2d(x, <span class="number">32</span>, <span class="number">5</span>, activation=tf.nn.relu) </span><br><span class="line"><span class="comment"># Max Pooling (down-sampling) with strides of 2 and kernel size of 2</span></span><br><span class="line">conv1 = tf.layers.max_pooling2d(conv1, <span class="number">2</span>, <span class="number">2</span>)</span><br></pre></td></tr></table></figure><h4 id="tf-nn-conv2d-v2">tf.nn.conv2d</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">strides = <span class="number">1</span></span><br><span class="line"><span class="comment"># Weights matrix looks like: [kernel_size(=5), kernel_size(=5), input_channels (=3), filters (= 32)]</span></span><br><span class="line"><span class="comment"># Similarly bias = looks like [filters (=32)]</span></span><br><span class="line">out = tf.nn.conv2d(input, weights, padding=<span class="string">"SAME"</span>, strides = [<span class="number">1</span>, strides, strides, <span class="number">1</span>])</span><br><span class="line">out = tf.nn.bias_add(out, bias)</span><br><span class="line">out = tf.nn.relu(out)</span><br></pre></td></tr></table></figure><h2 id="参考文献">参考文献</h2><p>1.<a href="https://www.tensorflow.org/api_docs/python/tf/contrib" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/contrib</a><br>2.<a href="https://stackoverflow.com/questions/48001759/what-is-right-batch-normalization-function-in-tensorflow" target="_blank" rel="noopener">https://stackoverflow.com/questions/48001759/what-is-right-batch-normalization-function-in-tensorflow</a><br>3.<a href="https://stackoverflow.com/a/48003210" target="_blank" rel="noopener">https://stackoverflow.com/a/48003210</a><br>4.<a href="https://stackoverflow.com/questions/42785026/tf-nn-conv2d-vs-tf-layers-conv2d" target="_blank" rel="noopener">https://stackoverflow.com/questions/42785026/tf-nn-conv2d-vs-tf-layers-conv2d</a><br>5.<a href="https://stackoverflow.com/a/53683545" target="_blank" rel="noopener">https://stackoverflow.com/a/53683545</a><br>6.<a href="https://stackoverflow.com/a/45308609" target="_blank" rel="noopener">https://stackoverflow.com/a/45308609</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;tf-contrib&quot;&gt;tf.contrib&lt;/h2&gt;
&lt;p&gt;根据tensorflow官网的说法，tf.contrib模块中包含了易修改的测试代码，&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;contrib module containing volatile or
      
    
    </summary>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/categories/tensorflow/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow rnn</title>
    <link href="http://mxxhcm.github.io/2019/05/18/tensorflow-rnn/"/>
    <id>http://mxxhcm.github.io/2019/05/18/tensorflow-rnn/</id>
    <published>2019-05-18T07:55:34.000Z</published>
    <updated>2019-05-19T11:40:49.946Z</updated>
    
    <content type="html"><![CDATA[<h2 id="常见cell和函数">常见Cell和函数</h2><ul><li>tf.nn.rnn_cell.BasicRNNCell: 最基本的RNN cell.</li><li>tf.nn.rnn_cell.LSTMCell: LSTM cell</li><li>tf.nn.rnn_cell.LSTMStateTuple: tupled LSTM cell</li><li>tf.nn.rnn_cell.MultiRNNCell: 多层Cell</li><li>tf.nn.rnn_cell.DropoutCellWrapper: 给Cell加上dropout</li><li>tf.nn.dynamic_rnn: 动态rnn</li><li>tf.nn.static_rnn: 静态rnn</li></ul><h2 id="basicrnncell">BasicRNNCell</h2><h3 id="api">API</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">__init__(</span><br><span class="line">    num_units,</span><br><span class="line">    activation=<span class="literal">None</span>,</span><br><span class="line">    reuse=<span class="literal">None</span>,</span><br><span class="line">    name=<span class="literal">None</span>,</span><br><span class="line">    dtype=<span class="literal">None</span>,</span><br><span class="line">    **kwargs</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="示例">示例</h3><p><a href>完整代码地址</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">   myrnn = rnn.BasicRNNCell(rnn_size,activation=tf.nn.relu)</span><br><span class="line">   zero_state = myrnn.zero_state(batch_size, dtype=tf.float32)</span><br><span class="line">   outputs, states = rnn.static_rnn(myrnn, x, initial_state=zero_state, dtype=tf.float32)</span><br><span class="line"><span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure><h3 id="其他">其他</h3><p>TF 2.0将会弃用，等价于tf.keras.layers.SimpleRNNCell()</p><h2 id="lstmcell">LSTMCell</h2><h3 id="api-v2">API</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">__init__(</span><br><span class="line">    num_units, <span class="comment"># 隐藏层的大小</span></span><br><span class="line">    use_peepholes=<span class="literal">False</span>, <span class="comment"># </span></span><br><span class="line">    cell_clip=<span class="literal">None</span>,</span><br><span class="line">    initializer=<span class="literal">None</span>, <span class="comment"># 权重的初始化构造器</span></span><br><span class="line">    num_proj=<span class="literal">None</span>,</span><br><span class="line">    proj_clip=<span class="literal">None</span>,</span><br><span class="line">    num_unit_shards=<span class="literal">None</span>,</span><br><span class="line">    num_proj_shards=<span class="literal">None</span>,</span><br><span class="line">    forget_bias=<span class="number">1.0</span>,</span><br><span class="line">    state_is_tuple=<span class="literal">True</span>, <span class="comment"># c_state和m_state的元组</span></span><br><span class="line">    activation=<span class="literal">None</span>,</span><br><span class="line">    reuse=<span class="literal">None</span>,</span><br><span class="line">    name=<span class="literal">None</span>,</span><br><span class="line">    dtype=<span class="literal">None</span>,</span><br><span class="line">    **kwargs</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="示例-v2">示例</h3><p><a href>完整代码地址</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">lstm = rnn.BasicLSTMCell(lstm_size, forget_bias=<span class="number">1</span>, state_is_tuple=<span class="literal">True</span>)</span><br><span class="line">   zero_state = lstm.zero_state(batch_size, dtype=tf.float32)</span><br><span class="line">   outputs, states = rnn.static_rnn(lstm, x, initial_state=zero_state, dtype=tf.float32)</span><br><span class="line"><span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure><h3 id="其他-v2">其他</h3><p>TF 2.0将会弃用，等价于tf.keras.layers.LSTMCell</p><h2 id="lstmstatetuple">LSTMStateTuple</h2><p>和LSTMCell一样，只不过state用的是tuple。</p><h3 id="其他-v3">其他</h3><p>TF 2.0将会弃用，等价于tf.keras.layers.LSTMCell</p><h2 id="multirnncell">MultiRNNCell</h2><p>这个类可以实现多层RNN。</p><h3 id="api-v3">API</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">__init__(</span><br><span class="line">    cells,</span><br><span class="line">    state_is_tuple=<span class="literal">True</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="示例-v3">示例</h3><h4 id="代码1">代码1</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">num_units = [<span class="number">128</span>, <span class="number">64</span>]</span><br><span class="line">cells = [BasicLSTMCell(num_units=n) <span class="keyword">for</span> n <span class="keyword">in</span> num_units]</span><br><span class="line">stacked_rnn_cell = MultiRNNCell(cells)</span><br><span class="line">outputs, state = tf.nn.dynamic_rnn(cell=stacked_rnn_cell,</span><br><span class="line">                                   inputs=data,</span><br><span class="line">                                   dtype=tf.float32)</span><br></pre></td></tr></table></figure><h4 id="代码2">代码2</h4><p><a href>完整代码地址</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">   lstm_cell = rnn.BasicLSTMCell(lstm_size, forget_bias=<span class="number">1</span>, state_is_tuple=<span class="literal">True</span>)</span><br><span class="line">   cell = rnn.MultiRNNCell([lstm_cell]*layers, state_is_tuple=<span class="literal">True</span>)</span><br><span class="line">   state = cell.zero_state(batch_size, dtype=tf.float32)</span><br><span class="line">   outputs = []</span><br><span class="line">   <span class="keyword">with</span> tf.variable_scope(<span class="string">"Multi_Layer_RNN"</span>, reuse=reuse):</span><br><span class="line">       <span class="keyword">for</span> time_step <span class="keyword">in</span> range(time_steps):</span><br><span class="line">           <span class="keyword">if</span> time_step &gt; <span class="number">0</span>:</span><br><span class="line">               tf.get_variable_scope().reuse_variables()</span><br><span class="line">           </span><br><span class="line">           cell_outputs, state = cell(x[time_step], state)</span><br><span class="line">           outputs.append(cell_outputs)</span><br><span class="line"><span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure><h3 id="其他-v4">其他</h3><p>TF 2.0将会弃用，等价于tf.keras.layers.StackedRNNCells</p><h2 id="dropoutcellwrapper">DropoutCellWrapper</h2><h3 id="api-v4">API</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">__init__(</span><br><span class="line">    cell, <span class="comment"># </span></span><br><span class="line">    input_keep_prob=<span class="number">1.0</span>,</span><br><span class="line">    output_keep_prob=<span class="number">1.0</span>,</span><br><span class="line">    state_keep_prob=<span class="number">1.0</span>,</span><br><span class="line">    variational_recurrent=<span class="literal">False</span>,</span><br><span class="line">    input_size=<span class="literal">None</span>,</span><br><span class="line">    dtype=<span class="literal">None</span>,</span><br><span class="line">    seed=<span class="literal">None</span>,</span><br><span class="line">    dropout_state_filter_visitor=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="示例-v4">示例</h3><p><a href>完整代码地址</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">   lstm_cell = rnn.BasicLSTMCell(lstm_size, forget_bias=<span class="number">1</span>, state_is_tuple=<span class="literal">True</span>)</span><br><span class="line">   lstm_cell = rnn.DropoutWrapper(lstm_cell, output_keep_prob=<span class="number">0.9</span>)</span><br><span class="line">   cell = rnn.MultiRNNCell([lstm_cell]*layers, state_is_tuple=<span class="literal">True</span>)</span><br><span class="line">   state = cell.zero_state(batch_size, dtype=tf.float32)</span><br><span class="line">   outputs = []</span><br><span class="line">   <span class="keyword">with</span> tf.variable_scope(<span class="string">"Multi_Layer_RNN"</span>):</span><br><span class="line">       <span class="keyword">for</span> time_step <span class="keyword">in</span> range(time_steps):</span><br><span class="line">           <span class="keyword">if</span> time_step &gt; <span class="number">0</span>:</span><br><span class="line">               tf.get_variable_scope().reuse_variables()</span><br><span class="line">           cell_outputs, state = cell(x[time_step], state)</span><br><span class="line">           outputs.append(cell_outputs)</span><br><span class="line"><span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure><h3 id="其他-v5">其他</h3><h2 id="static-rnn">static_rnn</h2><h3 id="api-v5">API</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.static_rnn(</span><br><span class="line">    cell, <span class="comment"># RNNCell的具体对象</span></span><br><span class="line">    inputs, <span class="comment"># 输入，长度为T的输入列表，列表中每一个Tensor的shape都是[batch_size, input_size]</span></span><br><span class="line">    initial_state=<span class="literal">None</span>, <span class="comment"># rnn的初始状态，如果cell.state_size是整数，它的shape需要是[batch_size, cell.state_size]，如果cell.state_size是元组，那么终究会是一个tensors的元组，[batch_size, s] for s in cell.state_size</span></span><br><span class="line">    dtype=<span class="literal">None</span>, <span class="comment"># </span></span><br><span class="line">    sequence_length=<span class="literal">None</span>, <span class="comment"># </span></span><br><span class="line">    scope=<span class="literal">None</span></span><br><span class="line">)</span><br><span class="line"><span class="comment"># 最简单形式的RNN，就是该API的参数都是用默认值，给定cell和inputs，相当于做了以下操作：</span></span><br><span class="line"><span class="comment">#    state = cell.zero_state(...)</span></span><br><span class="line"><span class="comment">#    outputs = []</span></span><br><span class="line"><span class="comment">#    for input_ in inputs:</span></span><br><span class="line"><span class="comment">#      output, state = cell(input_, state)</span></span><br><span class="line"><span class="comment">#      outputs.append(output)</span></span><br><span class="line"><span class="comment">#    return (outputs, state)</span></span><br></pre></td></tr></table></figure><h3 id="示例-v5">示例</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">myrnn = tf.nn.rnn_cell.BasicRNNCell(rnn_size,activation=tf.nn.relu)</span><br><span class="line">   zero_state = myrnn.zero_state(batch_size, dtype=tf.float32)</span><br><span class="line">   outputs, states = tf.nn.static_rnn(myrnn, x, initial_state=zero_state, dtype=tf.float32)</span><br></pre></td></tr></table></figure><h2 id="dynamic-rnn">dynamic rnn</h2><h3 id="api-v6">API</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.dynamic_rnn(</span><br><span class="line">    cell, <span class="comment"># RNNCell的具体对象</span></span><br><span class="line">    inputs, <span class="comment"># RNN的输入,time_major = False, [batch_size, max_time, ...],time_major=True, [max_time, batch_size, ...]</span></span><br><span class="line">    sequence_length=<span class="literal">None</span>, <span class="comment"># </span></span><br><span class="line">    initial_state=<span class="literal">None</span>, <span class="comment"># rnn的初始状态，如果cell.state_size是整数，它的shape需要是[batch_size, cell.state_size]，如果cell.state_size是元组，那么就会是一个tensors的元组，[batch_size, s] for s in cell.state_size</span></span><br><span class="line">    dtype=<span class="literal">None</span>,</span><br><span class="line">    parallel_iterations=<span class="literal">None</span>,</span><br><span class="line">    swap_memory=<span class="literal">False</span>, <span class="comment">#</span></span><br><span class="line">    time_major=<span class="literal">False</span>, <span class="comment"># 如果为True,如果为False，对应不同的inputs </span></span><br><span class="line">    scope=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="示例-v6">示例</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 例子1.创建一个BasicRNNCell</span></span><br><span class="line">rnn_cell = tf.nn.rnn_cell.BasicRNNCell(hidden_size)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义初始化状态</span></span><br><span class="line">initial_state = rnn_cell.zero_state(batch_size, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 'outputs' shape [batch_size, max_time, cell_state_size]</span></span><br><span class="line"><span class="comment"># 'state' shape [batch_size, cell_state_size]</span></span><br><span class="line">outputs, state = tf.nn.dynamic_rnn(rnn_cell, input_data,</span><br><span class="line">                                   initial_state=initial_state,</span><br><span class="line">                                   dtype=tf.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 例子2.创建两个LSTMCells</span></span><br><span class="line">rnn_layers = [tf.nn.rnn_cell.LSTMCell(size) <span class="keyword">for</span> size <span class="keyword">in</span> [<span class="number">128</span>, <span class="number">256</span>]]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个多层RNNCelss。</span></span><br><span class="line">multi_rnn_cell = tf.nn.rnn_cell.MultiRNNCell(rnn_layers)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 'outputs' is a tensor of shape [batch_size, max_time, 256]</span></span><br><span class="line"><span class="comment"># 'state' is a N-tuple where N is the number of LSTMCells containing a</span></span><br><span class="line"><span class="comment"># tf.contrib.rnn.LSTMStateTuple for each cell</span></span><br><span class="line">outputs, state = tf.nn.dynamic_rnn(cell=multi_rnn_cell,</span><br><span class="line">                                   inputs=data,</span><br><span class="line">                                   dtype=tf.float32)</span><br></pre></td></tr></table></figure><h2 id="static-rnn-vs-dynamic-rnn">static_rnn vs dynamic_rnn</h2><h3 id="tf-keras-layers-rnn-cell">tf.keras.layers.RNN(cell)</h3><p>在tensorflow 2.0中，上述两个API都会被弃用，使用新的keras.layers.RNN(cell)</p><h2 id="tf-nn-rnn-cell">tf.nn.rnn_cell</h2><p>该模块提供了许多RNN cell类和rnn函数。</p><h3 id="类">类</h3><ul><li>class BasicRNNCell: 最基本的RNN cell.</li><li>class BasicLSTMCell: 弃用了，使用tf.nn.rnn_cell.LSTMCell代替，就是下面那个</li><li>class LSTMCell: LSTM cell</li><li>class LSTMStateTuple: tupled LSTM cell</li><li>class GRUCell: GRU cell (引用文献 <a href="http://arxiv.org/abs/1406.1078" target="_blank" rel="noopener">http://arxiv.org/abs/1406.1078</a>).</li><li>class RNNCell: 表示一个RNN cell的抽象对象</li><li>class MultiRNNCell: 由很多个简单cells顺序组合成的RNN cell</li><li>class DeviceWrapper: 保证一个RNNCell在一个特定的device运行的op.</li><li>class DropoutWrapper: 添加droput到给定cell的的inputs和outputs的op.</li><li>class ResidualWrapper: 确保cell的输入被添加到输出的RNNCell warpper。</li></ul><h3 id="函数">函数</h3><ul><li>static_rnn(…) # 未来将被弃用，和tf.contrib.rnn.static_rnn是一样的。</li><li>dynamic_rnn(…) # 未来将被弃用</li><li>static_bidirectional_rnn(…) # 未来将被弃用</li><li>bidirectional_dynamic_rnn(…) # 未来将被弃用</li><li>raw_rnn(…)</li></ul><h2 id="tf-contrib-rnn">tf.contrib.rnn</h2><p>该模块提供了RNN和Attention RNN的类和函数op。</p><h3 id="类-v2">类</h3><ul><li>class RNNCell: # 抽象类，所有Cell都要继承该类。所有的Warpper都要直接继承该Cell。</li><li>class LayerRNNCell: # 所有的下列定义的Cell都要使用继承该Cell，该Cell继承RNNCell，所以所有下列Cell都间接继承RNNCell。</li><li>class BasicRNNCell:</li><li>class BasicLSTMCell: # 将被弃用，使用下面的LSTMCell。</li><li>class LSTMCell:</li><li>class LSTMStateTuple:</li><li>class GRUCell:</li><li>class MultiRNNCell:</li><li>class ConvLSTMCell:</li><li>class GLSTMCell:</li><li>class Conv1DLSTMCell:</li><li>class Conv2DLSTMCell:</li><li>class Conv3DLSTMCell:</li><li>class BidirectionalGridLSTMCell:</li><li>class AttentionCellWrapper:</li><li>class CompiledWrapper:</li><li>class CoupledInputForgetGateLSTMCell:</li><li>class DeviceWrapper:</li><li>class DropoutWrapper:</li><li>class EmbeddingWrapper:</li><li>class FusedRNNCell:</li><li>class FusedRNNCellAdaptor:</li><li>class GRUBlockCell:</li><li>class GRUBlockCellV2:</li><li>class GridLSTMCell:</li><li>class HighwayWrapper:</li><li>class IndRNNCell:</li><li>class IndyGRUCell:</li><li>class IndyLSTMCell:</li><li>class InputProjectionWrapper:</li><li>class IntersectionRNNCell:</li><li>class LSTMBlockCell:</li><li>class LSTMBlockFusedCell:</li><li>class LSTMBlockWrapper:</li><li>class LayerNormBasicLSTMCell:</li><li>class NASCell:</li><li>class OutputProjectionWrapper:</li><li>class PhasedLSTMCell:</li><li>class ResidualWrapper:</li><li>class SRUCell:</li><li>class TimeFreqLSTMCell:</li><li>class TimeReversedFusedRNN:</li><li>class UGRNNCell:</li></ul><h3 id="函数-v2">函数</h3><ul><li>static_rnn(…) # 将被弃用，和tf.nn.static_rnn是一样的</li><li>static_bidirectional_rnn(…) # 将被弃用</li><li>best_effort_input_batch_size(…)</li><li>stack_bidirectional_dynamic_rnn(…)</li><li>stack_bidirectional_rnn(…)</li><li>static_state_saving_rnn(…)</li><li>transpose_batch_time(…)</li></ul><h2 id="tf-contrib-rnn-vs-tf-nn-rnn-cell">tf.contrib.rnn vs tf.nn.rnn_cell</h2><p>事实上，这两个模块中都定义了许多RNN cell，contrib定义的是测试性的代码，而nn.rnn_cell是contrib中经过测试后的代码。<br>contrib中的代码会经常修改，而nn中的代码比较稳定。<br>contrib中的cell类型比较多，而nn中的比较少。<br>contrib和nn中有重复的cell，基本上nn中有的contrib中都有。</p><h2 id="参考文献">参考文献</h2><p>1.<a href="https://www.tensorflow.org/api_docs/python/tf/nn/rnn_cell/RNNCell" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/nn/rnn_cell/RNNCell</a><br>2.<a href="https://www.tensorflow.org/api_docs/python/tf/nn/rnn_cell/BasicRNNCell" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/nn/rnn_cell/BasicRNNCell</a><br>3.<a href="https://www.tensorflow.org/api_docs/python/tf/nn/rnn_cell/LSTMCell" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/nn/rnn_cell/LSTMCell</a><br>4.<a href="https://www.tensorflow.org/api_docs/python/tf/nn/rnn_cell/MultiRNNCell" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/nn/rnn_cell/MultiRNNCell</a><br>5.<a href="https://www.tensorflow.org/api_docs/python/tf/nn/rnn_cell/LSTMStateTuple" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/nn/rnn_cell/LSTMStateTuple</a><br>6.<a href="https://www.tensorflow.org/api_docs/python/tf/nn/rnn_cell/DropoutWrapper" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/nn/rnn_cell/DropoutWrapper</a><br>7.<a href="https://www.tensorflow.org/api_docs/python/tf/nn/static_rnn" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/nn/static_rnn</a><br>8.<a href="https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn</a><br>9.<a href="https://www.tensorflow.org/api_docs/python/tf/contrib/rnn" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/contrib/rnn</a><br>10.<a href="https://www.tensorflow.org/api_docs/python/tf/nn/rnn_cell" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/nn/rnn_cell</a><br>11.<a href="https://www.cnblogs.com/wuzhitj/p/6297992.html" target="_blank" rel="noopener">https://www.cnblogs.com/wuzhitj/p/6297992.html</a><br>12.<a href="https://stackoverflow.com/questions/48001759/what-is-right-batch-normalization-function-in-tensorflow" target="_blank" rel="noopener">https://stackoverflow.com/questions/48001759/what-is-right-batch-normalization-function-in-tensorflow</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;常见cell和函数&quot;&gt;常见Cell和函数&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;tf.nn.rnn_cell.BasicRNNCell: 最基本的RNN cell.&lt;/li&gt;
&lt;li&gt;tf.nn.rnn_cell.LSTMCell: LSTM cell&lt;/li&gt;
&lt;li&gt;t
      
    
    </summary>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/categories/tensorflow/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow layers</title>
    <link href="http://mxxhcm.github.io/2019/05/18/tensorflow-layers-module/"/>
    <id>http://mxxhcm.github.io/2019/05/18/tensorflow-layers-module/</id>
    <published>2019-05-18T07:37:50.000Z</published>
    <updated>2019-05-19T08:43:15.232Z</updated>
    
    <content type="html"><![CDATA[<h2 id="tf-layers">tf.layers</h2><p>这个模块定义在tf.contrib.layers中。主要是构建神经网络，正则化和summaries等op。它包括1个模块，19个类，以及一系列函数。</p><h2 id="模块">模块</h2><h3 id="experimental-module">experimental module</h3><p>tf.layers.experimental的公开的API</p><h2 id="类">类</h2><h3 id="class-conv2d">class Conv2D</h3><p>二维卷积类。</p><h4 id="api">API</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">__init__(</span><br><span class="line">    filters, <span class="comment"># 卷积核的数量</span></span><br><span class="line">    kernel_size, <span class="comment"># 卷积核的大小</span></span><br><span class="line">    strides=(<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">    padding=<span class="string">'valid'</span>,</span><br><span class="line">    data_format=<span class="string">'channels_last'</span>, <span class="comment"># string, "channels_last", "channels_first"</span></span><br><span class="line">    dilation_rate=(<span class="number">1</span>, <span class="number">1</span>), <span class="comment">#</span></span><br><span class="line">    activation=<span class="literal">None</span>, <span class="comment"># 激活函数</span></span><br><span class="line">    use_bias=<span class="literal">True</span>,</span><br><span class="line">    kernel_initializer=<span class="literal">None</span>, <span class="comment"># 卷积核的构造器</span></span><br><span class="line">    bias_initializer=tf.zeros_initializer(), <span class="comment"># bias的构造器</span></span><br><span class="line">    kernel_regularizer=<span class="literal">None</span>, <span class="comment">#  卷积核的正则化</span></span><br><span class="line">    bias_regularizer=<span class="literal">None</span>,</span><br><span class="line">    activity_regularizer=<span class="literal">None</span>,</span><br><span class="line">    kernel_constraint=<span class="literal">None</span>,</span><br><span class="line">    bias_constraint=<span class="literal">None</span>,</span><br><span class="line">    trainable=<span class="literal">True</span>, <span class="comment"># 如果为True的话，将变量添加到TRANABLE_VARIABELS collection中</span></span><br><span class="line">    name=<span class="literal">None</span>,</span><br><span class="line">    **kwargs</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h4 id="示例">示例</h4><h4 id="其他">其他</h4><h3 id="所有类">所有类</h3><ul><li>class AveragePooling1D</li><li>class AveragePooling2D</li><li>class AveragePooling3D</li><li>class BatchNormalization</li><li>class Conv1D</li><li>class Conv2D</li><li>class Conv2DTranspose</li><li>class Conv3D</li><li>class Conv3DTranspose</li><li>class Dense</li><li>class Dropout</li><li>class Flatten</li><li>class InputSpec</li><li>class Layer</li><li>class MaxPooling1D</li><li>class MaxPooling2D</li><li>class MaxPooling3D</li><li>class SeparableConv1D</li><li>class SeparableConv2D</li></ul><h2 id="函数">函数</h2><h3 id="conv2d">conv2d</h3><h4 id="api-v2">API</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">tf.layers.conv2d(</span><br><span class="line">    inputs, <span class="comment"># 输入</span></span><br><span class="line">    filters, <span class="comment">#  一个整数,输出的维度，就是有几个卷积核</span></span><br><span class="line">    kernel_size,</span><br><span class="line">    strides=(<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">    padding=<span class="string">'valid'</span>,</span><br><span class="line">    data_format=<span class="string">'channels_last'</span>,</span><br><span class="line">    dilation_rate=(<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">    activation=<span class="literal">None</span>,</span><br><span class="line">    use_bias=<span class="literal">True</span>,</span><br><span class="line">    kernel_initializer=<span class="literal">None</span>,</span><br><span class="line">    bias_initializer=tf.zeros_initializer(),</span><br><span class="line">    kernel_regularizer=<span class="literal">None</span>,</span><br><span class="line">    bias_regularizer=<span class="literal">None</span>,</span><br><span class="line">    activity_regularizer=<span class="literal">None</span>,</span><br><span class="line">    kernel_constraint=<span class="literal">None</span>,</span><br><span class="line">    bias_constraint=<span class="literal">None</span>,</span><br><span class="line">    trainable=<span class="literal">True</span>,</span><br><span class="line">    name=<span class="literal">None</span>,</span><br><span class="line">    reuse=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h4 id="示例-v2">示例</h4><h4 id="其他-v2">其他</h4><h3 id="所有函数">所有函数</h3><p>需要注意的是，下列所有函数在以后版本都将被弃用。</p><ul><li>average_pooling1d(…)</li><li>average_pooling2d(…)</li><li>average_pooling3d(…)</li><li>batch_normalization(…)</li><li>conv1d(…)</li><li>conv2d(…)</li><li>conv2d_transpose(…)</li><li>conv3d(…)</li><li>conv3d_transpose(…)</li><li>dense(…)</li><li>dropout(…)</li><li>flatten(…)</li><li>max_pooling1d(…)</li><li>max_pooling2d(…)</li><li>max_pooling3d(…)</li><li>separable_conv1d(…)</li><li>separable_conv2d(…)</li></ul><h2 id="tf-layers-conv2d-vs-tf-layers-conv2d">tf.layers.conv2d vs tf.layers.Conv2d</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">tf.layers.Conv2d.__init__(</span><br><span class="line">    filters,</span><br><span class="line">    kernel_size,</span><br><span class="line">    strides=(<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">    padding=<span class="string">'valid'</span>,</span><br><span class="line">    data_format=<span class="string">'channels_last'</span>,</span><br><span class="line">    dilation_rate=(<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">    activation=<span class="literal">None</span>,</span><br><span class="line">    use_bias=<span class="literal">True</span>,</span><br><span class="line">    kernel_initializer=<span class="literal">None</span>,</span><br><span class="line">    bias_initializer=tf.zeros_initializer(),</span><br><span class="line">    kernel_regularizer=<span class="literal">None</span>,</span><br><span class="line">    bias_regularizer=<span class="literal">None</span>,</span><br><span class="line">    activity_regularizer=<span class="literal">None</span>,</span><br><span class="line">    kernel_constraint=<span class="literal">None</span>,</span><br><span class="line">    bias_constraint=<span class="literal">None</span>,</span><br><span class="line">    trainable=<span class="literal">True</span>,</span><br><span class="line">    name=<span class="literal">None</span>,</span><br><span class="line">    **kwargs</span><br><span class="line">)</span><br><span class="line">tf.layers.conv2d(</span><br><span class="line">    inputs,</span><br><span class="line">    filters,</span><br><span class="line">    kernel_size,</span><br><span class="line">    strides=(<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">    padding=<span class="string">'valid'</span>,</span><br><span class="line">    data_format=<span class="string">'channels_last'</span>,</span><br><span class="line">    dilation_rate=(<span class="number">1</span>, <span class="number">1</span>),</span><br><span class="line">    activation=<span class="literal">None</span>,</span><br><span class="line">    use_bias=<span class="literal">True</span>,</span><br><span class="line">    kernel_initializer=<span class="literal">None</span>,</span><br><span class="line">    bias_initializer=tf.zeros_initializer(),</span><br><span class="line">    kernel_regularizer=<span class="literal">None</span>,</span><br><span class="line">    bias_regularizer=<span class="literal">None</span>,</span><br><span class="line">    activity_regularizer=<span class="literal">None</span>,</span><br><span class="line">    kernel_constraint=<span class="literal">None</span>,</span><br><span class="line">    bias_constraint=<span class="literal">None</span>,</span><br><span class="line">    trainable=<span class="literal">True</span>,</span><br><span class="line">    name=<span class="literal">None</span>,</span><br><span class="line">    reuse=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>conv2d是函数；Conv2d是类。<br>conv2d运行的时候需要传入卷积核参数，输入；Conv2d在构造的时候需要实例化卷积核参数，实例化后，可以使用不用的输入得到不同的输出。<br>调用conv2d就相当于调用Conv2d对象的apply(inputs)函数。</p><h2 id="参考文献">参考文献</h2><p>1.<a href="https://www.tensorflow.org/api_docs/python/tf/layers" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/layers</a><br>4.<a href="https://www.tensorflow.org/api_docs/python/tf/layers/Conv2D" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/layers/Conv2D</a><br>5.<a href="https://www.tensorflow.org/api_docs/python/tf/layers/conv2d" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/layers/conv2d</a><br>6.<a href="https://stackoverflow.com/questions/52011509/what-is-difference-between-tf-layers-conv2d-and-tf-layers-conv2d/52035621" target="_blank" rel="noopener">https://stackoverflow.com/questions/52011509/what-is-difference-between-tf-layers-conv2d-and-tf-layers-conv2d/52035621</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;tf-layers&quot;&gt;tf.layers&lt;/h2&gt;
&lt;p&gt;这个模块定义在tf.contrib.layers中。主要是构建神经网络，正则化和summaries等op。它包括1个模块，19个类，以及一系列函数。&lt;/p&gt;
&lt;h2 id=&quot;模块&quot;&gt;模块&lt;/h2&gt;
&lt;h3 
      
    
    </summary>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/categories/tensorflow/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow nn module</title>
    <link href="http://mxxhcm.github.io/2019/05/18/tensorflow-nn-module/"/>
    <id>http://mxxhcm.github.io/2019/05/18/tensorflow-nn-module/</id>
    <published>2019-05-18T07:25:34.000Z</published>
    <updated>2019-05-19T02:02:44.284Z</updated>
    
    <content type="html"><![CDATA[<h2 id="tf-nn">tf.nn</h2><p>提供神经网络op。包含构建RNN cell的rnn_cell模块和一些函数。</p><h2 id="tf-nn-rnn-cell">tf.nn.rnn_cell</h2><p>rnn_cell 用于构建RNN cells<br>包括以下几个类：</p><ul><li>class BasicLSTMCell: 弃用了，使用tf.nn.rnn_cell.LSTMCell代替。</li><li>class BasicRNNCell: 最基本的RNN cell.</li><li>class DeviceWrapper: 保证一个RNNCell在一个特定的device运行的op.</li><li>class DropoutWrapper: 添加droput到给定cell的的inputs和outputs的op.</li><li>class GRUCell: GRU cell (引用文献 <a href="http://arxiv.org/abs/1406.1078" target="_blank" rel="noopener">http://arxiv.org/abs/1406.1078</a>).</li><li>class LSTMCell: LSTM cell</li><li>class LSTMStateTuple: tupled LSTM cell</li><li>class MultiRNNCell: 由很多个简单cells顺序组合成的RNN cell</li><li>class RNNCell: 表示一个RNN cell的抽象对象</li><li>class ResidualWrapper: 确保cell的输入被添加到输出的RNNCell warpper。</li></ul><h2 id="函数">函数</h2><h3 id="conv2d">conv2d(…)</h3><p>给定一个4d输入和filter，计算2d卷积。</p><h4 id="api">API</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.conv2d(</span><br><span class="line">    input, <span class="comment"># 输入，[batch, in_height, in_width, in_channels]</span></span><br><span class="line">    filter, <span class="comment"># 4d tensor, [filter_height, filter_width, in_channels, out_channles]</span></span><br><span class="line">    strides, <span class="comment"># 长度为4的1d tensor。</span></span><br><span class="line">    padding, <span class="comment"># string, 可选"SAME"或者"VALID"</span></span><br><span class="line">    use_cudnn_on_gpu=<span class="literal">True</span>, <span class="comment">#</span></span><br><span class="line">    data_format=<span class="string">'NHWC'</span>, <span class="comment">#</span></span><br><span class="line">    dilations=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], <span class="comment">#</span></span><br><span class="line">    name=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h4 id="示例">示例</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv2d</span><span class="params">(inputs, output_dim, kernel_size, stride, initializer, activation_fn,</span></span></span><br><span class="line"><span class="function"><span class="params">           padding=<span class="string">'VALID'</span>, data_format=<span class="string">'NHWC'</span>, name=<span class="string">"conv2d"</span>, reuse=False)</span>:</span></span><br><span class="line">    kernel_shape = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(name, reuse=reuse):</span><br><span class="line">        <span class="keyword">if</span> data_format == <span class="string">'NCHW'</span>:</span><br><span class="line">            stride = [<span class="number">1</span>, <span class="number">1</span>, stride[<span class="number">0</span>], stride[<span class="number">1</span>]]</span><br><span class="line">            kernel_shape = [kernel_size[<span class="number">0</span>], kernel_size[<span class="number">1</span>], inputs.get_shape()[<span class="number">1</span>], output_dim]</span><br><span class="line">        <span class="keyword">elif</span> data_format == <span class="string">'NHWC'</span>:</span><br><span class="line">            stride = [<span class="number">1</span>, stride[<span class="number">0</span>], stride[<span class="number">1</span>], <span class="number">1</span>]</span><br><span class="line">            kernel_shape = [kernel_size[<span class="number">0</span>], kernel_size[<span class="number">1</span>], inputs.get_shape()[<span class="number">-1</span>], output_dim ]</span><br><span class="line"></span><br><span class="line">        w = tf.get_variable(<span class="string">'w'</span>, kernel_shape, tf.float32, initializer=initializer)</span><br><span class="line">        conv = tf.nn.conv2d(inputs, w, stride, padding, data_format=data_format)</span><br><span class="line"></span><br><span class="line">        b = tf.get_variable(<span class="string">'b'</span>, [output_dim], tf.float32, initializer=tf.constant_initializer(<span class="number">0.0</span>))</span><br><span class="line">        out = tf.nn.bias_add(conv, b, data_format=data_format)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> activation_fn <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        out = activation_fn(out)</span><br><span class="line">    <span class="keyword">return</span> out, w, b</span><br></pre></td></tr></table></figure><h3 id="convolution">convolution</h3><h4 id="api-v2">API</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.convolution(</span><br><span class="line">    input, <span class="comment"># 输入</span></span><br><span class="line">    filter, <span class="comment"># 卷积核</span></span><br><span class="line">    padding, <span class="comment"># string, 可选"SAME"或者"VALID"</span></span><br><span class="line">    strides=<span class="literal">None</span>, <span class="comment"># 步长</span></span><br><span class="line">    dilation_rate=<span class="literal">None</span>,</span><br><span class="line">    name=<span class="literal">None</span>,</span><br><span class="line">    data_format=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h4 id="和tf-nn-conv2d对比">和tf.nn.conv2d对比</h4><p>tf.nn.conv2d是2d卷积<br>tf.nn.convolution是nd卷积</p><h3 id="conv2d-transpose">conv2d_transpose</h3><p>反卷积</p><h4 id="api-v3">API</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.conv2d_transpose(</span><br><span class="line">    value, <span class="comment"># 输入，4d tensor，[batch, in_channels, height, width] for NCHW,或者[batch,height, width, in_channels] for NHWC</span></span><br><span class="line">    filter, <span class="comment"># 4d卷积核，shape是[height, width, output_channels, in_channels]</span></span><br><span class="line">    output_shape, <span class="comment"># 表示反卷积输出的shape一维tensor</span></span><br><span class="line">    strides, <span class="comment"># 步长</span></span><br><span class="line">    padding=<span class="string">'SAME'</span>,</span><br><span class="line">    data_format=<span class="string">'NHWC'</span>,</span><br><span class="line">    name=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h4 id="示例-v2">示例</h4><h3 id="max-pool">max_pool</h3><p>实现max pooling</p><h4 id="api-v4">API</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.max_pool(</span><br><span class="line">    value, <span class="comment"># 输入，4d tensor</span></span><br><span class="line">    ksize, <span class="comment"># 4个整数的list或者tuple，max pooling的kernel size</span></span><br><span class="line">    strides, <span class="comment"># 4个整数的list或者tuple</span></span><br><span class="line">    padding, <span class="comment"># string, 可选"VALID"或者"VALID"</span></span><br><span class="line">    data_format=<span class="string">'NHWC'</span>, <span class="comment"># string,可选"NHWC", "NCHW", NCHW_VECT_C"</span></span><br><span class="line">    name=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="几个常用的函数">几个常用的函数</h3><ul><li>bias_add(…)</li><li>raw_rnn(…)</li><li>static_rnn(…) # 未来将被弃用</li><li>dynamic_rnn(…) # 未来将被弃用</li><li>static_bidirectional_rnn(…) # 未来将被弃用</li><li>bidirectional_dynamic_rnn(…) # 未来将被弃用</li><li>dropout(…)</li><li>leaky_relu(…)</li><li>l2_loss(…)</li><li>log_softmax(…) # 参数弃用</li><li>softmax(…) # 参数弃用</li><li>softmax_cross_entropy_with_logits(…)# 未来将被弃用</li><li>softmax_cross_entropy_with_logits_v2(…) # 参数弃用</li><li>sparse_softmax_cross_entropy_with_logits(…)</li></ul><h4 id="全部函数">全部函数</h4><ul><li>all_candidate_sampler(…)</li><li>atrous_conv2d(…)</li><li>atrous_conv2d_transpose(…)</li><li>avg_pool(…)</li><li>avg_pool3d(…)</li><li>batch_norm_with_global_normalization(…)</li><li>batch_normalization(…)</li><li>bias_add(…)</li><li>bidirectional_dynamic_rnn(…)</li><li>collapse_repeated(…)</li><li>compute_accidental_hits(…)</li><li>conv1d(…)</li><li>conv2d(…)</li><li>conv2d_backprop_filter(…)</li><li>conv2d_backprop_input(…)</li><li>conv2d_transpose(…)</li><li>conv3d(…)</li><li>conv3d_backprop_filter(…)</li><li>conv3d_backprop_filter_v2(…)</li><li>conv3d_transpose(…)</li><li>convolution(…) - crelu(…)</li><li>ctc_beam_search_decoder(…)</li><li>ctc_beam_search_decoder_v2(…)</li><li>ctc_greedy_decoder(…)</li><li>ctc_loss(…)</li><li>ctc_loss_v2(…)</li><li>ctc_unique_labels(…)</li><li>depth_to_space(…)</li><li>depthwise_conv2d(…)</li><li>depthwise_conv2d_backprop_filter(…)</li><li>depthwise_conv2d_backprop_input(…)</li><li>depthwise_conv2d_native(…)</li><li>depthwise_conv2d_native_backprop_filter(…)</li><li>depthwise_conv2d_native_backprop_input(…)</li><li>dilation2d(…)</li><li>dropout(…)</li><li>dynamic_rnn(…)</li><li>elu(…)</li><li>embedding_lookup(…)</li><li>embedding_lookup_sparse(…)</li><li>erosion2d(…)</li><li>fixed_unigram_candidate_sampler(…)</li><li>fractional_avg_pool(…)</li><li>fractional_max_pool(…)</li><li>fused_batch_norm(…)</li><li>in_top_k(…)</li><li>l2_loss(…)</li><li>l2_normalize(…)</li><li>leaky_relu(…)</li><li>learned_unigram_candidate_sampler(…)</li><li>local_response_normalization(…)</li><li>log_poisson_loss(…)</li><li>log_softmax(…)</li><li>log_uniform_candidate_sampler(…)</li><li>lrn(…)</li><li>max_pool(…)</li><li>max_pool3d(…)</li><li>max_pool_with_argmax(…)</li><li>moments(…)</li><li>nce_loss(…)</li><li>normalize_moments(…)</li><li>pool(…)</li><li>quantized_avg_pool(…)</li><li>quantized_conv2d(…)</li><li>quantized_max_pool(…)</li><li>quantized_relu_x(…)</li><li>raw_rnn(…)</li><li>relu(…)</li><li>relu6(…)</li><li>relu_layer(…)</li><li>safe_embedding_lookup_sparse(…)</li><li>sampled_softmax_loss(…)</li><li>selu(…)</li><li>separable_conv2d(…)</li><li>sigmoid(…)</li><li>sigmoid_cross_entropy_with_logits(…)</li><li>softmax(…)</li><li>softmax_cross_entropy_with_logits(…)</li><li>softmax_cross_entropy_with_logits_v2(…)</li><li>softplus(…)</li><li>softsign(…)</li><li>space_to_batch(…)</li><li>space_to_depth(…)</li><li>sparse_softmax_cross_entropy_with_logits(…)</li><li>static_bidirectional_rnn(…)</li><li>static_rnn(…)</li><li>static_state_saving_rnn(…)</li><li>sufficient_statistics(…)</li><li>tanh(…)</li><li>top_k(…)</li><li>uniform_candidate_sampler(…)</li><li>weighted_cross_entropy_with_logits(…)</li><li>weighted_moments(…)</li><li>with_space_to_batch(…)</li><li>xw_plus_b(…)</li><li>zero_fraction(…)</li></ul><h2 id="参考文献">参考文献</h2><p>1.<a href="https://www.tensorflow.org/api_docs/python/tf/nn" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/nn</a><br>2.<a href="https://www.tensorflow.org/api_docs/python/tf/nn/rnn_cell" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/nn/rnn_cell</a><br>3.<a href="https://www.tensorflow.org/api_docs/python/tf/nn/conv2d" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/nn/conv2d</a><br>4.<a href="https://stackoverflow.com/questions/38601452/what-is-tf-nn-max-pools-ksize-parameter-used-for" target="_blank" rel="noopener">https://stackoverflow.com/questions/38601452/what-is-tf-nn-max-pools-ksize-parameter-used-for</a><br>5.<a href="https://www.tensorflow.org/api_docs/python/tf/nn/convolution" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/nn/convolution</a><br>6.<a href="https://stackoverflow.com/questions/47775244/difference-between-tf-nn-convolution-and-tf-nn-conv2d" target="_blank" rel="noopener">https://stackoverflow.com/questions/47775244/difference-between-tf-nn-convolution-and-tf-nn-conv2d</a><br>7.<a href="https://www.tensorflow.org/api_docs/python/tf/nn/conv2d_transpose" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/nn/conv2d_transpose</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;tf-nn&quot;&gt;tf.nn&lt;/h2&gt;
&lt;p&gt;提供神经网络op。包含构建RNN cell的rnn_cell模块和一些函数。&lt;/p&gt;
&lt;h2 id=&quot;tf-nn-rnn-cell&quot;&gt;tf.nn.rnn_cell&lt;/h2&gt;
&lt;p&gt;rnn_cell 用于构建RNN cell
      
    
    </summary>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/categories/tensorflow/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow softmax</title>
    <link href="http://mxxhcm.github.io/2019/05/16/tensorflow-softmax/"/>
    <id>http://mxxhcm.github.io/2019/05/16/tensorflow-softmax/</id>
    <published>2019-05-16T01:04:48.000Z</published>
    <updated>2019-09-28T13:25:45.387Z</updated>
    
    <content type="html"><![CDATA[<h2 id="各种softmax">各种softmax</h2><ul><li>tf.nn.softmax。</li><li>tf.nn.log_softmax。</li><li>tf.nn.softmax_cross_entropy_with_logits_v2中label是用稀疏的（one-hot）表示的。</li><li>tf.nn.sparse_softmax_cross_entropy_with_logits中label是非稀疏的。</li></ul><h2 id="对比">对比</h2><p>tf.nn.softmax()<br>tf.nn.log_softmax()<br>tf.nn.softmax_cross_entropy_with_logits_v2()<br>tf.nn.sparse_cross_entropy_with_logits()</p><h2 id="logits">logits</h2><p>什么是logits</p><h3 id="数学上">数学上</h3><p>假设一个事件发生的概率为 p，那么该事件的logits为$\text{logit}§ = \log\frac{p}{1-p}$.</p><h3 id="machine-learning中">Machine Learning中</h3><p>深度学习中的logits和数学上的logits没有太大联系。logits在机器学习中前向传播的输出，是未归一化的概率，总和不为$1$。将logits的输出输入softmax函数之后可以得到归一化的概率。</p><h2 id="tf-nn-softmax">tf.nn.softmax</h2><h3 id="api">API</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.softmax(</span><br><span class="line">logits,</span><br><span class="line">axis=<span class="literal">None</span>,</span><br><span class="line">name=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="功能">功能</h3><p>上面函数实现了如下的功能：<br>softmax = tf.exp(logits) / tf.reduce_sum(tf.exp(logits), axis)<br>就是将输入的logits经过softmax做归一化。</p><h3 id="示例">示例</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">logits = [<span class="number">1.0</span>, <span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">2.0</span>, <span class="number">2.0</span>, <span class="number">2.0</span>]</span><br><span class="line">res_op = tf.nn.softmax(logits)</span><br><span class="line">sess = tf.Session()</span><br><span class="line">result = sess.run(res_op)</span><br><span class="line">print(result)</span><br><span class="line">print(sum(result))</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line"><span class="comment"># [0.07768121 0.07768121 0.21115941 0.21115941 0.21115941 0.21115941]</span></span><br><span class="line"><span class="comment"># 1.0000000447034836</span></span><br><span class="line"><span class="comment"># 因为有指数运算，所以就不是整数</span></span><br></pre></td></tr></table></figure><h2 id="tf-nn-log-softmax">tf.nn.log_softmax</h2><h3 id="api-v2">API</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.log_softmax(</span><br><span class="line">logits,</span><br><span class="line">axis=<span class="literal">None</span>,</span><br><span class="line">name=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="功能-v2">功能</h3><p>该函数实现了如下功能。<br>logsoftmax = logits - log(reduce_sum(exp(logits), axis))</p><h3 id="示例-v2">示例</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">logits = [<span class="number">1.0</span>, <span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">2.0</span>, <span class="number">2.0</span>, <span class="number">2.0</span>]</span><br><span class="line">res_op = tf.nn.log_softmax(logits)</span><br><span class="line">sess = tf.Session()</span><br><span class="line">result = sess.run(res_op)</span><br><span class="line">print(result)</span><br><span class="line">print(sum(result))</span><br><span class="line"></span><br><span class="line"><span class="comment"># output</span></span><br><span class="line"><span class="comment"># [-2.555142  -2.555142  -1.5551419 -1.5551419 -1.5551419 -1.5551419]</span></span><br><span class="line"><span class="comment"># -11.330851554870605</span></span><br></pre></td></tr></table></figure><h2 id="tf-nn-softmax-cross-entropy-with-logits-v2">tf.nn.softmax_cross_entropy_with_logits_v2</h2><h3 id="api-v3">API</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.softmax_cross_entropy_with_logits_v2(</span><br><span class="line">    labels, <span class="comment"># shape是[batch_size, num_calsses]，每一个labels[i]都应该是一个有效的probability distribution</span></span><br><span class="line">    logits, <span class="comment"># 没有normalized的log probabilities</span></span><br><span class="line">    axis=<span class="literal">None</span>,</span><br><span class="line">    name=<span class="literal">None</span></span><br><span class="line">    dim=<span class="number">-1</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="功能-v3">功能</h3><p>计算logits经过softmax之后和labels之间的交叉熵</p><h2 id="tf-sparse-softmax-cross-entropy-with-logits">tf.sparse_softmax_cross_entropy_with_logits</h2><h3 id="api-v4">API</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.sparse_softmax_cross_entropy_with_logits(</span><br><span class="line">    _sentinel=<span class="literal">None</span>,  <span class="comment"># pylint: disable=invalid-name</span></span><br><span class="line">    labels=<span class="literal">None</span>,    <span class="comment"># shape是[d_0, d_1, ..., d_&#123;r-1&#125;]其中r是labels的秩，type是int32或int64，每一个entry都应该在[0, num_classes)之间</span></span><br><span class="line">    logits=<span class="literal">None</span>,    <span class="comment"># logits 是[d_0, d_1, ..., d_&#123;r-1&#125;, num_classes]，是float类型的，可以看成unnormalized log probabilities</span></span><br><span class="line">    name=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="功能-v4">功能</h3><p>计算logits和labels之间的稀疏softmax交叉熵</p><h2 id="参考文献">参考文献</h2><p>1.<a href="http://landcareweb.com/questions/789/shi-yao-shi-logits-softmaxhe-softmax-cross-entropy-with-logits" target="_blank" rel="noopener">http://landcareweb.com/questions/789/shi-yao-shi-logits-softmaxhe-softmax-cross-entropy-with-logits</a><br>2.<a href="https://stackoverflow.com/questions/41455101/what-is-the-meaning-of-the-word-logits-in-tensorflow" target="_blank" rel="noopener">https://stackoverflow.com/questions/41455101/what-is-the-meaning-of-the-word-logits-in-tensorflow</a><br>3.<a href="https://stackoverflow.com/a/43577384" target="_blank" rel="noopener">https://stackoverflow.com/a/43577384</a><br>4.<a href="https://stackoverflow.com/a/47852892" target="_blank" rel="noopener">https://stackoverflow.com/a/47852892</a><br>5.<a href="https://www.tensorflow.org/tutorials/estimators/cnn" target="_blank" rel="noopener">https://www.tensorflow.org/tutorials/estimators/cnn</a><br>6.<a href="https://www.zhihu.com/question/60751553" target="_blank" rel="noopener">https://www.zhihu.com/question/60751553</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;各种softmax&quot;&gt;各种softmax&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;tf.nn.softmax。&lt;/li&gt;
&lt;li&gt;tf.nn.log_softmax。&lt;/li&gt;
&lt;li&gt;tf.nn.softmax_cross_entropy_with_logits_v2中la
      
    
    </summary>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/categories/tensorflow/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow collection</title>
    <link href="http://mxxhcm.github.io/2019/05/13/tensorflow-collection/"/>
    <id>http://mxxhcm.github.io/2019/05/13/tensorflow-collection/</id>
    <published>2019-05-13T02:28:29.000Z</published>
    <updated>2019-09-28T08:41:47.549Z</updated>
    
    <content type="html"><![CDATA[<h2 id="tf-collection">tf.collection</h2><p>Tensorflow用graph collection来管理不同类型的对象。tf.GraphKeys中定义了默认的collection，tf通过调用各种各样的collection操作graph中的变量。比如tf.Optimizer只优化tf.GraphKeys.TRAINABLE_VARIABLES collection中的变量。常见的collection如下，它们其实都是字符串：</p><ul><li>GLOBAL_VARIABLES: 所有的Variable对象在创建的时候自动加入该colllection，且在分布式环境中共享（model variables是它的子集）。一般来说，TRAINABLE_VARIABLES包含在MODEL_VARIABLES中，MODEL_VARIABLES包含在GLOBAL_VARIABLES中。也就是说TRAINABLE_VARIABLES$\le$MODEL_VARIABLES$\le$GLOBAL_VARIABLES。一般tf.train.Saver()对应的是GLOBAL_VARIABLES的变量。</li><li>LOCAL_VARIABLES: 它是GLOBAL_VARIABLES不同的是在本机器上的Variable子集。使用tf.contrib.framework.local_variable将变量添加到这个collection.</li><li>MODEL_VARIABLES: 模型变量，在构建模型中，所有用于前向传播的Variable都将添加到这里。使用 tf.contrib.framework.model_variable向这个collection添加变量。</li><li>TRAINALBEL_VARIABLES: 所有用于反向传播的Variable，可以被optimizer训练，进行参数更新的变量。tf.Variable对象同样会自动加入这个collection。</li><li>SUMMARIES: graph创建的所有summary Tensor都会记录在这里面。</li><li>QUEUE_RUNNERS:</li><li>MOVING_AVERAGE_VARIABLES: 保持Movering average的变量子集。</li><li>REGULARIZATION_LOSSES: 创建graph的regularization loss。</li></ul><p>这里主要介绍三类collection，一种是GLOBAL_VARIABLES，一种是SUMMARIES，一种是自定义的collections。</p><p>下面的一些collection也被定义了，但是并不会自动添加</p><blockquote><p>The following standard keys are defined, but their collections are not automatically populated as many of the others are:</p></blockquote><ul><li>WEIGHTS</li><li>BIASES</li><li>ACTIVATIONS</li></ul><h2 id="global-variable-collection">GLOBAL_Variable collection</h2><p>tf.Variable()对象在生成时会被默认添加到tf.GraphKeys中的GLOBAL_VARIABLES和TRAINABLE_VARIABLES collection中。</p><h3 id="代码示例">代码示例</h3><p><a href="https://github.com/mxxhcm/code/blob/master/tf/ops/tf_global_trainable_variables_collections.py" target="_blank" rel="noopener">代码地址</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">a = tf.Variable([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">b = tf.get_variable(<span class="string">"bbb"</span>, shape=[<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">tf.constant([<span class="number">3</span>])</span><br><span class="line">c = tf.ones([<span class="number">3</span>])</span><br><span class="line">d = tf.random_uniform([<span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">e = tf.log(c)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看GLOBAL_VARIABLES collection中的变量</span></span><br><span class="line">global_variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)</span><br><span class="line"><span class="keyword">for</span> var <span class="keyword">in</span> global_variables:</span><br><span class="line">   print(var)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看TRAINABLE_VARIABLES collection中的变量</span></span><br><span class="line">trainable_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)</span><br><span class="line"><span class="keyword">for</span> var <span class="keyword">in</span> global_variables:</span><br><span class="line">   print(var)</span><br></pre></td></tr></table></figure><h2 id="summary-collection">Summary collection</h2><p>Summary op产生的变量会被添加到tf.GraphKeys.SUMMARIES collection中。<br><a href="https://mxxhcm.github.io/2019/05/08/tensorflow-summary/">点击查看关于tf.summary的详细介绍</a></p><h3 id="代码示例-v2">代码示例</h3><p><a href="https://github.com/mxxhcm/code/blob/master/tf/ops/tf_summary_collection.py" target="_blank" rel="noopener">代码地址</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成一个图</span></span><br><span class="line">graph = tf.Graph()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> graph.as_default():</span><br><span class="line">    <span class="comment"># 指定模型参数</span></span><br><span class="line">    w = tf.Variable([<span class="number">0.3</span>], name=<span class="string">"w"</span>, dtype=tf.float32)</span><br><span class="line">    b = tf.Variable([<span class="number">0.2</span>], name=<span class="string">"b"</span>, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 输入数据placeholder</span></span><br><span class="line">    x = tf.placeholder(tf.float32, name=<span class="string">"inputs"</span>)</span><br><span class="line">    y = tf.placeholder(tf.float32, name=<span class="string">"outputs"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 前向传播</span></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">'linear_model'</span>):</span><br><span class="line">        linear = w * x + b</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算loss</span></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">'cal_loss'</span>):</span><br><span class="line">        loss = tf.reduce_mean(input_tensor=tf.square(y - linear), name=<span class="string">'loss'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义summary saclar op</span></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">'add_summary'</span>):</span><br><span class="line">        summary_loss = tf.summary.scalar(<span class="string">'MSE'</span>, loss)</span><br><span class="line">        summary_b = tf.summary.scalar(<span class="string">'b'</span>, b[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义优化器</span></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">'train_model'</span>):</span><br><span class="line">        optimizer = tf.train.GradientDescentOptimizer(<span class="number">0.01</span>)</span><br><span class="line">        train = optimizer.minimize(loss)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session(graph=graph) <span class="keyword">as</span> sess:</span><br><span class="line">inputs = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]</span><br><span class="line">outputs = [<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]</span><br><span class="line">    <span class="comment"># 定义写入文件类</span></span><br><span class="line">    writer = tf.summary.FileWriter(<span class="string">"./summary/"</span>, graph)</span><br><span class="line">    <span class="comment"># 获取所有的summary op，不用一个一个去单独run</span></span><br><span class="line">    merged = tf.summary.merge_all()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化</span></span><br><span class="line">    init_op = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5000</span>):</span><br><span class="line"><span class="comment"># 运行summary op merged</span></span><br><span class="line">        _, summ = sess.run([train, merged], feed_dict=&#123;x: inputs, y: outputs&#125;)</span><br><span class="line"><span class="comment"># 将summary op返回的变量转化为事件，写入文件</span></span><br><span class="line">        writer.add_summary(summ, global_step=i)</span><br><span class="line"></span><br><span class="line">    w_, b_, l_ = sess.run([w, b, loss], feed_dict=&#123;x: inputs, y: outputs&#125;)</span><br><span class="line">    print(<span class="string">"w: "</span>, w_, <span class="string">"b: "</span>, b_, <span class="string">"loss: "</span>, l_)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 查看SUMMARIES collection</span></span><br><span class="line">    <span class="keyword">for</span> var <span class="keyword">in</span> tf.get_collection(tf.GraphKeys.SUMMARIES):</span><br><span class="line">        print(var)</span><br></pre></td></tr></table></figure><h2 id="自定义collection">自定义collection</h2><p>通过tf.add_collection()和tf.get_collection()可以添加和访问custom collection。</p><h3 id="示例代码">示例代码</h3><p><a href="https://github.com/mxxhcm/code/blob/master/tf/ops/tf_custom_collection.py" target="_blank" rel="noopener">代码地址</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义第1个loss</span></span><br><span class="line">x1 = tf.constant(<span class="number">1.0</span>)</span><br><span class="line">l1 = tf.nn.l2_loss(x1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义第2个loss</span></span><br><span class="line">x2 = tf.constant([<span class="number">2.5</span>, <span class="number">-0.3</span>])</span><br><span class="line">l2 = tf.nn.l2_loss(x2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将loss添加到losses collection中</span></span><br><span class="line">tf.add_to_collection(<span class="string">"losses"</span>, l1)</span><br><span class="line">tf.add_to_collection(<span class="string">"losses"</span>, l2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看losses collection中的内容</span></span><br><span class="line">losses = tf.get_collection(<span class="string">'losses'</span>)</span><br><span class="line"><span class="keyword">for</span> var <span class="keyword">in</span> tf.get_collection(<span class="string">'losses'</span>):</span><br><span class="line">    print(var)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 建立session运行</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    init = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init)</span><br><span class="line">    losses_val = sess.run(losses)</span><br><span class="line">    print(losses_val)</span><br></pre></td></tr></table></figure><h2 id="疑问">疑问</h2><p>collection是和graph绑定在一起的，那么如果定义了很多个图，如何获得非默认图的tf.GraphKeys中定义的collection？？</p><h2 id="参考文献">参考文献</h2><p>1.<a href="https://blog.csdn.net/shenxiaolu1984/article/details/52815641" target="_blank" rel="noopener">https://blog.csdn.net/shenxiaolu1984/article/details/52815641</a><br>2.<a href="https://blog.csdn.net/hustqb/article/details/80398934" target="_blank" rel="noopener">https://blog.csdn.net/hustqb/article/details/80398934</a><br>3.<a href="https://www.tensorflow.org/api_docs/python/tf/GraphKeys?hl=zh_cn" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/GraphKeys?hl=zh_cn</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;tf-collection&quot;&gt;tf.collection&lt;/h2&gt;
&lt;p&gt;Tensorflow用graph collection来管理不同类型的对象。tf.GraphKeys中定义了默认的collection，tf通过调用各种各样的collection操作grap
      
    
    </summary>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/categories/tensorflow/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow graph和session</title>
    <link href="http://mxxhcm.github.io/2019/05/12/tensorflow-graph%E5%92%8Csession/"/>
    <id>http://mxxhcm.github.io/2019/05/12/tensorflow-graph和session/</id>
    <published>2019-05-12T13:45:04.000Z</published>
    <updated>2019-07-18T12:21:17.513Z</updated>
    
    <content type="html"><![CDATA[<h2 id="tf-graph和tf-session">tf.Graph和tf.Session</h2><p>Graph和Session之间的区别和联系。</p><ul><li>Graph定义了如何进行计算，但是并没有进行计算，graph不会hold任何值，它仅仅定义code中指定的各种operation</li><li>Session用来执行graph或者graph的一部分。它会分配资源（一个机器或者多个机器），并且会保存中间结果和variables的值。在不同session的执行过程也是分开的。</li></ul><h2 id="tf-graph">tf.Graph</h2><p>tf.Graph包含两类信息：</p><ul><li>Node和Edge，用来表示各个op如何进行组合。</li><li>collections。使用tf.add_to_collection和tf.get_collection对collection进行操作。一个常见的例子是创建tf.Variable的时候，默认会将它加入到&quot;global variables&quot;和&quot;trainable variables&quot; collection中。<br>当调用tf.train.Saver和tf.train.Optimizer的时候，它会使用这些collection中的变量作为默认参数。<br>常见的定义在tf.GraphKeys上的collection:<br>VARIABLES, TRAINABLE_VARIABLES, MOVING_AVERAGE_VARIABLES, LOCAL_VARIABLES, MODEL_VARIABLE,SUMMARIES.<br><a href="https://mxxhcm.github.io/2019/05/13/tensorflow-collection/">关于collections的详细介绍可点击这里</a></li></ul><h2 id="构建tf-graph">构建tf.Graph</h2><p>调用tensorflow API就会构建新的tf.Operation和tf.Tensor，并将他们添加到tf.Graph实例中去。</p><ul><li>调用 tf.constant(42.0) 创建单个 tf.Operation，该操作可以生成值 42.0，将该值添加到默认图中，并返回表示常量值的 tf.Tensor。</li><li>调用 tf.matmul(x, y) 可创建单个 tf.Operation，该操作会将 tf.Tensor 对象 x 和 y 的值相乘，将其添加到默认图中，并返回表示乘法运算结果的 tf.Tensor。</li><li>执行 v = tf.Variable(0) 可向图添加一个 tf.Operation，该操作可以存储一个可写入的张量值，该值在多个 tf.Session.run 调用之间保持恒定。tf.Variable 对象会封装此操作，并可以像张量一样使用，即读取已存储值的当前值。tf.Variable 对象也具有 assign 和 assign_add 等方法，这些方法可创建 tf.Operation 对象，这些对象在执行时将更新已存储的值。（请参阅变量了解关于变量的更多信息。）</li><li>调用 tf.train.Optimizer.minimize 可将操作和张量添加到计算梯度的默认图中，并返回一个 tf.Operation，该操作在运行时会将这些梯度应用到一组变量上。</li></ul><h2 id="获得默认图">获得默认图</h2><p>用 tf.get_default_graph，它会返回一个 tf.Graph 对象：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Print all of the operations in the default graph.</span></span><br><span class="line">g = tf.get_default_graph()</span><br></pre></td></tr></table></figure><h2 id="清空默认图">清空默认图</h2><p>tf.reset_default_graph()</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 清空当前session的默认图</span></span><br><span class="line">tf.reset_default_graph()</span><br></pre></td></tr></table></figure><h2 id="命名空间">命名空间</h2><p>tf.Graph 对象会定义一个命名空间（为其包含的 tf.Operation 对象）。TensorFlow 会自动为图中的每个指令选择一个唯一名称，也可以指定描述性名称，让程序阅读和调试起来更轻松。TensorFlow API 提供两种方法来指定op名称：</p><ul><li>如果API会创建新的op或返回新的 tf.Tensor，就可选 name 参数。例如，tf.constant(42.0, name=“answer”) 会创建一个新的 tf.Operation（名为 “answer”）并返回一个 tf.Tensor（名为 “answer:0”）。如果默认图已包含名为 “answer” 的操作，则 TensorFlow 会在名称上附加 “_1”、&quot;_2&quot; 等字符，以便让名称具有唯一性。</li><li>借助 tf.name_scope 函数，可以向在特定上下文中创建的所有op添加name_scope。当前name_scope是一个用 “/” 分隔的名称列表，其中包含所有活跃的 tf.name_scope 上下文管理器名称。如果某个name_scope已在当前上下文中被占用，TensorFlow 将在该作用域上附加 “_1”、&quot;_2&quot; 等字符。例如：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">c_0 = tf.constant(<span class="number">0</span>, name=<span class="string">"c"</span>)  <span class="comment"># =&gt; operation named "c"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Already-used names will be "uniquified".</span></span><br><span class="line">c_1 = tf.constant(<span class="number">2</span>, name=<span class="string">"c"</span>)  <span class="comment"># =&gt; operation named "c_1"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Name scopes add a prefix to all operations created in the same context.</span></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">"outer"</span>):</span><br><span class="line">  c_2 = tf.constant(<span class="number">2</span>, name=<span class="string">"c"</span>)  <span class="comment"># =&gt; operation named "outer/c"</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Name scopes nest like paths in a hierarchical file system.</span></span><br><span class="line">  <span class="keyword">with</span> tf.name_scope(<span class="string">"inner"</span>):</span><br><span class="line">    c_3 = tf.constant(<span class="number">3</span>, name=<span class="string">"c"</span>)  <span class="comment"># =&gt; operation named "outer/inner/c"</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Exiting a name scope context will return to the previous prefix.</span></span><br><span class="line">  c_4 = tf.constant(<span class="number">4</span>, name=<span class="string">"c"</span>)  <span class="comment"># =&gt; operation named "outer/c_1"</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Already-used name scopes will be "uniquified".</span></span><br><span class="line">  <span class="keyword">with</span> tf.name_scope(<span class="string">"inner"</span>):</span><br><span class="line">    c_5 = tf.constant(<span class="number">5</span>, name=<span class="string">"c"</span>)  <span class="comment"># =&gt; operation named "outer/inner_1/c"</span></span><br></pre></td></tr></table></figure><p>请注意，tf.Tensor 对象以输出张量的op明确命名。张量名称的形式为 “&lt;OP_NAME&gt;:&lt;i&gt;”，其中：</p><ul><li>“&lt;OP_NAME&gt;” 是生成该张量的操作的名称。</li><li>“&lt;i&gt;” 是一个整数，表示该张量在该op的输出中的索引。</li></ul><h2 id="获得图中的op">获得图中的op</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">c_0 = tf.constant(<span class="number">0</span>, name=<span class="string">"c"</span>)  <span class="comment"># =&gt; operation named "c"</span></span><br><span class="line"><span class="comment"># Already-used names will be "uniquified".  c_1 = tf.constant(2, name="c")  # =&gt; operation named "c_1"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Name scopes add a prefix to all operations created in the same context.</span></span><br><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">"outer"</span>):</span><br><span class="line">  c_2 = tf.constant(<span class="number">2</span>, name=<span class="string">"c"</span>)  <span class="comment"># =&gt; operation named "outer/c"</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Name scopes nest like paths in a hierarchical file system.</span></span><br><span class="line">  <span class="keyword">with</span> tf.name_scope(<span class="string">"inner"</span>):</span><br><span class="line">    c_3 = tf.constant(<span class="number">3</span>, name=<span class="string">"c"</span>)  <span class="comment"># =&gt; operation named "outer/inner/c"</span></span><br><span class="line"></span><br><span class="line">g = tf.get_default_graph()</span><br><span class="line">print(g.get_operations())</span><br><span class="line"><span class="comment"># [&lt;tf.Operation 'c' type=Const&gt;, &lt;tf.Operation 'c_1' type=Const&gt;, &lt;tf.Operation 'outer/c' type=Const&gt;, &lt;tf.Operation 'outer/inner/c' type=Const&gt;]</span></span><br></pre></td></tr></table></figure><h2 id="类张量对象">类张量对象</h2><p>许多 TensorFlow op都会接受一个或多个 tf.Tensor 对象作为参数。例如，tf.matmul 接受两个 tf.Tensor 对象，tf.add_n 接受一个具有 n 个 tf.Tensor 对象的列表。为了方便起见，这些函数将接受类张量对象来取代 tf.Tensor，并将它明确转换为 tf.Tensor（通过 tf.convert_to_tensor 方法）。类张量对象包括以下类型的元素：</p><ul><li>tf.Tensor</li><li>tf.Variable</li><li>numpy.ndarray</li><li>list（以及类似于张量的对象的列表）</li><li>标量 Python 类型：bool、float、int、str</li></ul><p><strong>注意</strong> 默认情况下，每次使用同一个类张量对象时，TensorFlow 将创建新的 tf.Tensor。如果类张量对象很大（例如包含一组训练样本的 numpy.ndarray），且多次使用该对象，则可能会耗尽内存。要避免出现此问题，请在类张量对象上手动调用 tf.convert_to_tensor 一次，并使用返回的 tf.Tensor。</p><h2 id="tf-session">tf.Session</h2><h3 id="api">API</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tf.Session.init(</span><br><span class="line">target, <span class="comment"># 可选参数，指定设备。</span></span><br><span class="line">graph, <span class="comment">#可选参数，默认情况下，新的session绑定到默认graph</span></span><br><span class="line">confi <span class="comment"># 可选参数，常见的一个选择为gpu_options.allow_growth。将此参数设置为 True 可更改 GPU 内存分配器，使该分配器逐渐增加分配的内存量，而不是在启动时分配掉大多数内存。</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="创建session">创建session</h3><h4 id="默认session">默认session</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create a default in-process session.</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  <span class="comment"># ...</span></span><br></pre></td></tr></table></figure><h4 id="none"></h4><h3 id="执行op">执行op</h3><p>tf.Session.run 方法是运行 tf.Operation 或评估 tf.Tensor 的主要机制。传入一个或多个 tf.Operation 或 tf.Tensor 对象到 tf.Session.run，TensorFlow 将执行计算结果所需的操作。<br>tf.Session.run 需要指定一组 fetch，这些 fetch 可确定返回值，并且可能是 tf.Operation、tf.Tensor 或类张量类型，例如 tf.Variable。这些 fetch 决定了必须执行哪些子图（属于整体 tf.Graph）以生成结果：该子图包含 fetch 列表中指定的所有op，以及其输出用于计算 fetch 值的所有操作。例如，以下代码段说明了 tf.Session.run 的不同参数如何导致执行不同的子图：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">x = tf.constant([[<span class="number">37.0</span>, <span class="number">-23.0</span>], [<span class="number">1.0</span>, <span class="number">4.0</span>]])</span><br><span class="line">w = tf.Variable(tf.random_uniform([<span class="number">2</span>, <span class="number">2</span>]))</span><br><span class="line">y = tf.matmul(x, w)</span><br><span class="line">output = tf.nn.softmax(y)</span><br><span class="line">init_op = w.initializer</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  <span class="comment"># 初始化w</span></span><br><span class="line">  sess.run(init_op)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Evaluate `output`. `sess.run(output)` will return a NumPy array containing</span></span><br><span class="line">  <span class="comment"># the result of the computation.</span></span><br><span class="line">  <span class="comment"># 计算output</span></span><br><span class="line">  print(sess.run(output))</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Evaluate `y` and `output`. Note that `y` will only be computed once, and its</span></span><br><span class="line">  <span class="comment"># result used both to return `y_val` and as an input to the `tf.nn.softmax()`</span></span><br><span class="line">  <span class="comment"># op. Both `y_val` and `output_val` will be NumPy arrays.</span></span><br><span class="line">  <span class="comment"># 计算y和output</span></span><br><span class="line">  y_val, output_val = sess.run([y, output])</span><br></pre></td></tr></table></figure><p>tf.Session.run 也可以接受 feed dict，该字典是从 tf.Tensor 对象（通常是 tf.placeholder 张量），在执行时会替换这些张量的值（通常是 Python 标量、列表或 NumPy 数组）的映射。例如：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Define a placeholder that expects a vector of three floating-point values,</span></span><br><span class="line"><span class="comment"># and a computation that depends on it.</span></span><br><span class="line">x = tf.placeholder(tf.float32, shape=[<span class="number">3</span>])</span><br><span class="line">y = tf.square(x)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  <span class="comment"># Feeding a value changes the result that is returned when you evaluate `y`.</span></span><br><span class="line">  print(sess.run(y, &#123;x: [<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>]&#125;))  <span class="comment"># =&gt; "[1.0, 4.0, 9.0]"</span></span><br><span class="line">  print(sess.run(y, &#123;x: [<span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">5.0</span>]&#125;))  <span class="comment"># =&gt; "[0.0, 0.0, 25.0]"</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Raises &lt;a href="../api_docs/python/tf/errors/InvalidArgumentError"&gt;&lt;code&gt;tf.errors.InvalidArgumentError&lt;/code&gt;&lt;/a&gt;, because you must feed a value for</span></span><br><span class="line">  <span class="comment"># a `tf.placeholder()` when evaluating a tensor that depends on it.</span></span><br><span class="line">  sess.run(y)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Raises `ValueError`, because the shape of `37.0` does not match the shape</span></span><br><span class="line">  <span class="comment"># of placeholder `x`.</span></span><br><span class="line">  sess.run(y, &#123;x: <span class="number">37.0</span>&#125;)</span><br></pre></td></tr></table></figure><p>tf.Session.run 也接受可选的 options 参数（允许指定与调用有关的选项）和可选的 run_metadata 参数（允许收集与执行有关的元数据）。例如，可以同时使用这些选项来收集与执行有关的跟踪信息：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">y = tf.matmul([[<span class="number">37.0</span>, <span class="number">-23.0</span>], [<span class="number">1.0</span>, <span class="number">4.0</span>]], tf.random_uniform([<span class="number">2</span>, <span class="number">2</span>]))</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  <span class="comment"># Define options for the `sess.run()` call.</span></span><br><span class="line">  options = tf.RunOptions()</span><br><span class="line">  options.output_partition_graphs = <span class="literal">True</span></span><br><span class="line">  options.trace_level = tf.RunOptions.FULL_TRACE</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Define a container for the returned metadata.</span></span><br><span class="line">  metadata = tf.RunMetadata()</span><br><span class="line"></span><br><span class="line">  sess.run(y, options=options, run_metadata=metadata)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Print the subgraphs that executed on each device.</span></span><br><span class="line">  print(metadata.partition_graphs)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Print the timings of each operation that executed.</span></span><br><span class="line">  print(metadata.step_stats)</span><br></pre></td></tr></table></figure><h2 id="不同session的结果">不同session的结果</h2><p><a href>代码地址</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">graph = tf.Graph()</span><br><span class="line"></span><br><span class="line">with graph.as_default():</span><br><span class="line">    variable = tf.Variable(10, name=&quot;foo&quot;)</span><br><span class="line">    initialize = tf.global_variables_initializer()</span><br><span class="line">    assign = variable.assign(12)</span><br><span class="line"></span><br><span class="line">with tf.Session(graph=graph) as sess:</span><br><span class="line">    sess.run(initialize)</span><br><span class="line">    sess.run(assign)</span><br><span class="line">    print(sess.run(variable))</span><br><span class="line"></span><br><span class="line">with tf.Session(graph=graph) as sess:</span><br><span class="line">    print(sess.run(variable))</span><br></pre></td></tr></table></figure><h2 id="访问当前sess的图">访问当前sess的图。</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sess = tf.Session()</span><br><span class="line">sess.graph</span><br></pre></td></tr></table></figure><h2 id="可视化图">可视化图</h2><p>使用图可视化工具。最简单的方法是传递tf.Graph到tf.summary.FileWriter中。如下示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Build your graph.</span></span><br><span class="line">x = tf.constant([[<span class="number">37.0</span>, <span class="number">-23.0</span>], [<span class="number">1.0</span>, <span class="number">4.0</span>]])</span><br><span class="line">w = tf.Variable(tf.random_uniform([<span class="number">2</span>, <span class="number">2</span>]))</span><br><span class="line">y = tf.matmul(x, w)</span><br><span class="line"><span class="comment"># ...</span></span><br><span class="line">loss = ...</span><br><span class="line">train_op = tf.train.AdagradOptimizer(<span class="number">0.01</span>).minimize(loss)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  <span class="comment"># `sess.graph` provides access to the graph used in a &lt;a href="../api_docs/python/tf/Session"&gt;&lt;code&gt;tf.Session&lt;/code&gt;&lt;/a&gt;.</span></span><br><span class="line">  writer = tf.summary.FileWriter(<span class="string">"/tmp/log/..."</span>, sess.graph)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Perform your computation...</span></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">    sess.run(train_op)</span><br><span class="line">    <span class="comment"># ...</span></span><br><span class="line"></span><br><span class="line">  writer.close()</span><br></pre></td></tr></table></figure><p>然后可以在 tensorboard 中打开日志并转到“图”标签，查看图结构的概要可视化图表。</p><h2 id="创建多个图">创建多个图</h2><p>TensorFlow 提供了一个“默认图”，此图明确传递给同一上下文中的所有 API 函数。TensorFlow 提供了操作默认图的方法，在更高级的用例中，这些方法可能有用。</p><ul><li>tf.Graph 会定义 tf.Operation 对象的命名空间：单个图中的每个操作必须具有唯一名称。如果请求的名称已被占用，TensorFlow 将在操作名称上附加 “_1”、&quot;_2&quot; 等字符，以便确保名称的唯一性。通过使用多个明确创建的图，可以更有效地控制为每个op指定什么样的名称。</li><li>默认图会存储与添加的每个 tf.Operation 和 tf.Tensor 有关的信息。如果程序创建了大量未连接的子图，更有效的做法是使用另一个 tf.Graph 构建每个子图，以便回收不相关的状态。</li></ul><h3 id="创建两个图">创建两个图</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">g_1 = tf.Graph()</span><br><span class="line"><span class="keyword">with</span> g_1.as_default():</span><br><span class="line">  <span class="comment"># Operations created in this scope will be added to `g_1`.</span></span><br><span class="line">  c = tf.constant(<span class="string">"Node in g_1"</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Sessions created in this scope will run operations from `g_1`.</span></span><br><span class="line">  sess_1 = tf.Session()</span><br><span class="line"></span><br><span class="line">g_2 = tf.Graph()</span><br><span class="line"><span class="keyword">with</span> g_2.as_default():</span><br><span class="line">  <span class="comment"># Operations created in this scope will be added to `g_2`.</span></span><br><span class="line">  d = tf.constant(<span class="string">"Node in g_2"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Alternatively, you can pass a graph when constructing a &lt;a href="../api_docs/python/tf/Session"&gt;&lt;code&gt;tf.Session&lt;/code&gt;&lt;/a&gt;:</span></span><br><span class="line"><span class="comment"># `sess_2` will run operations from `g_2`.</span></span><br><span class="line">sess_2 = tf.Session(graph=g_2)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> c.graph <span class="keyword">is</span> g_1</span><br><span class="line"><span class="keyword">assert</span> sess_1.graph <span class="keyword">is</span> g_1</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> d.graph <span class="keyword">is</span> g_2</span><br><span class="line"><span class="keyword">assert</span> sess_2.graph <span class="keyword">is</span> g_2</span><br></pre></td></tr></table></figure><h2 id="参考文献">参考文献</h2><p>1.<a href="https://www.tensorflow.org/guide/graphs?hl=zh_cn" target="_blank" rel="noopener">https://www.tensorflow.org/guide/graphs?hl=zh_cn</a><br>2.<a href="https://blog.csdn.net/shenxiaolu1984/article/details/52815641" target="_blank" rel="noopener">https://blog.csdn.net/shenxiaolu1984/article/details/52815641</a><br>3.<a href="https://danijar.com/what-is-a-tensorflow-session/" target="_blank" rel="noopener">https://danijar.com/what-is-a-tensorflow-session/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;tf-graph和tf-session&quot;&gt;tf.Graph和tf.Session&lt;/h2&gt;
&lt;p&gt;Graph和Session之间的区别和联系。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Graph定义了如何进行计算，但是并没有进行计算，graph不会hold任何值，它仅仅定义co
      
    
    </summary>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/categories/tensorflow/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow Varaible</title>
    <link href="http://mxxhcm.github.io/2019/05/12/tensorflow-Varaible/"/>
    <id>http://mxxhcm.github.io/2019/05/12/tensorflow-Varaible/</id>
    <published>2019-05-12T12:41:34.000Z</published>
    <updated>2019-07-09T12:27:12.102Z</updated>
    
    <content type="html"><![CDATA[<h2 id="创建variable">创建Variable</h2><p>Tensorflow有两种方式创建Variable：tf.Variable()和tf.get_variable()，这两种方式获得的都是tensorflow.python.ops.variables.Variable类型的对象，但是他们的输入参数还有些不一样。</p><table><thead><tr><th style="text-align:center"></th><th style="text-align:center">tf.Variable()</th><th style="text-align:center">tf.get_variable()</th></tr></thead><tbody><tr><td style="text-align:center">name</td><td style="text-align:center">不需要，已存在的变量名，会在后面加上递增的数值用来区分</td><td style="text-align:center">必须，已存在的会报错</td></tr><tr><td style="text-align:center">shape</td><td style="text-align:center">不需要，或者说已经包含在初值中了</td><td style="text-align:center">需要</td></tr><tr><td style="text-align:center">初值</td><td style="text-align:center">必须</td><td style="text-align:center">不需要</td></tr><tr><td style="text-align:center">复用</td><td style="text-align:center">不可以</td><td style="text-align:center">可以</td></tr></tbody></table><p>两种方法事实上都可以指定name和初值。而tf.Variable()的初值中已经包含了shape，所以不需要再显示传入shape了。这里的需要和不需要指的是必要不必要，如果没有传入需要的参数，就会报错，不需要的参数则不会影响。</p><h2 id="tf-variable">tf.Variable()</h2><h3 id="一句话介绍">一句话介绍</h3><p>创建一个类操作全局变量。在TensorFlow内部，tf.Variable会存储持久性张量，允许各种op读取和修改它的值。这些修改在多个Session之间是可见的，因此对于一个tf.Variable，多个工作器可以看到相同的值。</p><h3 id="和tf-tensor对比">和tf.Tensor对比</h3><p>tf.Variable 表示可通过对其运行op来改变其值的张量。与 tf.Tensor对象不同，tf.Variable 存在于单个session.run调用的上下文之外。tf.Tensor的值是不可以改变的，tf.Tensor没有assign函数。</p><h3 id="api">API</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">tf.Variable.__init__(</span><br><span class="line">initial_value=<span class="literal">None</span>,  <span class="comment"># 指定变量的初值</span></span><br><span class="line">trainable=<span class="literal">True</span>,  <span class="comment"># 是否在BP时训练该参数</span></span><br><span class="line">collections=<span class="literal">None</span>, <span class="comment"># 指定变量的collection</span></span><br><span class="line">validate_shape=<span class="literal">True</span>, </span><br><span class="line">caching_device=<span class="literal">None</span>, </span><br><span class="line">name=<span class="literal">None</span>,  <span class="comment"># 指定变量的名字</span></span><br><span class="line">...</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="代码示例">代码示例</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor1 = tf.Variable([[<span class="number">1</span>,<span class="number">2</span>], [<span class="number">3</span>,<span class="number">5</span>]])</span><br><span class="line">tensor2 = tf.Variable(tf.constant([[<span class="number">1</span>,<span class="number">2</span>], [<span class="number">3</span>,<span class="number">5</span>]]))</span><br><span class="line">sess.run(tf.global_variables_initializer())</span><br><span class="line">sess.run(tensor1)</span><br><span class="line">sess.run(tensor2)</span><br></pre></td></tr></table></figure><h3 id="初始化">初始化</h3><p>tf.Variable()生成的变量必须初始化，tf.constant()可以不用初始化。</p><ul><li>使用全局初始化<br>sess.run(tf.global_variables_initializer())</li><li>使用checkpoint</li><li>使用tf.assign赋值</li></ul><h2 id="tf-get-variable">tf.get_variable()</h2><h3 id="一句话介绍-v2">一句话介绍</h3><p>获取一个已经存在的变量或者创建一个新的变量。主要目的，变量复用。</p><h3 id="api-v2">API</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">tf.get_variable(</span><br><span class="line">    name, <span class="comment"># 指定变量的名字，必选项</span></span><br><span class="line">    shape=<span class="literal">None</span>, <span class="comment"># 指定变量的shape，可选项</span></span><br><span class="line">    dtype=<span class="literal">None</span>, <span class="comment"># 指定变量类型</span></span><br><span class="line">    initializer=<span class="literal">None</span>, <span class="comment"># 指定变量初始化器</span></span><br><span class="line">    regularizer=<span class="literal">None</span>,</span><br><span class="line">    trainable=<span class="literal">None</span>,</span><br><span class="line">    collections=<span class="literal">None</span>,</span><br><span class="line">    caching_device=<span class="literal">None</span>,</span><br><span class="line">    partitioner=<span class="literal">None</span>,</span><br><span class="line">    validate_shape=<span class="literal">True</span>,</span><br><span class="line">    use_resource=<span class="literal">None</span>,</span><br><span class="line">    custom_getter=<span class="literal">None</span>,</span><br><span class="line">    constraint=<span class="literal">None</span>,</span><br><span class="line">    synchronization=tf.VariableSynchronization.AUTO,</span><br><span class="line">    aggregation=tf.VariableAggregation.NONE</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="代码示例-v2">代码示例</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"model"</span>) <span class="keyword">as</span> scope:</span><br><span class="line">  output1 = my_image_filter(input1)</span><br><span class="line">  scope.reuse_variables()</span><br><span class="line">  output2 = my_image_filter(input2)</span><br></pre></td></tr></table></figure><h2 id="variable和collection">Variable和collection</h2><p><a href="https://mxxhcm.github.io/2019/05/13/tensorflow-collection/">点击查看关于collecion的详细介绍</a><br>默认情况下，每个tf.Variable()都会添加到以下两个collection中：</p><ul><li>tf.GraphKeys.GLOBAL_VARIABLES - 可以在多台设备间共享的变量，</li><li>tf.GraphKeys.TRAINABLE_VARIABLES - TensorFlow 将计算其梯度的变量。</li></ul><p>如果不希望变量是可训练的，可以在创建时指定其collection为 tf.GraphKeys.LOCAL_VARIABLES collection中。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">my_local = tf.get_variable(<span class="string">"my_local"</span>, shape=(), collections=[tf.GraphKeys.LOCAL_VARIABLES])</span><br></pre></td></tr></table></figure><p>或者可以指定 trainable=False：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">my_non_trainable = tf.get_variable(<span class="string">"my_non_trainable"</span>,</span><br><span class="line">                                   shape=(),</span><br><span class="line">                                   trainable=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><h3 id="获取collection">获取collection</h3><p>要检索放在某个collection中的所有变量的列表，可以使用：</p><h4 id="代码示例-v3">代码示例</h4><p><a href="https://github.com/mxxhcm/code/tree/master/tf/ops/tf_Variable_collection.py" target="_blank" rel="noopener">代码地址</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">a = tf.Variable([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">b = tf.get_variable(<span class="string">"bbb"</span>, shape=[<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">tf.constant([<span class="number">3</span>])</span><br><span class="line">c = tf.ones([<span class="number">3</span>])</span><br><span class="line">d = tf.random_uniform([<span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">print(tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES))</span><br><span class="line"><span class="comment"># [&lt;tf.Variable 'Variable:0' shape=(3,) dtype=int32_ref&gt;, &lt;tf.Variable 'bbb:0' shape=(2, 3) dtype=float32_ref&gt;]</span></span><br><span class="line"><span class="comment"># 可以看出来，只有tf.Variable()和tf.get_variable()产生的变量会加入到这个图中</span></span><br></pre></td></tr></table></figure><h3 id="自定义collection">自定义collection</h3><h4 id="添加自定义collection">添加自定义collection</h4><p>可以使用自定义的collection。collection名称可为任何字符串，且无需显式创建。创建对象（包括Variable和其他）后调用 tf.add_to_collection将其添加到相应collection中。以下代码将 my_local 变量添加到名为 my_collection_name 的collection中：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.add_to_collection(<span class="string">"my_collection_name"</span>, my_local)</span><br></pre></td></tr></table></figure><h2 id="初始化变量">初始化变量</h2><h3 id="初始化所有变量">初始化所有变量</h3><p>调用 tf.global_variables_initializer()在训练开始前一次性初始化所有可训练变量。此函数会返回一个op，负责初始化 tf.GraphKeys.GLOBAL_VARIABLES collection中的所有变量。运行此op会初始化所有变量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sess.run(tf.global_variables_initializer())</span><br></pre></td></tr></table></figure><h3 id="初始化单个变量">初始化单个变量</h3><p>运行变量的初始化器op。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sess.run(my_variable.initializer)</span><br></pre></td></tr></table></figure><h3 id="查询未初始化变量">查询未初始化变量</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(sess.run(tf.report_uninitialized_variables()))</span><br></pre></td></tr></table></figure><h2 id="共享变量">共享变量</h2><p>TensorFlow 支持两种共享变量的方式：</p><ul><li>显式传递 tf.Variable 对象。</li><li>将 tf.Variable 对象隐式封装在 tf.variable_scope 对象内。</li></ul><h3 id="variable-scope">variable_scope</h3><h4 id="代码示例1">代码示例1</h4><p>使用variable_scope区分weights和biases。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_relu</span><span class="params">(input, kernel_shape, bias_shape)</span>:</span></span><br><span class="line">    <span class="comment"># Create variable named "weights".</span></span><br><span class="line">    weights = tf.get_variable(<span class="string">"weights"</span>, kernel_shape,</span><br><span class="line">        initializer=tf.random_normal_initializer())</span><br><span class="line">    <span class="comment"># Create variable named "biases".</span></span><br><span class="line">    biases = tf.get_variable(<span class="string">"biases"</span>, bias_shape,</span><br><span class="line">        initializer=tf.constant_initializer(<span class="number">0.0</span>))</span><br><span class="line">    conv = tf.nn.conv2d(input, weights,</span><br><span class="line">        strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line">    <span class="keyword">return</span> tf.nn.relu(conv + biases)</span><br></pre></td></tr></table></figure><h4 id="代码示例2">代码示例2</h4><p>使用variable_scope声明不同作用域</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_image_filter</span><span class="params">(input_images)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"conv1"</span>):</span><br><span class="line">        <span class="comment"># Variables created here will be named "conv1/weights", "conv1/biases".</span></span><br><span class="line">        relu1 = conv_relu(input_images, [<span class="number">5</span>, <span class="number">5</span>, <span class="number">32</span>, <span class="number">32</span>], [<span class="number">32</span>])</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"conv2"</span>):</span><br><span class="line">        <span class="comment"># Variables created here will be named "conv2/weights", "conv2/biases".</span></span><br><span class="line">        <span class="keyword">return</span> conv_relu(relu1, [<span class="number">5</span>, <span class="number">5</span>, <span class="number">32</span>, <span class="number">32</span>], [<span class="number">32</span>])</span><br></pre></td></tr></table></figure><h3 id="共享方式1">共享方式1</h3><p>设置reuse=True</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"model"</span>):</span><br><span class="line">  output1 = my_image_filter(input1)</span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"model"</span>, reuse=<span class="literal">True</span>):</span><br><span class="line">  output2 = my_image_filter(input2)</span><br></pre></td></tr></table></figure><h3 id="共享方式2">共享方式2</h3><p>调用scope.reuse_variables触发重用</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"model"</span>) <span class="keyword">as</span> scope:</span><br><span class="line">  output1 = my_image_filter(input1)</span><br><span class="line">  scope.reuse_variables()</span><br><span class="line">  output2 = my_image_filter(input2)</span><br></pre></td></tr></table></figure><h2 id="参考文献">参考文献</h2><p>1.<a href="https://blog.csdn.net/MrR1ght/article/details/81228087" target="_blank" rel="noopener">https://blog.csdn.net/MrR1ght/article/details/81228087</a><br>2.<a href="https://www.tensorflow.org/guide/variables?hl=zh_cn" target="_blank" rel="noopener">https://www.tensorflow.org/guide/variables?hl=zh_cn</a><br>3.<a href="https://www.tensorflow.org/api_docs/python/tf/get_variable?hl=zh_cn" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/get_variable?hl=zh_cn</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;创建variable&quot;&gt;创建Variable&lt;/h2&gt;
&lt;p&gt;Tensorflow有两种方式创建Variable：tf.Variable()和tf.get_variable()，这两种方式获得的都是tensorflow.python.ops.variables.V
      
    
    </summary>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/categories/tensorflow/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow list of placeholder</title>
    <link href="http://mxxhcm.github.io/2019/05/12/tensorflow-list-of-placeholder/"/>
    <id>http://mxxhcm.github.io/2019/05/12/tensorflow-list-of-placeholder/</id>
    <published>2019-05-12T07:55:49.000Z</published>
    <updated>2019-05-12T12:31:11.231Z</updated>
    
    <content type="html"><![CDATA[<h2 id="list-of-placeholder">list of placeholder</h2><h3 id="目的">目的</h3><p>计算图中定义了一个placeholder list，如何使用feed_dict传入值。</p><h3 id="代码示例">代码示例</h3><p><a href="https://github.com/mxxhcm/code/blob/master/tf/ops/tf_placeholder_list.py" target="_blank" rel="noopener">代码地址</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个长度为n的placeholder list</span></span><br><span class="line">n = <span class="number">4</span></span><br><span class="line">ph_list = [tf.placeholder(tf.float32, [<span class="literal">None</span>, <span class="number">10</span>]) <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">4</span>)]</span><br><span class="line"><span class="comment"># 对这个ph list的操作</span></span><br><span class="line">result = tf.Variable(<span class="number">0.0</span>)</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> ph_list:</span><br><span class="line">    result = tf.add(result, x)</span><br><span class="line">hhhh = tf.log(result)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    sess = tf.Session()</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 生成数据</span></span><br><span class="line">    inputs = []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(n):</span><br><span class="line">        x = np.random.rand(<span class="number">16</span>, <span class="number">10</span>)</span><br><span class="line">        inputs.append(x)</span><br><span class="line">    <span class="comment"># 声明一个字典，存放placeholder和value键值对</span></span><br><span class="line">    feed_dictionary = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> k,v <span class="keyword">in</span> zip(ph_list, inputs):</span><br><span class="line">       feed_dictionary[k] = v</span><br><span class="line">    <span class="comment"># feed 数据</span></span><br><span class="line">    print(sess.run(hhhh, feed_dict=feed_dictionary).shape)</span><br></pre></td></tr></table></figure><h2 id="参考文献">参考文献</h2><p>1.<a href="https://stackoverflow.com/questions/51128427/how-to-feed-list-of-values-to-a-placeholder-list-in-tensorflow" target="_blank" rel="noopener">https://stackoverflow.com/questions/51128427/how-to-feed-list-of-values-to-a-placeholder-list-in-tensorflow</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;list-of-placeholder&quot;&gt;list of placeholder&lt;/h2&gt;
&lt;h3 id=&quot;目的&quot;&gt;目的&lt;/h3&gt;
&lt;p&gt;计算图中定义了一个placeholder list，如何使用feed_dict传入值。&lt;/p&gt;
&lt;h3 id=&quot;代码示例&quot;&gt;代
      
    
    </summary>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/categories/tensorflow/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow gather</title>
    <link href="http://mxxhcm.github.io/2019/05/11/tensorflow-gather/"/>
    <id>http://mxxhcm.github.io/2019/05/11/tensorflow-gather/</id>
    <published>2019-05-11T13:03:00.000Z</published>
    <updated>2019-05-12T12:35:59.200Z</updated>
    
    <content type="html"><![CDATA[<h2 id="tf-gather-nd">tf.gather_nd</h2><h3 id="一句话介绍">一句话介绍</h3><p>按照索引将输入tensor的某些维度拼凑成一个新的tenosr</p><h3 id="api">API</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tf.gather_nd(</span><br><span class="line">    params, <span class="comment"># 输入参数</span></span><br><span class="line">    indices, <span class="comment"># 索引</span></span><br><span class="line">    name=<span class="literal">None</span> <span class="comment">#</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>indices是一个K维的整形tensor。<br>indices的最后一维至多和params的rank一样大，如果indices.shape==params.rank，那么对应的是elements，如果indices.shape $\lt$ params.rank，那么对应的是slices。输出的tensor shape是：<br>indices.shape[:-1] + params.shape[indices.shape[-1]:]<br>原文如下：</p><blockquote><p>The last dimension of indices corresponds to elements (if indices.shape[-1] == params.rank) or slices (if indices.shape[-1] &lt; params.rank) along dimension indices.shape[-1] of params. The output tensor has shape<br>indices.shape[:-1] + params.shape[indices.shape[-1]:]</p></blockquote><p>如果indices是两维的，那么就相当于用第二维的indices去访问params，然后indices的第一维度相当于把第二维的tensor放入一个列表。<br>indices是高维（大于两维）的话，反正就是找最后一维的维度，然后到params中找对应的数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">    indices = [[[<span class="number">1</span>]], [[<span class="number">0</span>]]]</span><br><span class="line">    params = [[[<span class="string">'a0'</span>, <span class="string">'b0'</span>], [<span class="string">'c0'</span>, <span class="string">'d0'</span>]],</span><br><span class="line">              [[<span class="string">'a1'</span>, <span class="string">'b1'</span>], [<span class="string">'c1'</span>, <span class="string">'d1'</span>]]]</span><br><span class="line">    output = [[[[<span class="string">'a1'</span>, <span class="string">'b1'</span>], [<span class="string">'c1'</span>, <span class="string">'d1'</span>]]],</span><br><span class="line">              [[[<span class="string">'a0'</span>, <span class="string">'b0'</span>], [<span class="string">'c0'</span>, <span class="string">'d0'</span>]]]]</span><br><span class="line"><span class="comment"># 直接看indices的最后一维，然后到params中找，比如[1]，找params[1]=[['a1', 'b1'], ['c1', 'd1']]],params[0]=[['a0', 'b0'], ['c0', 'd0']]。然后在组成output，shape怎么确定？我的理解是，直接用params[1]的结果去替换indices中的[1]，也就是[[params[1]]]</span></span><br><span class="line"></span><br><span class="line">    indices = [[[<span class="number">0</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">0</span>]], [[<span class="number">0</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">1</span>]]]</span><br><span class="line">    params = [[[<span class="string">'a0'</span>, <span class="string">'b0'</span>], [<span class="string">'c0'</span>, <span class="string">'d0'</span>]],</span><br><span class="line">              [[<span class="string">'a1'</span>, <span class="string">'b1'</span>], [<span class="string">'c1'</span>, <span class="string">'d1'</span>]]]</span><br><span class="line">    output = [[[<span class="string">'c0'</span>, <span class="string">'d0'</span>], [<span class="string">'a1'</span>, <span class="string">'b1'</span>]],</span><br><span class="line">              [[<span class="string">'a0'</span>, <span class="string">'b0'</span>], [<span class="string">'c1'</span>, <span class="string">'d1'</span>]]]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    indices = [[[<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>]], [[<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>]]]</span><br><span class="line">    params = [[[<span class="string">'a0'</span>, <span class="string">'b0'</span>], [<span class="string">'c0'</span>, <span class="string">'d0'</span>]],</span><br><span class="line">              [[<span class="string">'a1'</span>, <span class="string">'b1'</span>], [<span class="string">'c1'</span>, <span class="string">'d1'</span>]]]</span><br><span class="line">    output = [[<span class="string">'b0'</span>, <span class="string">'b1'</span>], [<span class="string">'d0'</span>, <span class="string">'c1'</span>]]</span><br></pre></td></tr></table></figure><h3 id="代码示例1">代码示例1</h3><p><a href>代码地址</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line"></span><br><span class="line">data = np.array([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>],</span><br><span class="line">          [<span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>],</span><br><span class="line">          [<span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>, <span class="number">16</span>, <span class="number">17</span>],</span><br><span class="line">          [<span class="number">18</span>, <span class="number">19</span>, <span class="number">20</span>, <span class="number">21</span>, <span class="number">22</span>, <span class="number">23</span>],</span><br><span class="line">          [<span class="number">24</span>, <span class="number">25</span>, <span class="number">26</span>, <span class="number">27</span>, <span class="number">28</span>, <span class="number">29</span>]])</span><br><span class="line">data = np.reshape(np.arange(<span class="number">30</span>), [<span class="number">5</span>, <span class="number">6</span>])</span><br><span class="line">x = tf.constant(data)</span><br><span class="line">print(sess.run(x))</span><br><span class="line"><span class="comment"># [[ 0  1  2  3  4  5]</span></span><br><span class="line"><span class="comment">#  [ 6  7  8  9 10 11]</span></span><br><span class="line"><span class="comment">#  [12 13 14 15 16 17]</span></span><br><span class="line"><span class="comment">#  [18 19 20 21 22 23]</span></span><br><span class="line"><span class="comment">#  [24 25 26 27 28 29]]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Collecting elements from a tensor of rank 2</span></span><br><span class="line">result = tf.gather_nd(x, [<span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line">print(sess.run(result))</span><br><span class="line"><span class="comment"># indices.shape=(2,), indices.shape[:-1]=(), indices.shape[-1]=2, params.shape=(5,6), params.shape[indices.shape[-1]:]=(), outputs.shape=()+() = () </span></span><br><span class="line"><span class="comment"># 8 </span></span><br><span class="line">result = tf.gather_nd(x, [[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">2</span>,<span class="number">3</span>]])</span><br><span class="line">print(sess.run(result))</span><br><span class="line"><span class="comment"># indices.shape=(2,2), indices.shape[:-1]=(2,), indices.shape[-1]=2, params.shape=(5,6), params.shape[indices.shape[-1]:]=(), outputs.shape=(2,)+() = (2,) </span></span><br><span class="line"><span class="comment"># [8, 15]</span></span><br><span class="line"><span class="comment"># Collecting rows from a tensor of rank 2</span></span><br><span class="line">result = tf.gather_nd(x, [[<span class="number">1</span>],[<span class="number">2</span>]])</span><br><span class="line">print(sess.run(result))</span><br><span class="line"><span class="comment"># indices.shape=(2, 1), indices.shape[:-1]=(2,), indices.shape[-1]=1, params.shape=(5,6), params.shape[indices.shape[-1]:]=(6,), outputs.shape=(2,)+(6,) = (2,6,) </span></span><br><span class="line"><span class="comment"># [[ 6  7  8  9 10 11]</span></span><br><span class="line"><span class="comment">#  [12 13 14 15 16 17]]</span></span><br></pre></td></tr></table></figure><h3 id="代码示例2">代码示例2</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line"></span><br><span class="line">data = np.array([[[<span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">          [<span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">          [<span class="number">4</span>, <span class="number">5</span>]],</span><br><span class="line">         [[<span class="number">6</span>, <span class="number">7</span>],</span><br><span class="line">          [<span class="number">8</span>, <span class="number">9</span>],</span><br><span class="line">          [<span class="number">10</span>,<span class="number">11</span>]]])</span><br><span class="line">data = np.reshape(np.arange(<span class="number">12</span>), [<span class="number">2</span>, <span class="number">3</span>, <span class="number">2</span>])</span><br><span class="line">x = tf.constant(data)</span><br><span class="line">print(sess.run(x))</span><br><span class="line"><span class="comment">#[[[ 0  1]</span></span><br><span class="line"><span class="comment">#  [ 2  3]</span></span><br><span class="line"><span class="comment">#  [ 4  5]]</span></span><br><span class="line"><span class="comment"># [[ 6  7]</span></span><br><span class="line"><span class="comment">#  [ 8  9]</span></span><br><span class="line"><span class="comment">#  [10 11]]]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Collecting elements from a tensor of rank 3</span></span><br><span class="line">result = tf.gather_nd(x, [[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>]])</span><br><span class="line">print(sess.run(result))</span><br><span class="line"><span class="comment"># indices.shape=(2, 3), indices.shape[:-1]=(2,), indices.shape[-1]=3, params.shape=(2, 3, 2), params.shape[indices.shape[-1]:]=(), outputs.shape=(2,)+() = (2,) </span></span><br><span class="line"><span class="comment"># [0 11]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Collecting batched rows from a tensor of rank 3</span></span><br><span class="line">result = tf.gather_nd(x, [[[<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">1</span>]], [[<span class="number">1</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">1</span>]]])</span><br><span class="line">print(sess.run(result))</span><br><span class="line"><span class="comment"># indices.shape=(2, 2, 2), indices.shape[:-1]=(2, 2, ), indices.shape[-1]=2, params.shape=(2, 3, 2), params.shape[indices.shape[-1]:]=(2,), outputs.shape=(2, 2)+(2, ) = (2, 2, 2) </span></span><br><span class="line"><span class="comment"># [[[0 1]</span></span><br><span class="line"><span class="comment">#  [2 3]]</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># [[6 7]</span></span><br><span class="line"><span class="comment">#  [8 9]]]</span></span><br><span class="line"></span><br><span class="line">result = tf.gather_nd(x, [[<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">1</span>]])</span><br><span class="line">print(sess.run(result))</span><br><span class="line"><span class="comment"># indices.shape=(4, 2), indices.shape[:-1]=(4,), indices.shape[-1]=2, params.shape=(2, 3, 2), params.shape[indices.shape[-1]:]=(2,), outputs.shape=(4,)+(2,) = (4, 2) </span></span><br><span class="line"><span class="comment"># [[0 1]</span></span><br><span class="line"><span class="comment">#  [2 3]</span></span><br><span class="line"><span class="comment">#  [6 7]</span></span><br><span class="line"><span class="comment">#  [8 9]]</span></span><br></pre></td></tr></table></figure><h2 id="参考文献">参考文献</h2><p>1.<a href="https://www.tensorflow.org/api_docs/python/tf/gather_nd" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/gather_nd</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;tf-gather-nd&quot;&gt;tf.gather_nd&lt;/h2&gt;
&lt;h3 id=&quot;一句话介绍&quot;&gt;一句话介绍&lt;/h3&gt;
&lt;p&gt;按照索引将输入tensor的某些维度拼凑成一个新的tenosr&lt;/p&gt;
&lt;h3 id=&quot;api&quot;&gt;API&lt;/h3&gt;
&lt;figure class
      
    
    </summary>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/categories/tensorflow/"/>
    
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow cond</title>
    <link href="http://mxxhcm.github.io/2019/05/10/tensorflow-cond/"/>
    <id>http://mxxhcm.github.io/2019/05/10/tensorflow-cond/</id>
    <published>2019-05-10T09:01:14.000Z</published>
    <updated>2019-05-12T12:35:04.910Z</updated>
    
    <content type="html"><![CDATA[<h2 id="tf-cond">tf.cond</h2><h3 id="一句话介绍">一句话介绍</h3><p>和if语句的功能和很像，如果条件为真，返回一个函数，如果条件为假，返回另一个函数。</p><h3 id="api">API</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">tf.cond(</span><br><span class="line">    pred, <span class="comment"># 条件</span></span><br><span class="line">    true_fn=<span class="literal">None</span>, <span class="comment"># 如果条件为真，执行该函数</span></span><br><span class="line">    false_fn=<span class="literal">None</span>, <span class="comment"># 如果条件为假，执行该函数</span></span><br><span class="line">    strict=<span class="literal">False</span>,</span><br><span class="line">    name=<span class="literal">None</span>,</span><br><span class="line">    fn1=<span class="literal">None</span>,</span><br><span class="line">    fn2=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>最后返回的是true_fn或者false_fn返回的还是tf.Tensor类型的变量。</p><h3 id="代码示例1">代码示例1</h3><p><a href>代码地址</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x = tf.placeholder(tf.int32, [<span class="number">10</span>])</span><br><span class="line">y = tf.constant([<span class="number">10</span>, <span class="number">3.2</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># for i in range(10):</span></span><br><span class="line"><span class="comment">#     if tf.equal(x[i], 0):</span></span><br><span class="line"><span class="comment">#         y = tf.add(y, 1)</span></span><br><span class="line"><span class="comment">#     else:</span></span><br><span class="line"><span class="comment">#         y = tf.add(y, 10)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 上面的代码起到了和下面代码相同的作用，但是上面的代码在tensorflow中会报错，不能运行，因为x[i]==0返回的不是python的bool类型，而是bool类型的tf.Tensor。</span></span><br><span class="line"><span class="comment"># TypeError: Using a tf.Tensor as a Python bool is not allowed.</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    y = tf.cond(tf.equal(x[i], <span class="number">0</span>), <span class="keyword">lambda</span>: tf.add(y, <span class="number">1</span>), <span class="keyword">lambda</span>: tf.add(y, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line">result = tf.log(y)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">   inputs = np.array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>])</span><br><span class="line">   print(sess.run(result, feed_dict=&#123;x: inputs&#125;))</span><br></pre></td></tr></table></figure><h3 id="代码示例2">代码示例2</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">myfunc</span><span class="params">(x)</span>:</span></span><br><span class="line">   <span class="keyword">if</span> (x &gt; <span class="number">0</span>):</span><br><span class="line">      <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">   <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    x = tf.constant(<span class="number">4</span>)</span><br><span class="line">    <span class="comment"># print(myfunc(x))</span></span><br><span class="line">    <span class="comment"># raise TypeError("Using a `tf.Tensor` as a Python `bool` is not allowed. "</span></span><br><span class="line">    <span class="comment"># TypeError: Using a `tf.Tensor` as a Python `bool` is not allowed. Use `if t is not None:` instead of `if t:` to test if a tensor is defined, and use TensorFlow ops such as tf.cond to execute subgraphs conditioned on the value of a tensor.</span></span><br><span class="line">    result = tf.cond(tf.greater(x, <span class="number">0</span>), <span class="keyword">lambda</span>: <span class="number">1</span>, <span class="keyword">lambda</span>: <span class="number">0</span>)</span><br><span class="line">    print(type(result))</span><br><span class="line">    print(result.eval())</span><br></pre></td></tr></table></figure><p>上述代码中定义了一个函数，实现判断某个值是否大于0。但是这个函数是错误的，因为$x\gt 0$返回一个bool类型的tf.Tensor不能用作if的判断条件，所以需要使用tf.cond语句。</p><h3 id="代码示例3">代码示例3</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Example 3</span></span><br><span class="line">x = tf.constant(<span class="number">4</span>)</span><br><span class="line">y = tf.constant(<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(x) </span><br><span class="line">    print(y) </span><br><span class="line">    <span class="keyword">if</span> x == y:</span><br><span class="line">      print(<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      print(<span class="literal">False</span>)</span><br><span class="line">    result = tf.equal(x, y)</span><br><span class="line">    print(result.eval())</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">f1</span><span class="params">()</span>:</span> </span><br><span class="line">      print(<span class="string">"f1 declare"</span>)</span><br><span class="line">      <span class="keyword">return</span> [<span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">f2</span><span class="params">()</span>:</span></span><br><span class="line">      print(<span class="string">"f2 declare"</span>)</span><br><span class="line">      <span class="keyword">return</span> [<span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">    res = tf.cond(tf.equal(x, y), f1, f2)</span><br><span class="line">    print(res)</span><br></pre></td></tr></table></figure><h2 id="参考文献">参考文献</h2><p>1.<a href="https://www.tensorflow.org/api_docs/python/tf/cond" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/cond</a><br>2.<a href="https://stackoverflow.com/questions/48571521/tensorflow-error-using-a-tf-tensor-as-a-python-bool-is-not-allowed" target="_blank" rel="noopener">https://stackoverflow.com/questions/48571521/tensorflow-error-using-a-tf-tensor-as-a-python-bool-is-not-allowed</a><br>3.<a href="https://blog.csdn.net/Cerisier/article/details/79819248" target="_blank" rel="noopener">https://blog.csdn.net/Cerisier/article/details/79819248</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;tf-cond&quot;&gt;tf.cond&lt;/h2&gt;
&lt;h3 id=&quot;一句话介绍&quot;&gt;一句话介绍&lt;/h3&gt;
&lt;p&gt;和if语句的功能和很像，如果条件为真，返回一个函数，如果条件为假，返回另一个函数。&lt;/p&gt;
&lt;h3 id=&quot;api&quot;&gt;API&lt;/h3&gt;
&lt;figure class
      
    
    </summary>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/categories/tensorflow/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>python cv2.imresize图像缩放</title>
    <link href="http://mxxhcm.github.io/2019/05/09/python-cv2-imresize/"/>
    <id>http://mxxhcm.github.io/2019/05/09/python-cv2-imresize/</id>
    <published>2019-05-09T13:37:30.000Z</published>
    <updated>2019-05-10T11:37:24.676Z</updated>
    
    <content type="html"><![CDATA[<h2 id="cv2-resize"><a href="#cv2-resize" class="headerlink" title="cv2.resize"></a>cv2.resize</h2><p>cv2是python的opencv包，实现的功能是对一个图片进行缩放。<br>python3下安装命令：<br>~$:pip install opencv-python</p><h3 id="示例代码"><a href="#示例代码" class="headerlink" title="示例代码"></a>示例代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">img = np.random.rand(<span class="number">210</span>, <span class="number">160</span> ,<span class="number">3</span>)</span><br><span class="line">print(img.shape)</span><br><span class="line">img_scale = cv2.resize(img, (<span class="number">84</span>, <span class="number">84</span>))</span><br><span class="line">print(img_scale.shape)</span><br></pre></td></tr></table></figure><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;cv2-resize&quot;&gt;&lt;a href=&quot;#cv2-resize&quot; class=&quot;headerlink&quot; title=&quot;cv2.resize&quot;&gt;&lt;/a&gt;cv2.resize&lt;/h2&gt;&lt;p&gt;cv2是python的opencv包，实现的功能是对一个图片进行缩放。&lt;br
      
    
    </summary>
    
      <category term="python" scheme="http://mxxhcm.github.io/categories/python/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="opencv" scheme="http://mxxhcm.github.io/tags/opencv/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow model save load</title>
    <link href="http://mxxhcm.github.io/2019/05/09/tensorflow-model-save-load/"/>
    <id>http://mxxhcm.github.io/2019/05/09/tensorflow-model-save-load/</id>
    <published>2019-05-09T07:14:26.000Z</published>
    <updated>2019-09-28T10:25:35.998Z</updated>
    
    <content type="html"><![CDATA[<h2 id="tf-train-saver保存和恢复模型">tf.train.Saver保存和恢复模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">saver = tf.train.Saver()</span><br><span class="line">saver.save()</span><br></pre></td></tr></table></figure><p>调用上述代码之后会存存储以下几个文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">checkpoint</span><br><span class="line">model.ckpt.data-00000-of-00001</span><br><span class="line">model.ckpt.index</span><br><span class="line">model.ckpt.meta</span><br></pre></td></tr></table></figure><p>其中checkpoint文件存储的是最近保存的文件的名字，meta文件存放的是计算图的定义，index和data文件存放的是权重文件。</p><p>下面介绍一下上述代码中出现的两个API，tf.train.Saver()和tf.train.Saver().save()。</p><h3 id="tf-train-saver">tf.train.Saver()</h3><p>Saver是类，不是函数。可以用来保存，恢复variable和model，Saver对象提供save()和restore()等函数，save()保存模型，restore()加载模型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">__init__(</span><br><span class="line">    var_list=<span class="literal">None</span>, <span class="comment"># 指定要保存的variablelist</span></span><br><span class="line">    reshape=<span class="literal">False</span>,</span><br><span class="line">    sharded=<span class="literal">False</span>,</span><br><span class="line">    max_to_keep=<span class="number">5</span>, <span class="comment"># 最多保留最近的几个checkpoints</span></span><br><span class="line">    keep_checkpoint_every_n_hours=<span class="number">10000.0</span>,</span><br><span class="line">    name=<span class="literal">None</span>,</span><br><span class="line">    restore_sequentially=<span class="literal">False</span>,</span><br><span class="line">    saver_def=<span class="literal">None</span>,</span><br><span class="line">    builder=<span class="literal">None</span>,</span><br><span class="line">    defer_build=<span class="literal">False</span>,</span><br><span class="line">    allow_empty=<span class="literal">False</span>,</span><br><span class="line">    write_version=tf.train.SaverDef.V2,</span><br><span class="line">    pad_step_number=<span class="literal">False</span>,</span><br><span class="line">    save_relative_paths=<span class="literal">False</span>,</span><br><span class="line">    filename=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="tf-train-saver-save">tf.train.Saver.save()</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">save(</span><br><span class="line">    sess,       \\传入当前要保存的session</span><br><span class="line">    save_path,  \\指定checkpoint的路径</span><br><span class="line">    global_step=<span class="literal">None</span>,   \\当前存的model的step</span><br><span class="line">    latest_filename=<span class="literal">None</span>,</span><br><span class="line">    meta_graph_suffix=<span class="string">'meta'</span>,</span><br><span class="line">    write_meta_graph=<span class="literal">True</span>,  \\指定是否要保存计算图</span><br><span class="line">    write_state=<span class="literal">True</span>,</span><br><span class="line">    strip_default_attrs=<span class="literal">False</span>,</span><br><span class="line">    save_debug_info=<span class="literal">False</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>这里说一下save_path，如果不指定的话，文件名默认是空的，在linux下是以.开头的（即当前目录），所以会显示成隐藏文件。通常情况下我们指定checkpoint要保存的路径，以及名字，比如叫model.ckpt，在load的时候还使用这个名字就行。指定了global_step之后，tf会自动在路径后面加上step进行区分。</p><h2 id="读取graph">读取graph</h2><h3 id="读取图的定义">读取图的定义</h3><p>meta文件中存放了计算图的定义，可以直接使用API tf.train.import_meta_graph()函数调用：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    saver = tf.train.import_meta_graph(<span class="string">"model.ckpt.meta"</span>)</span><br></pre></td></tr></table></figure><p>这时计算图就已经定义在当前sess中了。上述代码会保留原始的device信息，如果迁移到其他设备时，可能由于没有指定设备出错，这个问题可以通过指定一个特殊的参数clear_devices解决：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    saver = tf.train.import_meta_graph(<span class="string">"model.ckpt.meta"</span>, clear_devices=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><p>这样子就和device无关了。</p><h3 id="访问graph中的参数">访问graph中的参数</h3><h4 id="通过collection访问计算图中collection的键">通过collection访问计算图中collection的键</h4><p>这里的键指的是graph中都有哪些<a href>collections</a>。</p><ul><li><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(sess.graph.get_all_collection_keys())</span><br></pre></td></tr></table></figure></li><li><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(sess.graph.collections)</span><br></pre></td></tr></table></figure></li><li><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.get_default_graph().get_all_collection_keys()</span><br></pre></td></tr></table></figure></li></ul><h4 id="访问collection">访问collection</h4><ul><li><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sess.graph.get_collection(<span class="string">"summaries"</span>)</span><br></pre></td></tr></table></figure></li><li><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.get_collection(<span class="string">""</span>)</span><br></pre></td></tr></table></figure></li></ul><p>示例</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment">#saver = tf.train.Saver()</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    new_saver = tf.train.import_meta_graph(<span class="string">"saver1.ckpt.meta"</span>)</span><br><span class="line">    print(sess.graph)</span><br><span class="line">    <span class="keyword">for</span> var <span class="keyword">in</span> tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES):</span><br><span class="line">        print(var)</span><br></pre></td></tr></table></figure><h4 id="通过operation访问">通过operation访问</h4><ul><li><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sess.graph.get_opeartions()</span><br></pre></td></tr></table></figure></li><li><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> op <span class="keyword">in</span> sess.graph.get_opeartions():</span><br><span class="line">    print(op.name, op.values())</span><br></pre></td></tr></table></figure></li><li><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">sess.graph.get_operation_by_name(<span class="string">"op_name"</span>).node_def</span><br><span class="line">``` </span><br><span class="line"></span><br><span class="line"><span class="comment">## 保存和恢复variables</span></span><br><span class="line"><span class="comment">### 保存和恢复全部variables</span></span><br><span class="line">- 恢复variable时，无需初始化。</span><br><span class="line">- 恢复variable时，使用的是variable的name，不是op的name。只要知道variable的name即可。save和restore的op name不需要相同，只要variable name相同即可。</span><br><span class="line">- 对于使用tf.Variable()创建的variable，如果没有指定variable名字的话，系统会为其生成默认名字，在恢复的时候，需要使用tf.get_variable()恢复variable，同时传variable name和shape。</span><br><span class="line"></span><br><span class="line"><span class="comment">#### 保存全部variables</span></span><br><span class="line">``` python</span><br><span class="line">saver = tf.train.Saver()</span><br><span class="line">saver.save(sess, save_path) <span class="comment"># 需要指定的是checkpoint的名字而不是目录</span></span><br></pre></td></tr></table></figure></li></ul><h4 id="恢复全部variables">恢复全部variables</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">saver = tf.train.Saver()</span><br><span class="line">saver.restore(sess, save_path)</span><br></pre></td></tr></table></figure><h3 id="保存和恢复部分variables">保存和恢复部分variables</h3><h4 id="保存全部variable">保存全部variable</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">saver = tf.train.Saver(&#123;<span class="string">"variable_name1"</span>: op_name1,..., <span class="string">"variable_namen"</span>: op_namen&#125;)</span><br><span class="line">saver.save(sess, save_path) <span class="comment"># 需要指定的是checkpoint的名字而不是目录</span></span><br></pre></td></tr></table></figure><h4 id="恢复全部variable">恢复全部variable</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">saver = tf.train.Saver(&#123;<span class="string">"variable_name1"</span>: op_name1,..., <span class="string">"variable_namen"</span>: op_namen&#125;)</span><br><span class="line">saver.restore(sess, save_path)</span><br></pre></td></tr></table></figure><h2 id="保存和恢复模型">保存和恢复模型</h2><p>其实和保存恢复变量没有什么区别。只是把整个模型的variables都save和restore了。</p><h2 id="代码示例">代码示例</h2><p><a href="https://github.com/mxxhcm/code/tree/master/tf/some_ops/saver_restore" target="_blank" rel="noopener">代码地址</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">graph = tf.Graph()</span><br><span class="line"><span class="keyword">with</span> graph.as_default():</span><br><span class="line">    W = tf.Variable([<span class="number">0.3</span>], dtype=tf.float32)</span><br><span class="line">    b = tf.Variable([<span class="number">-0.3</span>], dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># input and output</span></span><br><span class="line">    x = tf.placeholder(tf.float32)</span><br><span class="line">    y = tf.placeholder(tf.float32)</span><br><span class="line">    predicted_y = W*x+b</span><br><span class="line"></span><br><span class="line">    <span class="comment"># MSE loss</span></span><br><span class="line">    loss = tf.reduce_mean(tf.square(y - predicted_y))</span><br><span class="line">    <span class="comment"># optimizer</span></span><br><span class="line">    optimizer = tf.train.GradientDescentOptimizer(<span class="number">0.01</span>)</span><br><span class="line">    train_op = optimizer.minimize(loss)</span><br><span class="line"></span><br><span class="line">inputs = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]</span><br><span class="line">outputs = [<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session(graph=graph) <span class="keyword">as</span> sess:</span><br><span class="line">    saver = tf.train.Saver()</span><br><span class="line">    init_op = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5000</span>):</span><br><span class="line">        sess.run(train_op, feed_dict=&#123;x: inputs, y: outputs&#125;)</span><br><span class="line">    l_, W_, b_ = sess.run([loss, W, b], feed_dict=&#123;x: inputs, y: outputs&#125;)</span><br><span class="line">    print(<span class="string">"loss: "</span>, l_, <span class="string">"w: "</span>, W_, <span class="string">"b:"</span>, b_)</span><br><span class="line">    checkpoint = <span class="string">"./checkpoint/saver1.ckpt"</span></span><br><span class="line">    save_path = saver.save(sess, checkpoint)</span><br><span class="line">    print(<span class="string">"Model has been saved in %s."</span> % save_path)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session(graph=graph) <span class="keyword">as</span> sess:</span><br><span class="line">    saver = tf.train.Saver()</span><br><span class="line">    saver.restore(sess, checkpoint)</span><br><span class="line">    l_, W_, b_ = sess.run([loss, W, b], feed_dict=&#123;x: inputs, y: outputs&#125;)</span><br><span class="line">    print(<span class="string">"loss: "</span>, l_, <span class="string">"w: "</span>, W_, <span class="string">"b:"</span>, b_)</span><br><span class="line">    print(<span class="string">"Model has been restored."</span>)</span><br></pre></td></tr></table></figure><h2 id="获取最新的checkpoint文件">获取最新的checkpoint文件</h2><h3 id="tf-train-get-checkpoint-state">tf.train.get_checkpoint_state()</h3><p>给出checkpoint文件所在目录，可以使用get_checkpoint_state()获得最新的checkpoint文件：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ckpt = tf.train.get_checkpoint_state(checkpoint_dir)</span><br><span class="line"><span class="keyword">if</span> ckpt <span class="keyword">and</span> ckpt.model_checkpoint_path:</span><br><span class="line">    save.restore(sess, ckpt.model_checkpoint_path)</span><br></pre></td></tr></table></figure><h3 id="使用inspect-checkpoint库">使用inspect_checkpoint库</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># import the inspect_checkpoint library</span></span><br><span class="line"><span class="keyword">from</span> tensorflow.python.tools <span class="keyword">import</span> inspect_checkpoint <span class="keyword">as</span> chkp</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印checkpoint文件中所有variable</span></span><br><span class="line">chkp.print_tensors_in_checkpoint_file(<span class="string">"saver/variables/all_variables.ckpt"</span>, tensor_name=<span class="string">''</span>, all_tensors=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印变量"v1"</span></span><br><span class="line">chkp.print_tensors_in_checkpoint_file(<span class="string">"saver/variables/all_variables.ckpt"</span>, tensor_name=<span class="string">'v1'</span>, all_tensors=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">chkp.print_tensors_in_checkpoint_file(<span class="string">"saver/variables/all_variables.ckpt"</span>, tensor_name=<span class="string">'v2'</span>, all_tensors=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><h2 id="模型的冻结">模型的冻结</h2><p>模型的冻结是不在训练模型，只用于正向推导，所以把变量转换成常量后，和计算图一起保存在协议缓冲区文件(.pb)文件中，因此需要在计算图中预先定义输出节点的名称，示例如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">output_nodes = [<span class="string">"Accuracy/prediction"</span>, <span class="string">"Metric/Dice"</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载计算图</span></span><br><span class="line">saver = tf.train.import_meta_graph(<span class="string">"model.ckpt.meta"</span>, clear_devices=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    input_graph_def = sess.graph.as_graph_def()</span><br><span class="line">    <span class="comment"># load model</span></span><br><span class="line">    saver.restore(sess, <span class="string">"model.ckpt"</span>)</span><br><span class="line">    <span class="comment"># 将变量转换为常量</span></span><br><span class="line">    output_graph_def = tf.graph_util.convert_variables_to_constants(sess, input_graph_def, output_nodes)</span><br><span class="line">    <span class="comment"># 写入pb文件</span></span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">"frozen_model.pb"</span>, <span class="string">"wb"</span>) <span class="keyword">as</span> f:</span><br><span class="line">        f.write(output_graph_def.SerializeToString())</span><br></pre></td></tr></table></figure><h2 id="模型的执行">模型的执行</h2><p>从协议缓冲区文件(.pb)文件中读取模型，导入计算图</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 读取模型并保存到序列化模型对象中</span></span><br><span class="line"><span class="keyword">with</span> open(frozen_graph_path, <span class="string">"rb"</span>) <span class="keyword">as</span> f:</span><br><span class="line">    graph_def = tf.GraphDef()</span><br><span class="line">    graph_def.ParseFromString(f.read())</span><br><span class="line"><span class="comment"># 导入计算图</span></span><br><span class="line">graph = tf.Graph()</span><br><span class="line"><span class="keyword">with</span> graph.as_default():</span><br><span class="line">    tf.import_graph_def(graph_def, name=<span class="string">"Test"</span>)</span><br></pre></td></tr></table></figure><p>获取输入和输出的张量，然后将测试数据feed给输入张量，得到结果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">x_tensor = graph.get_tensor_by_name(<span class="string">"Test/input/image-input:0"</span>)</span><br><span class="line">y_tensor = graph.get_tensor_by_name(<span class="string">"Test/input/label-input:0"</span>)</span><br><span class="line">keep_prob = graph.get_tensor_by_name(<span class="string">"Test/dropout/Placeholder:0"</span>)</span><br><span class="line">acc_op = graph.get_tensor_by_name(<span class="string">"Test/accuracy/prediction:0"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">"mnist_data"</span>, one_hot=<span class="literal">True</span>)</span><br><span class="line">x_values, y_values = mnist.test.next_batch(<span class="number">10000</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session(graph=graph) <span class="keyword">as</span> sess:</span><br><span class="line">    accuracy = sess.run(acc_op, feed_dict=&#123;x_tensor: x_values,</span><br><span class="line">                                          y_tensor: y_values,</span><br><span class="line">                                          keep_prob: <span class="number">1.0</span>&#125;)</span><br><span class="line">    print(accuracy)</span><br></pre></td></tr></table></figure><h2 id="参考文献">参考文献</h2><p>1.<a href="https://www.jarvis73.cn/2018/04/25/Tensorflow-Model-Save-Read/" target="_blank" rel="noopener">https://www.jarvis73.cn/2018/04/25/Tensorflow-Model-Save-Read/</a><br>2.<a href="https://www.tensorflow.org/guide/saved_model" target="_blank" rel="noopener">https://www.tensorflow.org/guide/saved_model</a><br>3.<a href="https://www.tensorflow.org/api_docs/python/tf/train/Saver" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/train/Saver</a><br>4.<a href="https://www.bilibili.com/read/cv681031/" target="_blank" rel="noopener">https://www.bilibili.com/read/cv681031/</a><br>5.<a href="https://cv-tricks.com/tensorflow-tutorial/save-restore-tensorflow-models-quick-complete-tutorial/" target="_blank" rel="noopener">https://cv-tricks.com/tensorflow-tutorial/save-restore-tensorflow-models-quick-complete-tutorial/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;tf-train-saver保存和恢复模型&quot;&gt;tf.train.Saver保存和恢复模型&lt;/h2&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;l
      
    
    </summary>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/categories/tensorflow/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/tags/tensorflow/"/>
    
      <category term="Saver" scheme="http://mxxhcm.github.io/tags/Saver/"/>
    
      <category term="save" scheme="http://mxxhcm.github.io/tags/save/"/>
    
      <category term="restore" scheme="http://mxxhcm.github.io/tags/restore/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow reduction</title>
    <link href="http://mxxhcm.github.io/2019/05/09/tensorflow-reduction/"/>
    <id>http://mxxhcm.github.io/2019/05/09/tensorflow-reduction/</id>
    <published>2019-05-09T03:19:00.000Z</published>
    <updated>2019-05-12T12:36:46.605Z</updated>
    
    <content type="html"><![CDATA[<h2 id="tf-reduction">tf.Reduction</h2><ul><li>tf.reduce_sum(input_tensor, reduction_indices=None, keep_dims=False, name=None)  # 计算input_tensor的和，可指定dim。</li><li>tf.reduce_mean(input_tensor, reduction_indices=None, keep_dims=False, name=None) # 计算input_tensor的均值，可指定dim。</li><li>tf.reduce_min(input_tensor, reduction_indices=None, keep_dims=False, name=None) # 计算input_tensor的最小值，可指定dim。</li><li>tf.reduce_max(input_tensor, reduction_indices=None, keep_dims=False, name=None) # 计算input_tensor的最大值，可指定dim。</li><li>tf.recude_proc(input_tensor, reduction_indices=None, keep_dims=False, name=None) # 计算input_tensor的乘积，可指定dim。</li><li>tf.reduce_all(input_tensor, reduction_indices=None, keep_dims=False, name=None) # 计算input_tensor中所有元素的逻辑与，可指定dim。</li><li>tf.reduce_any(input_tensor, reduction_indices=None, keep_dims=False, name=None) # 计算input_tensor的所有元素的逻辑或，可指定dim。</li><li>tf.accumulate_n(inputs, shape=None, tensor_dtype=None, name=None) # 计算inputs的和。</li><li>tf.cumsum(x, axis=0, exclusive=False, reverse=False, name=None) # 计算input_tensor的累积和。</li></ul><h2 id="代码示例">代码示例</h2><h3 id="tf-reduce-sum">tf.reduce_sum()</h3><p><a href="https://github.com/mxxhcm/code/blob/master/tf/some_ops/tf_reduce_sum.py" target="_blank" rel="noopener">代码地址</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x = tf.placeholder(dtype=tf.float32, shape=[<span class="literal">None</span>, <span class="number">2</span>])</span><br><span class="line">y = tf.log(x)</span><br><span class="line"><span class="comment"># 对所有y求和</span></span><br><span class="line">loss = tf.reduce_sum(y)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess :</span><br><span class="line">    <span class="comment"># inputs = tf.constant([1.0, 2.0])</span></span><br><span class="line">    inputs = np.array([[<span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line">    l = sess.run(loss, feed_dict=&#123;x: inputs&#125;)</span><br><span class="line">    print(l)</span><br></pre></td></tr></table></figure><h2 id="参考文献">参考文献</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;tf-reduction&quot;&gt;tf.Reduction&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;tf.reduce_sum(input_tensor, reduction_indices=None, keep_dims=False, name=None)  # 计算input_
      
    
    </summary>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/categories/tensorflow/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>pytorch</title>
    <link href="http://mxxhcm.github.io/2019/05/08/pytorch-basic/"/>
    <id>http://mxxhcm.github.io/2019/05/08/pytorch-basic/</id>
    <published>2019-05-08T14:07:42.000Z</published>
    <updated>2019-05-17T08:14:01.433Z</updated>
    
    <content type="html"><![CDATA[<h2 id="torch">torch</h2><p>torch提供了很多基础操作，包括数学操作等等。</p><h2 id="torch-cat">torch.cat</h2><h3 id="函数原型">函数原型</h3><p>将多个tensor在某一个维度上（默认是第0维）拼接到一起（除了拼接的维度上，其他维度的shape必须一定），最后返回一个tensor。<br>torch.cat(tensors, dim=0, out=None) → Tensor</p><blockquote><p>Concatenates the given sequence of seq tensors in the given dimension. All tensors must either have the same shape (except in the concatenating dimension) or be empty.</p></blockquote><h3 id="参数">参数</h3><p>tensors (sequence of Tensors) – 任意类型相同python序列或者tensor<br>dim (int, optional) - 在第几个维度上进行拼接(只有在拼接的维度上可以不同，其余维度必须相同。<br>out (Tensor, optional) – 输出的tensor</p><h3 id="例子">例子</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x1 = torch.randn(<span class="number">3</span>, <span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line">x2 = torch.randn(<span class="number">3</span>, <span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line">x = torch.cat([x1, x2], <span class="number">1</span>)</span><br><span class="line">print(x.size())</span><br></pre></td></tr></table></figure><p>输出如下：</p><blockquote><p>torch.Size([3, 5, 4])</p></blockquote><h2 id="torch中图像-img-格式">torch中图像(img)格式</h2><p>torch中图像的shape是(‘RGB’,width, height)，而numpy和matplotlib中都是(width, height, ‘RGB’)<br>matplotlib.pyplot.imshow()需要的参数是图像矩阵，如果矩阵中是整数，那么它的值需要在区间[0,255]之内，如果是浮点数，需要在[0,1]之间。</p><blockquote><p>Clipping input data to the valid range for imshow with RGB data ([0…1] for floats or [0…255] for integers).</p></blockquote><h2 id="参考文献">参考文献</h2><p>1.<a href="https://pytorch.org/docs/stable/torch.html" target="_blank" rel="noopener">https://pytorch.org/docs/stable/torch.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;torch&quot;&gt;torch&lt;/h2&gt;
&lt;p&gt;torch提供了很多基础操作，包括数学操作等等。&lt;/p&gt;
&lt;h2 id=&quot;torch-cat&quot;&gt;torch.cat&lt;/h2&gt;
&lt;h3 id=&quot;函数原型&quot;&gt;函数原型&lt;/h3&gt;
&lt;p&gt;将多个tensor在某一个维度上（默认是第
      
    
    </summary>
    
      <category term="pytorch" scheme="http://mxxhcm.github.io/categories/pytorch/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="pytorch" scheme="http://mxxhcm.github.io/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>pytorch initialize parameters</title>
    <link href="http://mxxhcm.github.io/2019/05/08/pytorch-initialize-parameters/"/>
    <id>http://mxxhcm.github.io/2019/05/08/pytorch-initialize-parameters/</id>
    <published>2019-05-08T14:01:24.000Z</published>
    <updated>2019-05-17T08:21:56.489Z</updated>
    
    <content type="html"><![CDATA[<h2 id="神经网络参数初始化">神经网络参数初始化</h2><h3 id="方法-1-model-apply-fn">方法$1$.Model.apply(fn)</h3><p><a href="https://github.com/mxxhcm/myown_code/blob/master/pytorch/tutorials/initialize/apply.py" target="_blank" rel="noopener">示例</a>如下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_weights</span><span class="params">(m)</span>:</span></span><br><span class="line">  print(m)</span><br><span class="line">  <span class="keyword">if</span> type(m) == nn.Linear:</span><br><span class="line">    m.weight.data.fill_(<span class="number">1.0</span>)</span><br><span class="line">    print(m.weight)</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(nn.Linear(<span class="number">2</span>, <span class="number">2</span>), nn.Linear(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">net.apply(init_weights)</span><br></pre></td></tr></table></figure><p>输出结果如下：</p><blockquote><p>Linear(in_features=2, out_features=2, bias=True)<br>Parameter containing:<br>tensor([[1., 1.],<br>[1., 1.]], requires_grad=True)<br>Linear(in_features=2, out_features=2, bias=True)<br>Parameter containing:<br>tensor([[1., 1.],<br>[1., 1.]], requires_grad=True)<br>Sequential(<br>(0): Linear(in_features=2, out_features=2, bias=True)<br>(1): Linear(in_features=2, out_features=2, bias=True)<br>)<br>Linear(in_features=2, out_features=2, bias=True)<br>Linear(in_features=2, out_features=2, bias=True)</p></blockquote><p>其中最后两行为net对象调用self.children()函数返回的模块，就是模型中所有网络的参数。事实上，调用net.apply(fn)函数，会对self.children()中的所有模块应用fn函数，</p><h2 id="参考文献">参考文献</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;神经网络参数初始化&quot;&gt;神经网络参数初始化&lt;/h2&gt;
&lt;h3 id=&quot;方法-1-model-apply-fn&quot;&gt;方法$1$.Model.apply(fn)&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/mxxhcm/myown_code/b
      
    
    </summary>
    
      <category term="pytorch" scheme="http://mxxhcm.github.io/categories/pytorch/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="pytorch" scheme="http://mxxhcm.github.io/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>pytorch optim</title>
    <link href="http://mxxhcm.github.io/2019/05/08/pytorch-optim/"/>
    <id>http://mxxhcm.github.io/2019/05/08/pytorch-optim/</id>
    <published>2019-05-08T13:58:19.000Z</published>
    <updated>2019-05-17T08:20:33.900Z</updated>
    
    <content type="html"><![CDATA[<h2 id="torch-optim">torch.optim</h2><h3 id="基类class-optimizer-object">基类class Optimizer(object)</h3><p>Optimizer是所有optimizer的基类。<br>调用任何优化器都要先初始化Optimizer类，这里拿Adam优化器举例子。Adam optimizer的init函数如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Adam</span><span class="params">(Optimizer)</span>:</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">        params (iterable): iterable of parameters to optimize or dicts defining</span></span><br><span class="line"><span class="string">            parameter groups</span></span><br><span class="line"><span class="string">        lr (float, optional): learning rate (default: 1e-3)</span></span><br><span class="line"><span class="string">        betas (Tuple[float, float], optional): coefficients used for computing</span></span><br><span class="line"><span class="string">            running averages of gradient and its square (default: (0.9, 0.999))</span></span><br><span class="line"><span class="string">        eps (float, optional): term added to the denominator to improve</span></span><br><span class="line"><span class="string">            numerical stability (default: 1e-8)</span></span><br><span class="line"><span class="string">        weight_decay (float, optional): weight decay (L2 penalty)</span></span><br><span class="line"><span class="string">        amsgrad (boolean, optional): whether to use the AMSGrad variant of this</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, params, lr=<span class="number">1e-3</span>, betas=<span class="params">(<span class="number">0.9</span>, <span class="number">0.999</span>)</span>, eps=<span class="number">1e-8</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 weight_decay=<span class="number">0</span>, amsgrad=False)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="number">0.0</span> &lt;= lr:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">"Invalid learning rate: &#123;&#125;"</span>.format(lr))</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="number">0.0</span> &lt;= eps:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">"Invalid epsilon value: &#123;&#125;"</span>.format(eps))</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="number">0.0</span> &lt;= betas[<span class="number">0</span>] &lt; <span class="number">1.0</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">"Invalid beta parameter at index 0: &#123;&#125;"</span>.format(betas[<span class="number">0</span>]))</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> <span class="number">0.0</span> &lt;= betas[<span class="number">1</span>] &lt; <span class="number">1.0</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">"Invalid beta parameter at index 1: &#123;&#125;"</span>.format(betas[<span class="number">1</span>]))</span><br><span class="line">        defaults = dict(lr=lr, betas=betas, eps=eps,</span><br><span class="line">                        weight_decay=weight_decay, amsgrad=amsgrad)</span><br><span class="line">        super(Adam, self).__init__(params, defaults)</span><br></pre></td></tr></table></figure><p>上述代码将学习率lr,beta,epsilon,weight_decay,amsgrad等封装在一个dict中，然后将其传给Optimizer的init函数，其代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Optimizer</span><span class="params">(object)</span>:</span></span><br><span class="line">    .. warning::</span><br><span class="line">        Parameters need to be specified <span class="keyword">as</span> collections that have a deterministic</span><br><span class="line">        ordering that <span class="keyword">is</span> consistent between runs. Examples of objects that don<span class="string">'t</span></span><br><span class="line"><span class="string">        satisfy those properties are sets and iterators over values of dictionaries.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        params (iterable): an iterable of :class:`torch.Tensor` s or</span></span><br><span class="line"><span class="string">            :class:`dict` s. Specifies what Tensors should be optimized.</span></span><br><span class="line"><span class="string">        defaults: (dict): a dict containing default values of optimization</span></span><br><span class="line"><span class="string">            options (used when a parameter group doesn'</span>t specify them).</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    def __init__(self, params, defaults):</span></span><br><span class="line"><span class="string">        self.defaults = defaults</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        if isinstance(params, torch.Tensor):</span></span><br><span class="line"><span class="string">            raise TypeError("params argument given to the optimizer should be "</span></span><br><span class="line"><span class="string">                            "an iterable of Tensors or dicts, but got " +</span></span><br><span class="line"><span class="string">                            torch.typename(params))</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        self.state = defaultdict(dict)</span></span><br><span class="line"><span class="string">        self.param_groups = []</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        param_groups = list(params)</span></span><br><span class="line"><span class="string">        if len(param_groups) == 0:</span></span><br><span class="line"><span class="string">            raise ValueError("optimizer got an empty parameter list")</span></span><br><span class="line"><span class="string">        if not isinstance(param_groups[0], dict):</span></span><br><span class="line"><span class="string">            param_groups = [&#123;'params': param_groups&#125;]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        for param_group in param_groups:</span></span><br><span class="line"><span class="string">            self.add_param_group(param_group)</span></span><br></pre></td></tr></table></figure><p>从这里可以看出来，每个pytorch给出的optimizer至少有以下三个属性和四个函数：<br>属性：</p><ul><li>self.defaults # 字典类型，主要包含学习率等值</li><li>self.state # defaultdict(&lt;class ‘dict’&gt;, {}) state存放的是</li><li>self.param_gropus # &lt;class ‘list’&gt;:[]，prama_groups是一个字典类型的列表，用来存放parameters。</li></ul><p>函数：</p><ul><li>self.zero_grad()  # 将optimizer中参数的梯度置零</li><li>self.step()  # 将梯度应用在参数上</li><li>self.state_dict() # 返回optimizer的state,包括state和param_groups。</li><li>self.load_state_dict()  # 加载optimizer的state。</li><li>self.add_param_group()  # 将一个param group添加到param_groups。可以用在fine-tune上，只添加我们需要训练的层数，然后其他层不动。</li></ul><p>如果param已经是一个字典列表的话，就无需操作，否则就需要把param转化成一个字典param_groups。然后对param_groups中的每一个param_group调用add_param_group(param_group)函数将param_group字典和defaults字典拼接成一个新的param_group字典添加到self.param_groups中。</p><h2 id="参考文献">参考文献</h2><p>1.<a href="https://pytorch.org/docs/stable/optim.html" target="_blank" rel="noopener">https://pytorch.org/docs/stable/optim.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;torch-optim&quot;&gt;torch.optim&lt;/h2&gt;
&lt;h3 id=&quot;基类class-optimizer-object&quot;&gt;基类class Optimizer(object)&lt;/h3&gt;
&lt;p&gt;Optimizer是所有optimizer的基类。&lt;br&gt;
调用任何
      
    
    </summary>
    
      <category term="pytorch" scheme="http://mxxhcm.github.io/categories/pytorch/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="pytorch" scheme="http://mxxhcm.github.io/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>pytorch utils data</title>
    <link href="http://mxxhcm.github.io/2019/05/08/pytorch-utils-data/"/>
    <id>http://mxxhcm.github.io/2019/05/08/pytorch-utils-data/</id>
    <published>2019-05-08T13:56:15.000Z</published>
    <updated>2019-05-17T07:45:03.279Z</updated>
    
    <content type="html"><![CDATA[<h2 id="torch-utils-data">torch.utils.data</h2><h3 id="dataloader">Dataloader</h3><h4 id="原型">原型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">torch</span>.<span class="title">utils</span>.<span class="title">data</span>.<span class="title">DataLoader</span><span class="params">(</span></span></span><br><span class="line"><span class="class"><span class="params">dataset, # 从哪加载数据</span></span></span><br><span class="line"><span class="class"><span class="params">batch_size=<span class="number">1</span>, # batch大小 <span class="params">(default: <span class="number">1</span>)</span>.</span></span></span><br><span class="line"><span class="class"><span class="params">shuffle=False,# 每个epoch的数据是否打乱 <span class="params">(default: False)</span>.</span></span></span><br><span class="line"><span class="class"><span class="params">sampler=None, # 定义采样策略。如果指定这个参数, shuffle必须是False.</span></span></span><br><span class="line"><span class="class"><span class="params">batch_sampler=None, </span></span></span><br><span class="line"><span class="class"><span class="params">num_workers=<span class="number">0</span>,# 多少个子进程用来进行数据加载。<span class="number">0</span>代表使用主进程加载数据 <span class="params">(default: <span class="number">0</span>)</span></span></span></span><br><span class="line"><span class="class"><span class="params">collate_fn=&lt;function default_collate&gt;, </span></span></span><br><span class="line"><span class="class"><span class="params">pin_memory=False, </span></span></span><br><span class="line"><span class="class"><span class="params">drop_last=False, </span></span></span><br><span class="line"><span class="class"><span class="params">timeout=<span class="number">0</span>, </span></span></span><br><span class="line"><span class="class"><span class="params">worker_init_fn=None</span></span></span><br><span class="line"><span class="class"><span class="params">)</span></span></span><br></pre></td></tr></table></figure><h4 id="例子">例子</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line">dataset = torchvision.datasets.CIFAR100(root=<span class="string">'./data'</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=<span class="literal">None</span>)</span><br><span class="line">train_loader = torch.utils.data.DataLoader(dataset, bath_size=<span class="number">16</span>, shuffle=<span class="literal">False</span>, num_worker=<span class="number">2</span>)</span><br></pre></td></tr></table></figure><h4 id="如何访问dataloader返回值">如何访问DataLoader返回值</h4><p>train_loader不是整数，所以不能用range，这里用enumerate()，i是</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i, data <span class="keyword">in</span> enumerate(train_loader):</span><br><span class="line">    images, labels = data</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;torch-utils-data&quot;&gt;torch.utils.data&lt;/h2&gt;
&lt;h3 id=&quot;dataloader&quot;&gt;Dataloader&lt;/h3&gt;
&lt;h4 id=&quot;原型&quot;&gt;原型&lt;/h4&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;ta
      
    
    </summary>
    
      <category term="pytorch" scheme="http://mxxhcm.github.io/categories/pytorch/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="pytorch" scheme="http://mxxhcm.github.io/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>pytorch torchvision</title>
    <link href="http://mxxhcm.github.io/2019/05/08/pytorch-torchvision/"/>
    <id>http://mxxhcm.github.io/2019/05/08/pytorch-torchvision/</id>
    <published>2019-05-08T13:55:57.000Z</published>
    <updated>2019-05-17T08:12:15.631Z</updated>
    
    <content type="html"><![CDATA[<h2 id="torchvision">torchvision</h2><p>torchvision是pytorch提供的一些工具包，主要包含下列几个模块</p><ul><li>torchvision.datasets</li><li>torchvision.utils</li><li>torchvision.transforms</li><li>torchvision.models</li></ul><h2 id="torchvision-datasets">torchvision.datasets</h2><p>torchvision提供了很多数据集</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line">print(torchvision.datasets.__all__)</span><br></pre></td></tr></table></figure><blockquote><p>(‘LSUN’, ‘LSUNClass’, ‘ImageFolder’, ‘DatasetFolder’, ‘FakeData’, ‘CocoCaptions’,     ‘CocoDetection’, ‘CIFAR10’, ‘CIFAR100’, ‘EMNIST’, ‘FashionMNIST’, ‘MNIST’, ‘STL10’,     ‘SVHN’, ‘PhotoTour’, ‘SEMEION’, ‘Omniglot’)</p></blockquote><h3 id="cifar10">CIFAR10</h3><h4 id="原型">原型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">calss torchvision.datasets.CIFAR10(root, train=<span class="literal">True</span>, transform=<span class="literal">None</span>, target_transform=<span class="literal">None</span>, download=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><h4 id="参数">参数</h4><p>root (string) – cifar-10-batches-py的存放目录或者download设置为True时将会存放的目录。<br>train (bool, optional) – 设置为True的时候, 从training set创建dataset, 否则从test set创建dataset.<br>transform (callable, optional) – 输入是一个 PIL image，返回一个transformed的版本。如，transforms.RandomCrop<br>target_transform (callable, optional) – A function/transform that takes in the target and transforms it.<br>download (bool, optional) – If true, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again.</p><h4 id="例子">例子</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line">trainset = torchvision.datasets.CIFAR100(root=<span class="string">"./datasets"</span>, train=<span class="literal">True</span>, transform=    <span class="literal">None</span>, download=<span class="literal">True</span>)</span><br><span class="line">testset = torchvision.datasets.CIFAR100(root=<span class="string">"./datasets"</span>, train=<span class="literal">False</span>, transform=    <span class="literal">None</span>, download=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h2 id="torchvision-models">torchvision.models</h2><p>模型</p><h2 id="torchvision-transforms">torchvision.transforms</h2><p>transform</p><h2 id="torchvision-utils">torchvision.utils</h2><p>一些工具包</p><h2 id="参考文献">参考文献</h2><p>1.<a href="https://pytorch.org/docs/stable/torchvision/index.html" target="_blank" rel="noopener">https://pytorch.org/docs/stable/torchvision/index.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;torchvision&quot;&gt;torchvision&lt;/h2&gt;
&lt;p&gt;torchvision是pytorch提供的一些工具包，主要包含下列几个模块&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;torchvision.datasets&lt;/li&gt;
&lt;li&gt;torchvision.utils
      
    
    </summary>
    
      <category term="pytorch" scheme="http://mxxhcm.github.io/categories/pytorch/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="pytorch" scheme="http://mxxhcm.github.io/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>pytorch nn functional</title>
    <link href="http://mxxhcm.github.io/2019/05/08/pytorch-nn-functional/"/>
    <id>http://mxxhcm.github.io/2019/05/08/pytorch-nn-functional/</id>
    <published>2019-05-08T13:55:37.000Z</published>
    <updated>2019-05-17T08:17:14.369Z</updated>
    
    <content type="html"><![CDATA[<h2 id="torch-nn-functional">torch.nn.functional</h2><p>该包提供了很多网络函数</p><h2 id="convoludion-functions">convoludion functions</h2><h3 id="conv2d">conv2d</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torch.nn as nn</span><br><span class="line">import torch.nn.functional as F</span><br><span class="line">from torch.autograd import Variable</span><br><span class="line"></span><br><span class="line">inputs = Variable(torch.randn(64,3,32,32))</span><br><span class="line"></span><br><span class="line">filters1 = Variable(torch.randn(16,3,3,3))</span><br><span class="line">output1 = F.conv2d(inputs,filters1)</span><br><span class="line">print(output1.size())</span><br><span class="line"></span><br><span class="line">filters2 = Variable(torch.randn(16,3,3,3))</span><br><span class="line">output2 = F.conv2d(inputs,filters2,padding=1)</span><br><span class="line">print(output2.size())</span><br></pre></td></tr></table></figure><p>输出</p><blockquote><p>torch.Size([64, 16, 30, 30])<br>torch.Size([64, 16, 32, 32])</p></blockquote><h2 id="relu-functions">relu functions</h2><h2 id="pooling-functions">pooling functions</h2><h2 id="dropout-functions">dropout functions</h2><h3 id="例子">例子</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"> </span><br><span class="line">x = torch.randn(<span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line">y = F.dropout(x, <span class="number">0.5</span>, <span class="literal">True</span>)</span><br><span class="line">y = F.dropout2d(x, <span class="number">0.5</span>)</span><br><span class="line"> </span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure><p>注意$2$中说的问题，不过可能已经被改正了，注意一些就是了。</p><h2 id="linear-functions">linear functions</h2><h2 id="loss-functions">loss functions</h2><h2 id="参考文献">参考文献</h2><p>1.<a href="https://pytorch.org/docs/stable/nn.html#torch-nn-functional" target="_blank" rel="noopener">https://pytorch.org/docs/stable/nn.html#torch-nn-functional</a><br>2.<a href="https://pytorch.org/docs/stable/nn.html#torch-nn-functional" target="_blank" rel="noopener">https://pytorch.org/docs/stable/nn.html#torch-nn-functional</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;torch-nn-functional&quot;&gt;torch.nn.functional&lt;/h2&gt;
&lt;p&gt;该包提供了很多网络函数&lt;/p&gt;
&lt;h2 id=&quot;convoludion-functions&quot;&gt;convoludion functions&lt;/h2&gt;
&lt;h3 id=&quot;c
      
    
    </summary>
    
      <category term="pytorch" scheme="http://mxxhcm.github.io/categories/pytorch/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="pytorch" scheme="http://mxxhcm.github.io/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>pytorch nn</title>
    <link href="http://mxxhcm.github.io/2019/05/08/pytorch-nn/"/>
    <id>http://mxxhcm.github.io/2019/05/08/pytorch-nn/</id>
    <published>2019-05-08T13:55:18.000Z</published>
    <updated>2019-06-07T10:14:29.318Z</updated>
    
    <content type="html"><![CDATA[<h2 id="parameter">Parameter</h2><h3 id="一句话介绍">一句话介绍</h3><p>torch.Tensor的子类，nn.Paramter()声明的变量被赋值给module的属性时，这个变量会自动添加到moudule的parameters list中，parameters()等函数返回的迭代器中可以访问。</p><h3 id="api">API</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Parameter</span><span class="params">(torch.Tensor)</span></span></span><br><span class="line">    # data是weights,requires_grad是是否需要梯度</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__new__</span><span class="params">(cls, data=None, requires_grad=True)</span></span></span><br></pre></td></tr></table></figure><h3 id="代码示例">代码示例</h3><p>下面的代码实现了和nn.Conv2d同样的功能，使用nn.Parameter()将手动创建的变量设置为module的paramters。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CNN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(CNN, self).__init__()</span><br><span class="line">        </span><br><span class="line">        self.cnn1_weight = nn.Parameter(torch.rand(<span class="number">16</span>, <span class="number">1</span>, <span class="number">5</span>, <span class="number">5</span>))</span><br><span class="line">        self.bias1_weight = nn.Parameter(torch.rand(<span class="number">16</span>))</span><br><span class="line">        </span><br><span class="line">        self.cnn2_weight = nn.Parameter(torch.rand(<span class="number">32</span>, <span class="number">16</span>, <span class="number">5</span>, <span class="number">5</span>))</span><br><span class="line">        self.bias2_weight = nn.Parameter(torch.rand(<span class="number">32</span>))</span><br><span class="line">        </span><br><span class="line">        self.linear1_weight = nn.Parameter(torch.rand(<span class="number">4</span> * <span class="number">4</span> * <span class="number">32</span>, <span class="number">10</span>))</span><br><span class="line">        self.bias3_weight = nn.Parameter(torch.rand(<span class="number">10</span>))</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = x.view(x.size(<span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line">        out = F.conv2d(x, self.cnn1_weight, self.bias1_weight)</span><br><span class="line">        out = F.relu(out)</span><br><span class="line">        out = F.max_pool2d(out)</span><br><span class="line">        </span><br><span class="line">        out = F.conv2d(x, self.cnn2_weight, self.bias2_weight)</span><br><span class="line">        out = F.relu(out)</span><br><span class="line">        out = F.max_pool2d(out)</span><br><span class="line">        </span><br><span class="line">        out = F.linear(x, self.linear1_weight, self.bias3_weight)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure><h2 id="container">Container</h2><h3 id="module">Module</h3><p>Module是所有模型的基类。<br>它有以下几个常用的函数和常见的属性。</p><h4 id="常见的函数">常见的函数</h4><ul><li>add_module(name, module)  # 添加一个子module到当前module</li><li>apply(fn) # 对模型中的每一个submodule（调用.children()得到的）都调用fn函数</li><li>_apply() # in-place</li><li>children() # 返回module中所有的子module，不包含整个module，<a href="https://github.com/mxxhcm/code/blob/master/pytorch/pytorch_test/torch_module_child_parameter.py" target="_blank" rel="noopener">详情可见</a></li><li>buffers(recurse=True)     # 返回module buffers的迭代器</li><li>cuda(device=None)    # 将model para和buffer转换到gpu</li><li>cpu()     # 将model para和buffer转换到cpu</li><li>double()  #将float的paramters和buffer转换成double</li><li>float()</li><li>forward(*input) # 前向传播</li><li>eval()    # 将module置为evaluation mode，只影响特定的traing和evaluation modules表现不同的module，比如Dropout和BatchNorm，一般Dropout在训练时使用，在测试时关闭。</li><li>train(mode=True)  # 设置模型为train mode</li><li>load_state_dict(state_dict, strict=True)  # 加载模型参数</li><li>modules()  # 返回network中所有的module的迭代器，包含整个module，<a href="https://github.com/mxxhcm/code/blob/master/pytorch/pytorch_test/torch_module_child_parameter.py" target="_blank" rel="noopener">详情可见</a></li><li>named_modules() # 同时返回包含module和module名字的迭代器</li><li>named_children() # 同时返回包含子module和子module名字的迭代器</li><li>named_parameters() # 同时返回包含parameter和paramter名字的迭代器</li><li>parameters() # 返回模型参数的迭代器</li><li>state_dict(destination=None, prefix=’’, keep_vars=False)  # 返回整个module的state，包含parameters和buffers。</li><li>zero_grad()   # 设置model parameters的gradients为$0$</li><li>to(*args, **kwargs) # 移动或者改变parameters和buffer的类型或位置</li></ul><h4 id="常见的属性">常见的属性</h4><ul><li>self._backend = thnn_backend</li><li>self._parameters = OrderedDict()</li><li>self._buffers = OrderedDict()</li><li>self._backward_hooks = OrderedDict()</li><li>self._forward_hooks = OrderedDict()</li><li>self._forward_pre_hooks = OrderedDict()</li><li>self._state_dict_hooks = OrderedDict()</li><li>self._load_state_dict_pre_hooks = OrderedDict()</li><li>self._modules = OrderedDict()</li><li>self.training = True</li></ul><h4 id="代码示例-v2">代码示例</h4><h5 id="apply">apply</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_weights</span><span class="params">(m)</span>:</span></span><br><span class="line">    print(m)</span><br><span class="line">    <span class="keyword">if</span> type(m) == nn.Linear:</span><br><span class="line">        m.weight.data.fill_(<span class="number">1.0</span>)</span><br><span class="line">        print(m.weight)</span><br><span class="line">net = nn.Sequential(nn.Linear(<span class="number">2</span>, <span class="number">2</span>), nn.Linear(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">net.apply(init_weights)</span><br></pre></td></tr></table></figure><h5 id="module-v2">module</h5><p>关于module,children_modules,parameters的<a href="https://github.com/mxxhcm/code/blob/master/pytorch/pytorch_test/torch_module_child_parameter.py" target="_blank" rel="noopener">代码</a></p><h3 id="sequential">Sequential</h3><h2 id="convolution-layers">Convolution Layers</h2><h3 id="torch-nn-conv2d">torch.nn.Conv2d</h3><h4 id="api-v2">API</h4><p>2维卷积。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.Conv2d(</span><br><span class="line">    in_channels,    <span class="comment"># int，输入图像的通道</span></span><br><span class="line">    out_channels,   <span class="comment"># int，卷积产生的输出通道数（也就是有几个kernel） </span></span><br><span class="line">    kernel_size,    <span class="comment"># int or tuple – kernel的大小 </span></span><br><span class="line">    stride=<span class="number">1</span>,       <span class="comment"># int or tuple, 可选，步长，默认为$1$</span></span><br><span class="line">    padding=<span class="number">0</span>,      <span class="comment"># int or tuple, 可选，向各边添加Zero-padding的数量，默认为$0$</span></span><br><span class="line">    dilation=<span class="number">1</span>,     <span class="comment"># int or tuple, 可选，Spacing between kernel elements. Default: 1</span></span><br><span class="line">    groups=<span class="number">1</span>,       <span class="comment"># int, 可选， Number of blocked connections from input channels to output channels. Default: 1</span></span><br><span class="line">    bias=<span class="literal">True</span>       <span class="comment"># bool，可选，如果为True,给output添加一个可以学习的bias</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h4 id="示例">示例</h4><h5 id="例子1">例子1</h5><p>用$6$个$5\times 5$的filter处理维度为$32\times 32\times 1$的图像。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">model = torch.nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">input = torch.randn(<span class="number">16</span>, <span class="number">1</span>, <span class="number">32</span>, <span class="number">32</span>)</span><br><span class="line">output = model(input)</span><br><span class="line">print(output.size())</span><br><span class="line"><span class="comment"># output: torch.Size([16, 6, 28, 28])</span></span><br></pre></td></tr></table></figure><h5 id="例子2-stride和padding">例子2，stride和padding</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"></span><br><span class="line">inputs = Variable(torch.randn(<span class="number">64</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>))</span><br><span class="line"></span><br><span class="line">m1 = nn.Conv2d(<span class="number">3</span>, <span class="number">16</span>, <span class="number">3</span>)</span><br><span class="line">print(m1)</span><br><span class="line"><span class="comment"># output: Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))</span></span><br><span class="line">output1 = m1(inputs)</span><br><span class="line">print(output1.size())</span><br><span class="line"><span class="comment"># output: torch.Size([64, 16, 30, 30])</span></span><br><span class="line"></span><br><span class="line">m2 = nn.Conv2d(<span class="number">3</span>, <span class="number">16</span>, <span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">print(m2)</span><br><span class="line"><span class="comment"># output: Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))</span></span><br><span class="line">output2 = m2(inputs)</span><br><span class="line">print(output2.size())</span><br><span class="line"><span class="comment"># output: torch.Size([64, 16, 32, 32])</span></span><br></pre></td></tr></table></figure><h2 id="pooling-layers">Pooling Layers</h2><h3 id="maxpool2dd">MaxPool2dd</h3><h4 id="api-v3">API</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">torch</span>.<span class="title">nn</span>.<span class="title">MaxPool2d</span><span class="params">(</span></span></span><br><span class="line"><span class="class"><span class="params">    kernel_size,</span></span></span><br><span class="line"><span class="class"><span class="params">    stride=None,</span></span></span><br><span class="line"><span class="class"><span class="params">    padding=<span class="number">0</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    dilation=<span class="number">1</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    return_indices=False,</span></span></span><br><span class="line"><span class="class"><span class="params">    ceil_mode=False</span></span></span><br><span class="line"><span class="class"><span class="params">)</span></span></span><br></pre></td></tr></table></figure><p>MaxPool2d默认layer stride默认是和kernel_size相同的</p><h4 id="代码示例-v3">代码示例</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch import nn</span><br><span class="line">from torch.autograd import Variable</span><br><span class="line"># maxpool2d</span><br><span class="line"></span><br><span class="line">input = Variable(torch.randn(30,20,32,32))</span><br><span class="line">print(input.size())</span><br><span class="line"># outputtorch.Size([30, 20, 32, 32])</span><br><span class="line"></span><br><span class="line">m1 = nn.MaxPool2d(2)</span><br><span class="line">output = m1(input)</span><br><span class="line">print(output.size())</span><br><span class="line"># output: torch.Size([30, 20, 16, 16])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">m2 = nn.MaxPool2d(5)</span><br><span class="line">print(m2)</span><br><span class="line"># output: MaxPool2d (size=(5, 5), stride=(5, 5), dilation=(1, 1))</span><br><span class="line"></span><br><span class="line">for param in m2.parameters():</span><br><span class="line">  print(param)</span><br><span class="line"></span><br><span class="line">print(m2.state_dict().keys())</span><br><span class="line"># output: []</span><br><span class="line"></span><br><span class="line">output = m2(input)</span><br><span class="line">print(output.size())</span><br><span class="line"># output: torch.Size([30, 20, 6, 6])</span><br></pre></td></tr></table></figure><h2 id="padding-layers">Padding Layers</h2><h2 id="linear-layers">Linear layers</h2><h3 id="linear">Linear</h3><h4 id="api-v4">API</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.Linear(</span><br><span class="line">    in_features, </span><br><span class="line">    out_features, </span><br><span class="line">    bias=<span class="literal">True</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h4 id="代码示例-v4">代码示例</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">m = nn.Linear(<span class="number">20</span>, <span class="number">30</span>)</span><br><span class="line">input = torch.randn(<span class="number">128</span>, <span class="number">20</span>)</span><br><span class="line">output = m(input)</span><br><span class="line">print(output.size())</span><br><span class="line">torch.Size([<span class="number">128</span>, <span class="number">30</span>])</span><br></pre></td></tr></table></figure><h2 id="dropout-layers">Dropout layers</h2><h3 id="drop2d">Drop2D</h3><h4 id="api-v5">API</h4><h4 id="代码示例-v5">代码示例</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line">m = nn.Dropout2d(<span class="number">0.3</span>)</span><br><span class="line">print(m)</span><br><span class="line">inputs = torch.randn(<span class="number">1</span>,<span class="number">28</span>,<span class="number">28</span>)</span><br><span class="line">outputs = m(inputs)</span><br><span class="line">print(outputs)</span><br></pre></td></tr></table></figure><p>输出：</p><blockquote><p>Dropout2d(p=0.3)<br>([[[ 0.8535,  1.0314,  2.7904,  1.2136,  2.7561, -2.0429,  0.0772,<br>-1.9372, -0.0864, -1.4132, -0.1648,  0.2403,  0.5727,  0.8102,<br>0.4544,  0.1414,  0.1547, -0.9266, -0.6033,  0.5813, -1.3541,<br>-0.0536,  0.9574,  0.0554,  0.8368,  0.7633, -0.3377, -1.4293],<br>[ 0.0000,  0.0000, -0.0000, -0.0000,  0.0000, -0.0000,  0.0000,<br>0.0000,  0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000,<br>0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000,<br>-0.0000,  0.0000,  0.0000, -0.0000, -0.0000, -0.0000, -0.0000],<br>…<br>[ 0.6452, -0.6455,  0.2370,  0.1088, -0.5421, -0.5120, -2.2915,<br>0.2061,  1.6384,  2.2276,  2.4022,  0.2033,  0.6984,  0.1254,<br>1.1627,  1.0699, -2.1868,  1.1293, -0.7030,  0.0454, -1.5428,<br>-2.4052, -0.3204, -1.5984,  0.1282,  0.2127, -2.3506, -2.2395]]])</p></blockquote><p>会发现输出的数组中有很多被置为$0$了。</p><h2 id="loss-function">Loss function</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;parameter&quot;&gt;Parameter&lt;/h2&gt;
&lt;h3 id=&quot;一句话介绍&quot;&gt;一句话介绍&lt;/h3&gt;
&lt;p&gt;torch.Tensor的子类，nn.Paramter()声明的变量被赋值给module的属性时，这个变量会自动添加到moudule的parameters
      
    
    </summary>
    
      <category term="pytorch" scheme="http://mxxhcm.github.io/categories/pytorch/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="pytorch" scheme="http://mxxhcm.github.io/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>pytorch Variable(torch.autograd.Variable)</title>
    <link href="http://mxxhcm.github.io/2019/05/08/pytorch-Variable/"/>
    <id>http://mxxhcm.github.io/2019/05/08/pytorch-Variable/</id>
    <published>2019-05-08T13:54:53.000Z</published>
    <updated>2019-05-17T08:14:27.886Z</updated>
    
    <content type="html"><![CDATA[<h3 id="variable-class-torch-autograd-variable">Variable(class torch.autograd.Variable)</h3><h4 id="声明一个tensor">声明一个tensor</h4><ul><li>torch.zeros</li><li>torch.ones</li><li>torch.rand</li><li>torch.full()</li><li>torch.empyt()</li><li>torch.rand()</li><li>torch.randn()</li><li>torch.ones_like()</li><li>torch.zeros_like()</li><li>torch.randn_like()</li><li>torch.Tensor</li></ul><h4 id="代码示例">代码示例</h4><p><a href="https://github.com/mxxhcm/code/blob/master/pytorch/pytorch_test/torch_tensor.py" target="_blank" rel="noopener">代码地址</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">torch.manual_seed(<span class="number">5</span>)</span><br><span class="line">x = torch.empty(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">print(torch.empty(<span class="number">5</span>, <span class="number">3</span>)) <span class="comment"># construct a 5x3 matrix, uninitialized</span></span><br><span class="line"><span class="comment"># tensor([[4.6179e-38, 4.5845e-41, 4.6179e-38],</span></span><br><span class="line"><span class="comment">#         [4.5845e-41, 6.3010e-36, 6.3010e-36],</span></span><br><span class="line"><span class="comment">#         [2.5204e-35, 6.3010e-36, 1.0082e-34],</span></span><br><span class="line"><span class="comment">#         [6.3010e-36, 6.3010e-36, 6.6073e-30],</span></span><br><span class="line"><span class="comment">#         [6.3010e-36, 6.3010e-36, 6.3010e-36]])</span></span><br><span class="line"></span><br><span class="line">print(torch.rand(<span class="number">3</span>, <span class="number">4</span>))  <span class="comment"># construct a 4x3 matrix, uniform [0,1] </span></span><br><span class="line"><span class="comment"># tensor([[0.8303, 0.1261, 0.9075, 0.8199],</span></span><br><span class="line"><span class="comment">#         [0.9201, 0.1166, 0.1644, 0.7379],</span></span><br><span class="line"><span class="comment">#         [0.0333, 0.9942, 0.6064, 0.5646]])</span></span><br><span class="line"></span><br><span class="line">print(torch.randn(<span class="number">5</span>, <span class="number">3</span>)) <span class="comment"># construct a 5x3 matrix, normal distribution</span></span><br><span class="line"><span class="comment"># tensor([[-1.4017, -0.7626,  0.6312],</span></span><br><span class="line"><span class="comment">#         [-0.8991, -0.5578,  0.6907],</span></span><br><span class="line"><span class="comment">#         [ 0.2225, -0.6662,  0.6846],</span></span><br><span class="line"><span class="comment">#         [ 0.5740, -0.5829,  0.7679],</span></span><br><span class="line"><span class="comment">#         [ 0.5740, -0.5829,  0.7679],</span></span><br><span class="line"></span><br><span class="line">print(torch.randn(<span class="number">2</span>, <span class="number">3</span>).type())</span><br><span class="line"><span class="comment"># torch.FloatTensor</span></span><br><span class="line"></span><br><span class="line">print(torch.zeros(<span class="number">5</span>, <span class="number">3</span>)) <span class="comment"># construct a 5x3 matrix filled zeros</span></span><br><span class="line"><span class="comment"># tensor([[0., 0., 0.],</span></span><br><span class="line"><span class="comment">#         [0., 0., 0.],</span></span><br><span class="line"><span class="comment">#         [0., 0., 0.],</span></span><br><span class="line"><span class="comment">#         [0., 0., 0.],</span></span><br><span class="line"><span class="comment">#         [0., 0., 0.]])</span></span><br><span class="line"></span><br><span class="line">print(torch.ones(<span class="number">5</span>, <span class="number">3</span>)) <span class="comment"># construct a 5x3 matrix filled ones</span></span><br><span class="line"><span class="comment"># tensor([[1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 1., 1.]])</span></span><br><span class="line"></span><br><span class="line">print(torch.ones(<span class="number">5</span>, <span class="number">3</span>, dtype=torch.long)) <span class="comment"># construct a tensor with dtype=torch.long</span></span><br><span class="line"><span class="comment"># tensor([[1, 1, 1],</span></span><br><span class="line"><span class="comment">#         [1, 1, 1],</span></span><br><span class="line"><span class="comment">#         [1, 1, 1],</span></span><br><span class="line"><span class="comment">#         [1, 1, 1],</span></span><br><span class="line"><span class="comment">#         [1, 1, 1]])</span></span><br><span class="line"></span><br><span class="line">print(torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])) <span class="comment"># construct a tensor direct from data</span></span><br><span class="line"><span class="comment"># tensor([1, 2, 3])</span></span><br><span class="line"></span><br><span class="line">print(x.new_ones(<span class="number">5</span>,<span class="number">4</span>)) <span class="comment"># constuct a tensor has the same property as x</span></span><br><span class="line"><span class="comment"># tensor([[1., 1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 1., 1., 1.]])</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(torch.full([<span class="number">4</span>,<span class="number">3</span>],<span class="number">9</span>))  <span class="comment"># construct a tensor with a value </span></span><br><span class="line"><span class="comment"># tensor([[9., 9., 9.],</span></span><br><span class="line"><span class="comment">#         [9., 9., 9.],</span></span><br><span class="line"><span class="comment">#         [9., 9., 9.],</span></span><br><span class="line"><span class="comment">#         [9., 9., 9.]])</span></span><br><span class="line"></span><br><span class="line">print(x.new_ones(<span class="number">5</span>,<span class="number">4</span>,dtype=torch.int)) <span class="comment"># construct a tensor with the same property as x, and also can have the specified type.</span></span><br><span class="line"><span class="comment"># tensor([[1, 1, 1, 1],</span></span><br><span class="line"><span class="comment">#         [1, 1, 1, 1],</span></span><br><span class="line"><span class="comment">#         [1, 1, 1, 1],</span></span><br><span class="line"><span class="comment">#         [1, 1, 1, 1],</span></span><br><span class="line"><span class="comment">#         [1, 1, 1, 1]], dtype=torch.int32)</span></span><br><span class="line"></span><br><span class="line">print(torch.randn_like(x,dtype=torch.float)) <span class="comment"># construct a tensor with the same shape with x, </span></span><br><span class="line"><span class="comment"># tensor([[ 0.4699, -1.9540, -0.5587],</span></span><br><span class="line"><span class="comment">#         [ 0.4295, -2.2643, -0.2017],</span></span><br><span class="line"><span class="comment">#         [ 1.0677,  0.3246, -0.0684],</span></span><br><span class="line"><span class="comment">#         [-0.9959,  1.1563, -0.3992],</span></span><br><span class="line"><span class="comment">#         [ 1.2153, -0.8115, -0.8848]])</span></span><br><span class="line"></span><br><span class="line">print(torch.ones_like(x))</span><br><span class="line"><span class="comment"># tensor([[1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 1., 1.],</span></span><br><span class="line"><span class="comment">#         [1., 1., 1.]])</span></span><br><span class="line"></span><br><span class="line">print(torch.zeros_like(x))</span><br><span class="line"><span class="comment"># tensor([[0., 0., 0.],</span></span><br><span class="line"><span class="comment">#         [0., 0., 0.],</span></span><br><span class="line"><span class="comment">#         [0., 0., 0.],</span></span><br><span class="line"><span class="comment">#         [0., 0., 0.],</span></span><br><span class="line"><span class="comment">#         [0., 0., 0.]])</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(torch.Tensor(<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line"><span class="comment"># tensor([[-3.8809e-21,  3.0948e-41,  2.3822e-44,  0.0000e+00],</span></span><br><span class="line"><span class="comment">#         [        nan,  7.2251e+28,  1.3733e-14,  1.8888e+31],</span></span><br><span class="line"><span class="comment">#         [ 4.9656e+28,  4.5439e+30,  7.1426e+22,  1.8759e+28]])</span></span><br><span class="line"></span><br><span class="line">print(torch.Tensor(<span class="number">3</span>,<span class="number">4</span>).uniform_(<span class="number">0</span>,<span class="number">1</span>))</span><br><span class="line"><span class="comment"># tensor([[0.8437, 0.1399, 0.2239, 0.3462],</span></span><br><span class="line"><span class="comment">#         [0.5668, 0.3059, 0.1890, 0.4087],</span></span><br><span class="line"><span class="comment">#         [0.2560, 0.5138, 0.1299, 0.3750]])</span></span><br><span class="line"></span><br><span class="line">print(torch.Tensor(<span class="number">3</span>,<span class="number">4</span>).normal_(<span class="number">0</span>,<span class="number">1</span>))</span><br><span class="line"><span class="comment"># tensor([[-0.5490, -0.0838, -0.1387, -0.5289],</span></span><br><span class="line"><span class="comment">#         [-0.4919, -0.4646, -0.0588,  1.2624],</span></span><br><span class="line"><span class="comment">#         [ 1.1935,  1.5696, -0.8977, -0.1139]])</span></span><br><span class="line"></span><br><span class="line">print(torch.Tensor(<span class="number">3</span>,<span class="number">4</span>).fill_(<span class="number">5</span>))</span><br><span class="line"><span class="comment"># tensor([[5., 5., 5., 5.],</span></span><br><span class="line"><span class="comment">#         [5., 5., 5., 5.],</span></span><br><span class="line"><span class="comment">#         [5., 5., 5., 5.]])</span></span><br><span class="line"></span><br><span class="line">print(torch.arange(<span class="number">1</span>, <span class="number">3</span>, <span class="number">0.4</span>))</span><br><span class="line"><span class="comment"># tensor([1.0000, 1.4000, 1.8000, 2.2000, 2.6000])</span></span><br></pre></td></tr></table></figure><h2 id="tensor的各种操作">tensor的各种操作</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">a = torch.ones(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">b = torch.ones(<span class="number">2</span>,<span class="number">3</span>)</span><br></pre></td></tr></table></figure><h3 id="加操作">加操作</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">print(a+b)                <span class="comment">#方法1</span></span><br><span class="line">c = torch.add(a,b)    <span class="comment">#方法2</span></span><br><span class="line">torch.add(a,b,result)    <span class="comment">#方法3</span></span><br><span class="line">a.add(b)                    <span class="comment">#方法4,将a加上b，且a不变</span></span><br><span class="line">a.add_(b)                <span class="comment">#方法5,将a加上b并将其赋值给a</span></span><br></pre></td></tr></table></figure><h3 id="转置操作">转置操作</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(a.t())               <span class="comment"># 打印出tensor a的转置</span></span><br><span class="line">print(a.t_())                 <span class="comment">#将tensor a 转置，并将其赋值给a</span></span><br></pre></td></tr></table></figure><h3 id="求最大行和列">求最大行和列</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.max(tensor,dim)</span><br><span class="line">np.max(array,dim)</span><br></pre></td></tr></table></figure><h3 id="和relu功能比较类似">和relu功能比较类似。</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.clamp(tensor, min, max,out=<span class="literal">None</span>)</span><br><span class="line">np.maximun(x1, x2)  <span class="comment"># x1 and x2 must hava the same shape</span></span><br></pre></td></tr></table></figure><h2 id="tensor和numpy转化">tensor和numpy转化</h2><h3 id="convert-tensor-to-numpy">convert tensor to numpy</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = torch.ones(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">b = a.numpy()</span><br></pre></td></tr></table></figure><h3 id="convert-numpy-to-tensor">convert numpy to tensor</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a =  numpy.ones(<span class="number">4</span>,<span class="number">3</span>)</span><br><span class="line">b = torch.from_numpy(a)</span><br></pre></td></tr></table></figure><h2 id="variable和tensor">Variable和Tensor</h2><p>Variable<br>图1.Variable</p><h3 id="属性">属性</h3><p>如图1,Variable wrap a Tensor,and it has six attributes,data,grad,requies_grad,volatile,is_leaf and grad_fn.We can acess the raw tensor through .data operation, we can accumualte gradients w.r.t this Variable into .grad,.Finally , creator attribute will tell us how the Variable were created,we can acess the creator attibute by .grad_fn,if the Variable was created by the user,then the grad_fn is None,else it will show us which Function created the Variable.<br>if the grad_fn is None,we call them graph leaves</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Variable.shape  <span class="comment">#查看Variable的size</span></span><br><span class="line">Variable.size()</span><br></pre></td></tr></table></figure><h3 id="parameters">parameters</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.autograd.Variable(data,requires_grad=<span class="literal">False</span>,volatile=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><p>requires_grad : indicate whether the backward() will ever need to be called</p><h3 id="backward">backward</h3><p>backward(gradient=None,retain_graph=None,create_graph=None,retain_variables=None)<br>如果Variable是一个scalar output，我们不需要指定gradient，但是如果Variable不是一个scalar，而是有多个element，我们就需要根据output指定一下gradient，gradient的type可以是tensor也可以是Variable，里面的值为梯度的求值比例，例如</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = Variable(torch.Tensor([<span class="number">3</span>,<span class="number">6</span>,<span class="number">4</span>]),requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = Variable(torch.Tensor([<span class="number">5</span>,<span class="number">3</span>,<span class="number">6</span>]),requires_grad=<span class="literal">True</span>)</span><br><span class="line">z = x+y</span><br><span class="line">z.backward(gradient=torch.Tensor([<span class="number">0.1</span>,<span class="number">1</span>,<span class="number">10</span>]))</span><br></pre></td></tr></table></figure><p>这里[0.1,1,10]分别表示的是对正常梯度分别乘上$0.1,1,10$，然后将他们累积在leaves Variable上</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">detach()    <span class="comment">#</span></span><br><span class="line">detach_()</span><br><span class="line">register_hook()</span><br><span class="line">register_grad()</span><br></pre></td></tr></table></figure><h2 id="参考文献">参考文献</h2><p>1.<a href="https://pytorch.org/docs/stable/tensors.html" target="_blank" rel="noopener">https://pytorch.org/docs/stable/tensors.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;variable-class-torch-autograd-variable&quot;&gt;Variable(class torch.autograd.Variable)&lt;/h3&gt;
&lt;h4 id=&quot;声明一个tensor&quot;&gt;声明一个tensor&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;to
      
    
    </summary>
    
      <category term="pytorch" scheme="http://mxxhcm.github.io/categories/pytorch/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="pytorch" scheme="http://mxxhcm.github.io/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>pytorch Function(torch.autograd.Function)</title>
    <link href="http://mxxhcm.github.io/2019/05/08/pytorch-Function/"/>
    <id>http://mxxhcm.github.io/2019/05/08/pytorch-Function/</id>
    <published>2019-05-08T13:54:22.000Z</published>
    <updated>2019-05-09T05:37:26.595Z</updated>
    
    <content type="html"><![CDATA[<h3 id="function-class-torch-autograd-funtion">Function(class torch.autograd.Funtion)</h3><h4 id="用法">用法</h4><p>Function一般只定义一个操作，并且它无法保存参数，一般适用于激活函数，pooling等，它需要定义三个方法，<strong>init</strong>(),forward(),backward()（这个需要自己定义怎么求导）<br>Model保存了参数，适合定义一层，如线性层(Linear layer),卷积层(conv layer),也适合定义一个网络。<br>和Model的区别，model只需要定义__init()__,foward()方法，backward()不需要我们定义，它可以由自动求导机制计算。</p><p>Function定义只是一个函数，forward和backward都只与这个Function的输入和输出有关</p><h4 id="functions">functions</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyReLU</span><span class="params">(torch.autograd.Function)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    We can implement our own custom autograd Functions by subclassing</span></span><br><span class="line"><span class="string">    torch.autograd.Function and implementing the forward and backward passes</span></span><br><span class="line"><span class="string">    which operate on Tensors.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        In the forward pass we receive a Tensor containing the input and return a</span></span><br><span class="line"><span class="string">        Tensor containing the output. You can cache arbitrary Tensors for use in the</span></span><br><span class="line"><span class="string">        backward pass using the save_for_backward method.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.save_for_backward(input)</span><br><span class="line">        <span class="keyword">return</span> input.clamp(min=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(self, grad_output)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        In the backward pass we receive a Tensor containing the gradient of the loss</span></span><br><span class="line"><span class="string">        with respect to the output, and we need to compute the gradient of the loss</span></span><br><span class="line"><span class="string">        with respect to the input.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        input, = self.saved_tensors</span><br><span class="line">        grad_input = grad_output.clone()</span><br><span class="line">        grad_input[input &lt; <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">        <span class="keyword">return</span> grad_input</span><br><span class="line"></span><br><span class="line">dtype = torch.FloatTensor</span><br><span class="line"><span class="comment"># dtype = torch.cuda.FloatTensor # Uncomment this to run on GPU</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></span><br><span class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></span><br><span class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create random Tensors to hold input and outputs, and wrap them in Variables.</span></span><br><span class="line">x = Variable(torch.randn(N, D_in).type(dtype), requires_grad=<span class="literal">False</span>)</span><br><span class="line">y = Variable(torch.randn(N, D_out).type(dtype), requires_grad=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create random Tensors for weights, and wrap them in Variables.</span></span><br><span class="line">w1 = Variable(torch.randn(D_in, H).type(dtype), requires_grad=<span class="literal">True</span>)</span><br><span class="line">w2 = Variable(torch.randn(H, D_out).type(dtype), requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">1e-6</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</span><br><span class="line">    <span class="comment"># Construct an instance of our MyReLU class to use in our network</span></span><br><span class="line">    relu = MyReLU()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Forward pass: compute predicted y using operations on Variables; we compute</span></span><br><span class="line">    <span class="comment"># ReLU using our custom autograd operation.</span></span><br><span class="line">    y_pred = relu(x.mm(w1)).mm(w2)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute and print loss</span></span><br><span class="line">    loss = (y_pred - y).pow(<span class="number">2</span>).sum()</span><br><span class="line">    print(t, loss.data[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Use autograd to compute the backward pass.</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update weights using gradient descent</span></span><br><span class="line">    w1.data -= learning_rate * w1.grad.data</span><br><span class="line">    w2.data -= learning_rate * w2.grad.data</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Manually zero the gradients after updating weights</span></span><br><span class="line">    w1.grad.data.zero_()</span><br><span class="line">    w2.grad.data.zero_()</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;function-class-torch-autograd-funtion&quot;&gt;Function(class torch.autograd.Funtion)&lt;/h3&gt;
&lt;h4 id=&quot;用法&quot;&gt;用法&lt;/h4&gt;
&lt;p&gt;Function一般只定义一个操作，并且它无法保存参
      
    
    </summary>
    
      <category term="pytorch" scheme="http://mxxhcm.github.io/categories/pytorch/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="pytorch" scheme="http://mxxhcm.github.io/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>pytorch distributions</title>
    <link href="http://mxxhcm.github.io/2019/05/08/pytorch-distributions/"/>
    <id>http://mxxhcm.github.io/2019/05/08/pytorch-distributions/</id>
    <published>2019-05-08T13:53:46.000Z</published>
    <updated>2019-05-17T08:15:22.519Z</updated>
    
    <content type="html"><![CDATA[<h2 id="torch-distributions">torch.distributions</h2><p>这个库和gym.space库很相似，都是提供一些分布，然后从中采样。<br>常见的有ExponentialFamily,Bernoulli,Binomial,Categorical,Exponential,Gamma,Independent,Laplace,Multinomial,MultivariateNormal。这里不做过程陈述，可以看<a href="http://localhost:4000/2019/04/12/gym%E4%BB%8B%E7%BB%8D/" target="_blank" rel="noopener">gym</a>中。</p><h3 id="categorical">Categorical</h3><p>对应tensorflow中的<a href="https://github.com/mxxhcm/myown_code/blob/master/tf/some_ops/tf_multinominal.py" target="_blank" rel="noopener">tf.multinomial</a>。<br>类原型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CLASS torch.distributions.categorical.Categorical(probs=<span class="literal">None</span>, logits=<span class="literal">None</span>, validate_args=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure><p>参数probs只能是$1$维或者$2$维，而且必须是非负，有限非零和的，然后将其归一化到和为$1$。<br>这个类和torch.multinormal是一样的，从${0,\cdots, K-1}$中按照probs的概率进行采样，$K$是probs.size(-1)，即是size()矩阵的最后一列，$2$维时把第$1$维当成了batch。</p><p>举一个简单的例子，<a href="https://github.com/mxxhcm/code/blob/master/pytorch/pytorch_test/torch_distribution.py" target="_blank" rel="noopener">代码</a>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.distributions <span class="keyword">as</span> diss</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">torch.manual_seed(<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">m = diss.Categorical(torch.tensor([<span class="number">0.25</span>, <span class="number">0.25</span>, <span class="number">0.25</span>, <span class="number">0.25</span> ]))</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">    print(m.sample())</span><br><span class="line"></span><br><span class="line">m = diss.Categorical(torch.tensor([[<span class="number">0.5</span>, <span class="number">0.25</span>, <span class="number">0.25</span>], [<span class="number">0.25</span>, <span class="number">0.25</span>, <span class="number">0.5</span>]]))</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">    print(m.sample())</span><br></pre></td></tr></table></figure><p>输出结果如下：</p><blockquote><p>tensor(2)<br>tensor(1)<br>tensor(1)<br>tensor(1)<br>tensor(1)<br>tensor([2, 2])<br>tensor([1, 2])<br>tensor([0, 1])<br>tensor([0, 2])<br>tensor([0, 0])</p></blockquote><p>作为对比，gym.spaces.Discrete示例如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> gym <span class="keyword">import</span> spaces</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.Discrete</span></span><br><span class="line"><span class="comment"># 取值是&#123;0, 1, ..., n - 1&#125;</span></span><br><span class="line">dis = spaces.Discrete(<span class="number">5</span>)</span><br><span class="line">dis.seed(<span class="number">5</span>)</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">    print(dis.sample())</span><br></pre></td></tr></table></figure><p>输出结果是：</p><blockquote><p>3<br>0<br>1<br>0<br>4</p></blockquote><h2 id="参考文献">参考文献</h2><p>1.<a href="https://pytorch.org/docs/stable/distributions.html" target="_blank" rel="noopener">https://pytorch.org/docs/stable/distributions.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;torch-distributions&quot;&gt;torch.distributions&lt;/h2&gt;
&lt;p&gt;这个库和gym.space库很相似，都是提供一些分布，然后从中采样。&lt;br&gt;
常见的有ExponentialFamily,Bernoulli,Binomial,Cat
      
    
    </summary>
    
      <category term="pytorch" scheme="http://mxxhcm.github.io/categories/pytorch/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="pytorch" scheme="http://mxxhcm.github.io/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>pytorch multiprocessing</title>
    <link href="http://mxxhcm.github.io/2019/05/08/pytorch-multiprocessing/"/>
    <id>http://mxxhcm.github.io/2019/05/08/pytorch-multiprocessing/</id>
    <published>2019-05-08T13:52:42.000Z</published>
    <updated>2019-05-17T08:21:22.708Z</updated>
    
    <content type="html"><![CDATA[<h2 id="torch-multiprocessing">torch.multiprocessing</h2><h3 id="join">join</h3><p>等待调用join()方法的线程执行完毕，然后继续执行。<br>可参见github<a href="https://github.com/mxxhcm/myown_code/tree/master/pytorch/tutorials/multiprocess_torch/mnist_hogwild" target="_blank" rel="noopener">官方demo</a>。</p><h3 id="share-memory">share_memory_()</h3><p>在多个线程之间共享参数，如下<a href="https://github.com/mxxhcm/myown_code/blob/master/pytorch/tutorials/multiprocess_torch/share_memory.py" target="_blank" rel="noopener">代码</a>所示。可以用来实现A3C。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.multiprocessing <span class="keyword">as</span> mp</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">proc</span><span class="params">(sec, x)</span>:</span></span><br><span class="line">   print(os.getpid(),<span class="string">"  "</span>, x)</span><br><span class="line">   time.sleep(sec)</span><br><span class="line">   print(os.getpid(), <span class="string">"  "</span>, x)</span><br><span class="line">   x += sec</span><br><span class="line">   print(str(os.getpid()) + <span class="string">"  over.  "</span>, x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">   num_processes = <span class="number">3</span></span><br><span class="line">   processes = []</span><br><span class="line">   x = torch.ones([<span class="number">3</span>,])</span><br><span class="line">   x.share_memory_()</span><br><span class="line">   <span class="keyword">for</span> rank <span class="keyword">in</span> range(num_processes):</span><br><span class="line">     p = mp.Process(target=proc, args=(rank + <span class="number">1</span>, x))</span><br><span class="line">     p.start() </span><br><span class="line">     processes.append(p)</span><br><span class="line">   <span class="keyword">for</span> p <span class="keyword">in</span> processes:</span><br><span class="line">     p.join()</span><br><span class="line">   print(x)</span><br></pre></td></tr></table></figure><p>输出结果如下所示：</p><blockquote><p>python share_memory.py<br>7739    tensor([1., 1., 1.])<br>7738    tensor([1., 1., 1.])<br>7737    tensor([1., 1., 1.])<br>7737    tensor([1., 1., 1.])<br>7737  over.   tensor([2., 2., 2.])<br>7738    tensor([2., 2., 2.])<br>7738  over.   tensor([4., 4., 4.])<br>7739    tensor([4., 4., 4.])<br>7739  over.   tensor([7., 7., 7.])<br>tensor([7., 7., 7.])</p></blockquote><p>我们可以发现$7739$这个线程中，传入的$x$还是和最开始的一样，但是在$7738$线程更新完$x$之后，$7739$使用的$x$就已经变成了更新后的$x$。所以，我猜测这里面应该是有一个对$x$的锁，保证$x$在同一时刻只能被一个线程访问。</p><h2 id="参考文献">参考文献</h2><p>1.<a href="https://pytorch.org/docs/stable/multiprocessing.html" target="_blank" rel="noopener">https://pytorch.org/docs/stable/multiprocessing.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;torch-multiprocessing&quot;&gt;torch.multiprocessing&lt;/h2&gt;
&lt;h3 id=&quot;join&quot;&gt;join&lt;/h3&gt;
&lt;p&gt;等待调用join()方法的线程执行完毕，然后继续执行。&lt;br&gt;
可参见github&lt;a href=&quot;https
      
    
    </summary>
    
      <category term="pytorch" scheme="http://mxxhcm.github.io/categories/pytorch/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="pytorch" scheme="http://mxxhcm.github.io/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>pytorch 常见问题（不定期更新）</title>
    <link href="http://mxxhcm.github.io/2019/05/08/pytorch-problems/"/>
    <id>http://mxxhcm.github.io/2019/05/08/pytorch-problems/</id>
    <published>2019-05-08T13:52:18.000Z</published>
    <updated>2019-05-25T16:07:43.924Z</updated>
    
    <content type="html"><![CDATA[<h2 id="问题1-cudnn-status-arch-mismatch">问题1-CUDNN_STATUS_ARCH_MISMATCH</h2><h3 id="报错">报错</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">RuntimeError: CUDNN_STATUS_ARCH_MISMATCH</span><br></pre></td></tr></table></figure><h3 id="原因">原因</h3><p>CUDNN doesn’t support CUDA arch 2.1 cards.<br>CUDNN requires Compute Capability 3.0, at least.<br>意思是GPU的加速能力不够，CUDNN只支持CUDA Capability 3.0以上的GPU加速，实验室主机是GT620的显卡，2.1的加速能力。<br>GPU对应的capability: <a href="https://developer.nvidia.com/cuda-gpus" target="_blank" rel="noopener">https://developer.nvidia.com/cuda-gpus</a><br>所以，对于不能使用cudnn对cuda加速的显卡，我们可以设置cudnn加速为False，这个默认是为True的<br>torch.backends.cudnn.enabled=False<br>但是，由于显卡版本为2.1，太老了，没有二进制版本。所以，还是会报其他错误，因此，就别使用cpu进行加速啦。</p><h3 id="查看cuda版本">查看cuda版本</h3><p>~#:nvcc --version</p><h2 id="参考文献">参考文献</h2><p>1.<a href="https://pytorch.org/docs/stable/torch.html" target="_blank" rel="noopener">https://pytorch.org/docs/stable/torch.html</a><br>2.<a href="https://pytorch.org/docs/stable/nn.html" target="_blank" rel="noopener">https://pytorch.org/docs/stable/nn.html</a><br>3.<a href="http://pytorch.org/tutorials/beginner/pytorch_with_examples.html" target="_blank" rel="noopener">http://pytorch.org/tutorials/beginner/pytorch_with_examples.html</a><br>4.<a href="https://discuss.pytorch.org/t/distributed-model-parallelism/10377" target="_blank" rel="noopener">https://discuss.pytorch.org/t/distributed-model-parallelism/10377</a><br>5.<a href="https://ptorch.com/news/40.html" target="_blank" rel="noopener">https://ptorch.com/news/40.html</a><br>6.<a href="https://discuss.pytorch.org/t/distributed-data-parallel-freezes-without-error-message/8009" target="_blank" rel="noopener">https://discuss.pytorch.org/t/distributed-data-parallel-freezes-without-error-message/8009</a><br>7.<a href="https://discuss.pytorch.org/t/runtimeerror-cudnn-status-arch-mismatch/3580" target="_blank" rel="noopener">https://discuss.pytorch.org/t/runtimeerror-cudnn-status-arch-mismatch/3580</a><br>8.<a href="https://discuss.pytorch.org/t/error-when-using-cudnn/577/7" target="_blank" rel="noopener">https://discuss.pytorch.org/t/error-when-using-cudnn/577/7</a><br>10.<a href="https://pytorch.org/docs/stable/distributions.html#categorical" target="_blank" rel="noopener">https://pytorch.org/docs/stable/distributions.html#categorical</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;问题1-cudnn-status-arch-mismatch&quot;&gt;问题1-CUDNN_STATUS_ARCH_MISMATCH&lt;/h2&gt;
&lt;h3 id=&quot;报错&quot;&gt;报错&lt;/h3&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;
      
    
    </summary>
    
      <category term="pytorch" scheme="http://mxxhcm.github.io/categories/pytorch/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="常见问题" scheme="http://mxxhcm.github.io/tags/%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/"/>
    
      <category term="pytorch" scheme="http://mxxhcm.github.io/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow assign</title>
    <link href="http://mxxhcm.github.io/2019/05/08/tensorflow-assign/"/>
    <id>http://mxxhcm.github.io/2019/05/08/tensorflow-assign/</id>
    <published>2019-05-08T12:48:06.000Z</published>
    <updated>2019-05-08T13:51:32.261Z</updated>
    
    <content type="html"><![CDATA[<h2 id="tf-assign">tf.assign</h2><h3 id="简单解释">简单解释</h3><p>op = x.assign(y)<br>将y的值赋值给x，执行sess.run(op)后，x的值就变成和y一样了。</p><h3 id="代码示例">代码示例</h3><p><a href="https://github.com/mxxhcm/code/blob/master/tf/some_ops/tf_assign.py" target="_blank" rel="noopener">代码地址</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 声明两个Variable</span></span><br><span class="line">x1 = tf.Variable([<span class="number">3</span>,<span class="number">4</span>])</span><br><span class="line">x2 = tf.Variable([<span class="number">9</span>,<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># y是将x2 assign 给x1的op</span></span><br><span class="line">y = x1.assign(x2)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">  sess.run(tf.global_variables_initializer())</span><br><span class="line">  xx1 = sess.run(x1)</span><br><span class="line">  <span class="comment"># 输出x1</span></span><br><span class="line">  print(xx1)</span><br><span class="line">  <span class="comment"># [3 4]</span></span><br><span class="line"></span><br><span class="line">  xx2 = sess.run(x2)</span><br><span class="line">  <span class="comment"># 输出x2</span></span><br><span class="line">  print(xx2)</span><br><span class="line">  <span class="comment"># [9 1]</span></span><br><span class="line"></span><br><span class="line">  print(sess.run(x1))</span><br><span class="line">  <span class="comment"># [3 4]</span></span><br><span class="line">  </span><br><span class="line">  <span class="comment"># 执行y操作</span></span><br><span class="line">  yy = sess.run(y)</span><br><span class="line">  print(yy)</span><br><span class="line">  <span class="comment"># [9 1]</span></span><br><span class="line">  </span><br><span class="line">  <span class="comment"># 发现x1已经用x2赋值了</span></span><br><span class="line">  print(sess.run(x1))</span><br><span class="line">  <span class="comment"># [9 1]</span></span><br><span class="line">  print(sess.run(x2))</span><br><span class="line">  <span class="comment"># [9 1]</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;tf-assign&quot;&gt;tf.assign&lt;/h2&gt;
&lt;h3 id=&quot;简单解释&quot;&gt;简单解释&lt;/h3&gt;
&lt;p&gt;op = x.assign(y)&lt;br&gt;
将y的值赋值给x，执行sess.run(op)后，x的值就变成和y一样了。&lt;/p&gt;
&lt;h3 id=&quot;代码示例&quot;&gt;代码
      
    
    </summary>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/categories/tensorflow/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow Tensor</title>
    <link href="http://mxxhcm.github.io/2019/05/08/tensorflow-Tensor/"/>
    <id>http://mxxhcm.github.io/2019/05/08/tensorflow-Tensor/</id>
    <published>2019-05-08T12:47:50.000Z</published>
    <updated>2019-05-23T08:01:57.086Z</updated>
    
    <content type="html"><![CDATA[<h2 id="tf-tensor">tf.Tensor</h2><h3 id="目的">目的</h3><ul><li>当做另一个op的输入，各个op通过Tensor连接起来，形成数据流。</li><li>可以使用t.eval()得到Tensor的值。。。</li></ul><h3 id="属性">属性</h3><ul><li>数据类型，float32, int32, string等</li><li>形状</li></ul><p>tf.Tensor一般是各种op操作后产生的变量，如tf.add,tf.log等运算，它的值是不可以改变的，没有assign()方法。</p><h2 id="维度">维度</h2><ul><li>0 标量</li><li>1 向量</li><li>2 矩阵</li><li>3 3阶张量</li><li>n n阶张量</li></ul><h3 id="创建0维">创建0维</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">string_scalar = tf.Variable(<span class="string">"Elephat"</span>, tf.string)</span><br><span class="line">int_scalar = tf.Variable(<span class="number">414</span>, tf.int16)</span><br><span class="line">float_scalar = tf.Variable(<span class="number">3.2345</span>, tf.float64)</span><br><span class="line"><span class="comment"># complex_scalar = tf.Variable(12.3 - 5j, tf.complex64)</span></span><br></pre></td></tr></table></figure><h3 id="创建1维">创建1维</h3><p>需要列表作为初值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">string_vec = tf.Variable([<span class="string">"Elephat"</span>], tf.string)</span><br><span class="line">int_vec = tf.Variable([<span class="number">414</span>, <span class="number">32</span>], tf.int16)</span><br><span class="line">float_vec = tf.Variable([<span class="number">3.2345</span>, <span class="number">32</span>], tf.float64)</span><br><span class="line"><span class="comment"># complex_vec = tf.Variable([12.3 - 5j, 1 + j], tf.complex64)</span></span><br></pre></td></tr></table></figure><h3 id="创建2维">创建2维</h3><p>至少需要包含一行和一列</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">bool_mat = tf.Variable([[<span class="literal">True</span>], [<span class="literal">False</span>]], tf.bool)</span><br><span class="line">string_mat = tf.Variable([<span class="string">"Elephat"</span>], tf.string)</span><br><span class="line">int_mat = tf.Variable([[<span class="number">414</span>], [<span class="number">32</span>]], tf.int16)</span><br><span class="line">float_mat = tf.Variable([[<span class="number">3.2345</span>, <span class="number">32</span>]], tf.float64)</span><br><span class="line"><span class="comment"># complex_mat = tf.Variable([[12.3 - 5j], [1 + j]], tf.complex64)</span></span><br></pre></td></tr></table></figure><h3 id="获取维度">获取维度</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.rank(tensor)</span><br></pre></td></tr></table></figure><h2 id="切片">切片</h2><p>0阶标量不需要索引，本身就是一个数字<br>1阶向量，可以传递一个索引访问某个数字<br>2阶矩阵，可以传递两个数字，返回一个标量，传递1个数字返回一个向量。<br>可以使用:访问，表示不操作该维度。</p><h2 id="获得tensor的shape">获得Tensor的shape</h2><ul><li>tf.Tensor.shape</li><li>tf.shape(tensor) # 返回tensor的shape</li><li>tf.Tensor.get_shape()</li></ul><h2 id="改变tensor的shape">改变tensor的shape</h2><h3 id="api">api</h3><p>tf.reshape(tensor, shape, name=None)</p><ul><li>tensor 输入待操作tensor</li><li>shape reshape后的shape</li></ul><h3 id="代码示例">代码示例</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># t = [1, 2, 3, 4, 5, 6, 7, 8, 9]</span></span><br><span class="line">tf.reshape(t, [<span class="number">3</span>, <span class="number">3</span>])  <span class="comment"># [[1, 2, 3,], [4, 5, 6], [7, 8, 9]]</span></span><br></pre></td></tr></table></figure><h2 id="增加数据维度">增加数据维度</h2><h3 id="api-v2">API</h3><p>tf.expand_dims(input, axis=None, name=None, dim=None)</p><h3 id="代码示例-v2">代码示例</h3><p><a href="https://github.com/mxxhcm/code/blob/master/tf/some_ops/tf_expand_dims.py" target="_blank" rel="noopener">代码地址</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x = tf.placeholder(tf.int32, [<span class="literal">None</span>, <span class="number">10</span>])</span><br><span class="line">y1 = tf.expand_dims(x, <span class="number">0</span>)</span><br><span class="line">y2 = tf.expand_dims(x, <span class="number">1</span>)</span><br><span class="line">y3 = tf.expand_dims(x, <span class="number">2</span>)</span><br><span class="line">y4 = tf.expand_dims(x, <span class="number">-1</span>) <span class="comment"># -1表示最后一维</span></span><br><span class="line"><span class="comment"># y5 = tf.expand_dims(x, 3) error</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">   inputs = np.random.rand(<span class="number">12</span>, <span class="number">10</span>)</span><br><span class="line">   r1, r2, r3, r4 = sess.run([y1, y2, y3, y4], feed_dict=&#123;x: inputs&#125;)</span><br><span class="line">   print(r1.shape)</span><br><span class="line">   print(r2.shape)</span><br><span class="line">   print(r3.shape)</span><br><span class="line">   print(r4.shape)</span><br></pre></td></tr></table></figure><h2 id="改变数据类型">改变数据类型</h2><h3 id="api-v3">API</h3><p>tf.cast(x, dtype, name=None)</p><ul><li>x  # 待转换数据</li><li>dtype # 待转换数据类型</li></ul><h3 id="代码示例-v3">代码示例</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = tf.constant([<span class="number">1.8</span>, <span class="number">2.2</span>], dtype=tf.float32)</span><br><span class="line">tf.cast(x, tf.int32)</span><br></pre></td></tr></table></figure><h2 id="评估张量">评估张量</h2><p>tf.Tensor.eval() 返回一个与Tensor内容相同的numpy数组</p><h3 id="代码示例-v4">代码示例</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">constant = tf.constant([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">tensor = constant * constant</span><br><span class="line">print(tensor.eval()) <span class="comment"># 注意，只有eval()处于活跃的Session中才会起作用。</span></span><br></pre></td></tr></table></figure><h2 id="特殊类型">特殊类型</h2><ul><li>tf.Variable 和tf.Tensor还不一样，<a href>点击查看tf.Variable详细介绍</a></li><li>tf.constant</li><li>tf.placeholder</li><li>tf.SparseTensor</li></ul><h3 id="tf-placeholder">tf.placeholder</h3><h4 id="api-v4">API</h4><p>返回一个Tensor<br>tf.placeholder(dtype, shape=None, name = None)</p><ul><li>dtype  # 类型</li><li>shape  # 形状</li></ul><h4 id="代码示例-v5">代码示例</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x = tf.placeholder(tf.float32, shape=(<span class="literal">None</span>, <span class="number">1024</span>))</span><br><span class="line">y = tf.matmul(x, x)</span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line"><span class="comment"># print(sess.run(y))  this will fail</span></span><br><span class="line">rand_array = np.random.rand(<span class="number">1024</span>, <span class="number">1024</span>)</span><br><span class="line">print(sess.run(y, feed_dict=&#123;x: rand_array&#125;))</span><br></pre></td></tr></table></figure><h3 id="tf-constant">tf.constant</h3><h4 id="api-v5">api</h4><p>tf.constant(values, dtype=None, shape=None, name=‘Const’, verify_shape=False)<br>返回一个constant的Tensor。</p><ul><li>values # 初始值</li><li>dtype # 类型</li><li>shape # 形状</li><li>name  # 可选</li><li>verify_shape</li></ul><h4 id="代码示例-v6">代码示例</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor = tf.constant([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>])</span><br><span class="line">tensor = tf.constant(<span class="number">-1.0</span>, shape=[<span class="number">3</span>, <span class="number">4</span>])</span><br></pre></td></tr></table></figure><h3 id="tf-variable">tf.Variable</h3><h4 id="api-v6">api</h4><p>tf.Variable.__init__(initial_value=None, trainable=True, collections=None, validate_shape=True, caching_device=None, name=None, …)</p><h4 id="代码示例-v7">代码示例</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor1 = tf.Variable([[<span class="number">1</span>,<span class="number">2</span>], [<span class="number">3</span>,<span class="number">5</span>]])</span><br><span class="line">tensor2 = tf.Variable(tf.constant([[<span class="number">1</span>,<span class="number">2</span>], [<span class="number">3</span>,<span class="number">5</span>]]))</span><br><span class="line">sess.run(tf.global_variables_initializer())</span><br><span class="line">sess.run(tensor1)</span><br><span class="line">sess.run(tensor2)</span><br></pre></td></tr></table></figure><h3 id="创建常量tensor">创建常量Tensor</h3><ul><li><p>tf.ones(shape, dtype=tf.float32, name=None)</p></li><li><p>tf.zeros(shape, dtype=tf.float32, name=None)</p></li><li><p>tf.fill(shape, value, name=None)</p></li><li><p>tf.constant(value, dtype=None, shape=None, name=‘Const’)</p></li><li><p>tf.ones_like(tensor, dtype=None, name=None)</p></li><li><p>tf.zeros_like(tensor, dtype=None, name=None)</p></li><li><p>tf.linspace()</p></li></ul><h3 id="创建随机tensor">创建随机Tensor</h3><ul><li>tf.random_uniform(shape, minval=0, maxval=None, dtype=tf.float32, seed=None, name=None)<br><a href="https://www.tensorflow.org/versions/r1.8/api_docs/python/tf/random_uniform" target="_blank" rel="noopener">https://www.tensorflow.org/versions/r1.8/api_docs/python/tf/random_uniform</a></li><li>tf.random_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name=None)<br>均值为mean，方差为stddev的正态分布<br><a href="https://www.tensorflow.org/versions/r1.8/api_docs/python/tf/random_normal" target="_blank" rel="noopener">https://www.tensorflow.org/versions/r1.8/api_docs/python/tf/random_normal</a></li><li>tf.truncated_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name=None)<br>均值为mean，方差为stddev的正态分布，保留[mean-2*stddev, mean+2*stddev]之内的随机数。</li><li>tf.random_shuffle(value, seed=None, name=None)<br>对value的第一维重新排列</li></ul><h2 id="代码示例-v8">代码示例</h2><p><a href="https://github.com/mxxhcm/code/blob/master/tf/some_ops/tf_tensor.py" target="_blank" rel="noopener">代码地址</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line"></span><br><span class="line">x = tf.constant([[<span class="number">3</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">8</span>]])</span><br><span class="line">print(sess.run(tf.constant([<span class="number">3</span>,<span class="number">4</span>])))</span><br><span class="line"><span class="comment"># [3 4]</span></span><br><span class="line"></span><br><span class="line">print(sess.run(tf.ones_like(x)))</span><br><span class="line">[[<span class="number">1</span> <span class="number">1</span>]</span><br><span class="line"> [<span class="number">1</span> <span class="number">1</span>]]</span><br><span class="line"></span><br><span class="line">print(sess.run(tf.zeros_like(x)))</span><br><span class="line">[[<span class="number">0</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">0</span> <span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出正态分布的随机采样值</span></span><br><span class="line">print(sess.run(tf.random_normal([<span class="number">2</span>,<span class="number">2</span>])))</span><br><span class="line"><span class="comment"># [[-0.5188188   0.77538687]</span></span><br><span class="line"> [ <span class="number">1.2343276</span>  <span class="number">-0.58534193</span>]]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出均匀[0,1]分布的随机采样值。</span></span><br><span class="line">print(sess.run(tf.random_uniform([<span class="number">2</span>,<span class="number">2</span>])))</span><br><span class="line">[[<span class="number">0.8851745</span>  <span class="number">0.12824357</span>]</span><br><span class="line"> [<span class="number">0.28489232</span> <span class="number">0.76961493</span>]]</span><br><span class="line"></span><br><span class="line">print(sess.run(tf.random_uniform([<span class="number">2</span>,<span class="number">2</span>], dtype=tf.int32, maxval=<span class="number">4</span>)))</span><br><span class="line">[[<span class="number">0</span> <span class="number">2</span>]</span><br><span class="line"> [<span class="number">2</span> <span class="number">1</span>]]</span><br><span class="line"></span><br><span class="line">print(sess.run(tf.ones([<span class="number">3</span>, <span class="number">4</span>])))</span><br><span class="line">[[<span class="number">1.</span> <span class="number">1.</span> <span class="number">1.</span> <span class="number">1.</span>]</span><br><span class="line"> [<span class="number">1.</span> <span class="number">1.</span> <span class="number">1.</span> <span class="number">1.</span>]</span><br><span class="line"> [<span class="number">1.</span> <span class="number">1.</span> <span class="number">1.</span> <span class="number">1.</span>]]</span><br><span class="line"></span><br><span class="line">print(sess.run(tf.zeros([<span class="number">2</span>,<span class="number">2</span>])))</span><br><span class="line">[[<span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span>]]</span><br></pre></td></tr></table></figure><h2 id="参考文献">参考文献</h2><p>1.<a href="https://www.tensorflow.org/guide/tensors?hl=zh_cn" target="_blank" rel="noopener">https://www.tensorflow.org/guide/tensors?hl=zh_cn</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;tf-tensor&quot;&gt;tf.Tensor&lt;/h2&gt;
&lt;h3 id=&quot;目的&quot;&gt;目的&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;当做另一个op的输入，各个op通过Tensor连接起来，形成数据流。&lt;/li&gt;
&lt;li&gt;可以使用t.eval()得到Tensor的值。。。&lt;/li&gt;
&lt;/
      
    
    </summary>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/categories/tensorflow/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow cnn demo</title>
    <link href="http://mxxhcm.github.io/2019/05/08/tensorflow-cnn-demo/"/>
    <id>http://mxxhcm.github.io/2019/05/08/tensorflow-cnn-demo/</id>
    <published>2019-05-08T11:35:01.000Z</published>
    <updated>2019-05-08T12:43:31.108Z</updated>
    
    <content type="html"><![CDATA[<h2 id="tf-nn-conv2d">tf.nn.conv2d</h2><h3 id="代码示例">代码示例</h3><p><a href="https://github.com/mxxhcm/code/blob/master/tf/some_ops/tf_conv2d.py" target="_blank" rel="noopener">代码地址</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv</span><span class="params">(img)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> len(img.shape) == <span class="number">3</span>:</span><br><span class="line">        img = tf.reshape(img, [<span class="number">1</span>]+img.get_shape().as_list())</span><br><span class="line">    fiter = tf.random_normal([<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">1</span>])</span><br><span class="line">    img = tf.nn.conv2d(img, fiter, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">'SAME'</span>)</span><br><span class="line">    print(img.get_shape())</span><br><span class="line">    <span class="keyword">return</span> img</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> skimage <span class="keyword">import</span> data</span><br><span class="line"><span class="comment"># img = data.text()</span></span><br><span class="line">img = data.astronaut()</span><br><span class="line">print(img.shape)</span><br><span class="line">plt.imshow(img)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">x = tf.placeholder(tf.float32, shape=(img.shape))</span><br><span class="line">result = tf.squeeze(conv(x)).eval(feed_dict=&#123;x:img&#125;)</span><br><span class="line">plt.imshow(result)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;tf-nn-conv2d&quot;&gt;tf.nn.conv2d&lt;/h2&gt;
&lt;h3 id=&quot;代码示例&quot;&gt;代码示例&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/mxxhcm/code/blob/master/tf/some_ops/tf_conv2d
      
    
    </summary>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/categories/tensorflow/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow basic operation</title>
    <link href="http://mxxhcm.github.io/2019/05/08/tensorflow-basic-operation/"/>
    <id>http://mxxhcm.github.io/2019/05/08/tensorflow-basic-operation/</id>
    <published>2019-05-08T10:57:41.000Z</published>
    <updated>2019-05-16T01:08:47.714Z</updated>
    
    <content type="html"><![CDATA[<h2 id="创建session">创建Session</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">n = <span class="number">32</span></span><br><span class="line">x = tf.linspace(<span class="number">-3.0</span>, <span class="number">3.0</span>, n)</span><br></pre></td></tr></table></figure><h3 id="普通session">普通Session</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sess = tf.Session()</span><br></pre></td></tr></table></figure><h3 id="交互式session">交互式Session</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">sess = tf.InteractiveSession()</span><br></pre></td></tr></table></figure><h3 id="在sess内执行op">在sess内执行op</h3><h4 id="方法1">方法1</h4><p>sess.run(tf.global_variables_initializer())<br>sess.run(op)<br>代码示例</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sess.run(tf.global_variables_initializer())</span><br><span class="line">result  = sess.run(x)</span><br></pre></td></tr></table></figure><h4 id="方法2">方法2</h4><p>tf.global_variables_initializer().run()<br>sess.run(op)<br>op.eval()<br>代码示例</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tf.global_variables_initializer().run()</span><br><span class="line">x.eval(session=sess)</span><br><span class="line">sess.close()</span><br></pre></td></tr></table></figure><h2 id="新op添加到默认图上">新op添加到默认图上</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">sigma = <span class="number">1.0</span></span><br><span class="line">mean = <span class="number">0.0</span></span><br><span class="line"><span class="comment"># 和x的shape是一样的</span></span><br><span class="line">z = (tf.exp(tf.negative(tf.pow(x - mean, <span class="number">2.0</span>) /</span><br><span class="line">                        (<span class="number">2.0</span> * tf.pow(sigma, <span class="number">2.0</span>)))) *</span><br><span class="line">     (<span class="number">1.0</span> / (sigma * tf.sqrt(<span class="number">2.0</span> * <span class="number">3.1415</span>))))</span><br><span class="line">print(type(z))</span><br><span class="line">print(z.graph <span class="keyword">is</span> tf.get_default_graph())</span><br><span class="line"></span><br><span class="line">plt.plot(z.eval())</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h2 id="查看shape">查看shape</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">print(z.shape)</span><br><span class="line">print(z.get_shape())</span><br><span class="line">print(z.get_shape().as_list())</span><br><span class="line">print(tf.shape(z).eval())</span><br></pre></td></tr></table></figure><h2 id="常用function">常用function</h2><h3 id="tf-stack">tf.stack</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">print(tf.stack([tf.shape(z),tf.shape(z),[<span class="number">3</span>]]).eval())</span><br><span class="line"><span class="comment"># tf.reshape, tf.matmul</span></span><br><span class="line">z_ = tf.matmul(tf.reshape(z, (n, <span class="number">1</span>)), tf.reshape(z, (<span class="number">1</span>, n)))</span><br><span class="line">plt.imshow(z_.eval()) plt.show()</span><br></pre></td></tr></table></figure><h3 id="tf-ones-like-tf-multiply">tf.ones_like, tf.multiply</h3><p>tf.ones_like返回与输入tensor具有相同shape的tensor</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">x = tf.reshape(tf.sin(tf.linspace(- <span class="number">3.0</span>, <span class="number">3.0</span>, n)), (n, <span class="number">1</span>))</span><br><span class="line">print(x.shape)</span><br><span class="line">y = tf.reshape(tf.ones_like(x), (<span class="number">1</span>, n))</span><br><span class="line">print(y.shape)</span><br><span class="line">print(y.eval())</span><br><span class="line">z = tf.multiply(tf.matmul(x,y), z_)</span><br><span class="line">print(z.shape)</span><br><span class="line">plt.imshow(z.eval())</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h2 id="列出graph中所有操作">列出graph中所有操作</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">ops = tf.get_default_graph().get_operations()</span><br><span class="line">print([op <span class="keyword">for</span> op <span class="keyword">in</span> ops])</span><br></pre></td></tr></table></figure><h2 id="代码">代码</h2><p><a href="https://github.com/mxxhcm/code/blob/master/tf/some_ops/tf_basic.py" target="_blank" rel="noopener">完整地址</a></p><h2 id="参考文献">参考文献</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;创建session&quot;&gt;创建Session&lt;/h2&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span c
      
    
    </summary>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/categories/tensorflow/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow boolean_mask</title>
    <link href="http://mxxhcm.github.io/2019/05/08/tensorflow-boolean-mask/"/>
    <id>http://mxxhcm.github.io/2019/05/08/tensorflow-boolean-mask/</id>
    <published>2019-05-08T09:46:26.000Z</published>
    <updated>2019-05-12T09:04:21.196Z</updated>
    
    <content type="html"><![CDATA[<h2 id="tf-boolean-mask">tf.boolean_mask</h2><h3 id="简单解释">简单解释</h3><p>用一个mask数组和输入的tensor做与操作，忽略为0的值。</p><h3 id="api">api</h3><p>定义在tensorflow/python/ops/array_ops.py</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tf.boolean_mask(</span><br><span class="line">    tensor, <span class="comment"># 要处理的tensor</span></span><br><span class="line">    mask, <span class="comment"># 掩码，也需要是一个tensor</span></span><br><span class="line">    name=<span class="string">'boolean_mask'</span>, <span class="comment"># 这个op的名字</span></span><br><span class="line">    axis=<span class="literal">None</span> <span class="comment">#</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="代码示例">代码示例</h3><p><a href="https://github.com/mxxhcm/code/blob/master/tf/some_ops/tf_boolean_mask.py" target="_blank" rel="noopener">代码地址</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line">a = tf.Variable([1, 2, 3])</span><br><span class="line">b = tf.Variable([2, 1.0, 4.0])</span><br><span class="line">c = tf.Variable([2, 1.0, 0.0])</span><br><span class="line">d = tf.Variable([2, 0.0, 4.0])</span><br><span class="line">e = tf.Variable([0, 1.0, 4.0])</span><br><span class="line">f = tf.Variable([0, 1.0, 0.0])</span><br><span class="line">g = tf.Variable([0, 0.0, 0.0])</span><br><span class="line"></span><br><span class="line">sess.run(tf.global_variables_initializer())</span><br><span class="line">print(&quot;a: &quot;, sess.run(a))</span><br><span class="line">print(&quot;b: &quot;, sess.run(b))</span><br><span class="line">print(&quot;c: &quot;, sess.run(c))</span><br><span class="line">print(&quot;d: &quot;, sess.run(d))</span><br><span class="line">print(&quot;e: &quot;, sess.run(e))</span><br><span class="line">print(&quot;f: &quot;, sess.run(f))</span><br><span class="line">print(&quot;g: &quot;, sess.run(g))</span><br><span class="line"># c = tf.maximum(a, b)</span><br><span class="line">a1 = tf.boolean_mask(a, b)</span><br><span class="line">a2 = tf.boolean_mask(a, c)</span><br><span class="line">a3 = tf.boolean_mask(a, d)</span><br><span class="line">a4 = tf.boolean_mask(a, e)</span><br><span class="line">a5 = tf.boolean_mask(a, f)</span><br><span class="line">a6 = tf.boolean_mask(a, g)</span><br><span class="line"></span><br><span class="line">print(&quot;tf.boolean(a, b):\n  &quot;, sess.run(a1))</span><br><span class="line">print(&quot;tf.boolean(a, c):\n  &quot;, sess.run(a2))</span><br><span class="line">print(&quot;tf.boolean(a, d):\n  &quot;, sess.run(a3))</span><br><span class="line">print(&quot;tf.boolean(a, e):\n  &quot;, sess.run(a4))</span><br><span class="line">print(&quot;tf.boolean(a, f):\n  &quot;, sess.run(a5))</span><br><span class="line">print(&quot;tf.boolean(a, g):\n  &quot;, sess.run(a6))</span><br></pre></td></tr></table></figure><p>输出如下：</p><blockquote><p>a:  [1 2 3]<br>b:  [2. 1. 4.]<br>c:  [2. 1. 0.]<br>d:  [2. 0. 4.]<br>e:  [0. 1. 4.]<br>f:  [0. 1. 0.]<br>g:  [0. 0. 0.]<br>tf.boolean(a, b):<br>[1 2 3]<br>tf.boolean(a, c):<br>[1 2]<br>tf.boolean(a, d):<br>[1 3]<br>tf.boolean(a, e):<br>[2 3]<br>tf.boolean(a, f):<br>[2]<br>tf.boolean(a, g):<br>[]</p></blockquote><h2 id="参考文献">参考文献</h2><p>1.<a href="http://landcareweb.com/questions/27920/zai-tensorflowzhong-ru-he-cong-pythonde-zhang-liang-zhong-huo-qu-fei-ling-zhi-ji-qi-suo-yin" target="_blank" rel="noopener">http://landcareweb.com/questions/27920/zai-tensorflowzhong-ru-he-cong-pythonde-zhang-liang-zhong-huo-qu-fei-ling-zhi-ji-qi-suo-yin</a><br>2.<a href="https://www.tensorflow.org/api_docs/python/tf/boolean_mask" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/boolean_mask</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;tf-boolean-mask&quot;&gt;tf.boolean_mask&lt;/h2&gt;
&lt;h3 id=&quot;简单解释&quot;&gt;简单解释&lt;/h3&gt;
&lt;p&gt;用一个mask数组和输入的tensor做与操作，忽略为0的值。&lt;/p&gt;
&lt;h3 id=&quot;api&quot;&gt;api&lt;/h3&gt;
&lt;p&gt;定义在ten
      
    
    </summary>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/categories/tensorflow/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow summary</title>
    <link href="http://mxxhcm.github.io/2019/05/08/tensorflow-summary/"/>
    <id>http://mxxhcm.github.io/2019/05/08/tensorflow-summary/</id>
    <published>2019-05-08T09:39:43.000Z</published>
    <updated>2019-06-30T15:19:34.526Z</updated>
    
    <content type="html"><![CDATA[<h2 id="tf-summary">tf.summary</h2><h3 id="目的">目的</h3><p>该模块定义在tensorflow/_api/v1/summary/__init__.py文件中，主要用于可视化。<br>每次运行完一个op之后，调用writer.add_summary()将其写入事件file。因为summary操作实在数据流的外面进行操作的，并不会操作数据，所以需要每次运行完之后，都调用一次写入函数。</p><h2 id="常用api">常用API</h2><h3 id="函数">函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1.定义一个summary scalar op，同时会将这个op加入到tf.GraphKeys.SUMMARIES collection中。</span></span><br><span class="line">tf.summary.scalar(</span><br><span class="line">name, </span><br><span class="line">tensor, <span class="comment"># 一个实数型的Tensor，包含单个的值。</span></span><br><span class="line">collections=<span class="literal">None</span>, <span class="comment"># 可选项，是graph collections keys的list，新的summary op会被添加到这个list of collection。默认的list是[GraphKeys.SUMMARIES]。</span></span><br><span class="line">family=<span class="literal">None</span></span><br><span class="line">)</span><br><span class="line"><span class="comment"># 2.定义一个summary histogram op，同时会将这个op加入到tf.GraphKeys.SUMMARIES collection中。</span></span><br><span class="line">tf.summary.histogram(</span><br><span class="line">    name,</span><br><span class="line">    values, <span class="comment"># 一个实数型的Tensor，任意shape，用来生成直方图。</span></span><br><span class="line">    collections=<span class="literal">None</span>, <span class="comment"># 可选项，是graph collections keys的list，新的summary op会被添加到这个list of collection。默认的list是[GraphKeys.SUMMARIES].</span></span><br><span class="line">    family=<span class="literal">None</span></span><br><span class="line">)</span><br><span class="line"><span class="comment"># 3.将所有定义的summary op集中到一块，如scalar，text，histogram等。</span></span><br><span class="line">tf.summary.merge_all(</span><br><span class="line">    key=tf.GraphKeys.SUMMARIES, <span class="comment">#指定用哪个GraphKey来collect summaries。默认设置为GraphKeys.SUMMARIES.并不是说将他们加入到哪个GraphKey的意思，tf.summary.scalar()等会将op加入到相应的colleection。</span></span><br><span class="line">    scope=<span class="literal">None</span>, <span class="comment">#</span></span><br><span class="line">    name=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h4 id="scalar和histogram的区别">scalar和histogram的区别</h4><p>scalar记录的是一个标量。<br>而histogram记录的是一个分布，可以是任何shape。</p><h4 id="函数示例">函数示例</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">summary_loss = tf.summary.scalar(<span class="string">'loss'</span>, loss)</span><br><span class="line">summary_weights = tf.summary.scalar(<span class="string">'weights'</span>, weights)</span><br><span class="line"><span class="comment"># merged可以代替sumary_loss和summary_weights op。</span></span><br><span class="line">merged = tf.summary.merge_all()</span><br></pre></td></tr></table></figure><p>关于tf.summary.histogram()的示例，<a href="https://github.com/mxxhcm/code/blob/master/tf/some_ops/tf_summary_histogram.py" target="_blank" rel="noopener">可以点击查看。</a></p><h3 id="类">类</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义将Summary数据写入event文件的类</span></span><br><span class="line">tf.summary.FileWriter(</span><br><span class="line">self, </span><br><span class="line">logdir,　</span><br><span class="line">graph=<span class="literal">None</span>, </span><br><span class="line">max_queue=<span class="number">10</span>,</span><br><span class="line">flush_secs=<span class="number">120</span>, </span><br><span class="line">graph_def=<span class="literal">None</span>, </span><br><span class="line">filename_suffix=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h4 id="类内函数">类内函数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将summary op的输出存到event文件(Adds a Summary protocol buffer to the event file.)</span></span><br><span class="line">tf.summary.FileWriter.add_summary(</span><br><span class="line">self,</span><br><span class="line">summary,  <span class="comment"># 一个Summary protocol buffer，一般是sess.run(summary_op)的结果</span></span><br><span class="line">global_step=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h4 id="类示例">类示例</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">writer = tf.summary.FileWriter(<span class="string">"./summary/"</span>)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    summ = sess.run([merged], feed_dict=&#123;x: inputs, y: outputs&#125;)</span><br><span class="line">    writer.add_summary(summ, global_step=i)</span><br></pre></td></tr></table></figure><h2 id="使用流程">使用流程</h2><ol><li>summary_op = tf.summary_scalar() # 声明summary op，会将该op变量加入tf.GraphKeys.SUMMARIES collection</li><li>merged = tf.summary.merge_all() # 将所有summary op合并</li><li>writer = tf.summary.FileWriter() # 声明一个FileWrite文件，用于将Summary数据写入event文件</li><li>output = sess.run([merged]) # 运行merge后的summary op</li><li>writer.add_summary(output) # 将op运行后的结果写入事件文件</li></ol><h2 id="代码示例">代码示例</h2><p><a href="https://github.com/mxxhcm/code/blob/master/tf/some_ops/tf_summary.py" target="_blank" rel="noopener">代码地址</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">graph = tf.Graph()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> graph.as_default():</span><br><span class="line">    <span class="comment"># model parameters</span></span><br><span class="line">    w = tf.Variable([<span class="number">0.3</span>], name=<span class="string">"w"</span>, dtype=tf.float32)</span><br><span class="line">    b = tf.Variable([<span class="number">0.2</span>], name=<span class="string">"b"</span>, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">    x = tf.placeholder(tf.float32, name=<span class="string">"inputs"</span>)</span><br><span class="line">    y = tf.placeholder(tf.float32, name=<span class="string">"outputs"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">'linear_model'</span>):</span><br><span class="line">        linear = w * x + b</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">'cal_loss'</span>):</span><br><span class="line">        loss = tf.reduce_mean(input_tensor=tf.square(y - linear), name=<span class="string">'loss'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">'add_summary'</span>):</span><br><span class="line">        summary_loss = tf.summary.scalar(<span class="string">'MSE'</span>, loss)</span><br><span class="line">        summary_b = tf.summary.scalar(<span class="string">'b'</span>, b[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">'train_model'</span>):</span><br><span class="line">        optimizer = tf.train.GradientDescentOptimizer(<span class="number">0.01</span>)</span><br><span class="line">        train = optimizer.minimize(loss)</span><br><span class="line"></span><br><span class="line">inputs = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]</span><br><span class="line">outputs = [<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session(graph=graph) <span class="keyword">as</span> sess:</span><br><span class="line">    writer = tf.summary.FileWriter(<span class="string">"./summary/"</span>, graph)</span><br><span class="line">    merged = tf.summary.merge_all()</span><br><span class="line"></span><br><span class="line">    init_op = tf.global_variables_initializer()</span><br><span class="line">    sess.run(init_op)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5000</span>):</span><br><span class="line">        _, summ = sess.run([train, merged], feed_dict=&#123;x: inputs, y: outputs&#125;)</span><br><span class="line">        writer.add_summary(summ, global_step=i)</span><br><span class="line"></span><br><span class="line">    w_, b_, l_ = sess.run([w, b, loss], feed_dict=&#123;x: inputs, y: outputs&#125;)</span><br><span class="line">    print(<span class="string">"w: "</span>, w_, <span class="string">"b: "</span>, b_, <span class="string">"loss: "</span>, l_)</span><br><span class="line">    <span class="keyword">for</span> var <span class="keyword">in</span> tf.get_collection(tf.GraphKeys.SUMMARIES):</span><br><span class="line">    <span class="comment">#for var in tf.get_collection(tf.GraphKeys.MODEL_VARIABLES):</span></span><br><span class="line">        print(var)</span><br></pre></td></tr></table></figure><p>使用tensorboard --logdir ./summary/打开tensorboard<br>打开之后在每个图中会看到两个曲线，一个深色，一个浅色，浅色的是真实的值，深色的是在真实值的基础上进行了平滑。在左侧可以调整平滑系数，默认是0.6，如果是0表示不进行平滑，如果是1就成了一条直线。<br>如果多次运行的话，多次的结果都会在图中显示出来，鼠标移动到图中只能看到最新的那次结果。浅色的线是最新运行的结果的真实值，深色的线是平滑后的，设置为0可以看到深色和浅色重合了。横轴STEP表示按步长，RELATIVE表示按相对时间，WALL表示将它们分开显示。<br>对于histogram来说的话，这个它是把每一步中list的值做成了一个直方图，统计在每个范围内出现的值的个数，然后按照时间步展现出来每一步的直方图。但是这个直方图是做了一定优化的，如果拿几个值来测试，最后的结果跟你想的并不一定一样。<br>所以histogram就是展现出了每一步list的值主要集中在哪个地方。有两个mode，overlay和offset，overlay是重叠的。<br>overlay中横轴是bin的取值，纵轴是每个bin的频率，所有的时间步都在一起，每一条线都代表一个时间步的直方图，鼠标悬停上去会显示每一条线的时间步。<br>offset中横轴是bin的取值，纵轴是时间步，所有的直方图按照时间步进行展开，每一时间步都是一条单独的线，鼠标悬停上去会显示每一条线的频率。<br>。</p><h3 id="官网示例">官网示例</h3><p>加了一定注释，<a href="https://github.com/mxxhcm/code/blob/master/tf/ops/tf_summary_example.py" target="_blank" rel="noopener">可以点击查看</a></p><h2 id="所有api">所有API</h2><h3 id="类-v2">类</h3><ul><li>class Event: A ProtocolMessage</li><li>class FileWriter: Writes Summary protocol buffers to event files.</li><li>class FileWriterCache: Cache for file writers.</li><li>class SessionLog: A ProtocolMessage</li><li>class Summary: A ProtocolMessage</li><li>class SummaryDescription: A ProtocolMessage</li><li>class TaggedRunMetadata: A ProtocolMessage</li></ul><h3 id="函数-v2">函数</h3><ul><li>scalar(…): Outputs a Summary protocol buffer containing a single scalar value.</li><li>histogram(…): Outputs a Summary protocol buffer with a histogram.</li><li>image(…): Outputs a Summary protocol buffer with images.</li><li>tensor_summary(…): Outputs a Summary protocol buffer with a serialized tensor.proto.</li><li>audio(…): Outputs a Summary protocol buffer with audio.</li><li>text(…): Summarizes textual data.</li><li>merge(…): Merges summaries.</li><li>merge_all(…): Merges all summaries collected in the default graph.</li><li>get_summary_description(…): Given a TensorSummary node_def, retrieve its SummaryDescription.</li></ul><h2 id="参考文献">参考文献</h2><p>1.<a href="https://www.tensorflow.org/api_docs/python/tf/summary" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/summary</a><br>2.<a href="https://www.tensorflow.org/api_docs/python/tf/summary/scalar" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/summary/scalar</a><br>3.<a href="https://www.tensorflow.org/api_docs/python/tf/summary/histogram" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/summary/histogram</a><br>4.<a href="https://www.tensorflow.org/api_docs/python/tf/summary/merge_all" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/summary/merge_all</a><br>5.<a href="https://www.tensorflow.org/guide/graphs#visualizing_your_graph" target="_blank" rel="noopener">https://www.tensorflow.org/guide/graphs#visualizing_your_graph</a><br>6.<a href="https://www.tensorflow.org/guide/summaries_and_tensorboard" target="_blank" rel="noopener">https://www.tensorflow.org/guide/summaries_and_tensorboard</a><br>7.<a href="https://www.tensorflow.org/tensorboard/r1/histograms" target="_blank" rel="noopener">https://www.tensorflow.org/tensorboard/r1/histograms</a><br>8.<a href="https://ask.csdn.net/questions/760881" target="_blank" rel="noopener">https://ask.csdn.net/questions/760881</a><br>9.<a href="https://gaussic.github.io/2017/08/16/tensorflow-tensorboard/" target="_blank" rel="noopener">https://gaussic.github.io/2017/08/16/tensorflow-tensorboard/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;tf-summary&quot;&gt;tf.summary&lt;/h2&gt;
&lt;h3 id=&quot;目的&quot;&gt;目的&lt;/h3&gt;
&lt;p&gt;该模块定义在tensorflow/_api/v1/summary/__init__.py文件中，主要用于可视化。&lt;br&gt;
每次运行完一个op之后，调用writer
      
    
    </summary>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/categories/tensorflow/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow math</title>
    <link href="http://mxxhcm.github.io/2019/05/08/tensorflow-math/"/>
    <id>http://mxxhcm.github.io/2019/05/08/tensorflow-math/</id>
    <published>2019-05-08T09:38:46.000Z</published>
    <updated>2019-05-10T11:37:24.676Z</updated>
    
    <content type="html"><![CDATA[<h2 id="tf-math">tf.math</h2><ul><li>tf.add(x, y, name=None) # 求和</li><li>tf.sub(x, y, name=None) # 减法</li><li>tf.mul(x, y, name=None) # 乘法</li><li>tf.div(x, y, name=None) # 除法</li><li>tf.mod(x, y, name=None) # 取模</li><li>tf.maximumd(x, y, name=None) # x &gt; y?x:y</li><li>tf.minimum(x, y, name=None) # x &lt; y?x:y</li><li>tf.abs(x, name=None) # 求绝对值</li><li>tf.neg(x, name=None) # 取负</li><li>tf.sign(x, name=None) # 返回符号</li><li>tf.inv(x, name=None) # 取反</li><li>tf.square(x, name=None) # 平方</li><li>tf.round(x, name=None) # 四舍五入</li><li>tf.sqrt(x, name=None) # 开根号</li><li>tf.pow(x, name=None) #</li><li>tf.exp(x, name=None) #</li><li>tf.log(x, name=None) #</li><li>tf.sin(x, name=None) #</li><li>tf.cos(x, name=None) #</li><li>tf.tan(x, name=None) #</li><li>tf.atan(x, name=None) #</li></ul><h2 id="代码示例">代码示例</h2><h3 id="tf-maximum">tf.maximum</h3><p>比较两个tensor，返回element-wise两个tensor的最大值。<br><a href="https://github.com/mxxhcm/myown_code/blob/master/tf/some_ops/tf_maximum.py" target="_blank" rel="noopener">代码地址示例</a>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line">a = tf.Variable([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">b = tf.Variable([<span class="number">2</span>, <span class="number">1</span>, <span class="number">4</span>])</span><br><span class="line"></span><br><span class="line">sess.run(tf.global_variables_initializer())</span><br><span class="line">print(<span class="string">"a: "</span>, sess.run(a))</span><br><span class="line">print(<span class="string">"b: "</span>, sess.run(b))</span><br><span class="line">c = tf.maximum(a, b)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"tf.maximum(a, b):\n  "</span>, sess.run(c))</span><br></pre></td></tr></table></figure><p>输出如下：</p><blockquote><p>a:  [1 2 3]<br>b:  [2 1 4]<br>tf.maximum(a, b):<br>[2 2 4]</p></blockquote><h2 id="参考文献">参考文献</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;tf-math&quot;&gt;tf.math&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;tf.add(x, y, name=None) # 求和&lt;/li&gt;
&lt;li&gt;tf.sub(x, y, name=None) # 减法&lt;/li&gt;
&lt;li&gt;tf.mul(x, y, name=None) #
      
    
    </summary>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/categories/tensorflow/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow multinomial</title>
    <link href="http://mxxhcm.github.io/2019/05/08/tensorflow-multinomial/"/>
    <id>http://mxxhcm.github.io/2019/05/08/tensorflow-multinomial/</id>
    <published>2019-05-08T09:37:45.000Z</published>
    <updated>2019-05-12T09:12:06.018Z</updated>
    
    <content type="html"><![CDATA[<h2 id="tf-multinomial-1-tf-random-categorical-2">tf.multinomial[1] (tf.random.categorical[2])</h2><p>多项分布，采样。</p><h3 id="更新">更新</h3><p>在tensorflow 13.1版本中，提示这个API在未来会被弃用，需要使用tf.random.categorical替代。</p><h3 id="api">API</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tf.multinomial(</span><br><span class="line">    logits, <span class="comment"># 指定样本概率的tf.Tensor</span></span><br><span class="line">    num_samples, <span class="comment"># 样本个数</span></span><br><span class="line">    seed=<span class="literal">None</span>, <span class="comment">#, 0-D</span></span><br><span class="line">    name=<span class="literal">None</span>,</span><br><span class="line">    output_dtype=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="代码示例">代码示例</h3><p><a href="https://github.com/mxxhcm/myown_code/blob/master/tf/some_ops/tf_multinominal.py" target="_blank" rel="noopener">代码地址</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># tf.multinomial(logits, num_samples, seed=None, name=None)</span></span><br><span class="line"><span class="comment"># logits 是一个二维张量，指定概率，num_samples是采样个数</span></span><br><span class="line">sess = tf.Session()</span><br><span class="line">sample = tf.multinomial([[<span class="number">5.0</span>, <span class="number">5.0</span>, <span class="number">5.0</span>], [<span class="number">5.0</span>, <span class="number">4</span>, <span class="number">3</span>]], <span class="number">10</span>) <span class="comment"># 注意logits必须是float</span></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">  print(sess.run(sample))</span><br></pre></td></tr></table></figure><p>输出结果如下:</p><blockquote><p>[[2 1 2 1 0 2 1 1 1 0]<br>[1 0 0 1 0 1 0 1 0 0]]<br>[[2 2 0 2 2 0 2 0 1 2]<br>[1 0 0 2 0 1 0 1 1 0]]<br>[[0 0 0 2 0 0 1 2 0 1]<br>[0 0 0 1 0 1 0 0 0 0]]<br>[[2 1 0 1 1 1 0 0 2 0]<br>[1 0 0 2 0 0 0 0 0 1]]<br>[[1 0 1 0 0 1 2 2 0 0]<br>[1 0 0 0 0 1 1 1 2 0]]</p></blockquote><h2 id="参考文献">参考文献</h2><p>1.<a href="https://www.tensorflow.org/api_docs/python/tf/random/multinomial" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/random/multinomial</a><br>2.<a href="https://www.tensorflow.org/api_docs/python/tf/random/categorical" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/random/categorical</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;tf-multinomial-1-tf-random-categorical-2&quot;&gt;tf.multinomial[1] (tf.random.categorical[2])&lt;/h2&gt;
&lt;p&gt;多项分布，采样。&lt;/p&gt;
&lt;h3 id=&quot;更新&quot;&gt;更新&lt;/h3&gt;
&lt;p&gt;在
      
    
    </summary>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/categories/tensorflow/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow app</title>
    <link href="http://mxxhcm.github.io/2019/05/08/tensorflow-app/"/>
    <id>http://mxxhcm.github.io/2019/05/08/tensorflow-app/</id>
    <published>2019-05-08T09:35:39.000Z</published>
    <updated>2019-05-08T14:09:23.063Z</updated>
    
    <content type="html"><![CDATA[<h2 id="tf-app-flags">tf.app.flags</h2><h3 id="代码示例">代码示例</h3><p><a href="https://github.com/mxxhcm/code/blob/master/tf/some_ops/tf_app.py" target="_blank" rel="noopener">代码地址</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">flags.py</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">flags = tf.app.flags</span><br><span class="line">flags.DEFINE_string(<span class="string">'model'</span>, <span class="string">'mxx'</span>, <span class="string">'Type of model'</span>)</span><br><span class="line">flags.DEFINE_boolean(<span class="string">'gpu'</span>,<span class="string">'True'</span>, <span class="string">'use gpu?'</span>)</span><br><span class="line">FLAGS = flags.FLAGS</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(_)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> k,v <span class="keyword">in</span> FLAGS.flag_values_dict().items():</span><br><span class="line">        print(k, v)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    tf.app.run(main)</span><br></pre></td></tr></table></figure><p>传递参数的方法有两种，一种是命令行~$:python <a href="http://flags.py" target="_blank" rel="noopener">flags.py</a> --model hhhh ，一种是pycharm中传递参数。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;tf-app-flags&quot;&gt;tf.app.flags&lt;/h2&gt;
&lt;h3 id=&quot;代码示例&quot;&gt;代码示例&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/mxxhcm/code/blob/master/tf/some_ops/tf_app.py
      
    
    </summary>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/categories/tensorflow/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow where</title>
    <link href="http://mxxhcm.github.io/2019/05/08/tensorflow-where/"/>
    <id>http://mxxhcm.github.io/2019/05/08/tensorflow-where/</id>
    <published>2019-05-08T09:34:47.000Z</published>
    <updated>2019-05-08T14:12:24.529Z</updated>
    
    <content type="html"><![CDATA[<h2 id="tf-where">tf.where</h2><h3 id="简单解释">简单解释</h3><p>tf.where(conditon) 返回条件为True的下标。<br>tf.where(condition, x=X, y=Y) 条件为True的对应位置值替换为1,为False替换成0。</p><h3 id="api">API</h3><p>定义在tensorflow/python/ops/array_ops.py中。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tf.where(</span><br><span class="line">    condition, <span class="comment"># 条件</span></span><br><span class="line">    x=<span class="literal">None</span>,  <span class="comment"># 操作数1</span></span><br><span class="line">    y=<span class="literal">None</span>,  <span class="comment"># 操作数2</span></span><br><span class="line">    name=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="tf-where-condition-代码示例">tf.where(condition)代码示例</h3><p><a href="https://github.com/mxxhcm/code/blob/master/tf/some_ops/tf_where.py" target="_blank" rel="noopener">代码地址</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">X = tf.placeholder(tf.int32, [<span class="literal">None</span>, <span class="number">7</span>])</span><br><span class="line"></span><br><span class="line">zeros = tf.zeros_like(X)</span><br><span class="line">index = tf.not_equal(X, zeros)</span><br><span class="line">loc = tf.where(index)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    inputs = np.array([[<span class="number">1</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">0</span>, <span class="number">8</span>, <span class="number">6</span>], [<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>]])</span><br><span class="line">    out = sess.run(loc, feed_dict=&#123;X: inputs&#125;)</span><br><span class="line">    print(np.array(out))</span><br><span class="line">    <span class="comment"># 输出12个坐标，表示这个数组中不为0元素的索引。</span></span><br></pre></td></tr></table></figure><h3 id="tf-where-condition-x-x-y-y-代码示例">tf.where(condition, x=X, y=Y)代码示例</h3><p><a href="https://github.com/mxxhcm/code/blob/master/tf/some_ops/tf_where.py" target="_blank" rel="noopener">代码地址</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">inputs = np.array([[<span class="number">1</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">0</span>, <span class="number">8</span>, <span class="number">6</span>], [<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>]])</span><br><span class="line">X = tf.placeholder(tf.int32, [<span class="literal">None</span>, <span class="number">7</span>])</span><br><span class="line">zeros = tf.zeros_like(X)</span><br><span class="line">ones = tf.ones_like(X)</span><br><span class="line">loc = tf.where(inputs, x=ones, y=zeros)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    out = sess.run(loc, feed_dict=&#123;X: inputs&#125;)</span><br><span class="line">    print(np.array(out))</span><br></pre></td></tr></table></figure><h2 id="参考文献">参考文献</h2><p>1.<a href="https://www.tensorflow.org/api_docs/python/tf/where" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/where</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;tf-where&quot;&gt;tf.where&lt;/h2&gt;
&lt;h3 id=&quot;简单解释&quot;&gt;简单解释&lt;/h3&gt;
&lt;p&gt;tf.where(conditon) 返回条件为True的下标。&lt;br&gt;
tf.where(condition, x=X, y=Y) 条件为True的对应位置值替
      
    
    </summary>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/categories/tensorflow/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>ubuntu 18.04 alt tab快捷键</title>
    <link href="http://mxxhcm.github.io/2019/05/07/linux-ubuntu-18-04-alt-tab%E5%BF%AB%E6%8D%B7%E9%94%AE/"/>
    <id>http://mxxhcm.github.io/2019/05/07/linux-ubuntu-18-04-alt-tab快捷键/</id>
    <published>2019-05-07T13:21:07.000Z</published>
    <updated>2019-06-12T03:11:14.516Z</updated>
    
    <content type="html"><![CDATA[<h2 id="问题描述">问题描述</h2><p>ubuntu 16.04中，可以使用alt+tab快捷键在相同的应用中进行切换，在18.04中alt+tab是在不同的应用中切换。事实上，可以使用alt+`在该应用内切换，但是我还是想用alt+tab。</p><h2 id="设置方法">设置方法</h2><p>打开setting &gt;&gt; Devices &gt;&gt; Keyboard<br>找到Switch windows，设置快捷键为alt+tab即可。</p><h2 id="参考文献">参考文献</h2><p>1.<a href="https://superuser.com/questions/394376/how-to-prevent-gnome-shells-alttab-from-grouping-windows-from-similar-apps" target="_blank" rel="noopener">https://superuser.com/questions/394376/how-to-prevent-gnome-shells-alttab-from-grouping-windows-from-similar-apps</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;问题描述&quot;&gt;问题描述&lt;/h2&gt;
&lt;p&gt;ubuntu 16.04中，可以使用alt+tab快捷键在相同的应用中进行切换，在18.04中alt+tab是在不同的应用中切换。事实上，可以使用alt+`在该应用内切换，但是我还是想用alt+tab。&lt;/p&gt;
&lt;h2 id=
      
    
    </summary>
    
      <category term="linux" scheme="http://mxxhcm.github.io/categories/linux/"/>
    
    
      <category term="linux" scheme="http://mxxhcm.github.io/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>linux file perission</title>
    <link href="http://mxxhcm.github.io/2019/05/07/linux-file-perission/"/>
    <id>http://mxxhcm.github.io/2019/05/07/linux-file-perission/</id>
    <published>2019-05-07T09:08:26.000Z</published>
    <updated>2019-05-08T08:26:22.877Z</updated>
    
    <content type="html"><![CDATA[<h2 id="创建文件和目录">创建文件和目录</h2><h3 id="创建文件">创建文件</h3><p>~$:touch file</p><h3 id="创建目录mkdir">创建目录mkdir</h3><p>~$:mkdir -p /dir/my_dir<br>~$:mkdir -p /dir/{test,test1,test2}</p><h3 id="递归创建">递归创建</h3><p>~$:sudo mkdir -p /test/test<br>选项-R 递归的将某目录下所有的文件以及目录全部修改<br>~$:sudo chown -R root:root test</p><h2 id="输出文件">输出文件</h2><h3 id="一次性输出">一次性输出</h3><ul><li>cat<br>-n --打印行号(包含空白行)<br>-b --打印行号(不含空白行)<br>-a --将一些看不见的字符用特殊符号打出来　空格用&quot;<sup>I</sup>&quot; 回车用&quot;$&quot;</li><li>tac(反向列出)  cat --&gt; tac</li><li>nl(添加行号打印)</li></ul><h3 id="分屏输出">分屏输出</h3><ul><li>more</li><li>less</li><li>head [-n number]</li><li>tail</li><li>od<br>非纯文本文件(二进制文件)<br>-t   c ASCII<br>a   默认字符<br>o   octal 八进制<br>d   decimal 十进制<br>f   浮点数<br>x   十六进制</li></ul><h3 id="输出文件类型">输出文件类型</h3><p>file 命令<br>查看文件类型,data或者ASCII或者binary<br>~$:file /usr/bin/passwd</p><h2 id="rwx权限">rwx权限</h2><h3 id="r-读权限查询文件名数据">r-读权限查询文件名数据</h3><h3 id="w-写权限">w-写权限</h3><ul><li>新建文件与目录</li><li>删除文件或者目录</li><li>重命名以及转移文件或者目录</li></ul><h3 id="x-可执行权限">x-可执行权限</h3><ul><li>进入某目录</li><li>切换到该目录（cd命令）<br>!!!能不能进入某一目录只与该目录的x权限有关，如果不拥有某目录的x权限，即使拥有r权限，那么也无法执行该目录下的任何命令<br>但是即使拥有了x权限，但是没有r权限，能进入该目录但是不能打开该目录，因为没有读取的权限。<br>cd - 回到上一次工作的目录</li></ul><h3 id="改变文件或者目录的权限">改变文件或者目录的权限</h3><p>~$:sudo chmod 777 test<br>~$:sudo chmod +x test<br>~$:sudo chmod u=rwx,g=r,o=r test<br>r. u–user  g–group  o–others  a–all</p><h3 id="改变文件或者目录的属主">改变文件或者目录的属主</h3><p>~$:sudo chown root:root test<br>~$:sudo chown root test</p><h3 id="改变属组">改变属组</h3><p>~$:sudo chgrp root test</p><h2 id="umask">umask</h2><p>用户创建文件时一般不应有执行的权限，所以创建文件的默认权限为666也即-rw-rw-rw-，但是目录需要有执行的权限，应为777,即-rwxrwxrwx，使用如下命令查看当前的umask：<br>~$:umask</p><blockquote><p>0002</p></blockquote><p>~$:umask -S</p><blockquote><p>u=rwx,g=rwx,o=rx</p></blockquote><p>第一个与特殊权限有关，后三个与一般权限有关，在创建文件或者目录时，会将um-ask所对应的权限拿掉，即新建文件时:<br>(-rw-rw-rw-)-(--------w-) = (-rw-rw-r–)所以创建文件的一般权限为-rw-rw-r–<br>同理可得创建目录时的权限应该为drwxrwxr-x即775。要修改umask的值，可直接在输入umask后接所要减去的权限<br>即<br>~$:umask 002<br>一般情况下root用户的umask为022，这是为了安全考虑，一般用户的umask是022,即保留了同用户组的写入权利。如果同一个用户组的不同用户无法修改另一个用户的文件，那么就可能是同组成员的创建文件时的默认权限不同，可以用umask修改。</p><h2 id="修改文件时间">修改文件时间</h2><h3 id="stat-filename">stat filename</h3><p>列出该文件的各种时间</p><h3 id="touch">touch</h3><p>-a 仅修改访问时间<br>-t 后面接欲修改的时间而不用当前时间，格式为[YYMMNNhhmm]<br>-d 接欲修改的日趋而不用当前日期，也可以用–date=“时间或者日期”<br>-c 仅修改文件的时间(文件状态改变的时间)<br>-m 仅修改mtime(文件内容被更改的时间)<br>-d和-t修改的是mtime和atime 但是不能修改　ctime<br>~$:touch -d “2 days ago” testtouch<br>~$:touch -t 150929 testtouch</p><p>ls -l 默认显示的是mtime,是内容修改的时间(modify)<br>touch --time=ctime　　ctime 显示的是状态被改变的时间,指的是文件属性和权限发生改变。<br>touch --time=atime    atime 访问时间显示的是最近文件被访问的时间(acess),cat and more可以,但是像ls和stat不会改变</p><p>ls -lc  # chagne state<br>ls -lu  # acess time访问时间<br>ls -l# modify time</p><h2 id="chattr与lsattr-设置文件的隐藏属性">chattr与lsattr 设置文件的隐藏属性。</h2><p>change attributes<br>chattr -i 设置文件不可以被删除(包括root用户)<br>-a 设置文件只能增加数据，而不能删除或者修改文件(如登陆文件)</p><p>chattr +i +a 可以增加文件的隐藏属性，其他属性不变<br>-i -a 可以除去文件的隐藏属性，其他属性不变<br>=i a  仅有=后面的属性</p><p>lsattr 查看文件的隐藏属性</p><h2 id="文件特殊权限">文件特殊权限</h2><p>在文件或者目录中除了rwx外，还会出现s,t,S,T权限</p><h3 id="suid">SUID</h3><p>当s出现在文件所有者的x权限上时，</p><p>如<br>~$:ls -l /usr/bin/passwd<br>-rwsr-xr-x 1 root root …<br>~$:ls -l /etc/shadow<br>-rw-r----- 1 root shadow</p><p>用户密码存在/etc/shadow内，当用户想要修改密码时，可以使用passwd进行修改<br>用户mxx对于/etc/shadow没有任何权限，但是对于/usr/bin/passwd拥有r-x权限，所以可以执行passwd命令，由于在passwd命令中有SUID权限，所以mxx在执行pass-wd命令时，会暂时获得passwd拥有者即root的权限，所以接下来可以用passwd修改/etc/shadow。</p><p>！！！此权限仅可用于二进制程序中，且仅在执行该程序的过程中有效，此外只对于文件有效，对于目录也是无效的</p><h3 id="sgid">SGID</h3><p>当s权限出现在用户组的x权限时，如<br>~$:ls -l /usr/bin/mlocate /var/lib/mlocate/mlocate.db</p><blockquote><p>-rwx–s--x 1 root mlocate</p></blockquote><p>同SUID类似，程序执行者会获得该程序用户组的支持，还可以用在目录上，若该用户在此目录下具有w权限，用户创建的新文件的用户组与此目录组的用户组相同<br>~$:su root<br>~$:mkdir test<br>~$:ls -l test</p><blockquote><p>drwxrwxr-x 2 root root …</p></blockquote><p>~$:chmod 2777 test<br>~$:ls -l test</p><blockquote><p>drwxrwsrwx 2 root root</p></blockquote><p>~$:su mxx<br>~$:cd test<br>~$:touch test</p><blockquote><p>-rw-rw-r-- 1 mxx root …</p></blockquote><p>！！！此权限对于目录以及文件都有效</p><h3 id="sbit">SBIT</h3><p>当t出现在others的x权限上时<br>~$:ls -l /tmp</p><blockquote><p>drwxrwxrwt 13 root root …</p></blockquote><p>用户对于某个目录具有wx的权限，即可以写入的权限，相当于说目录的属主给了用户属组或者其他人的身份，并拥有w的权限，那么也就是说这个用户具有删除属主<br>创建的文件或者目录的删除等权限。但是如果该目录拥有了SBIT的权限，那么该用户就只能删除自己所创建的文件，而不能删除属主所创建的文件。<br>！！！此权限只针对目录有效</p><h3 id="如何设置文件以及目录的特殊权限-suid-4-sgid-2-sbit-1">如何设置文件以及目录的特殊权限(SUID 4,SGID 2 ,SBIT 1)</h3><p>最前面的一位为文件的特殊权限<br>直接用chmod 4755 filename就可以了<br>还可以通过加法来实现，如SUID为u+s,SGID为g+s,SBIT为o+t<br>此外还有大写的S和T，代表空，如<br>~$:chmod 7666 test<br>~$:ls -l test</p><blockquote><p>-rwSrwSrwT 1 mxx mxx</p></blockquote><p>因为s和t都是替代x的，而当文件所有者以及其他用户用户组都没有x的时候，所以就不用说其他的操作了，所以也就为空了</p><h2 id="常见配置文件">常见配置文件</h2><ul><li>/bin:可以被单用户执行的命令。其下的命令可以被root用户和普通用户执行，如cat,cd,cp,date,chown,chmod,等等</li><li>/sbin/:开机过程所需要的，只能被root用户所执行，普通用户只能进行查询，包括与开机，还原系统所需要的命令</li><li>/usr/bin:绝大部分的用户可使用命令都在这里</li><li>/usr/sbin/:服务器所需要的某些软件程序</li><li>/usr/local/sbin:本机自行安装的软件产生的系统执行文件</li><li>/根目录</li><li>/etc  系统的配置文件</li><li>/lib  执行文件所需要的函数库与内核所需要的模块</li><li>/bin  重要执行文件</li><li>/sbin 重要的系统执行文件</li><li>/dev  所需要的设备文件</li></ul><p>这五个目录必须和根目录放在一块。</p><p>根目录最好小一些，将一些经常用到的文件目录(/home:/usr:/var:/tmp与根目录分到不同的分区。因为越大的分区，放入的数据也就越多，出错的几率也就越大，而如果根目录出现问题，系统就可能会出现问题。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;创建文件和目录&quot;&gt;创建文件和目录&lt;/h2&gt;
&lt;h3 id=&quot;创建文件&quot;&gt;创建文件&lt;/h3&gt;
&lt;p&gt;~$:touch file&lt;/p&gt;
&lt;h3 id=&quot;创建目录mkdir&quot;&gt;创建目录mkdir&lt;/h3&gt;
&lt;p&gt;~$:mkdir -p /dir/my_dir&lt;br&gt;

      
    
    </summary>
    
      <category term="linux" scheme="http://mxxhcm.github.io/categories/linux/"/>
    
    
      <category term="linux" scheme="http://mxxhcm.github.io/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>linux search file command</title>
    <link href="http://mxxhcm.github.io/2019/05/07/linux-search-command/"/>
    <id>http://mxxhcm.github.io/2019/05/07/linux-search-command/</id>
    <published>2019-05-07T09:03:36.000Z</published>
    <updated>2019-06-16T09:06:12.689Z</updated>
    
    <content type="html"><![CDATA[<h2 id="命令与文件的查询">命令与文件的查询</h2><h3 id="文件查询">文件查询</h3><ul><li>find</li><li>whereis</li><li>locate</li></ul><p>whereis和locate利用数据库查找，find查找硬盘。</p><h3 id="命令查询">命令查询</h3><ul><li>which</li></ul><h2 id="find">find</h2><p>find从硬盘中查找文件，还可以查找具有特殊要求的文件，如查找文件所有者，文件大小,SUID等等</p><h3 id="与时间有关的参数">与时间有关的参数</h3><p>~$:find /tmp mtime n/+n/-n</p><h3 id="与用户或者用户组有关的文件">与用户或者用户组有关的文件</h3><p>find / -uid n<br>-gid n<br>-user name<br>-group name<br>-nouser<br>-nogroup</p><h3 id="与文件权限或者名称有关的参数">与文件权限或者名称有关的参数</h3><p>find / -name filename<br>-size [±]SIZE<br>-type TYPE[-fbcdls]<br>-perm mode刚好等于mode<br>-perm -mode全部包含<br>-perm /mode任意一个</p><h3 id="find示例">find示例</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> 查找/home/maddpg目录下所有__pycache__目录和文件</span><br><span class="line">find /home/maddpg -name **__pycache__ </span><br><span class="line"><span class="meta">#</span> 查看根目录下所有权限为7000的文件</span><br><span class="line">find / -perm +7000 -exec ls -l &#123;&#125; \;</span><br><span class="line"><span class="meta">#</span> 查找当前目录下size在1k到5k之间的文件，+表示大于，-表示小于</span><br><span class="line">find . -size -5k -a -size +1k # 是会把当前目录也列出来的</span><br></pre></td></tr></table></figure><h2 id="whereis">whereis</h2><h3 id="参数介绍">参数介绍</h3><p>whereis [-bmsu]<br>-b 二进制文件<br>-m manualz路径下的文件(说明文件)<br>-s source源文件<br>-u 不在上述范围的其他特殊文件</p><h2 id="locate">locate</h2><p>locate  查找/var/lic/mlocate数据库内的数据，该数据库每天更新一次可手动更新，updatedb,因为他是每天更新一次，所以可能会找到已删除的文件或者是找不到新建立的文件。</p><h2 id="which">which</h2><h3 id="参数介绍-v2">参数介绍</h3><p>which -a command 列出所有的位置。<br>which command 列出第一个找到的位置</p><h3 id="示例">示例</h3><p>~$:which -a python</p><blockquote><p>/home/mxxmhh/anaconda3/bin/python<br>/usr/bin/python</p></blockquote><p>~$:which pip</p><blockquote><p>/home/mxxmhh/anaconda3/bin/pip</p></blockquote><h2 id="参考文献">参考文献</h2><p>1.《鸟哥的LINUX私房菜》<br>2.<a href="http://www.cnblogs.com/wanqieddy/archive/2011/06/09/2076785.html" target="_blank" rel="noopener">http://www.cnblogs.com/wanqieddy/archive/2011/06/09/2076785.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;命令与文件的查询&quot;&gt;命令与文件的查询&lt;/h2&gt;
&lt;h3 id=&quot;文件查询&quot;&gt;文件查询&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;find&lt;/li&gt;
&lt;li&gt;whereis&lt;/li&gt;
&lt;li&gt;locate&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;whereis和locate利用数据库查找，f
      
    
    </summary>
    
      <category term="linux" scheme="http://mxxhcm.github.io/categories/linux/"/>
    
    
      <category term="linux" scheme="http://mxxhcm.github.io/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>linux file system</title>
    <link href="http://mxxhcm.github.io/2019/05/07/linux-file-system/"/>
    <id>http://mxxhcm.github.io/2019/05/07/linux-file-system/</id>
    <published>2019-05-07T09:02:37.000Z</published>
    <updated>2019-06-16T08:16:52.986Z</updated>
    
    <content type="html"><![CDATA[<h2 id="碎片整理">碎片整理</h2><p>文件写入的block太碎了，文件的读写性能太差，所以可以通过碎片整理将一个文件的block回合在一起FAT文件系统经常需要碎片整理,但是Ext2文件类型是索引式文件系统，所以不太需要经常碎片整理的。</p><h2 id="dumpe2fs-bh">dumpe2fs [-bh]</h2><p>查询每个区段以及superblock的信息</p><h3 id="参数介绍">参数介绍</h3><p>dumpe2fs [-bh]<br>-b<br>-h 仅列出superblock的数据</p><h2 id="df">df</h2><p>查询挂载的设备</p><h3 id="参数介绍-v2">参数介绍</h3><p>df [-haiT]　[dir/file]显示文件系统的整体磁盘用量<br>-a 列出所有的文件<br>-h 显示文件系统的大写，自行显示格式<br>-i 可用的inode<br>-T 联通分区文件系统的名称</p><h2 id="du">du</h2><p>目录或者文件所占的容量</p><h3 id="参数介绍-v3">参数介绍</h3><p>du [-ashkm] [dir/filename] 默认显示的是目录的容量，不包含文件<br>-s 该目录下所有文件的容量，不细列出来<br>-a 显示所有的目录与文件的容量<br>-h 以人们熟悉的大小方式显示出来<br>-k 以kb列出容量<br>-m 以mb列出容量:</p><h2 id="ln-s-链接文件">ln [-s] 链接文件</h2><p>hard-link硬链接，将某个目录下的block多写入一个数据,磁盘的inode与block数量一般不会改变，磁盘容量也不会改变，而且删除一个文件并不会响另一个文件的读写，但是其对于目录是没有作用的，对于不同的文件系统也是没有用的。<br>sybomlic</p><h2 id="磁盘分区-格式化-检验与挂载">磁盘分区，格式化，检验与挂载</h2><p>df + 目录  查看某个目录挂载的磁盘位置<br>eg: df /</p><p>sudo fdisk [-l]　+ 设备　输出后面设备所有的分区内容　如不加设备名称，列　出整个系统。</p><h2 id="新增或者删除分区">新增或者删除分区</h2><p>sudo fdisk + 设备   对设备进行操作<br>partprobe</p><p>sudo mkfs [-t ext2/ext2/vfat] + 设备名　 将某个设备格式化为某种文件系统</p><p>sudo mke2fs [-b block_size] [-i inode_size]  [-L 卷标] [-cj -c 检查磁盘错误　-j 加入日志文件] + 设备名</p><p>sudo fsck [-CAay] [-t filesystem] + 设备<br>-C  使用直方图显示进度<br>-A  依据/etc/fstab内容，扫描设备<br>-a  自动修复检查到的有问题的扇区<br>-y  与-a 类似<br>ext2 ext3 支持额外参数　　[-fD] -f 强制进入设备进行检查<br>-D 对文件系统下的目录进行优化配置<br>sudo badblocks [-sv] + 设备  -s 在屏幕上列出进度 -v 在屏幕上看到进度</p><h2 id="mount">mount</h2><p>挂载文件系统与磁盘 P227</p><h3 id="参数介绍-v4">参数介绍</h3><p>mount [-aoltnL]<br>-a　按照/etc/fstab的配置信息将所有未挂载的磁盘挂载上来<br>-l　可增加label名称<br>-t　加上文件类型<br>-n　默认情况下会将挂载情况写入/etc/mtab，加入-n可以不写入<br>-L　可以利用卷标名来挂载<br>-o 加一些挂载时的额外参数　<br>ro(只读)　rw(可写)<br>async sync 此文件系统是否使用同步写入或者异步的内存机制<br>auto noauto 允许此分区以mount -a自动挂载(auto)<br>dev nodev 是否运行在此分区创建设备文件<br>suid nosuid<br>exec noexec 是否可拥有可执行binary文件<br>user,nouer 设置user参数可以让一般user能对此分区挂载<br>defaults　默认为rw,suid,dev,exec,auto,nouser,async<br>remount 重新挂载，在系统出错，或者重新更新参数时</p><h3 id="示例">示例</h3><p>mount 设备文件名　挂载点</p><p>用卷标名挂载设备<br>~$:mount -L mxx_logical /medic/mxx</p><p>用磁盘设备名挂载<br>~$:mount /dev/sdb1 /mnt/usb<br>~$:mount -t iso9660 /dev/cdrom /media/cdrom<br>~$:mount -o remount,rw,auto /dir<br>~$:mount -o loop ~/my.iso/myfile.iso /mnt/iso</p><h2 id="磁盘参数修改">磁盘参数修改</h2><p>设备用文件来代表通过文件的major与minor数值来替代<br>Major 主设备代码，Minor　次设备代码<br>/dev/hd*  major = 3<br>/dev/sd*  minor = 8</p><h3 id="mknod">mknod</h3><p>mknod [bcp]<br>b   设置设备名称成为一个外部存储文件，如硬盘<br>c   设置设备名称成为一个外部输入文件，如鼠标/键盘<br>p   设置设备名称成为一个FIFO文件</p><h3 id="e2label">e2label</h3><p>e2label /dev/sdb5 + 新的label名称</p><h3 id="tune2fs">tune2fs</h3><p>tune2fs [-jlL]<br>-l  类似 dupme2fs -h<br>-j  将ext2转换为ext3<br>-L  类似于　e2label</p><h3 id="hdparm">hdparm</h3><p>hdparm -Tt /dev/sd*  测试SATA硬盘的读取性能</p><h2 id="挂载-iso文件">挂载.iso文件</h2><p>mount -o loop /home/mxx/ubuntu16.04 /mnt/ubuntu16.04</p><p>dd命令　创建一个大文件<br>dd if=/dev/zero of=/home/mxx/filename bs=1M count=512<br>if–input file/dev/zero 一直输出0<br>of–output file将if中的内容加入到of接的文件名中<br>bs–block size<br>count共有多少个bs</p><h2 id="构建swap空间">构建swap空间</h2><p>例如将第二快硬盘的第五个分区改为swap分区<br>~$:sudo fdisk -l /dev/sdb<br>p<br>t 5<br>82<br>w<br>partprobe<br>将/dev/sdb5更改为swap类型的文件系统<br>~$:mkswap /dev/sdb5<br>~$:free 查看memory以及swap分区的使用情况<br>~$:swapon /dev/sdb5 使用/deb/sdb5的swap分区<br>~$:swapon -s 查看目前使用的swap设备有哪些<br>~$:swapoff /dev/sdb5</p><h2 id="boost-sector与superblock-的关系">boost sector与superblock 的关系</h2><ol><li>superblock的大小为1024b<br>boost sector与superblock 各占一个block ,可以查看/boot的挂载目录<br>0号block给boost ，1号block给superblock</li><li>superblock的大小大于1024b,如为4096b<br>superblock在0号blok ,但是superblock 只有1024b,所以为了防止空间浪费，于是在0号block内，superblock(1024-2047),boost sector(0-1023),2048后面的空间保留。<br>实际情况中，由于在比较大的block中，我们能将引导装载程序安装到superblock所在的0号block,但事实上还是安装到启动扇区的保留区域。<br>比较正确的说法是，安装到文件系统最前面的1024b内的区域，就是启动扇区</li></ol><h2 id="查看文件的inode编号">查看文件的inode编号</h2><p>~$:ls -i<br>目录并不一定只占一个block，当目录内的文件数太多时，会增加该目录的block</p><h2 id="参考文献">参考文献</h2><p>1.《鸟哥的LINUX私房菜》</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;碎片整理&quot;&gt;碎片整理&lt;/h2&gt;
&lt;p&gt;文件写入的block太碎了，文件的读写性能太差，所以可以通过碎片整理将一个文件的block回合在一起FAT文件系统经常需要碎片整理,但是Ext2文件类型是索引式文件系统，所以不太需要经常碎片整理的。&lt;/p&gt;
&lt;h2 id=&quot;d
      
    
    </summary>
    
      <category term="linux" scheme="http://mxxhcm.github.io/categories/linux/"/>
    
    
      <category term="linux" scheme="http://mxxhcm.github.io/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>linux 压缩和备份</title>
    <link href="http://mxxhcm.github.io/2019/05/07/linux-compress-uncompress/"/>
    <id>http://mxxhcm.github.io/2019/05/07/linux-compress-uncompress/</id>
    <published>2019-05-07T09:01:24.000Z</published>
    <updated>2019-06-16T07:46:34.565Z</updated>
    
    <content type="html"><![CDATA[<h2 id="压缩">压缩</h2><ul><li>gzip</li><li>zcat</li><li>bzip2</li><li>bzcat</li><li>gunzip</li><li>bunzip2</li><li>7z</li><li>zip</li><li>rar</li></ul><h3 id="gzip和bzip2">gzip和bzip2</h3><p>gzip和bzip2公用参数</p><h4 id="参数介绍">参数介绍</h4><p>gzip(bzip2)<br>-d 解压缩参数<br>-$ $取1-9　压缩等级 -1最快<br>-v 显示压缩比<br>-k 保留原文件<br>-z 压缩参数<br>-c 将压缩过程中产生的数据输出到屏幕上(压缩后的数据)可以将其输出重定向<br>-t 检验一个压缩文件的一致性</p><h3 id="zip">zip</h3><h4 id="参数介绍-v2">参数介绍</h4><p>zip [-dmbrfFg]<br>-d　从zip文件中移除一个文件<br>-m  将特定文件移入zip文件，且删除特定文件<br>-g　将文件压缩附加到zip文件中<br>-r  包括子目录<br>-f  以新文件取代旧文件<br>-F　修复已经损毁的压缩文件<br>-b　暂存文件的路径<br>-v　显示详细信息<br>-u　值更新改变过的文件<br>-T　测试zip文件是否正常<br>-x　不需要压缩的文件</p><h4 id="示例">示例</h4><p>~$:zip -r myfile.zip ./*<br>~$:zip -d myfile.zip myfile　 //删除压缩文件内的某个文件<br>~$:zip -g myfile.zip myfile   //向一个压缩文件内添加新文件<br>~$:zip -u myzip</p><h3 id="unzip">unzip</h3><h4 id="参数介绍-v3">参数介绍</h4><p>unzip [-dnovj]<br>-v  查看压缩文件目录，但是不解压<br>-d  指定解压到的目录<br>-n　不覆盖已有文件<br>-o　覆盖已有文件<br>-j  不重建文档的目录结构，把所有文件解压到同一目录下</p><h3 id="7z">7z</h3><h4 id="安装">安装</h4><p>~$:apt-get install p7zip</p><h4 id="参数介绍-v4">参数介绍</h4><p>7z [x|a] [-rotr]<br>a　代表添加文件到压缩包<br>x　代表解压缩文件<br>-r 表示递归所有文件<br>-t 制定压缩类型<br>-o 指定解压到的目录</p><h4 id="示例-v2">示例</h4><p>~$:7z a -t 7z -r myfile.7z  ~/myfile<br>~$:7z x myfile.7z -r -o ~/</p><h3 id="rar">rar</h3><h4 id="安装-v2">安装</h4><p>~$:apt-get install rar</p><h4 id="示例-v3">示例</h4><p>~$:rar x myfile.rar<br>~$:rar a myfile.rar myfile</p><h3 id="tar">tar</h3><p>打包</p><h4 id="参数介绍-v5">参数介绍</h4><p>tar [-cxtvfjzCpP]<br>-c  --create<br>-x  --extract<br>-t  --list<br>-v  --verbose<br>-f  --file<br>-j  --bzip2<br>-z  --gzip --gunzip<br>-C  --directory DIR<br>change to directory DIR<br>-p  --preserve-permissions, --same-permissions<br>-P  --absolute-names</p><pre><code>--exclude=file</code></pre><h4 id="示例-v4">示例</h4><p>~$:tar -cvj -f ~/my_bak/etc.newer.passwd.tar.bz2 --newer-mtime=“2016/09/23”/etc/*<br>~$:tar -cvj -f ~/my_bak/etc.tar.gz   /etc<br>~$:tar -xvj -f ~/my_bak/etc.tar.gz -C /tmp<br>~$:tar -xvj -f ~/my_bak/etc.tar.gz |etc/shadow<br>~$:tar -tfj -f ~/my_bak/etc.tar.gz | grep ‘shadow’<br>~$:tar -cv -f /dev/st0 /home /root /etc     # 磁带机/dev/st0</p><h2 id="备份">备份</h2><ul><li>dump</li><li>restore</li><li>mkisofs</li><li>dd</li></ul><h3 id="dump">dump</h3><h4 id="参数介绍-v6">参数介绍</h4><p>dump    [-SujvWf]<br>-Ssize<br>-uupdate    recode the dump time to /var/lib/dumpdates<br>-u只能对level 0 操作<br>-jadd compress bz2(默认压缩等级为 2)<br>-vverbose   详细的<br>-W列出/etc/fstab中的具有dump设置的分区是否被备份过<br>-f<br>-level备份的等级(0-9) 对于文件系统有九个等级<br>对于单个目录只有0级</p><h4 id="示例-v5">示例</h4><p>~$:dump -0u -f /root/etc.dump /etc</p><h3 id="restore">restore</h3><h4 id="参数介绍-v7">参数介绍</h4><p>restore [-tCir]<br>-tlist<br>-Ccompare<br>-iitera<br>-rr</p><h4 id="示例-v6">示例</h4><p>~$:restore -t -f /root/boot.dump<br>~$:restore -C -f /root/boot.dump<br>~$:mkdir test_restore<br>~$:cd test_restore<br>~$:restore -r -f /root/boot.dump<br>~$:restore -i -f /root/etc.dump.bz2</p><h3 id="mkisofs">mkisofs</h3><p>生成iso文件</p><h4 id="参数介绍-v8">参数介绍</h4><p>mkisofs [-orvVm]<br>-o +生成的镜像名<br>-r 记录更完整的文件信息，包括UID,GID与权限等<br>-v 显示构建iso的过程<br>-V 新建Volume<br>-m exclude 排除某文件<br>-graft-point</p><h4 id="示例-v7">示例</h4><p>~$:mkisofs -r -v -o ~/my.iso/system.iso -m /home/lost+found -graft-point/home=/home /root=/root /etc=/etc</p><h3 id="dd">dd</h3><p>可以备份整块硬盘或者整块磁盘包括superblocks以及boot sector等等</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;压缩&quot;&gt;压缩&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;gzip&lt;/li&gt;
&lt;li&gt;zcat&lt;/li&gt;
&lt;li&gt;bzip2&lt;/li&gt;
&lt;li&gt;bzcat&lt;/li&gt;
&lt;li&gt;gunzip&lt;/li&gt;
&lt;li&gt;bunzip2&lt;/li&gt;
&lt;li&gt;7z&lt;/li&gt;
&lt;li&gt;zip&lt;/li&gt;
      
    
    </summary>
    
      <category term="linux" scheme="http://mxxhcm.github.io/categories/linux/"/>
    
    
      <category term="linux" scheme="http://mxxhcm.github.io/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>linux vim</title>
    <link href="http://mxxhcm.github.io/2019/05/07/linux-vim/"/>
    <id>http://mxxhcm.github.io/2019/05/07/linux-vim/</id>
    <published>2019-05-07T08:58:22.000Z</published>
    <updated>2019-06-11T07:30:17.572Z</updated>
    
    <content type="html"><![CDATA[<h2 id="vim-配置文件">vim 配置文件</h2><p>个人vim配置文件一般在~/.vimrc下，可以自定义各种配置。<br><a href>我的vimrc文件</a></p><h3 id="前缀符号">前缀符号</h3><p>为了缓解快捷键冲突问题，就引入了前缀键，跟参考文献[0]一样，设置;号为前缀键。<br>let mapleader=&quot;;&quot;</p><h3 id="设置显示行号">设置显示行号</h3><p>set number</p><h3 id="底部显示文件路径">底部显示文件路径</h3><p>set laststatus=2 “设置底部状态栏可见<br>set statusline=%F%m%r%h%w\ %=#%n\ [%{&amp;fileformat}:%{(&amp;fenc==”&quot;?&amp;enc:&amp;fenc)    .((exists(&quot;+bomb&quot;)\ &amp;&amp;\ &amp;bomb)?&quot;+B&quot;:&quot;&quot;).&quot;&quot;}:%{strlen(&amp;ft)?&amp;ft:’**’    }]\ [%L\%l,%c]\ %p%%    &quot;statusline显示的信息，来自参考文献[8]。<br>&quot;其中%L是当前文件缓冲区的行数，%P是当前行占总行数的百分比。<br>set ruler &quot;显示光标当前位置</p><h2 id="vim复制到系统寄存器">vim复制到系统寄存器</h2><h3 id="vim寄存器">vim寄存器</h3><p>vim有9种寄存器:</p><ol><li>&quot;是未命名寄存器，vim的默认寄存器，存放删除和复制的文本。</li><li>small delete寄存器 -，存放不超过一行的delete操作（不包括x操作）产生的文本。</li><li>编号为$0,1,2,\cdot, 9$的寄存器，</li><li>$a-za-z$的$26$个字母寄存器,</li><li>只读寄存器 : .,%,$</li><li>表达式寄存器 =</li><li>搜索寄存器 /</li><li>GUI选择寄存器$*,+$。</li><li>黑洞寄存器，向这个寄存器写入的话，什么都不会发生。</li></ol><p>详细介绍可见参考文献[4]。</p><h3 id="使用系统剪切板">使用系统剪切板</h3><p>*和+寄存器适合系统相关的，*和系统缓冲区关联，+和系统剪切板关联。<br>使用+y复制当前行到系统剪切板。<br>使用+ny复制n行到系统剪切板。<br>使用+p粘贴系统剪切板到当前位置。<br>但是有些vim发行版不支持系统剪切板，可以使用如下命令查看自己的系统是否支持系统剪切板。<br>~\$:vim --version|grep clipboard<br>在我的系统上，输出如下：</p><blockquote><p>-clipboard         +jumplist          +persistent_undo   +virtualedit<br>-ebcdic            -mouseshape        +statusline        -xterm_clipboard</p></blockquote><p>如果输出+clipboard说明当前vim支持剪切板，-clipboard说明当前vim不支持系统剪切板，所以就卸载安装支持的版本呗。<br>~\$:sudo apt remove vim<br>~\$:sudo apt install vim-gtk3<br>然后再次查看<br>~\$:vim --version|grep clipboard<br>输出如下：</p><blockquote><p>+clipboard         +jumplist          +persistent_undo   +virtualedit<br>-ebcdic            +mouseshape        +statusline        +xterm_clipboard</p></blockquote><p>说明已经支持系统剪切板，可以使用了。注意记得把之前打开的vim关闭后再试。<br>使用以下命令进行操作：<br>+nyy # 复制从当前行开始的n行到+寄存器<br>+yy # 复制当前行行到+寄存器<br>+p # 粘贴+寄存器中的内容到文本中。<br>这个时候还有一个问题，就是一般的笔记本键盘的+和=号是在一起的，如果要打出=行，需要按一下shit +=，这个时候会向下移动一行，但是无伤大雅，为什么会这样，我还不知道。详细流程可参见参考文献[5]。<br>但是后来我发现这个还不能用。然后就只能继续查找了。在知乎上找到一个回答，发现还要在这些命令前加上一个&quot;号，表示将默认&quot;寄存器中的内容复制到+寄存器中。也就是使用如下命令：<br>&quot;+nyy # 复制从当前行开始的n行到+寄存器<br>&quot;+yy # 复制当前行行到+寄存器<br>&quot;+p # 粘贴+寄存器中的内容到文本中。</p><h3 id="将未命名寄存器和系统寄存器设为同一个">将未命名寄存器和系统寄存器设为同一个。</h3><p>修改vim配置文件<br>~$:vim ~/.vimrc<br>添加下面一句话，重新打开vim即可<br>set clipboard=unnamed</p><h2 id="vim模式和常用命令">vim模式和常用命令</h2><h3 id="vim模式">vim模式</h3><ul><li>正常模式，用vim打开一个文件之后就处于正常模式</li><li>插入模式，在正常模式下输入i,a,o或者I,A,O之后，就进入了命令模式，可以修改文件，按Esc退出。</li><li>Visual模式，可以移动光标选中某些行，进行复制或者删除，在正常模式按v或者V进入Visual模式。</li><li>命令模式，在正常模式按:进入命令模式，可以在窗口底部输入命令。</li><li>替换模式，使用r替换当前字符，使用R从当前字符开始连续替换。</li></ul><h3 id="正常模式">正常模式</h3><h4 id="移动光标">移动光标</h4><p>0 移动到行首<br>$ 移动到行尾<br>h 向左移动一个character<br>j 向下移动一行<br>k 向上移动一行<br>l 向右移动一个character<br>nj nk nh nl 移动n次<br>oO o在当前行的下一行插入，O在当前行的上一行插入<br>iI i在当前光标处插入，I在行首插入<br>aA a在当前光标后插入，A在行尾插入<br>1G 跳到第一行<br>gg 跳到首行<br>G 跳到尾行<br>nG 跳到尾行<br>n-space 跳到光标后第n个character<br>n-ENTER nG 跳到第n行<br>wW w移动到下一个word的开头，W移动到隔了一个空白符的下一个word的开头<br>bB b移动到前一个word的开头，B移动到隔了一个空白符的前一个word开头<br>eE 移动到当前word的结尾，W移动到隔了一个空白符的word结尾。<br>ctrl+f 跳到下一页<br>ctrl+b 跳到上一页</p><h4 id="删除和复制">删除和复制</h4><p>x 删除一个character<br>nx 删除n个characters<br>dd 删除当前行<br>ndd 删除n行<br>yy 复制一行<br>nyy 复制n行<br>p 粘贴</p><h3 id="命令模式">命令模式</h3><h4 id="切屏">切屏</h4><p>:sp [filename]<br>输入:进入命令模式，然后输入sp，空格，要打开的文件名。使用ctrl w在分开的屏幕之间进行切换。</p><h4 id="查找">查找</h4><p>/word ?word<br>n N</p><h4 id="替换和删除">替换和删除</h4><p>:1,10s/word/word.rp/g©<br>:1,$s/word/word.rp/g©<br>利用正则表达式可以实现下面的一些常用命令<br><a href="https://github.com/mxxhcm/code/tree/master/shell/vim_regex" target="_blank" rel="noopener">代码地址</a></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> 替换所有的^为\^</span><br><span class="line">:0,$s/\^/\\^/g</span><br><span class="line"><span class="meta">#</span> 替换所有的\*\*为##</span><br><span class="line">:0,$s/[0-9][0-9]\./## /g</span><br><span class="line"><span class="meta">#</span> 删除所有以tab开头的tab</span><br><span class="line">:0,$s/^\t//g</span><br><span class="line"><span class="meta">#</span> 删除所有以#开头的行</span><br><span class="line">:g/^#\.\*$/d</span><br><span class="line"><span class="meta">#</span> 删除所有空行</span><br><span class="line">:g/^\s\*$/d</span><br><span class="line"><span class="meta">#</span> 用newlines替换,</span><br><span class="line">:0,$s/,/\r/g</span><br><span class="line"><span class="meta">#</span> 在re.*后面加上括号</span><br><span class="line"><span class="meta">#</span> re.Ire.IGNORECASE)</span><br><span class="line"><span class="meta">#</span> re.Lre.LOCALE)</span><br><span class="line"><span class="meta">#</span> re.Mre.MULTILINE)</span><br><span class="line"><span class="meta">#</span> re.sre.DOTALL)</span><br><span class="line"><span class="meta">#</span> re.Ure.UNICODE)</span><br><span class="line"><span class="meta">#</span> re.Xre.VERBOSE)</span><br><span class="line">:m,ns/\(^re\.[A-Z]\)/\1(/g</span><br></pre></td></tr></table></figure><h4 id="其他">其他</h4><p>:w [filename]<br>:r [filename]<br>:n1,n2 w [filename]<br>:set nu</p><h3 id="visual模式">Visual模式</h3><p>见参考文献[9]。</p><h2 id="快捷键映射">快捷键映射</h2><ul><li>namp 正常模式下的递归映射</li><li>vmap Visual模式</li><li>imap 插入模式</li><li>cmap 命令模式</li><li>nnoremap 正常模式下的非递归映射</li><li>vnoremap Visual模式下的非递归映射</li><li>inoremap 插入模式下的非递归映射</li><li>cnoremap 命令模式下的非递归映射</li></ul><h2 id="其他vim使用事项">其他vim使用事项</h2><h3 id="编码">编码</h3><p>tty1-tty6默认不支持中文编码　　<br>修改终端接口语系　<br>LANG=zh_CN.big5</p><h3 id="dos和unix转换">dos和UNIX转换</h3><p>ubuntu  don’t have dos2UNIX or UNIX2dos  but is has tofrodos</p><p>frodos filename<br>todos filename<br>-b .bak<br>-v ver</p><h3 id="语系编码转换">语系编码转换</h3><p>iconv -o保留原文件，-o加新文件名<br>iconv -f big5 -t utf8 filename -o filename</p><h2 id="问题-vim中设置了setexpand不起作用">问题-vim中设置了setexpand不起作用</h2><p>~/.vimrc中进行了如下设置：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">set expandtab</span><br><span class="line">set tabstop=4</span><br><span class="line">set shiftwidth=4</span><br><span class="line">set softtabstop=4</span><br></pre></td></tr></table></figure><p>但是发现在markdown甚至~/.vimrc中expandtab都没有设置成功，但是py文件是正常的，后来发现是多加了一个set paste的原因，把它删了就好了。</p><h3 id="原因">原因</h3><p>因为set paste覆盖了set expandtab。</p><h3 id="解决方案">解决方案</h3><p>删除set paste行。</p><h2 id="我的vimrc文件">我的vimrc文件</h2><p>vimrc文件如下，<a href="https://github.com/mxxhcm/code/blob/master/shell/vimrc" target="_blank" rel="noopener">代码地址</a></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> 使用四个空格代替tab键</span><br><span class="line">set expandtab</span><br><span class="line">set tabstop=4</span><br><span class="line">set shiftwidth=4</span><br><span class="line">set softtabstop=4</span><br><span class="line"></span><br><span class="line">" 打开文件类型检测</span><br><span class="line">filetype on</span><br><span class="line">" 根据不同的文件类型加载插件</span><br><span class="line">filetype plugin on</span><br><span class="line">set ignorecase</span><br><span class="line"></span><br><span class="line">" 定义前缀键</span><br><span class="line">" let mappleader=";"</span><br><span class="line"></span><br><span class="line">" 设置ctrl+a全选，ctrl+c复制，;y粘贴</span><br><span class="line">vnoremap ; "+</span><br><span class="line">nnoremap ; "+</span><br><span class="line">nmap ;p "+p</span><br><span class="line">nnoremap &lt;C-C&gt; "+y</span><br><span class="line">vnoremap &lt;C-C&gt; "+y</span><br><span class="line">nnoremap &lt;C-A&gt; ggVG</span><br><span class="line">vnoremap &lt;C-A&gt; ggVG</span><br><span class="line"></span><br><span class="line">" 删除#号开头</span><br><span class="line">nnoremap ;d3 :g/^#.*$/d&lt;CR&gt;</span><br><span class="line">nnoremap ;d# :g/^#.*$/d&lt;CR&gt;</span><br><span class="line">" 删除空行</span><br><span class="line">nnoremap ;ds :g/^\s*$/d&lt;CR&gt;</span><br><span class="line">" 删除以tab开头的tab</span><br><span class="line">nnoremap ;rt :0,$s/^\t//g&lt;CR&gt;</span><br><span class="line">" 用\^代替^</span><br><span class="line">nnoremap ;r6 :0,$s/\^/\\^/g&lt;CR&gt;</span><br><span class="line">nnoremap ;r^ :0,$s/\^/\\^/g&lt;CR&gt;</span><br><span class="line">" 用\\\\代替\\</span><br><span class="line">nnoremap ;r/ :0,$s/\\\\/\\\\\\\\/g&lt;CR&gt;</span><br><span class="line">nnoremap ;r? :0,$s/\\\\/\\\\\\\\/g&lt;CR&gt;</span><br><span class="line"></span><br><span class="line">" 给选中行加注释</span><br><span class="line">" cnoremap &lt;C-#&gt; s/^/# /g&lt;CR&gt;</span><br><span class="line">nmap ;ic :s/^/# /g&lt;CR&gt;</span><br><span class="line">vmap ;ic :s/^/# /g&lt;CR&gt;</span><br><span class="line">nmap ;dc :s/^# //g&lt;CR&gt;</span><br><span class="line">vmap ;dc :s/^# //g&lt;CR&gt;</span><br><span class="line">"vmap &lt;C-#&gt; :s/^/#/g&lt;CR&gt;</span><br><span class="line">"nmap &lt;C-#&gt; :s/^/#/g&lt;CR&gt;</span><br><span class="line"></span><br><span class="line">""" 状态栏设置</span><br><span class="line">" 总是显示状态栏</span><br><span class="line">set laststatus=2</span><br><span class="line">" 状态信息</span><br><span class="line">set statusline=%f%m%r%h%w\ %=#%n\ [%&#123;&amp;fileformat&#125;:%&#123;(&amp;fenc==\"\"?&amp;enc:&amp;fenc).((exists(\"\+bomb\")\ &amp;&amp;\ &amp;bomb)?\"\+B\":\"\").\"\"&#125;:%&#123;strlen(&amp;ft)?&amp;ft:'**'&#125;]\ [%c,%l/%L]\ %p%%</span><br><span class="line"></span><br><span class="line">"""光标设置</span><br><span class="line">" 设置显示光标当前位置</span><br><span class="line">set ruler</span><br><span class="line"></span><br><span class="line">" 开启行号显示</span><br><span class="line">set number</span><br><span class="line">" 高亮显示当前行/列</span><br><span class="line">set cursorline</span><br><span class="line">" set cursorcolumn</span><br><span class="line">" 高亮显示搜索结果</span><br><span class="line">set hlsearch</span><br><span class="line">" 显示文件名</span><br><span class="line"></span><br><span class="line">" 开启语法高亮</span><br><span class="line">syntax enable</span><br><span class="line">" 允许用指定语法高亮配色方案替换默认方案</span><br><span class="line">syntax on</span><br><span class="line">" 将制表符扩展为空格</span><br><span class="line">" 设置编辑时制表符占用空格数</span><br><span class="line">" 设置格式化时制表符占用空格数</span><br><span class="line">" 让 vim 把连续数量的空格视为一个制表符</span><br><span class="line">set autoindent</span><br></pre></td></tr></table></figure><h2 id="参考文献">参考文献</h2><p>0.<a href="https://github.com/yangyangwithgnu/use_vim_as_ide" target="_blank" rel="noopener">https://github.com/yangyangwithgnu/use_vim_as_ide</a><br>1.<a href="https://askubuntu.com/a/1027647" target="_blank" rel="noopener">https://askubuntu.com/a/1027647</a><br>2.<a href="https://www.zhihu.com/question/19863631/answer/89354508" target="_blank" rel="noopener">https://www.zhihu.com/question/19863631/answer/89354508</a><br>3.鸟哥的LINUX私房菜<br>4.<a href="http://vimdoc.sourceforge.net/htmldoc/change.html#registers" target="_blank" rel="noopener">http://vimdoc.sourceforge.net/htmldoc/change.html#registers</a><br>5.<a href="https://stackoverflow.com/a/11489440" target="_blank" rel="noopener">https://stackoverflow.com/a/11489440</a><br>6.<a href="https://www.brianstorti.com/vim-registers/" target="_blank" rel="noopener">https://www.brianstorti.com/vim-registers/</a><br>7.<a href="http://landcareweb.com/questions/3593/ru-he-zai-vimzhong-yong-jiu-xian-shi-dang-qian-wen-jian-de-lu-jing" target="_blank" rel="noopener">http://landcareweb.com/questions/3593/ru-he-zai-vimzhong-yong-jiu-xian-shi-dang-qian-wen-jian-de-lu-jing</a><br>8.<a href="https://forum.ubuntu.org.cn/viewtopic.php?t=319408" target="_blank" rel="noopener">https://forum.ubuntu.org.cn/viewtopic.php?t=319408</a><br>9.<a href="https://stackoverflow.com/a/1676659/8939281" target="_blank" rel="noopener">https://stackoverflow.com/a/1676659/8939281</a><op selected lines><br>10.<a href="https://vi.stackexchange.com/questions/9028/what-is-the-command-for-select-all-in-vim-and-vsvim/9029" target="_blank" rel="noopener">https://vi.stackexchange.com/questions/9028/what-is-the-command-for-select-all-in-vim-and-vsvim/9029</a><ctrl a><br>11.<a href="https://stackoverflow.com/a/37962622/8939281" target="_blank" rel="noopener">https://stackoverflow.com/a/37962622/8939281</a><set paste><br>12.<a href="https://vim.fandom.com/wiki/Search_and_replace_in_a_visual_selection" target="_blank" rel="noopener">https://vim.fandom.com/wiki/Search_and_replace_in_a_visual_selection</a><br>13.<a href="https://stackoverflow.com/questions/71323/how-to-replace-a-character-by-a-newline-in-vim/71334" target="_blank" rel="noopener">https://stackoverflow.com/questions/71323/how-to-replace-a-character-by-a-newline-in-vim/71334</a>&lt;用newline替换逗号&gt;</set></ctrl></op></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;vim-配置文件&quot;&gt;vim 配置文件&lt;/h2&gt;
&lt;p&gt;个人vim配置文件一般在~/.vimrc下，可以自定义各种配置。&lt;br&gt;
&lt;a href&gt;我的vimrc文件&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;前缀符号&quot;&gt;前缀符号&lt;/h3&gt;
&lt;p&gt;为了缓解快捷键冲突问题，就引入
      
    
    </summary>
    
      <category term="linux" scheme="http://mxxhcm.github.io/categories/linux/"/>
    
    
      <category term="linux" scheme="http://mxxhcm.github.io/tags/linux/"/>
    
      <category term="vim" scheme="http://mxxhcm.github.io/tags/vim/"/>
    
  </entry>
  
  <entry>
    <title>linux bash</title>
    <link href="http://mxxhcm.github.io/2019/05/07/linux-bash/"/>
    <id>http://mxxhcm.github.io/2019/05/07/linux-bash/</id>
    <published>2019-05-07T08:55:17.000Z</published>
    <updated>2019-06-19T03:31:15.534Z</updated>
    
    <content type="html"><![CDATA[<p>jq 解析json字符串</p><p>&amp;&amp; || 命令从左到右依次执行　根据回传码$0的值，继续向右执行命令</p><h2 id="diff-文本比较-通常比较一个文件的不同版本">diff 文本比较，通常比较一个文件的不同版本</h2><p>diff [-bBi] file1 file2<br>-b　忽略一行中仅有多个空白的区别<br>-B  忽略空白行的区别<br>-i  忽略大小写<br>diff test.old test.new<br>diff -Naur test.olc test.new &gt; test.patch<br>patch 补丁<br>cat test.patch<br>patch -pN test.patch 更新旧版<br>patch -R -pN test.patch 恢复为旧版</p><h2 id="nl-打印出文件并加上行号">nl 打印出文件并加上行号</h2><h2 id="echo-与-unset">echo 与 unset</h2><p>~$:echo $PATH<br>~$:echo ${PATH}</p><p>&quot;&quot;内的特殊字符可以保持原有特性　var=“lang is $LANG” 那么 echo $var 输出var=en_US.UTF-8<br>’'内的特殊字符仅保存为一般文本</p><p>反单引号<code>可以获得其他命令的信息 version=</code>uname -r`  =$(uname -r)</p><h2 id="env以及export查看常见变量">env以及export查看常见变量</h2><p>/etc/profile<br>/etc/bash.bashrc</p><p>RANDOM产生0~32767的随机数<br>产生0-9的用declare -i number=$RANDOM*10/32767    echo $number<br>HOME<br>SHELL<br>HISTSIZE<br>MAIL<br>PATH/etc/environment<br>LANG<br>RANDOM</p><p>set查所有变量</p><p>HISTFILE=~/.bash_history<br>MAILCHECK<br>PS1提示符的设置<br>$关于本shell的PID<br>?上个变量的回传码，正确返回0，错误返回其他值，可以利用代码差错<br>OSTYPE HOSTTYPE MACHTYPE主机硬件与内核的等级</p><pre><code>export将自定义变量转换为环境变量locale -a 文件的语系read 赌球来自键盘输入的变量</code></pre><p>-p用户可以输入提示语<br>-t光标等待用户输入时间</p><p>~$:read -p “hello” -t 10 variable</p><pre><code>declare 声明变量的类型　　默认为字符串</code></pre><p>-x声明环境变量<br>-i将变量定义为整形<br>-a将变量定义为数组<br>-r将变量设置为readonly  若要删除该变量，必须退出该bash重进<br>-p单独列出变量的类型</p><pre><code>ulimit 与文件系统以及程序的限制关系</code></pre><p>-a 后面不接任何参数,可以列出所有的限制额度<br>-c 某些进程发生错误，系统可能会将该进程在内存中的信息写成文件，这种文件就称为内核文件(core file)。此为限制每个内核文件的最大容量<br>-f 此文件可以创建的最大文件容量,一般为2G:<br>-d 进程可使用的最大断裂内存(segment)容量<br>-l 可用于锁定(lock)的内存量<br>-t 可使用的最大CPU时间<br>-u 用户可使用的最大进程(process)数量</p><p>-H hard limit 严格的限制　　必须不能超过<br>-S soft limit 警告的限制　　可以超过，但要有警告信息</p><h2 id="变量的使用">变量的使用</h2><p>变量内容的测试与内容替换<br>echo ${variable#<em>}<br>echo ${variable##</em>}</p><pre><code>echo ${variable%*}echo ${variable%%*}echo ${variable/bin/BIN}echo ${variable//bin/BIN}变量的测试与替换new_var=${old_var-content}用新的变量的值区替代旧的变量的值，新旧变量可为同一个，若old_var不存在，则将</code></pre><p>content的值给new_var,而若old_var的值存在则将其赋给new_var;<br>　　加上:的话，即使old_var为空的话，也会用content的值去赋给new_var</p><pre><code>username=&quot;&quot;username=${username:-root}echo $username将-换成=是将原变量一同更改</code></pre><p>将-换成?是当变量不存在时，可以发出错误信息</p><h2 id="bash-shell的操作环境">Bash Shell的操作环境</h2><p>路径与命令查找顺序<br>先由相对路径或者绝对路径寻找<br>a.alias<br>b.builtin<br>c.$PATH这个变量的顺序找到的第一个命令<br>bash的登陆界面以及欢迎信息<br>/etc/issue  #<br>/etc/issue.net  #提供telnet远程登陆，当使用telnet连接到主机时显示该内容<br>/etc/motd(?)-&gt;/etc/update-motd.d/　<br>/etc/issue\d \l \m \n \o \r \t \s \v<br>\d 本地端时间的日期<br>\l 显示第几个终端机<br>\m 显示硬件等级<br>\n 显示主机的网络名称<br>\o 显示domain name<br>\r 操作系统的版本<br>\t 显示本地端时间的时间<br>\s 操作系统的名称<br>\v 操作系统的版本</p><pre><code>  bash的环境配置文件</code></pre><p>login shell 以及non-login shell<br>/etc/profile系统整体的设置<br>~/.profile用户个人设置</p><p>login shell<br>/etc/profile<br>/etc/inputrc/etc/profile.d/*sh<br>~/.profile<br>~/.bashrc/etc/bashrc<br>开始操作bash</p><p>non-login shell<br>取得non-login shell 时，该bash配置文件仅会读取~/.bashrc</p><p>source 配置文件名<br>如<br>source ~/.bashrc<br>. ~/.bashrc</p><p>/etc/manpath.config使用man时man page的路径到哪里去找<br>用tarball的方式安装的时候,那么man page可能放置在/usr/local/softpackage/\man里，需要以手动的方式将该路径加入到/etc/man.config里面</p><pre><code> 终端机的环境设置</code></pre><p>stty　　setting tty(终端机的意思)<br>-a 将所有的stty参数列出来</p><pre><code>如何设置呢  比如将erase设置为ctrl+h来控制stty erase ^hctrl + c 终止目前的命令</code></pre><p>ctrl + d 输入结束，例如邮件结束<br>　　ctrl + m ENTER<br>ctrl + u 在提示符下，将整行命令删除<br>　　ctrl + z 暂停目前的命令</p><p>set<br>　set $-　那个$-变量内容是set的所有设置<br>uvxhHmBC</p><p>/etc/inputrc其他的按键设置功能</p><pre><code> 通配符与特殊符号</code></pre><p>通配符* ? [] - ^<br>特殊字符　# \ | ; ~ $ &amp; ! / &gt;,&gt;&gt; &lt;,&lt;&lt; ‘’ “” ``或者$() () {}</p><h2 id="seq-产生一系列数">seq 产生一系列数</h2><p>seq [-s]</p><p>~$:seq -s &quot; &quot; 3 10<br>3 4 5 6 7 8 9 10</p><h2 id="sh-vxn-my-sh">sh [-vxn] <a href="http://my.sh" target="_blank" rel="noopener">my.sh</a></h2><p>sh -x执行过程<br>sh -n查询语法问题</p><h2 id="id和finger">id和finger</h2><p>id 用来显示某个用户的id信息<br>finger 用来分析某个用户信息</p><h2 id="none"></h2><p>type 查看命令的来自于哪里　　<br>是bash还是外部命令还是别名<br>file外部命令<br>alias别名<br>builtin内置在bash内<br>-t -p -a</p><p>type -t ls<br>~$:alias 以file builtin alias 列出该命令的类型<br>type -a ls 列出所有的名为ls的命令</p><p>学习shell script<br>看一下自己写的/home/mxx/scripts/delete_dir</p><h2 id="echo-num1-operand-num2">echo $(($num1 operand $num2))</h2><p>进行运算</p><h2 id="source-file-sh-sh-file-sh-file-sh">source <a href="http://file.sh" target="_blank" rel="noopener">file.sh</a>   sh <a href="http://file.sh" target="_blank" rel="noopener">file.sh</a>   ./file.sh</h2><p>source 是将该shell拿到父进程中来执行，所以各项操作都会在该bash内执行<br>sh和./是开启一个新的shell来执行</p><h2 id="test">test</h2><p>test [-rwxfd]<br>[-nt -ot -ef ]<br>[-eq -nq -gt -lt -ge -le]<br>[-z ]<br>[-a -o]</p><p>test -r filename<br>test “$filename” == “content”</p><p>[ “$filename” == “$varible” ]</p><h2 id="none-v2">$# $@ $*</h2><p>$#:变量个数<br>$@:变量内容<br>$*:</p><h2 id="别名">别名</h2><p>alias<br>alias lm=‘ls -al’</p><p>unalias</p><h2 id="history命令与文件">history命令与文件</h2><p>history (n)列出最近的第(n)条命令</p><p>!number执行history的第number条命令<br>!command　由最近的命令开始搜寻开头为command的命令<br>!!执行上一个命令</p><p>last最近登录的用户</p><h2 id="参考文献">参考文献</h2><ol><li>《鸟哥的LINUX私房菜》<br>2.<a href="https://www.tomczhen.com/2017/10/15/parsing-json-with-shell-script/" target="_blank" rel="noopener">https://www.tomczhen.com/2017/10/15/parsing-json-with-shell-script/</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;jq 解析json字符串&lt;/p&gt;
&lt;p&gt;&amp;amp;&amp;amp; || 命令从左到右依次执行　根据回传码$0的值，继续向右执行命令&lt;/p&gt;
&lt;h2 id=&quot;diff-文本比较-通常比较一个文件的不同版本&quot;&gt;diff 文本比较，通常比较一个文件的不同版本&lt;/h2&gt;
&lt;p&gt;diff
      
    
    </summary>
    
      <category term="linux" scheme="http://mxxhcm.github.io/categories/linux/"/>
    
    
      <category term="linux" scheme="http://mxxhcm.github.io/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>linux account</title>
    <link href="http://mxxhcm.github.io/2019/05/07/linux-account/"/>
    <id>http://mxxhcm.github.io/2019/05/07/linux-account/</id>
    <published>2019-05-07T08:54:11.000Z</published>
    <updated>2019-06-16T08:20:24.095Z</updated>
    
    <content type="html"><![CDATA[<h2 id="账户配置文件">账户配置文件</h2><ul><li>/etc/passwd</li><li>/etc/shadow</li><li>/etc/group</li><li>/etc/gshadow</li></ul><h3 id="etc-passwd">/etc/passwd</h3><p>mxx❌1000:1000:mxx,:/home/mxx:/bin/bash<br>账号名称,密码，UID,GID,用户信息说明列，主文件夹，shell</p><h3 id="etc-shadow">/etc/shadow</h3><p>mxx:…:17034:0:99999:7:::<br>账号名称，密码，最近密码更改日期，密码更改多久后才能重新更改，密码多长时间需要重新修改，密码需要修改前多少天发送警告，密码过期后宽限时间，账号失效日期，(形式和密码更改日期一样)，保留字段</p><h3 id="etc-group">/etc/group</h3><p>用户组名称，用户组密码,GID,此用户组支持的账号名称</p><h3 id="etc-gshawod">/etc/gshawod</h3><p>用户组名，<br>密码列，开头为!表示无合法密码<br>用户组管理员的账号<br>该用户组的所属账号</p><table><thead><tr><th>UID</th><th>用户</th></tr></thead><tbody><tr><td>0</td><td>系统管理员</td></tr><tr><td>1-99</td><td>系统账户</td></tr><tr><td>100-499</td><td>用户创建的系统账号</td></tr><tr><td>500-65535</td><td>一般用户</td></tr></tbody></table><h2 id="修改密码">修改密码</h2><p>一般账户:passwd<br>root账户:重启后进入单用户维护模式<br>忘记密码后，以各种方式清空/etc/shadow中root的密码字段。登陆后再用passwd修改密码</p><h2 id="uesr管理">uesr管理</h2><p>usermod -G group user将一个用户加入其他用户组<br>初始用户组用户的/etc/passwd的第四个字段即为该用户的初始用户组的GID<br>groups查看当前登陆用户的用户组。第一个为有效用户组<br>newgrp更改用户的有效用户组，但是用户组必须当前用户支持的用户组<br>UID/GID密码参数的设置在 /etc/login.defs</p><h3 id="useradd添加用户">useradd添加用户</h3><h4 id="参数介绍">参数介绍</h4><p>useradd [-ugGmMcdrsef]　　调用/etc/default/useradd的数据<br>-u UID          /etc/skel用户主文件加参考基准目录<br>-g initial group<br>-G 这个账户可以加入的其他用户组<br>-m 创建用户主文件　<br>-M 不创建用户主文件<br>-s 接一个默认shell<br>-r 创建一个系统账户<br>-c /etc/passwd的第五列说明<br>-d 制定某个目录成为主文件夹<br>-e 后面跟一个日期YYYYMMDD写入shadow的第八字段，账号的失效日期从1970年来总日数，若账号失效，无论密码是否正确，都无法登陆<br>-f 后面接shadow的第七字段,判定密码是否会失效,0为立即失效,-1为永不失效(密码只会过期强制登陆时重新设置),大于0的表示如n，如果在n天后，没有登陆修改密码，那么在n天后密码会失效，再也无法登陆，但是在如果在n天内登陆并修改密码，就可以继续使用。<br>-D useradd的默认值</p><h2 id="例子">例子</h2><p>~$:useradd -d /home/mxxhcm -k /etc/skel/ -m mxxhcm -s /bin/bash</p><h3 id="passwd修改密码">passwd修改密码</h3><h4 id="参数介绍-v2">参数介绍</h4><p>passwd [账号]　[–stdin] -[luSnxwi]<br>-l lock<br>-u unlock<br>-S 密码相关参数<br>-n next　多长时间不能修改第四个字段<br>-x 多少天必须修改　第五个字段<br>-w warn第六个字段<br>-i 失效日期　第七个字段</p><h4 id="示例">示例</h4><p>~#:passwd　后面没有接密码，就是修改当前用户的密码<br>~#:echo “passwd” | passwd --stdin user</p><h3 id="change修改user信息">change修改user信息</h3><h4 id="参数介绍-v3">参数介绍</h4><p>chage [-ldEImMW] 账号名<br>-l　列出详细参数<br>-d　第三字段<br>-E　第八字段　账号失效　<br>-I　第七字段　密码失效<br>-m　第四字段<br>-M　第五字段<br>-W　第六字段</p><h4 id="示例-v2">示例</h4><p>chage -d 0 user</p><h3 id="user信息修改">user信息修改</h3><h4 id="参数介绍-v4">参数介绍</h4><p>usermod [-;cdefgGasuLU]<br>-l 修改账户名称<br>-L lock<br>-U unlock<br>修改/etc/shadow<br>-f 第七字段<br>-e 第六字段<br>-c /etc/passwd 第五字段<br>-d /etc/passwd 主文件夹第六字段<br>-g /etc/passwd 第四个字段GID<br>-G 后面接次要用户组，修改这个用户能支持的用户组，修改/etc/group<br>-a 与-G连用，增加次要用户组的支持而非设置<br>-u UID /etc/passwd的第三个字段,UID<br>-s 接shell的实际文件</p><h4 id="示例-v3">示例</h4><p>~#:usermod -l ‘my_usename’ username</p><h3 id="user删除">user删除</h3><h4 id="参数介绍-v5">参数介绍</h4><p>userdel</p><h3 id="示例-v4">示例</h3><p>~#:userdel -r username # 删除主文件夹<br>~#:find / -user username<br>~#:userdel username</p><h2 id="finger查看用户的数据">finger查看用户的数据</h2><p>finger 查看当前用户的数据<br>finger username 查看某用户的信息</p><h2 id="chfn">chfn</h2><p>chfn 就是相当于-c参数,修改当前用户/etc/passwd的第五个字段值<br>chsh -s　修改当前用户的shell<br>chsh -s /bin/bash</p><h2 id="id">id</h2><p>id [username]<br>列出当前用户或者username的所有id</p><h2 id="group操作">group操作</h2><p>groupadd [-gr]<br>-g　指定GID<br>-r　新建系统用户组<br>groupmod [-gn]<br>-n　修改组名<br>groupdel [groupname]</p><p>尽量少修改GID否则会造成系统资源的混乱<br>当用户组为某个用户的初始用户组时，就无法删除该用户组</p><h3 id="gpasswd修改group信息">gpasswd修改group信息</h3><p>gpasswd　[-AMrR] groupname<br>-A 将groupname的控制权交给后面用户<br>gpasswd -A mxx groupname<br>-M 将某些账号加入到这个用户组中<br>-r 将groupname的密码删除<br>-R 将groupname的密码失效</p><p>gpasswd groupname 设置groupname管理密码<br>gpasswd groupname<br>-A 增加groupname的管理员<br>-r让密码删除</p><p>gpasswd -ad username groupname<br>-a增加<br>-d删除</p><h2 id="acl-acess-control-list">ACL  Acess Control List</h2><p>针对单一用户或者目录来进行rwx的权限设置<br>setfacl　[-m|-x]    -m设置acl参数   -x删除后续acl参数<br>[-bkRd]<br>-b删除所有的acl参数;-k删除默认的acl参数;-R递归设置acl;-d设置默认的acl，只对目录有效<br>setfacl [-m|-x] [bkRd]<br>-b　删除所有ACL参数<br>-k　删除默认ACL参数<br>-R　递归设置ACL参数，包括子目录<br>-d  设置默认ACL参数，只对目录有效</p><pre><code>-x　删除后续的ACL参数-m  设置后续的ACL参数-m u:mxx:rw my_file</code></pre><p>getfacl my_file</p><p>针对有效权限mask的设置<br>setfacl -m m:rwx my_file<br>mask在此可以来规定最大允许的权限。取得是mask和用户以及用户组的权限交集。        若用户mxx的权限为rwx 但是mask为r–，那么mxx的权限只能为r–.</p><p>~#:setfacl -m d:u:mxx:rwx file　递归设置目录的acl<br>~#:setfacl -m m:rw acl_test<br>~#:setfacl -m g:mxx:rwx acl_test<br>~#:getfacl acl_test<br>~#:setfacl -b file 删除acl</p><h2 id="切换用户-切换账号">切换用户，切换账号</h2><p>su[- -l -m -c]<br>su - 切换到root用户以login shell变量的读取方式<br>su 切换到root用户，以nologin shell变量的读取方式登陆系统<br>｀｀    su -l 加想要切换的账号login shell<br>su -c 只提升一次到root权限<br>su -m 使用目前用户的环境变量，不读取新用户的配置文件</p><p>su - -c cat /etc/shadow</p><p>sudo -u mxx …提升到mxx权限</p><h2 id="visudo的设置">visudo的设置</h2><p>1.visudo 修改/etc/sudoers<br>其他用户使用root身份<br>root ALL=(ALL) ALL<br>用户账号<br>登陆者的来源主机名，<br>可切换的身份<br>可执行的命令<br>2.最左边加一个%表示用户组<br>利用用户组以及免密码<br>%wheel ALL=(ALL) ALL<br>usermod -a -G wheel user</p><p>免密<br>%wheel ALL=(ALL) NOPASSWD: ALL<br>3.mxx ALL=(root) /usr/bin/passwd<br>mxx可以切换到root的身份使用passwd命令</p><p>mxx ALL=(root) !/usr/bin/passwd, /usr/bin/passwd [A-Za-z]*,<br>!/usr/bin/passwd root<br>4.别名设置<br>User_Alias MYUSER=mxx,mahuihui<br>Cmnd_Alias MYCOMMAND=!/usr/bin/passwd, /usr/bin/passwd [A-Za-z]*,<br>!/usr/bin/passwd root<br>MYUSER all=(root) MYCOMMAND</p><p>5.用自己的密码切换成root</p><p>sudo su -</p><h2 id="用户信息传递">用户信息传递</h2><p>查询用户</p><ul><li>w</li><li>who</li><li>last</li><li>lastlog</li></ul><p>用户对谈<br>mesg y<br>mesg n</p><p>write<br>write mxx tty1</p><p>wall “hello”　每个人都会收到</p><p>mail mahuihui -s “Hi,mahuihui,nihaoa”<br>…<br>…<br>ctrl+d</p><p>mail 收信<br>?查看命令</p><p>q离开,离开后，会将该信件移动到~/home/mbox，收信箱<br>读取<br>mail -f /home/mxx/mbox</p><h2 id="手工添加账号">手工添加账号</h2><ul><li><p>pwck<br>pwconv将/etc/passwd相关信息移动到/etc/shadow,把在/etc/passwd中存在的账号            但是在/etc/shadow没有对应密码的列新增密码</p></li><li><p>grpconv</p></li><li><p>pwunconv</p></li><li><p>chpasswd    修改密码<br>echo “mxx:mypaswd” | chpasswd -m</p></li></ul><h3 id="新建账号">新建账号</h3><p>vim /etc/group<br>mygroup❌1020:</p><p>grpconv</p><p>vim /etc/passwd<br>myuser❌1200:1020::/home/myuser:/bin/bash<br>pwconv</p><p>passwd myuser</p><p>cp -a /etc/skel /home/myuser<br>chmod -R myuser:mygroup /home/myuser<br>chmod 700 /home/myuser</p><h2 id="参考文献">参考文献</h2><p>1.《鸟哥的LINUX私房菜》</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;账户配置文件&quot;&gt;账户配置文件&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;/etc/passwd&lt;/li&gt;
&lt;li&gt;/etc/shadow&lt;/li&gt;
&lt;li&gt;/etc/group&lt;/li&gt;
&lt;li&gt;/etc/gshadow&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;etc-passw
      
    
    </summary>
    
      <category term="linux" scheme="http://mxxhcm.github.io/categories/linux/"/>
    
    
      <category term="linux" scheme="http://mxxhcm.github.io/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>linux quota</title>
    <link href="http://mxxhcm.github.io/2019/05/07/linux-quota/"/>
    <id>http://mxxhcm.github.io/2019/05/07/linux-quota/</id>
    <published>2019-05-07T08:53:04.000Z</published>
    <updated>2019-06-16T08:22:05.854Z</updated>
    
    <content type="html"><![CDATA[<h2 id="quota">quota</h2><p>显示磁盘使用情况和限额<br>~$:sudo apt-get install quota</p><h2 id="quota示例">quota示例</h2><p>文件系统开启quota<br>~#:df -h /home<br>~#:mount -o remount,usrquota,grpquota /home<br>~#:mount | grep ‘home’</p><p>查看/etc/mtab文件<br>cat /etc/mtab</p><p>或者直接写入/etc/fstab<br>~#:vim /etc/fstab<br>添加：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">LABEL=/home /home ext4 defaults,usrquota,groquota</span><br></pre></td></tr></table></figure><p>~#:umount /home<br>~#:mount -a<br>~#:mount | grep ‘home’</p><h2 id="新建quota配置文件">新建quota配置文件</h2><p>quotacheck [-avugmf]<br>-c 创建磁盘配额数据文件<br>-a 创建在/etc/mtab所有磁盘的配额数据库文件，使用此参数，后无需加挂载点<br>-u 创建用户的磁盘配额数据库文件<br>-g 用户组的<br>-m 把一起的磁盘配额信息清除，对/分区创建时，必须用此参数<br>-v 显示创建的过程</p><h2 id="启动quota">启动quota</h2><p>quotaon [-avug]　[挂载点]<br>-a 根据/etc/fstab的设置来启动有关的quota<br>-v</p><h2 id="关闭quota">关闭quota</h2><p>quotaoff [-aug] [mount-point]</p><h2 id="编辑quota">编辑quota</h2><p>edquota [-ugtp]<br>-t 修改宽限时间<br>-p 复制范本，，模板账号为已存在并设置好quota的账号<br>edquota -p 范本账号 -u 新账号<br>edquota -u myquota<br>edquota -g myquotagrp<br>edquota -t<br>edquota -p quotagrp -u myquota</p><h2 id="quota报表">quota报表</h2><p>单一用户<br>quota [-ugvs]<br>-s 以1024的整数倍显示</p><p>repquota [-a] [-uvgs]<br>/dev/sda[012]<br>warnquotaroot给用户以及root发邮件　P461<br>在 /etc/warnquota.conf 中设置邮件内容<br>setquota [-u|-g] name block(soft) block(hard) inode(soft) inode(hard) 文件系统<br>setquota -u myquota5 2000 3000 0 0 /home<br>quota-uv myquota5</p><h2 id="参考文献">参考文献</h2><p>1.《鸟哥的LINUX私房菜》</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;quota&quot;&gt;quota&lt;/h2&gt;
&lt;p&gt;显示磁盘使用情况和限额&lt;br&gt;
~$:sudo apt-get install quota&lt;/p&gt;
&lt;h2 id=&quot;quota示例&quot;&gt;quota示例&lt;/h2&gt;
&lt;p&gt;文件系统开启quota&lt;br&gt;
~#:df -h /ho
      
    
    </summary>
    
      <category term="linux" scheme="http://mxxhcm.github.io/categories/linux/"/>
    
    
      <category term="linux" scheme="http://mxxhcm.github.io/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>linux raid lvm</title>
    <link href="http://mxxhcm.github.io/2019/05/07/linux-raid-lvm/"/>
    <id>http://mxxhcm.github.io/2019/05/07/linux-raid-lvm/</id>
    <published>2019-05-07T08:52:12.000Z</published>
    <updated>2019-06-16T08:26:01.500Z</updated>
    
    <content type="html"><![CDATA[<h2 id="磁盘阵列">磁盘阵列</h2><h3 id="mdadm新建raid">mdadm新建raid</h3><h4 id="参数介绍">参数介绍</h4><p>mdadm --detail  后面接的那个磁盘阵列设备的具体信息<br>madam --create 为新建raid的参数<br>–auto=yes /dev/md[01…]<br>–raid-device=N 使用几个raid作为磁盘阵列的设备<br>–spare-device=N　使用几个磁盘作为备用<br>–level=[0125]  这组磁盘阵列的等级是0,1还是5之类的</p><h4 id="示例">示例</h4><p>~#:mdadm --create --auto=yes /dev/md0 --level=5 --raid-device=3 --spare-device=1 /dev/sdb{5,6,7,8}<br>创建raid需要时间，创建好之后<br>~#:mdadm --detail /dev/md0<br>查看建好的RAID<br>~#:cat /proc/mdstat<br>~#:mkfs -t ext4 /dev/md0<br>~#:mkdir /mnt/raid<br>~#:mount /dev/md0 /mnt/raid<br>~#:df</p><h3 id="mdadm管理raid">mdadm管理raid</h3><h4 id="参数介绍-v2">参数介绍</h4><p>mdadm --manage /dev/md[0-9] [–add 设备] [–remove 设备] [–fail 设备]<br>~#:mdadm --manage /dev/md0 --fail /dev/sdb6<br>~#:mdadm --detail /dev/md0<br>~#:cat /proc/mdstat<br>过一段时间在执行会发现以及将坏的设备更新了<br>~#:mdadm --detail /dev/md0<br>~#:mdadm --manage /dev/md0 --add /dev/sdb9 --remove /dev/sdb10</p><h3 id="开机自动加载raid">开机自动加载raid</h3><p>~#:mdadm --detail /dev/md0 | grep -i ‘uuid’<br>~#:vim /etc/mdadm/mdadm.conf<br>ARRAY /dev/md0 UUID=…<br>~#:vim /etc/fstab<br>/dev/md0 /mnt/raid ext4 defaults 1 2<br>~#:umount /dev/md0<br>~#:mount -a<br>~#:df</p><h3 id="关闭raid">关闭raid</h3><p>~#:vim /etc/fstab<br># /dev/md0 …<br>~#:mdadm --stop /dev/md0<br>~#:cat /proc/mdstat<br>~#:vim /etc/mdadm/mdadm.conf</p><h2 id="lvm的制作">LVM的制作</h2><p>LVM Logical Volume Manager<br>PV physical volume<br>VG volume group<br>PE physical extend<br>LV logical volume</p><h3 id="lv的写入机制">LV的写入机制</h3><ul><li>线性机制<br>若有两个设备/dev/sda1,/dev/sdb1,他们都在一个VG中，并且只有一个LV，线性机制就是在一个设备完全写满之后，再向另一个设备写入</li><li>交错模式</li></ul><h3 id="新建分区">新建分区</h3><p>~#:sudo fdisk /dev/sdb<br>new /dev/sdb{5,6,7,8,9,10}<br>t 8e(Linux LVM)<br>w<br>~#:partprobe</p><h3 id="安装应用">安装应用</h3><p>sudo apt-get install lvm2</p><h3 id="pv物理卷的新建">PV物理卷的新建</h3><p>pvcreate 将物理分区新建为PV分区<br>pvscan 查询目前系统里具有PV的磁盘<br>pvdisplay 显示目前系统上面的PV状态<br>pvremove 将PV属性删除</p><p>pvmove 将某个设备内的pe给移动到另一个设备<br>pvmove /dev/sdb5 /dev/sdb9</p><p>~#:pvscan<br>~#:pvcreate /dev/sdb{5,6,7,8}<br>~#:pvscan<br>~#:pvdisplay</p><h3 id="vg卷用户组的新建">VG卷用户组的新建</h3><p>vgcreate 新建VG<br>vgscan 查找目前系统上的VG<br>vgdisplay 显示目前系统上的VG状态<br>vgextend 在VG内新增额外的VG<br>vgreduce 在VG内删除PV<br>vgchange 设置VG是否启动<br>vgremove 删除一个VG<br>VG名称是自己定义的。而PV名称实际上是分区的设备文件名</p><p>vgcreate [-s] VG名称 PV名称<br>-s　后面接PE的大写,单位可以是m,g,t (支持大小写)<br>~#:vgcreate -s 16M mxxvg /dev/sdb{5,6,7}<br>~#:vgscan<br>~#:pvscan<br>~#:vgdisplay<br>vgextend VG名称　PV名称<br>~#:vgextend mxxvg /dev/sdb8<br>~#:vgdisplay</p><h3 id="lv逻辑卷的新建">LV逻辑卷的新建</h3><p>lvcreate 新建lv<br>lvscan 查询系统上的lv<br>lvdisplay 展示系统上的lv<br>lvextend 在lv里增加容量<br>lvreduce 在lv里减少容量<br>lvremove 删除一个lv<br>lvresise 对lv的大小进行重新调整</p><p>lvcreate [-lLs] [-n lv名称]　vg名称<br>-l 后接的是PE的个数<br>-L 后接的是vg的容量<br>-n 后接lv的名称<br>-s snapshot 快照<br>~#:lvcreate -l 252 -n mxxlv mxxvg<br>~#:ls -l /dev/mxxvg/mxxlv<br>~#:lvscan<br>~#:lvdisplay</p><h3 id="文件系统新建">文件系统新建</h3><p>~#:mkfs -t ext4 /dev/mxxvg/mxxlv<br>~#:mkdir /mnt/lvm<br>~#:mount /dev/mxxvg/mxxlv /mnt/lvm<br>~#:df -h .</p><h3 id="增加lv容量">增加lv容量</h3><p>~#:sudo fdisk /dev/sdb<br>new /dev/sdb9<br>~#:pvcreate /dev/sdb9<br>~#:pvscan<br>~#:vgextend mxxvg /dev/sdb9<br>~#:vgdisplay<br>增加lv的容量<br>~#:lvresize -l +63 /dev/mxxvg/mxxlv<br>~#:lvdisplay<br>~#:df -h /mnt/lvm</p><p>此时虽然lv显示的容量增大，但是对应的/dev/mxxvg/mxxlv文件系统还没有改变<br>~#:dumpe2fs /dev/mxxvg/mxxlv<br>重新计算文件系统<br>resizefs [-f] [device] [size]<br>-f 强制进行resize<br>device 后接的文件系统或者是设备名<br>size 如果没有size默认为整个文件系统，如果有size的话，必须给一个             单位<br>~#:resize2fs /dev/mxxvg/mxxlv   //可在线进行resize</p><h3 id="缩小lv容量">缩小lv容量</h3><p>先计算需要缩小多少<br>~#:pvscan<br>~#:pvdisplay<br>　　　缩小文件系统容量<br>放大可以直接进行，但是缩小需要先卸载<br>~#:umount /dev/mxxvg/mxxlv<br>~#:resize2fs /dev/mxxvg/mxxlv 3900M<br>报错需要用e2fsck<br>~#:e2fsckk -f /dev/mxxvg/mxxlv<br>~#:resize2fs /dev/mxxvg/mxxlv 3900M<br>~#:mount /dev/mxxvg/mxxlv /mnt/lvm<br>~#:df -h /mnt/lvm<br>　　　降低lv容量<br>~#:lvresize -l -63 /dev/mxxvg/mxxlv<br>~#:lvdisplay<br>转移pv<br>~#:pvdisplay<br>~#:pvmove /dev/sdb5 /dev/sdb9<br>　　　删除vg<br>~#:vgreduce mxxvg /dev/sdb5<br>删除pv<br>~#:pvscan<br>~#:pvremove /dev/sdb5</p><h2 id="lvm的快照">LVM的快照</h2><p>需要有未使用的PE块<br>所以需要新加入一个PV块</p><p>~#:vgdisplay<br>~#:pvcreate /dev/sdb5<br>~#:vgextend mxxvg /dev/sdb5<br>~#:vgdisplay<br>~#:lvcreate -l 40 -s -n mxxlv_ss /dev/mxxvg/mxxlv<br>-s snapshot<br>~#:lvdisplay<br>复原的数据是不能比快照区的大小大的，此处不能大于40个PE</p><p>接下来改变LVM中的数据，会发现lvm与快照区是不同的<br>~#:cd /mnt/lvm<br>~#:cp -a /home/mxx/my.iso /mnt/lvm<br>~#:lvdisplay 会发现lv的快照区已经被使用了<br>~#:df 会发现原始文件与快照区文件系统也是不同的</p><h3 id="利用快照区进行备份">利用快照区进行备份</h3><p>~#:tar -cvj -f /home/mxx/my.bak/lvm.bak.tar.bz2 *<br>~#:umount /mnt/snapshot</p><p>将快照区进行删除，因为已经被备份<br>~#:lvremove /dev/mxxvg/mxxlv_ss</p><p>~#:umount /mnt/lvm<br>~#:mkfs -t ext4 /mnt/lvm<br>~#:mount /dev/mxxvg/mxxlv /mnt/lvm</p><p>将备份的数据还原，那么这个文件系统就会和原来一样了<br>~#:tar -xvj -f /home/mxx/my.bak/lvm.bak.tar.bz2 /mnt/lvm<br>~#:ls -l /mnt/lvm</p><h3 id="lvm的关闭">LVM的关闭</h3><p>先卸载lvm系统，包括快照与原系统<br>再使用lvremove删除LV<br>使用vgchange -a n VG 名称 让其不再为active<br>使用vgremove删除VG<br>使用pvremove删除PV<br>最后使用sudo fdisk 修改System ID</p><p>~#:umount /mnt/lvm<br>~#:lvremove /dev/mxxvg/mxxlv<br>~#:vgchage -a n mxxvg<br>~#:vgremove mxxvg<br>~#:pvremover /dev/sdb{5,6,7,8}<br>~#:sudo fdisk -l</p><h2 id="参考文献">参考文献</h2><p>1.《鸟哥的LINUX私房菜》</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;磁盘阵列&quot;&gt;磁盘阵列&lt;/h2&gt;
&lt;h3 id=&quot;mdadm新建raid&quot;&gt;mdadm新建raid&lt;/h3&gt;
&lt;h4 id=&quot;参数介绍&quot;&gt;参数介绍&lt;/h4&gt;
&lt;p&gt;mdadm --detail  后面接的那个磁盘阵列设备的具体信息&lt;br&gt;
madam --creat
      
    
    </summary>
    
      <category term="linux" scheme="http://mxxhcm.github.io/categories/linux/"/>
    
    
      <category term="linux" scheme="http://mxxhcm.github.io/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>linux at cron anacron mail</title>
    <link href="http://mxxhcm.github.io/2019/05/07/linux-at-cron-anacron-mail/"/>
    <id>http://mxxhcm.github.io/2019/05/07/linux-at-cron-anacron-mail/</id>
    <published>2019-05-07T08:50:33.000Z</published>
    <updated>2019-06-23T01:12:20.608Z</updated>
    
    <content type="html"><![CDATA[<h2 id="例行性工作">例行性工作</h2><ul><li>at    仅执行一次</li><li>cron  周期性执行</li><li>anacron   适合不常开机的设置</li></ul><h2 id="at仅执行一次的工作调度">at仅执行一次的工作调度</h2><h3 id="参数说明">参数说明</h3><p>at [-lmdvc] TIME<br>-m 当at完成时，即使没有输出信息，以mail通知用户<br>-v 可以使用较明显的时间格式列出at调度中的工作<br>-c 可以列出后面接的该项工作的实际命令内容<br>-d 相当于atrm，可以取消一个at工作<br>-l 相当于atq，列出目前系统上所有该用户的at调度<br>-b 相当于batch<br>TIME<br>HH:MM04:00<br>HH:MM YYYY-MM-DD    05:00 2016-10-05<br>HH:MM[pm|am] [Month] [Date] 04 January 10<br>HH:MM [am|pm] + number [minutes|hours|days|weeks]<br>now + 5 minutes<br>05pm + 3days<br>04pm + 10 days</p><h3 id="示例">示例</h3><h4 id="创建一个job">创建一个job</h4><p>~$:at now+1minutes<br>at&gt;echo &quot;create a job&quot;<br>按ctrl+D结束<br>OK，但是这样子我找不到任何程序的输出在哪里。<br>所以可以改成这样子<br>at&gt;echo “create a job” &gt; at_job.output<br>或者<br>~$:echo “create a job” &gt; at_job.output | at now</p><h4 id="列出所有at-jobs">列出所有at jobs</h4><p>~$:at -l   # 列出at的所有任务<br>~$:atq</p><h4 id="列出某个job">列出某个job</h4><p>~$:at -c [number](1, 2…) # 如果当前没有相应的job，会输出cannot find jobid x</p><h4 id="删除某个job">删除某个job</h4><p>~$:at -r 8<br>~$:atrm 1</p><h3 id="配置文件">配置文件</h3><p>/etc/at.allow   # 哪些人能使用<br>/etc/at.deny    # 哪些人不能使用<br>使用at命令的话，先查找at.allow，如果存在并且有内容，那么只有这些人能使用。如果不存在的话，就去找at.deny。</p><h2 id="batch">batch</h2><p>当空闲时执行，空闲指的是CPU占用率在$0.8$以下</p><h2 id="crond例行性工作调度">crond例行性工作调度</h2><h3 id="参数介绍">参数介绍</h3><p>crontab [-u user] [-ler]<br>-u　只有root能设置这个参数<br>-l　列出当前用户的所有crontab工作内容<br>-e  编辑crontab的内容<br>-r　删除所有crontab的内容</p><h3 id="示例-v2">示例</h3><h4 id="新建crontab">新建crontab</h4><p><strong>注意：周与月日不可共存</strong><br>~$:crontab -e<br>* * * * * cmd<br>分钟　小时　日期　月份　星期<br>*表示任何取值，<br>-表示时间范围 0-59,<br>&quot;,“表示分隔 3,6,9<br>”/n&quot;，如*/5每过五个单位(分钟，小时，天)<br>比如添加每一小时给荟荟发一封邮件，需要添加以下内容</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">* */1 * * * echo &quot;I love you.&quot; | mail -s &quot;huhui&quot; 18811367922@163.com</span><br></pre></td></tr></table></figure><h4 id="删除一个crontab">删除一个crontab</h4><p>~$:crontab -e<br>然后手动编辑要删除的crontab</p><h4 id="删除所有crontab">删除所有crontab</h4><p>~$:crontab -r # 删除所有的crontab</p><h3 id="开启-var-log-cron-log">开启/var/log/cron.log</h3><p>~$:vim /etc/rsyslog.d/50-default.conf<br>将rsylog文件中的#cron.*前的#去掉<br>~$:service rsyslog restart<br>~$:service cron restart<br>~$:vim /var/log/cron.log</p><h3 id="系统任务">系统任务</h3><p>/etc/crontab 为系统的例行性任务，它会执行以下run-parts</p><ul><li>/etc/cron.daily/</li><li>/etc/cron.hourly/</li><li>/etc/cron.monthly/</li><li>/etc/cron.weekly/</li></ul><h3 id="自定义run-parts">自定义run-parts</h3><p>直接编辑/etc/crontab文件，在其中添加</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># m h dom mon dow usercommand</span><br><span class="line">\*/2 \* \* \* \* root   run-parts /etc/cron.minutely</span><br><span class="line">\*/5 \* \* \* \* root   run-parts /root/runcron</span><br><span class="line"># 上述两条命令中，需要对应的目录存在或者直接执行一个shell脚本</span><br><span class="line">\* \* \* \* \* mxxmhh /bin/bash /home/mxxmhh/outputtime_minutes.sh</span><br></pre></td></tr></table></figure><p>outputtime_minutes.sh脚本如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#! /bin/bash</span><br><span class="line">time=`date`</span><br><span class="line">echo $time &gt;&gt; /home/mxxmhh/test.log</span><br></pre></td></tr></table></figure><h3 id="crontab-e-vs-vim-etc-crontab">crontab -e vs vim /etc/crontab</h3><p>他们的格式不同，一个需要指定用户，一个不需要<br>只有root能够修改/etc/crontab，而crontab -e所有不在cron.deny中的用户都可以<br>/etc/crontab是系统的任务，crontab -e是用户的任务</p><h3 id="配置文件-v2">配置文件</h3><p>ubuntu中没有下面两个配置项<br>/etc/cron.allow<br>/etc/cron.deny<br>即默认为所有用户都可以使用crontab</p><h3 id="cron-spool">cron spool</h3><p>/var/spool/cron/crontabs/<br>该目录下为不同账号的crontab内容</p><h2 id="anacron-处理非24小时开机的系统">anacron 处理非24小时开机的系统</h2><h3 id="参数介绍-v2">参数介绍</h3><p>anacron [-usfn] [job]<br>-u 更新记录文件的时间戳<br>-s 开始连续执行各项job，依据记录文件的时间戳判断是否进行<br>-f 强制执行，不管时间戳<br>-n 立即进行未进行的任务，而不延迟</p><h3 id="示例-v3">示例</h3><p>系统的anacron文件都在目录/etc/cron*/*ana*存放<br>/etc/cron.daily/0anacron<br>0表示最先被执行，让时间戳先被更新，避免anacron误判<br>/etc/anacronanacron的设置</p><p>/var/spool/anacron/*<br>记录最近一次执行anacron的时间戳</p><h2 id="mail命令介绍">mail命令介绍</h2><p>mail -s “title” target_email_address<br>echo &quot;content |mail -s “title” target_email_address<br>mail -s &quot;title target_email_address &lt; file #将file的内容当做邮件正文</p><h2 id="mail发送邮件">mail发送邮件</h2><h3 id="安装相应软件">安装相应软件</h3><p>~$:sudo apt-get install postfix mailutils libsasl2-2 ca-certificates libsasl2-modules<br>编辑/etc/postfix/main.cf文件，在文件末尾添加下列内容</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># 指定默认的邮件发送服务器</span><br><span class="line">relayhost = [smtp.gmail.com]:587</span><br><span class="line"># 激活sasl认证</span><br><span class="line">smtp_sasl_auth_enable = yes</span><br><span class="line"># 指定sasl密码配置文件</span><br><span class="line">smtp_sasl_password_maps = hash:/etc/postfix/sasl_passwd</span><br><span class="line"># 非匿名登录</span><br><span class="line">smtp_sasl_security_options = noanonymous</span><br><span class="line"># linux用户与发件人的对应关系配置文件</span><br><span class="line">sender_canonical_maps = hash:/etc/postfix/sender_canonical </span><br><span class="line">smtp_tls_CApath = /etc/ssl/certs</span><br><span class="line">smtpd_tls_CApath = /etc/ssl/certs</span><br><span class="line">smtp_use_tls = yes</span><br></pre></td></tr></table></figure><h3 id="创建密码配置文件">创建密码配置文件</h3><p>~$:vim /etc/postfix/sasl_passwd<br>添加如下内容</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 163邮箱格式</span><br><span class="line">[smtp.163.com]:25 your163mail:your163mailpassword   #注意这里如果直接用passwd是会报错的，需要使用授权码</span><br><span class="line"># gamil邮箱格式</span><br><span class="line">[smtp.gmail.com]:587 yourgmail:yourgmailpassword</span><br></pre></td></tr></table></figure><p>~$:sudo postmap /etc/postfix/sasl_passwd</p><h3 id="创建用户与发件人对应文件">创建用户与发件人对应文件</h3><p>~$:vim /etc/postfix/sender_canonical<br>添加如下内容</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">root your163mail</span><br><span class="line">user1 yourgmail</span><br></pre></td></tr></table></figure><p>~$:sudo postmap /etc/postfix/sender_canonical</p><h3 id="重启postfix服务">重启postfix服务</h3><p>~$:sudo /etc/init.d/postfix reload<br>或者<br>~$:sudo systemctl relaod postfix.service<br>或者<br>~$:sudo service postfix restart</p><h3 id="测试">测试</h3><p>~$:echo “Hello.” |mail -s “I love you.” <a href="mailto:18811376816@163.com" target="_blank" rel="noopener">18811376816@163.com</a><br>这种方式应该是不支持中文的。。</p><h2 id="附录">附录</h2><p>更多at命令的TIME格式</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">noon</span><br><span class="line">midnight</span><br><span class="line">teatime</span><br><span class="line">tomorrow</span><br><span class="line">noon tomorrow</span><br><span class="line">next week</span><br><span class="line">next monday</span><br><span class="line">fri</span><br><span class="line">NOV</span><br><span class="line">9:00 AM</span><br><span class="line">2:30 PM</span><br><span class="line">1430</span><br><span class="line">2:30 PM tomorrow</span><br><span class="line">2:30 PM next month</span><br><span class="line">2:30 PM Fri</span><br><span class="line">2:30 PM 10/21</span><br><span class="line">2:30 PM Oct 21</span><br><span class="line">2:30 PM 10/21/2014</span><br><span class="line">2:30 PM 21.10.14</span><br><span class="line">now + 30 minutes</span><br><span class="line">now + 1 hour</span><br><span class="line">now + 2 days</span><br><span class="line">4 PM + 2 days</span><br><span class="line">now + 3 weeks</span><br><span class="line">now + 4 months</span><br><span class="line">now + 5 years</span><br></pre></td></tr></table></figure><h2 id="参考文献">参考文献</h2><p>1.《鸟哥的LINUX私房菜》<br>2.<a href="https://zhidao.baidu.com/question/249718018.html" target="_blank" rel="noopener">https://zhidao.baidu.com/question/249718018.html</a><br>3.<a href="https://askubuntu.com/questions/1112772/send-system-mail-ubuntu-18-04" target="_blank" rel="noopener">https://askubuntu.com/questions/1112772/send-system-mail-ubuntu-18-04</a><br>4.<a href="https://www.cnblogs.com/tugeler/p/6620150.html" target="_blank" rel="noopener">https://www.cnblogs.com/tugeler/p/6620150.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;例行性工作&quot;&gt;例行性工作&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;at    仅执行一次&lt;/li&gt;
&lt;li&gt;cron  周期性执行&lt;/li&gt;
&lt;li&gt;anacron   适合不常开机的设置&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;at仅执行一次的工作调度&quot;&gt;at仅执行一次的工作
      
    
    </summary>
    
      <category term="linux" scheme="http://mxxhcm.github.io/categories/linux/"/>
    
    
      <category term="linux" scheme="http://mxxhcm.github.io/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>linux service and daemon</title>
    <link href="http://mxxhcm.github.io/2019/05/07/linux-service/"/>
    <id>http://mxxhcm.github.io/2019/05/07/linux-service/</id>
    <published>2019-05-07T08:47:38.000Z</published>
    <updated>2019-06-23T14:39:36.630Z</updated>
    
    <content type="html"><![CDATA[<h2 id="service和daemon">service和daemon</h2><p>service（服务）：系统提供某些功能的一些服务(包括系统本身以及网络service)<br>daemon：实现service的程序叫做daemon</p><h2 id="daemon的分类">daemon的分类</h2><ul><li>stand alone daemon</li><li>super daemon</li></ul><h3 id="stand-alone-daemon">stand_alone daemon</h3><p>独立启动，启动并加载到内存后就一直占用内存与系统资源运行。因此对于客户端的请求响应特别快。比如WWW的daemon(httpd)，FTP的daemon(vsftpd)</p><h3 id="super-daemon">super daemon</h3><p>由一个统一daemon唤起的service，这个特殊的daemon叫做super daemon早期是inetd,后来被xinetd替代了。没有客户端请求时，service被关闭，收到客户端请求时，super daemon唤醒相应的service，请求结束后，这个service就会关闭，service反应时间会比较慢。常见的有telnetservice。<br>signal-control和interval-control，信号管理的daemon以及每隔一段时间主动执行某项job的daemon每一个service程序文件名都会加上d，d代表daemon。</p><h2 id="sysvinit-service">SysVInit service</h2><h3 id="配置文件路径">配置文件路径</h3><ul><li>/etc/rc.d/rcX.d/ (X 代表运行级别 0-6) # 不同runlevel的service存放位置</li><li>/etc/rc.d/rc.local    # 用户自定义的service</li></ul><h3 id="自定义文件示例">自定义文件示例</h3><p>创建/etc/init.d/shadowsocks_client service如下所示</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/sh</span><br><span class="line"></span><br><span class="line">### BEGIN INIT INFO</span><br><span class="line"># Provides:          shadowsocks client</span><br><span class="line"># Required-Start:    $local_fs $remote_fs $network $syslog</span><br><span class="line"># Required-Stop:     $local_fs $remote_fs $network $syslog</span><br><span class="line"># Default-Start:     2 3 4 5</span><br><span class="line"># Default-Stop:      0 1 6</span><br><span class="line"># Short-Description: shadowsocks service</span><br><span class="line"># Description:       shadowsocks service daemon</span><br><span class="line">### END INIT INFO</span><br><span class="line">start()&#123;</span><br><span class="line">　　  sslocal -c /etc/shadowsocks.json -d start</span><br><span class="line">&#125;</span><br><span class="line">stop()&#123;</span><br><span class="line">　　  sslocal -c /etc/shadowsocks.json -d stop</span><br><span class="line">&#125;</span><br><span class="line">case “$1” in</span><br><span class="line">start)</span><br><span class="line">　　　start</span><br><span class="line">　　　;;</span><br><span class="line">stop)</span><br><span class="line">　　　stop</span><br><span class="line">　　　;;</span><br><span class="line">reload)</span><br><span class="line">　　　stop</span><br><span class="line">　　　start</span><br><span class="line">　　　;;</span><br><span class="line">\*)</span><br><span class="line">　　　echo “Usage: $0 &#123;start|reload|stop&#125;”</span><br><span class="line">　　　exit 1</span><br><span class="line">　　　;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure><p>然后执行以下命令进行更新<br>~$:sudo chomod a+x /etc/init.d/shadowsocks_client<br>~$:sudo update_rc.d shadowsocks defaults</p><h3 id="运行方式">运行方式</h3><p>service shadowsocks_client start</p><h2 id="systemd">SystemD</h2><h3 id="配置文件路径-v2">配置文件路径</h3><ul><li>/etc/systemd/system系统service，不要动。大部分是软连接，指向/usr/lib/systemd/sytem</li><li>/run/systemd/systemRuntime units</li><li>/usr/local/lib/systemd/system管理员安装的System units</li><li>/usr/lib/systemd/system包管理器安装的System units(for centos)</li><li>/lib/systemd/system   包管理器安装的System units(for debian/ubuntu)</li><li>/etc/systemd/system/**.service.wants/*：此目录内的文件为链接文件，设置相依服务的链接。意思是启动了 **.service 之后，最好再加上这目录下面建议的服务。</li><li>/etc/systemd/system/vsftpd.service.requires/*：此目录内的文件为链接文件，设置相依服务的链接。意思是在启动 vsftpd.service 之前，需要事先启动哪些服务的意思。</li></ul><h3 id="自定义unit文件示例">自定义unit文件示例</h3><p>在/lib/systemd/system/创建ss_client.service，如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[Unit]</span><br><span class="line">Description=ss v4 client daemon</span><br><span class="line"> </span><br><span class="line">[Service]</span><br><span class="line">ExecStart=/usr/bin/sslocal -c /etc/shadowsocks_v4_client.json &lt;/dev/null &amp;&gt;&gt;/home/mxxmhh/.log/ss-local.log </span><br><span class="line">WorkingDirectory=/home/mxxmhh/</span><br><span class="line"># Restart=on-failure</span><br><span class="line">StartLimitBurst=2</span><br><span class="line">StartLimitInterval=30</span><br><span class="line">User=mxxmhh</span><br><span class="line">ExecReload=/bin/kill -SIGHUP $MAINPID</span><br><span class="line">ExecStop=/bin/kill -SIGINT $MAINPID</span><br><span class="line"> </span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure><p>然后执行以下命令<br>~$:sudo systemctl start ss-client.service<br>~$:sudo systemctl enable ss-client.service</p><blockquote><p>Created symlink /etc/systemd/system/multi-user.target.wants/ss-client.service → /lib/systemd/system/ss-client.service.</p></blockquote><p>执行以下命令发现报错<br>~$:sudo systemctl status ss-client.service</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ss-client.service: Start request repeated too</span><br><span class="line">ss-client.service: Failed with result &apos;exit-c</span><br></pre></td></tr></table></figure><p>根据参考文献13使用下列命令常看详细log<br>~$:journalctl -u ss-client.service<br>发现报错：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ss-client.service: Failed at step USER spawning /usr/bin/sslocal: No such proces</span><br></pre></td></tr></table></figure><p>然后根据参考文献12发现可能是自己的文件写的有问题，最后发现是user复制的时候出错了，修改之后就好了。执行以下命令加载修改后的配置文件，然后restart服务。<br>~$: sudo systemctl daemon-reload<br>~$: sudo systemctl restart ss-client.service<br>~$: sudo systemctl status ss-client.service</p><h3 id="unit文件的编写">Unit文件的编写</h3><p>每个unit都有一个配置文件，定义了这个unit启动的条件。</p><h4 id="unit格式">Unit格式</h4><p>下面是 SSH service的unit文件，service unit文件以.service 为文件名后缀。<br>~$:cat /etc/systemd/system/sshd.service</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[Unit]</span><br><span class="line">Description=OpenSSH server daemon</span><br><span class="line">[Service]</span><br><span class="line">EnvironmentFile=/etc/sysconfig/sshd</span><br><span class="line">ExecStartPre=/usr/sbin/sshd-keygen</span><br><span class="line">ExecStart=/usrsbin/sshd –D $OPTIONS</span><br><span class="line">ExecReload=/bin/kill –HUP $MAINPID</span><br><span class="line">KillMode=process</span><br><span class="line">Restart=on-failure</span><br><span class="line">RestartSec=42s</span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure><p>Unit部分仅仅有一个描述信息;Service中，ExecStartPre定义启动service之前应该运行的命令；ExecStart定义启动service的具体命令行语法；Install部分，WangtedBy 表明这个service是在多用户模式下所需要的，multi-user.target</p><h4 id="unit的配置文件区块">Unit的配置文件区块</h4><p>一个文件通常由[Unit]，[Service]（或者其他unit类型）和[Install]构成。<br>[Unit]区块通常是配置文件的第一个区块，用来定义 Unit 的元数据，以及配置与其他 Unit 的关系。它的主要字段如下。</p><ul><li>Description：简短描述</li><li>Documentation：文档地址</li><li>Requires：当前 Unit 依赖的其他 Unit，如果它们没有运行，当前 Unit 会启动失败</li><li>Wants：与当前 Unit 配合的其他 Unit，如果它们没有运行，当前 Unit 不会启动失败</li><li>BindsTo：与Requires类似，它指定的 Unit 如果退出，会导致当前 Unit 停止运行</li><li>Before：如果该字段指定的 Unit 也要启动，那么必须在当前 Unit 之后启动</li><li>After：如果该字段指定的 Unit 也要启动，那么必须在当前 Unit 之前启动</li><li>Conflicts：这里指定的 Unit 不能与当前 Unit 同时运行</li><li>Condition…：当前 Unit 运行必须满足的条件，否则不会运行</li><li>Assert…：当前 Unit 运行必须满足的条件，否则会报启动失败</li></ul><p>[Install]通常是配置文件的最后一个区块，用来定义如何启动，以及是否开机启动。它的主要字段如下。</p><ul><li>WantedBy：它的值是一个或多个 Target，当前 Unit 激活时（enable）符号链接会放入/etc/systemd/system目录下面以 Target 名 + .wants后缀构成的子目录中</li><li>RequiredBy：它的值是一个或多个 Target，当前 Unit 激活时，符号链接会放入/etc/systemd/system目录下面以 Target 名 + .required后缀构成的子目录中</li><li>Alias：当前 Unit 可用于启动的别名</li><li>Also：当前 Unit 激活（enable）时，会被同时激活的其他 Unit</li></ul><p>[Service]区块用来 Service 的配置，只有 Service 类型的 Unit 才有这个区块。它的主要字段如下。</p><ul><li>Type：定义启动时的进程行为。它有以下几种值。</li><li>Type=simple：默认值，执行ExecStart指定的命令，启动主进程</li><li>Type=forking：以 fork 方式从父进程创建子进程，创建后父进程会立即退出</li><li>Type=oneshot：一次性进程，Systemd 会等当前服务退出，再继续往下执行</li><li>Type=dbus：当前服务通过D-Bus启动</li><li>Type=notify：当前服务启动完毕，会通知Systemd，再继续往下执行</li><li>Type=idle：若有其他任务执行完毕，当前服务才会运行</li><li>ExecStart：启动当前服务的命令</li><li>ExecStartPre：启动当前服务之前执行的命令</li><li>ExecStartPost：启动当前服务之后执行的命令</li><li>ExecReload：重启当前服务时执行的命令</li><li>ExecStop：停止当前服务时执行的命令</li><li>ExecStopPost：停止当其服务之后执行的命令</li><li>RestartSec：自动重启当前服务间隔的秒数</li><li>Restart：定义何种情况 Systemd 会自动重启当前服务，可能的值包括always（总是重启）、on-success、on-failure、on-abnormal、on-abort、on-watchdog</li><li>TimeoutSec：定义 Systemd 停止当前服务之前等待的秒数</li><li>Environment：指定环境变量</li></ul><h3 id="日志">日志</h3><p>Systemd统一管理所有Unit的启动日志。带来的好处就是，可以只用journalctl一个命令，查看所有日志（内核日志和应用日志）。日志的配置文件是/etc/systemd/journald.conf。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">~$:sudo journalctl  # 查看所有日志（默认情况下 ，只保存本次启动的日志）</span><br><span class="line">~$: sudo journalctl -k    # 查看内核日志（不显示应用日志）</span><br><span class="line"><span class="meta">#</span> 查看系统本次启动的日志</span><br><span class="line">~$:sudo journalctl -b    </span><br><span class="line">~$:sudo journalctl -b -0</span><br><span class="line">~$:sudo journalctl -b -1 # 查看上一次启动的日志（需更改设置）</span><br><span class="line"><span class="meta">#</span> 查看指定时间的日志</span><br><span class="line">~$:sudo journalctl --since="2012-10-30 18:17:16"</span><br><span class="line">~$:sudo journalctl --since "20 min ago"</span><br><span class="line">~$:sudo journalctl --since yesterday</span><br><span class="line">~$:sudo journalctl --since "2015-01-10" --until "2015-01-11 03:00"</span><br><span class="line">~$:sudo journalctl --since 09:00 --until "1 hour ago"</span><br><span class="line">~$:sudo journalctl -n   # 显示尾部的最新10行日志</span><br><span class="line">~$:sudo journalctl -n 20    # 显示尾部指定行数的日志</span><br><span class="line">~$:sudo journalctl -f   # 实时滚动显示最新日志</span><br><span class="line">~$:sudo journalctl /usr/lib/systemd/systemd # 查看指定服务的日志</span><br><span class="line">~$:sudo journalctl _PID=1   # 查看指定进程的日志</span><br><span class="line">~$:sudo journalctl /usr/bin/bash    # 查看某个路径的脚本的日志</span><br><span class="line">~$:sudo journalctl _UID=33 --since today    # 查看指定用户的日志</span><br><span class="line"><span class="meta">#</span> 查看某个 Unit 的日志</span><br><span class="line">~$:sudo journalctl -u nginx.service</span><br><span class="line">~$:sudo journalctl -u nginx.service --since today</span><br><span class="line">~$:sudo journalctl -u nginx.service -f  # 实时滚动显示某个 Unit 的最新日志</span><br><span class="line">~$:journalctl -u nginx.service -u php-fpm.service --since today # 合并显示多个 Unit 的日志</span><br><span class="line"><span class="meta">#</span> 查看指定优先级（及其以上级别）的日志，共有8级</span><br><span class="line"><span class="meta">#</span> 0: emerg</span><br><span class="line"><span class="meta">#</span> 1: alert</span><br><span class="line"><span class="meta">#</span> 2: crit</span><br><span class="line"><span class="meta">#</span> 3: err</span><br><span class="line"><span class="meta">#</span> 4: warning</span><br><span class="line"><span class="meta">#</span> 5: notice</span><br><span class="line"><span class="meta">#</span> 6: info</span><br><span class="line"><span class="meta">#</span> 7: debug</span><br><span class="line">~$:sudo journalctl -p err -b</span><br><span class="line">~$:sudo journalctl --no-pager   # 日志默认分页输出，--no-pager 改为正常的标准输出</span><br><span class="line">~$:sudo journalctl -b -u nginx.service -o json  # 以 JSON 格式（单行）输出</span><br><span class="line">~$:sudo journalctl -b -u nginx.serviceqq -o json-pretty # 以 JSON 格式（多行）输出，可读性更好</span><br><span class="line">~$:sudo journalctl --disk-usage # 显示日志占据的硬盘空间</span><br><span class="line">~$:sudo journalctl --vacuum-size=1G # 指定日志文件占据的最大空间</span><br><span class="line">~$:sudo journalctl --vacuum-time=1years # 指定日志文件保存多久</span><br></pre></td></tr></table></figure><h3 id="systemctl-工具">systemctl 工具</h3><p>~$:systemctl list-units     列出正在运行的 Unit<br>~$:systemctl list-units --all   列出所有Unit，包括没有找到配置文件的或者启动失败的<br>~$:systemctl list-units --all --state=inactive      列出所有没有运行的 Unit<br>~$:systemctl list-units --failed    列出所有加载失败的 Unit<br>~$:systemctl list-units --type=service  列出所有正在运行的、类型为 service 的 Unit<br>~$:systemctl list-unit-files    列出所有配置文件<br>~$:systemctl list-unit-files --type=service     列出指定类型的配置文件<br>~$:systemctl start foo.service用来启动一个service (并不会重启现有的)<br>~$:systemctl stop foo.service用来停止一个service (并不会重启现有的)。<br>~$:systemctl restart foo.service用来停止并启动一个service。<br>~$:systemctl reload foo.service当支持时，重新装载配置文件而不中断等待操作。<br>~$:systemctl condrestart foo.service如果service正在运行那么重启它。<br>~$:systemctl status foo.service汇报service是否正在运行。<br>~$:systemctl list-unit-files --type=service用来列出可以启动或停止的service列表。<br>~$:systemctl enable foo.service在下次启动时或满足其他触发条件时设置service为启用。创建一个符号链接从/etc/systemd/system/some_target.target.wants指向/lib/systemd/system或者/etc/systemd/system。<br>~$:systemctl disable foo.service在下次启动时或满足其他触发条件时设置service为禁用<br>~$:systemctl is-enabled foo.service用来检查一个service在当前环境下被配置为启用还是禁用。<br>~$:systemctl list-unit-files --type=service输出在各个运行级别下service的启用和禁用情况<br>~$:systemctl daemon-reload当您创建新service文件或者变更设置时使用。<br>~$:systemctl isolate multi-user.target (OR systemctl isolate runlevel3.target OR telinit 3)改变至多用户运行级别。<br>~$:ls /etc/SystemD/system/*.wants/foo.service用来列出该service在哪些运行级别下启用和禁用。</p><h2 id="配置文件">配置文件</h2><p>/etc/init.d/*　# 基本上所有的service启动脚本都被放置在该目录<br>/etc/rcX.d/    # X指的是数字，从$0-6$，代表不同的run-level，是/etc/init.d/目录下service的软连接<br>/etc/systemd/system     # systemd的service文件位置，是/usr/lib/systemd/sytem的软连接<br>/etc/default    # 一些配置文件<br>/etc/*  # 各service各自的配置文件</p><h2 id="service-initctl-systemctl命令对照表">service，initctl，systemctl命令对照表</h2><p>Service 命令|UpStart initctl 命令|SystemD 命令|备注<br>—|---|<br>service foo start|initctl start|systemctl start foo.service用来启动一个service (并不会重启现有的)<br>service foo stop|initctl stop|systemctl stop foo.service用来停止一个service (并不会重启现有的)<br>service foo reload|systemctl reload foo.service当支持时，重新装载配置文件而不中断等待操作。<br>service foo restart|initctl restart|systemctl restart foo.service用来停止并启动一个service<br>service foo status|initctl status|systemctl status foo.service汇报service是否正在运行。<br>service foo reload|initctl reload|systemctl reload foo.service当支持时，重新装载配置文件而不中断等待操作。<br>service foo condrestart||systemctl condrestart foo.service如果service正在运行那么重启它。</p><h2 id="xinted">xinted</h2><p>xinted是/etc/init.d/目录中的一个脚本。<br>xinted 是inted的扩展，是super daemon，它本身管理了一系列的daemon，只有在用户调用时才由xinetd启动，他们要比独立的daemon启动晚。<br>!!!xinted默认在ubuntu中是不存在的,<br>~$:sudo apt-get install xinetd<br>/etc/xinetd.conf   #super daemon配置文件<br>/etc/xinetd.d/*    #它所管理的进程<br>/var/lib/*  各service产生的数据库<br>/var/run/*  各service的程序的pid记录处</p><h3 id="stand-alone的启动">stand alone的启动</h3><ol><li>用/etc/init.d/*启动<br>~#:/etc/init.d/cron start|stop|status|restart|reload|force-reload</li></ol><p>2.用service [service-name] (start|…)启动<br>service-name必须与/etc/init.d/相照应<br>–status-all 将所有的stand_aloneservice列出来<br>~#:service --status-all<br>~#:service cron</p><h3 id="super-daemon的启动方式">super daemon的启动方式</h3><p>super daemon本身也是一个stand alone的service，但是它所管理的其他文件就不是了。<br>查看某个service是否可用。<br>~#:grep -i ‘disable’ /etc/xinted.d/* # disable表示取消，若为yes，表示该service未开启，no表示开启</p><h4 id="示例">示例</h4><p>开启timeservice<br>~#:vim /etc/xinted.d/time<br>将disable改为no<br>重新启动xinted service<br>~#:service xinted restart<br>!!!注意是重启xinted service</p><p>查看该service的信息<br>~#:grep -i ‘time’ /etc/services<br>~#:netstat -nltp | grep ‘time port’</p><h3 id="默认值配置文件以及参数介绍">默认值配置文件以及参数介绍</h3><p>/etc/xinetd.conf<br>log_type    SYSLOG daemon info 日志文件的记录service类型<br>log_on_failure  发生错误时需要记录的信息<br>log_on_success  成功启动时的记录信息<br>cps 同一秒内的最大连接个数，若超过则暂停<br>instance    同一service的最大连接数<br>per_source  同一来源的客户端的最大连接数<br>v6only  是否运行ipv6<br>groups<br>umask</p><p>/etc/xinetd.d/<br>service <service name><br>{<br>disable 启动与否<br>id  service识别<br>server  程序文件名  这个service的启动程序<br>server_args 程序参数    设置server_args=–daemon<br>user    service所属id<br>group   用户组<br>socket_type 数据包类型  stream|dgram|raw stream使用tcp,   udp使用dgram,raw代表erver需要与ip直接交互。<br>protocol    数据包类型  tcp|udp与socket_type重复，<br>wait    连接机制    yes(single) no(multi) 一般udp为yes，tcp为no<br>instances   最大连接数<br>per_source  单用户来源  (一个数字或者NULIMTED)<br>cps 新连接限制<br>log_type    日志文件类型    以什么日志选项记载和需要记载的等级(默认为info)<br>log_on_success,log_on_failure,设置值,[PID,HOST,USERID,EXIT,DURATION]<br>PID为service启动时的pid,host为远程主机的ip，userid为登陆者的账号，EXIT为离开时记录的项目，DURATION为该用户使用此service多久。<br>env 额外环境变量设置    设置环境变量<br>port    非正规端口号    设置不同的service与对应的端口号，port与service名必须与/etc/services的值相同<br>redirect    service转址    [IP port] 将客户端的请求转到另一台主机<br>includedir  调用外部设置    表示将某个目录所有文件都放入xinetd.conf中，<br>bind    service端口锁定    运行此service的适配卡<br>interface   与bind相同<br>only_from   [0.0.0.0,192.168.1.0/24,hostname,domainname]设置为这里面的ip或者主机名才能访问，0.0.0.0表示所有主机皆能访问，如果是192.168.1.0/24则表示为C　class的域，即由(192.168.1.1~192.168.1.255)皆可登录。另外，也可选择域名，如bit.edu.cn表示运行北理工的ip登录你的主机<br>no_acess    表示的是不可登录的主机<br>acess_time  时间控制    [00:00-24:00,HH:MM-HH:MM]<br>umask   设置用户新建目录或者文件时候的属性<br>}</service></p><h2 id="参考文献">参考文献</h2><p>1.《鸟哥的LINUX私房菜》<br>2.<a href="https://askubuntu.com/questions/911525/difference-between-systemctl-init-d-and-service" target="_blank" rel="noopener">https://askubuntu.com/questions/911525/difference-between-systemctl-init-d-and-service</a><br>3.<a href="http://www.r9it.com/20180613/ubuntu-18.04-auto-start.html" target="_blank" rel="noopener">http://www.r9it.com/20180613/ubuntu-18.04-auto-start.html</a><br>4.<a href="https://www.ibm.com/developerworks/cn/linux/1407_liuming_init1/index.html" target="_blank" rel="noopener">https://www.ibm.com/developerworks/cn/linux/1407_liuming_init1/index.html</a><br>5.<a href="https://www.ibm.com/developerworks/cn/linux/1407_liuming_init2/index.html?ca=drs-" target="_blank" rel="noopener">https://www.ibm.com/developerworks/cn/linux/1407_liuming_init2/index.html?ca=drs-</a><br>6.<a href="https://www.ibm.com/developerworks/cn/linux/1407_liuming_init3/index.html?ca=drs-" target="_blank" rel="noopener">https://www.ibm.com/developerworks/cn/linux/1407_liuming_init3/index.html?ca=drs-</a><br>7.<a href="http://www.ruanyifeng.com/blog/2016/03/systemd-tutorial-commands.html" target="_blank" rel="noopener">http://www.ruanyifeng.com/blog/2016/03/systemd-tutorial-commands.html</a><br>8.<a href="https://wizardforcel.gitbooks.io/vbird-linux-basic-4e/content/150.html" target="_blank" rel="noopener">https://wizardforcel.gitbooks.io/vbird-linux-basic-4e/content/150.html</a><br>9.<a href="https://www.freedesktop.org/software/systemd/man/systemd.unit.html" target="_blank" rel="noopener">https://www.freedesktop.org/software/systemd/man/systemd.unit.html</a><br>10.<a href="https://unix.stackexchange.com/questions/206315/whats-the-difference-between-usr-lib-systemd-system-and-etc-systemd-system" target="_blank" rel="noopener">https://unix.stackexchange.com/questions/206315/whats-the-difference-between-usr-lib-systemd-system-and-etc-systemd-system</a><br>11.<a href="https://stackoverflow.com/questions/35452591/start-request-repeated-too-quickly" target="_blank" rel="noopener">https://stackoverflow.com/questions/35452591/start-request-repeated-too-quickly</a><br>12.<a href="https://superuser.com/questions/1156676/what-causes-systemd-failed-at-step-user-spawning-usr-sbin-opendkim-no-such-p" target="_blank" rel="noopener">https://superuser.com/questions/1156676/what-causes-systemd-failed-at-step-user-spawning-usr-sbin-opendkim-no-such-p</a><br>13.<a href="https://stackoverflow.com/questions/39202644/caddy-service-start-request-repeated-too-quickly" target="_blank" rel="noopener">https://stackoverflow.com/questions/39202644/caddy-service-start-request-repeated-too-quickly</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;service和daemon&quot;&gt;service和daemon&lt;/h2&gt;
&lt;p&gt;service（服务）：系统提供某些功能的一些服务(包括系统本身以及网络service)&lt;br&gt;
daemon：实现service的程序叫做daemon&lt;/p&gt;
&lt;h2 id=&quot;daem
      
    
    </summary>
    
      <category term="linux" scheme="http://mxxhcm.github.io/categories/linux/"/>
    
    
      <category term="linux" scheme="http://mxxhcm.github.io/tags/linux/"/>
    
      <category term="SystemD" scheme="http://mxxhcm.github.io/tags/SystemD/"/>
    
      <category term="serveice" scheme="http://mxxhcm.github.io/tags/serveice/"/>
    
      <category term="daemon" scheme="http://mxxhcm.github.io/tags/daemon/"/>
    
      <category term="initd" scheme="http://mxxhcm.github.io/tags/initd/"/>
    
  </entry>
  
  <entry>
    <title>linux log文件</title>
    <link href="http://mxxhcm.github.io/2019/05/07/linux-log%E6%96%87%E4%BB%B6/"/>
    <id>http://mxxhcm.github.io/2019/05/07/linux-log文件/</id>
    <published>2019-05-07T08:44:36.000Z</published>
    <updated>2019-06-20T01:47:30.578Z</updated>
    
    <content type="html"><![CDATA[<h2 id="常见的日志文件">常见的日志文件</h2><p>/var/log/cron.logcrontab调度有没有执行，有没有错误以及/etc/crontab是否正确编写<br>/var/log/lastlog所有账号最后一次的登录信息，非ASCII文件<br>/var/log/mail.log所有邮件的往来信息<br>/var/log/messages各种错误信息<br>/var/log/secure<br>/var/log/wtmp登录成功与识别的账号信息<br>/var/log/apport.log应用程序崩溃记录<br>/var/log/apt/*apt-get 安装卸载软件的日志<br>/var/log/auth.log登录认证log(与/etc/var/secure挺像)<br>/var/log/boot.log系统启动的日志<br>/var/log/btmp记录所有失败者的信息<br>/var/log/cups/* <br>/var/log/dist-upgradedist-upgrade这种更新方式的日志<br>/var/log/dmesg内核缓冲信息<br>/var/log/dpkg.log安装或dpkg命令清除软件包的日志<br>/var/log/faillog用户登录失败信息，错误登录命令也会显示<br>/var/log/fontconfig.log字体设置有关的日志<br>/var/log/fsck文件系统日志<br>/var/log/hp<br>/var/log/install<br>/var/log/kern.log内核产生的日志<br>/var/log/sambasamba存储的信息<br>/var/log/syslog系统登录信息<br>/var/log/upstart<br>/var/log/wtmp包含登录信息，找出谁正在登录进入系统以及谁用命令显示这个文件或者信息等<br>/var/log/xorg.*.log来自X的日志信息</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;常见的日志文件&quot;&gt;常见的日志文件&lt;/h2&gt;
&lt;p&gt;/var/log/cron.log	crontab调度有没有执行，有没有错误以及/etc/crontab是否正确编写&lt;br&gt;
/var/log/lastlog	所有账号最后一次的登录信息，非ASCII文件&lt;br&gt;

      
    
    </summary>
    
      <category term="linux" scheme="http://mxxhcm.github.io/categories/linux/"/>
    
    
      <category term="linux" scheme="http://mxxhcm.github.io/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>linux-启动流程</title>
    <link href="http://mxxhcm.github.io/2019/05/07/linux-%E5%90%AF%E5%8A%A8%E6%B5%81%E7%A8%8B/"/>
    <id>http://mxxhcm.github.io/2019/05/07/linux-启动流程/</id>
    <published>2019-05-07T08:41:27.000Z</published>
    <updated>2019-05-18T09:40:46.238Z</updated>
    
    <content type="html"><![CDATA[<h2 id="linux的启动流程">Linux的启动流程</h2><p>BIOS   MBR    boot loader    boot sector</p><h2 id="bios">BIOS</h2><p>BIOS(Basic Input Ouput System)是一套程序,它被写死到主板上面的一个内存芯片，这个内存芯片没有电时也能将数据记录下来，那就是一个ROM(Read Only Memory)。BIOS是系统开机时首先会去读取的一个小程序，它控制着开机时候的各项硬件参数的取得，它掌握了系统硬件的详细信息以及开机设备的选择，BIOS程序代码也会被适度修改，但是如果写在ROM中是无法修改的，现在多把BIOS写入Flash Memory 或者EEPROM中。</p><p>BIOS通过硬件的INT13中断功能来读取MBR的，所以只要BIOS能检测到磁盘那么他就能够通过INT13这条信道来读取该磁盘的第一个扇区内的MBR，这样就能够执行boot loader</p><h2 id="boot-loader">Boot Loader</h2><p>boot loader的最主要功能就是认识操作系统的文件格式并且加载该操作系统的内核到内存中执行。不同操作系统的文件类型不同，所以boot loader也是不同的，那么如果通过一个MBR来安装多操作系统呢。</p><h2 id="boot-sector">Boot Sector</h2><p>对于文件系统来说，每个文件系统都会有保留一个引导扇区(boot sector)提供给操作系统来安装boot loader。<br>每个操作系统默认会安装一个boot loader到它的文件系统中。对于Linux来说，我们可以选择将boot loader安装到MBR，也可以不选择，那样boot loader只会安装在它自己的文件系统中的即是(boot sector)。但是Windows操作系统会默认直接将boot loader安装在MBR以及boot sector中，所以说安装双系统时，最好先装Windows，再装Linux，否则反过来的话，那么Windows的boot loader可能就会覆盖掉Linux的boot loader.<br>但是，系统的MBR只有一个，所以，如何执行boot sector中的boot loader呢，那就需要谈到boot loader的功能了。<br>boot loader的功能<br>提供菜单<br>加载内核文件<br>转交其他loader<br>我们可以通过MBR中的boot loader选择其他的loader，这样就可以选择其他的操作系统运行了。<br>通过boot loader的管理读取了内核文件之后，那么就要进行工作了，重新检测硬件等。<br>但是从某些版本之后，内核是可以动态加载内核模块的，这些模块被放在/lib/modules/目录内，模块放置到磁盘根目录内，因此，启动过程中内核必须要挂载根目录，这样才能动态读取内核模块提供加载驱动程序的功能。<br>一般来说，非必要的功能可以编译成模块的内核功能，许多Linux会将内核编译成模块。USB，SATA,SCSI等设备的驱动程序都是通过模块的方式存在的。<br>那么问题来了，内核是不认识SATA硬盘的，所以根目录无法挂载，更无法通过根目录下的/lib/modules来驱动SATA硬盘了。这时候，就用到了虚拟文件系统(/boot/initrd)来管理。<br>虚拟文件系统能够通过boot loader加载到内存中，解压缩被当成一个根目录，从而通过该程序加载启动过程中所最需要的内核模块,通常是USB,RAID,LVM,SCSI等文件系统以及硬盘的驱动程序。<br>需要initrd的原因是因为启动时无法挂载根目录，如果根目录能被挂载，那么就不需要了，根目录再USB,SATA,SCSI等磁盘，或者文件系统比较特殊，为LVM，RAID等，那么是需要<br>载入这些模块之后，initd就会帮助内核重新调用/sbin/init进行后续正常的启动</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;linux的启动流程&quot;&gt;Linux的启动流程&lt;/h2&gt;
&lt;p&gt;BIOS   MBR    boot loader    boot sector&lt;/p&gt;
&lt;h2 id=&quot;bios&quot;&gt;BIOS&lt;/h2&gt;
&lt;p&gt;BIOS(Basic Input Ouput System
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>linux cpu信息查看</title>
    <link href="http://mxxhcm.github.io/2019/05/07/linux-cpu%E4%BF%A1%E6%81%AF%E6%9F%A5%E7%9C%8B/"/>
    <id>http://mxxhcm.github.io/2019/05/07/linux-cpu信息查看/</id>
    <published>2019-05-07T08:30:27.000Z</published>
    <updated>2019-06-12T03:16:52.798Z</updated>
    
    <content type="html"><![CDATA[<h2 id="查看cpu核数和cpu信息">查看cpu核数和cpu信息</h2><p>~$:lscpu</p><blockquote><p>Architecture:          x86_64<br>CPU op-mode(s):        32-bit, 64-bit<br>Byte Order:            Little Endian<br>CPU(s):                16<br>On-line CPU(s) list:   0-15<br>Thread(s) per core:    2<br>Core(s) per socket:    8<br>Socket(s):             1<br>NUMA node(s):          1<br>Vendor ID:             AuthenticAMD<br>CPU family:            23<br>Model:                 8<br>Model name:            AMD Ryzen 7 2700X Eight-Core Processor<br>Stepping:              2<br>CPU MHz:               3921.420<br>CPU max MHz:           3700.0000<br>CPU min MHz:           2200.0000<br>BogoMIPS:              7385.61<br>Virtualization:        AMD-V<br>L1d cache:             32K<br>L1i cache:             64K<br>L2 cache:              512K<br>L3 cache:              8192K<br>NUMA node0 CPU(s):     0-15<br>Flags:                 fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb hw_pstate sme ssbd ibpb vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt sha_ni xsaveopt xsavec xgetbv1 xsaves clzero irperf xsaveerptr arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif overflow_recov succor smca</p></blockquote><p>总核数 = 物理CPU个数 X 每颗物理CPU的核数<br>总逻辑CPU数 = 物理CPU个数 X 每颗物理CPU的核数 X 超线程数<br>拿我做测试的机器来说，一个cpu，每个cpu八核，每个核两个超线程。</p><h2 id="查看物理cpu个数">查看物理CPU个数</h2><p>~$:cat /proc/cpuinfo| grep “physical id”| sort| uniq |wc -l</p><blockquote><p>1</p></blockquote><h2 id="查看每个物理cpu中core的个数-即核数">查看每个物理CPU中core的个数(即核数)</h2><p>~$:cat /proc/cpuinfo| grep “cpu cores”</p><blockquote><p>8</p></blockquote><h2 id="查看逻辑cpu的个数">查看逻辑CPU的个数</h2><p>~$:cat /proc/cpuinfo| grep “processor”| wc -l</p><blockquote><p>processor: 0<br>processor: 1<br>processor: 2<br>processor: 3<br>processor: 4<br>processor: 5<br>processor: 6<br>processor: 7<br>processor: 8<br>processor: 9<br>processor: 10<br>processor: 11<br>processor: 12<br>processor: 13<br>processor: 14<br>processor: 15</p></blockquote><h2 id="参考文献">参考文献</h2><p>1.鸟哥的Linux私房菜</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;查看cpu核数和cpu信息&quot;&gt;查看cpu核数和cpu信息&lt;/h2&gt;
&lt;p&gt;~$:lscpu&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Architecture:          x86_64&lt;br&gt;
CPU op-mode(s):        32-bit, 6
      
    
    </summary>
    
      <category term="linux" scheme="http://mxxhcm.github.io/categories/linux/"/>
    
    
      <category term="linux" scheme="http://mxxhcm.github.io/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>linux jobs nohup bg fg ...</title>
    <link href="http://mxxhcm.github.io/2019/05/07/linux-jobs-nohup-fg/"/>
    <id>http://mxxhcm.github.io/2019/05/07/linux-jobs-nohup-fg/</id>
    <published>2019-05-07T08:19:29.000Z</published>
    <updated>2019-06-17T11:26:52.617Z</updated>
    
    <content type="html"><![CDATA[<h2 id="nohup">nohup</h2><p>nohup　[command parameters] [&amp;] nohup不挂断地运行命令。<br>nohup命令忽略所有挂断（SIGHUP）信号，有&amp;表示在后台执行，没有&amp;表示在机前台执行，即使脱机或者注销系统后仍然会执行，输出为nohup.out</p><h2 id="none">&amp;</h2><p>在后台运行。<br>一般nohup和&amp;会在一起使用。即nohup command &amp;，表示在后台不挂断的执行command命令<br>STDOUT以及STDERR都会被显示在屏幕上，可以采用数据流重定向将其输入文件<br>tar -cvj -f ~/my.bak/etc20161006.tar.bz2 /etc &gt; ~/tmp/log.txt 2&gt;&amp;1 &amp;<br>这样stdout以及stderr会被输入进~/tmp/log.txt</p><h2 id="示例">示例</h2><p>~$:nohup sslocal -c /etc/shadowsocks_v6.json &lt;/dev/null &amp;&gt;&gt;~/.log/ss-local.log &amp;</p><h2 id="jobs">jobs</h2><p>jobs -l 查看运行的后台进程，当打开该进程的终端关闭时，就无法看到使用jobs查看该程序了。需要使用ps命令<br>jobs [-lsr] 查看目前后台的jobs<br>-l 列出所有的后台jobs，包含pid<br>-s 列出停止的后台jobs，<br>-r 列出正在运行的jobs,</p><h2 id="fg-bg-ctrl-z">fg, bg, ctrl+z</h2><p>fg(foreground)将后台的工作拿到前台<br>fg %jobnumber<br>fg +/- [jobnumber]表示第几个后台工作，+表示最后一个被丢入后台，-表示最后第二个被丢入后台，最后第三个以及以上不显示</p><p>bg继续后台运行某个程序</p><p>ctrl+z挂起程序，将正在工作的程序放入后台(避免被ctrl+c终止,而非系统的后台)</p><h2 id="参考文献">参考文献</h2><p>1.<a href="https://baike.baidu.com/item/nohup/5683841?fr=aladdin" target="_blank" rel="noopener">https://baike.baidu.com/item/nohup/5683841?fr=aladdin</a><br>2.<a href="https://www.cnblogs.com/baby123/p/6477429.html" target="_blank" rel="noopener">https://www.cnblogs.com/baby123/p/6477429.html</a><br>3.<a href="https://www.cnblogs.com/hf8051/p/4494735.html" target="_blank" rel="noopener">https://www.cnblogs.com/hf8051/p/4494735.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;nohup&quot;&gt;nohup&lt;/h2&gt;
&lt;p&gt;nohup　[command parameters] [&amp;amp;] nohup不挂断地运行命令。&lt;br&gt;
nohup命令忽略所有挂断（SIGHUP）信号，有&amp;amp;表示在后台执行，没有&amp;amp;表示在机前台执行，即使脱
      
    
    </summary>
    
      <category term="linux" scheme="http://mxxhcm.github.io/categories/linux/"/>
    
    
      <category term="linux" scheme="http://mxxhcm.github.io/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>ubuntu 编译安装gcc</title>
    <link href="http://mxxhcm.github.io/2019/05/06/linux-gcc%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85/"/>
    <id>http://mxxhcm.github.io/2019/05/06/linux-gcc编译安装/</id>
    <published>2019-05-06T06:17:40.000Z</published>
    <updated>2019-06-12T02:45:33.982Z</updated>
    
    <content type="html"><![CDATA[<h2 id="下载相应版本的安装包">下载相应版本的安装包</h2><p>国科大源：<a href="https://mirrors.ustc.edu.cn/gnu/gcc/" target="_blank" rel="noopener">https://mirrors.ustc.edu.cn/gnu/gcc/</a><br>官网源：<a href="http://ftp.gnu.org/gnu/gcc/" target="_blank" rel="noopener">http://ftp.gnu.org/gnu/gcc/</a><br>我选择的是官方源，执行以下命令下载：<br>~$:wget <a href="http://ftp.gnu.org/gnu/gcc/gcc-7.3.0.tar.gz" target="_blank" rel="noopener">http://ftp.gnu.org/gnu/gcc/gcc-7.3.0.tar.gz</a></p><h2 id="解压">解压</h2><p>~$:tar xvf gcc-7.3.0.tar.gz<br>~$:sudo cp -r gcc-7.3.0 /usr/local/src/<br>~$:cd /usr/local/src/gcc-7.3.0/</p><h2 id="创建安装目录">创建安装目录</h2><p>~$:sudo mkdir /usr/local/gcc-7.3.0<br>~$:sudo mkdir /usr/local/src/gcc-7.3.0/build<br>~$:cd /usr/local/src/gcc-7.3.0/build</p><h2 id="配置">配置</h2><p>~$:sudo …/configure --prefix=/usr/local/gcc-7.3.0/ --enable-threads=posix --disable-multilib --enable-languages=c,c++<br>~$:sudo make -j8<br>~$:sudo make install</p><h2 id="修改gcc版本">修改gcc版本</h2><p>~$:sudo update-alternativess --install /usr/bin/cc cc /usr/local/gcc-4.6.0/bin/gcc-4.6 30<br>~$:sudo update-alternativess --install /usr/bin/c++ c++ /usr/local/gcc-4.6.0/bin/g+±4.6 30</p><p>~$:sudo update-alternativess --config cc</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;下载相应版本的安装包&quot;&gt;下载相应版本的安装包&lt;/h2&gt;
&lt;p&gt;国科大源：&lt;a href=&quot;https://mirrors.ustc.edu.cn/gnu/gcc/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://mirrors.us
      
    
    </summary>
    
      <category term="linux" scheme="http://mxxhcm.github.io/categories/linux/"/>
    
    
      <category term="gcc" scheme="http://mxxhcm.github.io/tags/gcc/"/>
    
      <category term="linux" scheme="http://mxxhcm.github.io/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>linux 扩展boot分区</title>
    <link href="http://mxxhcm.github.io/2019/05/04/linux-boot%E5%88%86%E5%8C%BA%E6%89%A9%E5%B1%95/"/>
    <id>http://mxxhcm.github.io/2019/05/04/linux-boot分区扩展/</id>
    <published>2019-05-04T04:06:18.000Z</published>
    <updated>2019-05-06T16:22:27.708Z</updated>
    
    <content type="html"><![CDATA[<p>扩展linux的/boot分区</p><h2 id="使用gpared或者fdisk创建一个新的partition">使用gpared或者fdisk创建一个新的partition</h2><h2 id="find-the-uuid-of-the-new-partition">find the uuid of the new partition</h2><p>使用命令<br>~$:ls -l /dev/disk/by-uuid/<br>获得分区的uuid<br>lrwxrwxrwx 1 root root 10 11月 24 14:34 19d6c114-8859-4209-aef9-60ee3cc108c1 -&gt; …/…/sda9<br>lrwxrwxrwx 1 root root 10 11月 24 14:34 1C48-1828 -&gt; …/…/sda2<br>lrwxrwxrwx 1 root root 10 11月 24 14:34 2840620D4061E254 -&gt; …/…/sda4<br>lrwxrwxrwx 1 root root 11 11月 24 14:34 66ab484d-0bbc-41cb-b2ca-8f436a330e2b -&gt; …/…/sda10<br>lrwxrwxrwx 1 root root 10 11月 24 14:34 71640978-4b7b-49aa-9a3e-ef22c994a183 -&gt; …/…/sda6<br>lrwxrwxrwx 1 root root 10 11月 24 14:34 8856E16256E1518C -&gt; …/…/sdb1<br>lrwxrwxrwx 1 root root 10 11月 24 14:34 99f1b75b-eb7b-41bb-9aa8-3c5ab2446f01 -&gt; …/…/sda7<br>lrwxrwxrwx 1 root root 10 11月 24 14:34 B4CEF361CEF31A76 -&gt; …/…/sdb2<br>lrwxrwxrwx 1 root root 10 11月 24 14:34 B836469636465592 -&gt; …/…/sda1<br>lrwxrwxrwx 1 root root 10 11月 24 14:34 C14D581BDA18EBFA -&gt; …/…/sda5<br>lrwxrwxrwx 1 root root 10 11月 24 14:34 e9b32a21-5e8a-4c53-9982-a31cd67c464e -&gt; …/…/sda8</p><h2 id="更新配置文件-etc-fstab">更新配置文件/etc/fstab</h2><p>通过改变uuid将/boot目录挂在到新的挂载点上<br>from<br>UUID=99f1b75b-eb7b-41bb-9aa8-3c5ab2446f01 /boot           ext4    defaults        0       2<br>to<br>UUID=66ab484d-0bbc-41cb-b2ca-8f436a330e2b /boot           ext4    defaults        0       2<br>here we can use the device name /dev/sda10 but it may change if we add some other devices, uuid is unique so that it won’t change.</p><h2 id="重启">重启</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;扩展linux的/boot分区&lt;/p&gt;
&lt;h2 id=&quot;使用gpared或者fdisk创建一个新的partition&quot;&gt;使用gpared或者fdisk创建一个新的partition&lt;/h2&gt;
&lt;h2 id=&quot;find-the-uuid-of-the-new-partitio
      
    
    </summary>
    
      <category term="linux" scheme="http://mxxhcm.github.io/categories/linux/"/>
    
    
      <category term="linux" scheme="http://mxxhcm.github.io/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>pytorch tensorflow常用函数对应</title>
    <link href="http://mxxhcm.github.io/2019/05/04/pytorch-tensorflow%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0%E5%AF%B9%E5%BA%94/"/>
    <id>http://mxxhcm.github.io/2019/05/04/pytorch-tensorflow常用函数对应/</id>
    <published>2019-05-04T02:39:44.000Z</published>
    <updated>2019-05-08T14:17:58.044Z</updated>
    
    <content type="html"><![CDATA[<h2 id="对应">对应</h2><table><thead><tr><th style="text-align:center">tensorflow</th><th style="text-align:center">pytorch</th></tr></thead><tbody><tr><td style="text-align:center">tensor.shape</td><td style="text-align:center">tensor.size()</td></tr><tr><td style="text-align:center"><a href="https://github.com/mxxhcm/myown_code/blob/master/tf/some_ops/tf_maximum.py" target="_blank" rel="noopener">tf.maximum</a></td><td style="text-align:center"><a href="https://github.com/mxxhcm/myown_code/blob/master/pytorch/pytorch_test/torch_max.py" target="_blank" rel="noopener">torch.max</a></td></tr><tr><td style="text-align:center"><a href="https://github.com/mxxhcm/myown_code/blob/master/tf/some_ops/tf_multinominal.py" target="_blank" rel="noopener">tf.multinomial</a></td><td style="text-align:center"><a href="https://github.com/mxxhcm/myown_code/blob/master/pytorch/pytorch_test/torch_distribution.py" target="_blank" rel="noopener">torch.distributions.Categorical</a></td></tr></tbody></table><h2 id="参考文献">参考文献</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;对应&quot;&gt;对应&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&quot;text-align:center&quot;&gt;tensorflow&lt;/th&gt;
&lt;th style=&quot;text-align:center&quot;&gt;pytorch&lt;/th&gt;
&lt;/tr&gt;
&lt;/th
      
    
    </summary>
    
      <category term="pytorch" scheme="http://mxxhcm.github.io/categories/pytorch/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/tags/tensorflow/"/>
    
      <category term="pytorch" scheme="http://mxxhcm.github.io/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>reinforcement learning an introduction 第5章笔记</title>
    <link href="http://mxxhcm.github.io/2019/04/29/reinforcement-learning-an-introduction-%E7%AC%AC5%E7%AB%A0%E7%AC%94%E8%AE%B0/"/>
    <id>http://mxxhcm.github.io/2019/04/29/reinforcement-learning-an-introduction-第5章笔记/</id>
    <published>2019-04-29T07:53:02.000Z</published>
    <updated>2019-08-04T06:58:54.818Z</updated>
    
    <content type="html"><![CDATA[<h2 id="mc-methods">MC Methods</h2><p>这章主要介绍了MC算法，MC算法通过采样，估计state-value function或者action value function。为了找到最好的policy，需要让policy不断的进行探索，但是我们还需要找到最好的action，减少exploration。这两个要求是矛盾的，这一章主要介绍了两种方法来尽量满足这两个要求。一种是on-policy的方法，使用soft policy，即有一定概率随机选择action，其余情况下选择最好的action。这种情况下学习到的policy不是greedy的，同时也能进行一定的exploration。一种是off-policy的方法，这种方法使用两个不同的policy，一个用来采样的behaviour policy，一个用来评估的target policy。target policy是一个deterministic policy，而behaviour policy用来exploration。<br>MC方法通过采样估计值函数有三个优势，从真实experience中学习，从仿真环境中学习，以及每个state value的计算独立于其他state。<br>MC和DP不一样的是，它不需要环境的信息，只需要experience即可，不管是从真实交互还是从仿真环境中得到的state,action,reward序列都行。从真实交互中学习不需要环境的信息，从仿真环境中学习需要一个model，但是这个model只用于生成sample transition，并不需要像DP那样需要所有transition的完整概率分布。在很多情况下，生成experience sample要比显示的得到概率分布容易很多。<br>MC基于average sample returns估计值函数。为了保证returns是可用的，这里定义蒙特卡洛算法是episodic的，即所有的experience都有一个terminal state。只有在一个episode结束的时候，value estimate和policy才会改变。蒙塔卡洛算法可以在episode和episode实现增量式，不能在step和step之间实现增量式。(Monte Carlo methods can thus be incremental in an episode-by-episode sense, but not in a step-by-step online sense.)<br>在一个state采取action得到的return取决于同一个episode后续状态的action，因为所有的action都是在不断学习中采取，从早期state的角度来看，这个问题是non-stationary的。为了解决non-stationary问题，采用GPI中的idea。DP从已知的MDP中计算value function，蒙特卡洛使用MDP的sample returns学习value function。然后value function和对应的policy交互获得好的value和policy。<br>这一章就是把DP中的各种想法推广到了MC上，解决prediction和control问题，DP使用的是整个MDP，而MC使用的是MDP的采样。</p><h2 id="mc-prediction">MC Prediction</h2><p>Prediction problem就是估计value function，value function又分为state value function和action value function。这里会分别给出state value function和action value function的估计方法。</p><h3 id="state-value-function">State value function</h3><p>从state value function说起。最简单的想法就是使用experience估计value function，通过对每个state experience中return做个average。</p><h4 id="first-visti-mc-method">First visti MC method</h4><p>这里主要介绍两个算法，一个叫做first visit MC method，另一个是every visit MC method。比如要估计策略$\pi$下的$v(s)$，使用策略$\pi$采样一系列经过$s$的episodes，$s$在每一个episode中出现一次叫做一个visit，一个$s$可能在一个episode中出现多次。First visit就是只取第一次visit估计$v(s)$，every visit就是每一次visit都用。<br>下面给出first visit的算法：<br>算法1 <strong>First visit MC preidction</strong><br><strong>输入</strong> 被评估的policy $\pi$<br><strong>初始化</strong>:<br>$\qquad V(s)\in R,\forall s \in S$<br>$\qquad Returns(s) \leftarrow empty list,\forall s \in S$<br><strong>Loop</strong> for each episeode:<br>$\qquad$生成一个episode<br>$\qquad G\leftarrow 0$<br>$\qquad$<strong>Loop</strong> for each step, $t= T-1,T-2, \cdots, 1$<br>$\qquad\qquad G\leftarrow G + \gamma R_t$<br>$\qquad\qquad$ IF $S_t$没有在$S_0, \cdots , S_{t-1}$中出现过<br>$\qquad\qquad\qquad Returns(S_t).apppend(G)$<br>$\qquad\qquad\qquad V(S_t)\leftarrow average(Returns(S_t))$<br>$\qquad\qquad END IF$<br>Every visit算法的话，不用判断$S_t$是否出现。当$s$的visit趋于无穷的时候，first vist和every visit算法$v_{\pi}(s)$都能收敛。First visit中，每一个return都是$v_{\pi}(s)$的一个独立同分布估计。根据大数定律，估计平均值（$average(Returns(S_0),\cdots, average(Returns(S_t)$）的序列收敛于它的期望。每一个average都是它自己的一个无偏估计，标准差是$\frac{1}{\sqrt{n}}$。every visit的收敛更难直观的去理解，但是它二次收敛于$v_{\pi}(s)$。<br>补充一点：<br>大数定律：无论抽象分布如何，均值服从正态分布。<br>中心极限定理：样本大了，抽样分布近似于整体分布。</p><p>这里再次对比一下DP和MC，在扑克牌游戏中，我们知道环境的所有信息，但是我们不知道摸到下一张牌的概率，比如我们手里有很多牌了，我们知道下一张摸到什么牌会赢，但是我们不知道这件事发生的概率。使用MC可以采样获得，所以说，即使有时候知道环境信息，MC方法可能也比DP方法好。</p><h4 id="mc-backup-diagram">MC backup diagram</h4><p>能不能推广DP中的backup图到MC中？什么是backup图？backup图顶部是一个root节点，表示要被更新的节点，下面是所有的transitions，leaves是对于更新有用的reward或者estimated values。<br>MC中的backup图，root节点是一个state，下面是一个episode中的所有transtion轨迹，以terminal state为终止节点。DP backup diagram展示了所有可能的transitions，而MC backup diagram只展示了采样的那个episode；DP backup diagram只包含一步的transitions，而MC backup diagram包含一个episode的所有序列。<br><img src="/2019/04/29/reinforcement-learning-an-introduction-第5章笔记/" alt="mc backup"><br><img src="/2019/04/29/reinforcement-learning-an-introduction-第5章笔记/" alt="dp backup page 59"></p><h4 id="mc的特点">MC的特点</h4><p>DP中每个state的估计都依赖于它的后继state，而MC中每个state value的计算都不依赖于任何其他state value（MC算法不进行bootstrap），所以可以单独估计某一个state或者states的一个子集。而且估计单个state的计算复杂度和states的数量无关，我们可以只取感兴趣的states子集进行评估，这是MC的第三个优势。前两个优势是从actural experience中学习和从simulated的experience中学习。</p><h3 id="action-value-function">Action value function</h3><p>如果没有model的话，需要估计state-action value而不是state value。有model的话，只有state value就可以确定policy，选择使reward和next_state value加起来最大的action即可。没有model的话，只有state value是不够的，因为不知道下一个state是什么。而使用action value，就可以确定policy，选择$q$值最大的那个action value，取相应的action即可。<br>所以这一节的目标是学习action value function。有一个问题是许多state-action可能一次也没有被访问过，如果$\pi$是deterministic的，每一个state只输出一个action，其他action的MC估计没有returns进行平均，就无法进行更新。所以，我们需要估计每一个state对应的所有action，这是exploration问题。<br>对于action value的policy evaluation，必须保证continual exploration。一种实现方式是指定episode开始的state-action pair，每一个pair都有大于$0$的概率被选中,这就保证了每一个action-pair在无限个episode中会被访问无限次，这叫做exploring starts。这种假设有时候有用，但是在某些时候，我们无法控制环境产生的experience，可行的方法是使用stochastic policy。</p><h2 id="mc-control">MC Control</h2><p>MC control使用的还是GPI的想法，估计当前policy的action value，基于action value改进policy，不断迭代。考虑经典的policy iteration，执行一次完全的iterative policy evaluation，再执行一次完全的policy improvement，不断迭代。对于policy evaluation，每次evaluation都使用多个episodes的experience，每次action value都会离true value function更近。假设我们有无限个exploring starts生成的episodes，满足这些条件时，对于任意$\pi_k$都会精确计算出$q_{\pi_k}$。进行policy improvement时，只要对于当前的action value function进行贪心即可，即：<br>$$\pi(s) = arg\ max_a q(s,a)\tag{1}$$<br>第$4$章给出了证明，即policy improvement theorem。在每一轮improvement中，对所有的$s\in S$，执行：<br>\begin{align*}<br>q_{\pi_k}(s,\pi_{k+1}(s)) &amp;=q_{\pi_k}(s, argmax_a q_{\pi_k}(s,a))\\<br>&amp;=max_a q_{\pi_k}(s,a)\\<br>&amp;\ge q_{\pi_k}(s, \pi_k(s))\\<br>&amp;\ge v_{\pi_k}(s)\\<br>\end{align*}<br>MC算法的收敛保证需要满足两个假设，一个是exploring start，一个是policy evaluation需要无限个episode的experience。但是现实中，这两个条件是不可能满足的，我们需要替换掉这些条件近似接近最优解。</p><h3 id="mc-control-without-infinte-episodes">MC Control without infinte episodes</h3><p>无限个episodes的条件比较容易去掉，在DP方法中也有这些问题。在DP和MC任务中，都有两种方法去掉无限episode的限制，第一种方法是像iterative policy evaluation一样，规定一个误差的bound，在每一次evaluation迭代，逼近$q_{\pi_k}$，通过足够多的迭代确保误差小于bound，可能需要很多个episode才能达到这个bound。第二种是进行不完全的policy evaluation，和DP一样，使用小粒度的policy evaluation，可以只执行iterative policy evaluation的一次迭代，也可以执行一次单个state的improvement和evaluation。对于MC方法来说，很自然的就想到基于一个episode进行evaluation和improvement。每经历一个episode，执行该episode内相应state的evaluation和improvement。也就是说一个是规定每次迭代的bound，一个是规定每次迭代的次数。</p><h4 id="伪代码">伪代码</h4><p>算法2 <strong>First visit MCES</strong><br><strong>初始化</strong><br>$\qquad$任意初始化$\pi(s)\in A(s), \forall s\in S$<br>$\qquad$任意初始化$Q(s, a)\in R, \forall s\in S, \forall a \in A(s)$<br>$\qquad$Returns(s,a)$\leftarrow$ empty list, $\forall s\in S, \forall a \in A(s)$<br><strong>Loop forever(for each episode)</strong><br>$\qquad$随机选择满足$S_0\in S, A_0\in A(S_0)$的state-action$(S_0,A_0)$，满足概率大于$0$<br>$\qquad$从$S_0,A_0$生成策略$\pi$下的一个episode，$S_0,A_0,R_1,\cdots,S_{T-1},A_{T-1},R_T$<br>$\qquad G\leftarrow 0$<br>$\qquad$<strong>Loop for each step of episode</strong>,$t=T-1,T-2,\cdots,0$<br>$\qquad\qquad G\leftarrow \gamma G+R_{t+1}$<br>$\qquad\qquad$如果$S_t,A_t$没有在$S_0,A_0,\cdots, S_{t-1},A_{t-1}$中出现过<br>$\qquad\qquad\qquad$Returns($S_t,A_t$).append(G)<br>$\qquad\qquad\qquad Q(S_t,A_t) \leftarrow average(Returns(S_t, A_t)$<br>$\qquad\qquad\qquad \pi(S_t) \leftarrow argmax_a Q(S_t,a)$<br>这个算法一定会收敛到全局最优解，因为如果收敛到一个suboptimal policy，value function在迭代过程中会收敛到该policy的true value function，接下来的policy improvement会改进该suboptimal policy。</p><h2 id="on-policy-mc-control-without-es">On-policy MC Control without ES</h2><p>上节主要是去掉了无穷个episode的限制，这节需要去掉ES的限制，解决方法是需要agents一直能够去选择所有的actions。目前有两类方法实现，一种是on-policy，一种是off-policy。</p><h3 id="on-policy和off-policy">on-policy和off-policy</h3><p>On-policy算法中，用于evaluation或者improvement的policy和用于决策的policy是相同的，而off-policy算法中，evaluation和improvement的policy和决策的policy是不同的。</p><h3 id="varepsilon-soft和-varepsilon-greedy">$\varepsilon$ soft和$\varepsilon$ greedy</h3><p>在on-policy算法中，policy一般是soft的，整个policy整体上向一个deterministic policy偏移。<br>在$\varepsilon$ soft算法中，只要满足$\pi(a|s)\gt 0,\forall s\in S, a\in A$即可。<br>在$\varepsilon$ greedy算法中，用$\frac{\varepsilon}{|A(s)|}$的概率选择non-greedy的action，使用$1 -\varepsilon + \frac{\varepsilon}{|A(s)|}$的概率选择greedy的action。<br>$\varepsilon$ greedy是$\varepsilon$ soft算法中的一类，可以看成一种特殊的$\varepsilon$ soft算法。<br>本节介绍的on policy方法使用$\varepsilon$ greedy算法。</p><h3 id="on-policy-first-visit-mc">On-policy first visit MC</h3><p>本节介绍的on policy MC算法整体的思路还是GPI，首先使用first visit MC估计当前policy的action value function。去掉exploring starting条件之后，为了保证exploration，不能直接对所有的action value进行贪心，使用$\varepsilon$ greedy算法保持exploration。<br>算法3 <strong>On policy first visit MC Control</strong><br>$\varepsilon \gt 0$<br><strong>初始化</strong><br>$\qquad$用任意$\varepsilon$ soft算法初始化$\pi$<br>$\qquad$任意初始化$Q(s, a)\in R, \forall s\in S, \forall a \in A(s)$<br>$\qquad$Returns(s,a) $\leftarrow$ empty list, $\forall s\in S, \forall a \in A(s)$<br><strong>Loop forever(for each episode)</strong><br>$\qquad$根据policy $\pi$生成一个episode，$S_0,A_0,R_1,\cdots,S_{T-1},A_{T-1},R_T$<br>$\qquad G\leftarrow 0$<br>$\qquad$<strong>Loop for each step of episode</strong>,$t=T-1,T-2,\cdots,0$<br>$\qquad\qquad G\leftarrow \gamma G+R_{t+1}$<br>$\qquad\qquad$如果$S_t,A_t$没有在$S_0,A_0,\cdots, S_{t-1},A_{t-1}$中出现过<br>$\qquad\qquad\qquad$Returns($S_t,A_t$).append(G)<br>$\qquad\qquad\qquad Q(S_t,A_t) \leftarrow average(Returns(S_t, A_t)$<br>$\qquad\qquad\qquad A^{*}\leftarrow argmax_a Q(S_t,a)$<br>$\qquad\qquad\qquad$<strong>For all</strong> $a \in A(S_t) : $<br>$\qquad\qquad\qquad\qquad\pi(a|S_t)\leftarrow \begin{cases}1-\varepsilon+\frac{\varepsilon}{|A(S_t)|}\qquad if\ a = A^{*}\\ \frac{\varepsilon}{|A(S_t)|}\qquad a\neq A^{*}\end{cases}$</p><p>对于任意的$\varepsilon$ soft policy $\pi$，相对于$q_{\pi}$的$\varepsilon$ greedy算法至少和$\pi$一样好。用$\pi’$表示$\varepsilon$ greedy policy，对于$\forall s\in S$，都满足policy improvement theorem的条件：<br>\begin{align*}<br>q_{\pi}(s,\pi’(s))&amp;=\sum_a\pi’(a|s)q_{\pi}(s,a)\\<br>&amp;=\frac{\varepsilon}{|A(s)|} \sum_aq_{\pi}(s,a) + (1- \varepsilon) max_a q_{\pi}(s,a) \tag{2}\\<br>&amp;\ge \frac{\varepsilon}{|A(s)|} \sum_aq_{\pi}(s,a) + (1-\varepsilon) \sum_a\frac{\pi(a|s) - \frac{\varepsilon}{|A(s)|}}{1-\varepsilon}q_{\pi}(s,a) \tag{3}\\<br>&amp;=\frac{\varepsilon}{|A(s)|} \sum_aq_{\pi}(s,a) - \frac{\varepsilon}{|A(s)|} \sum_aq_{\pi}(s,a) + \sum_a \pi(a|s)\sum_aq_{\pi}(s,a)\\<br>&amp;=v(s)<br>\end{align*}<br>式子2到式子3是怎么变换的，我有点没看明白！！！（不懂）。后来终于想明白了，式子3的第二项分子服从的是$\pi(a|s)$，而式子2的第二项这个$a$是新的$\pi’(a|s)$。<br>接下来证明，当$\pi$和$\pi’$都是optimal $\varepsilon$ policy的时候，可以取到等号。这个我看这没什么意思，就不证明了。。在p102。</p><h2 id="off-policy-prediction-via-importance-sampling">Off-policy Prediction via Importance Sampling</h2><p>所有的control方法都要面临一个问题：一方面需要选择optimal的action估计action value，另一方面需要exploration，不能一直选择optimal action，那么该如何控制这两个问题之间的比重。on-policy方法采样的方法是学习一个接近但不是optimal的policy保持exploriation。off-policy的方法使用两个policy，一个用于采样的behavior policy，一个用于evaluation的target policy。用于学习target policy的data不是target policy自己产生的，所以叫做off-policy learning。</p><h3 id="on-policy-vs-off-policy">on-policy vs off-policy</h3><p>on policy更简单，off policy使用两个不同的policy，所以variance更大，收敛的更慢，但是off-policy效果更好，更通用。On-policy可以看成off-policy的特例，target policy和behaviour policy是相同的。Off-policy可以使用非学习出来的data，比如人工生成的data。</p><h3 id="off-policy-prediction-problem">off-policy prediction problem</h3><p>对于prediction problem，target policy和behaviour policy都是固定的。$\pi$是target policy，$b$是behaviour policy，我们要使用$b$生成的episode去估计$q_{\pi}$或者$v_{\pi}$。为了使用$b$生成的episodes估计$\pi$，需要满足一个假设，policy $\pi$中采取的action在$b$中也要能有概率被采取，即$\pi(a|s)\gt 0$表明$b(a|s) \gt 0$，这是coverage假设。<br>在control问题中，target policy通常是相对于当前action value的deterministic greedy policy，最后target policy是一个deterministic optimal policy而behaviour policy通常是$\varepsilon$ greedy的探索策略。</p><h3 id="importance-sampling和importance-sampling-ratio">importance sampling和importance sampling ratio</h3><p>很多off policy方法使用importance sampling，利用一个distribution的samples估计另一个distribution的value function。Importance sampling通过计算trajectoried在target和behaviour policy中出现的概率比值对returns进行加权，这个相对概率称为importance sampling ratio。给定以$S_t$为初始状态的sate-action trajectory，它在任何一个policy $\pi$中发生的概率如下：<br>\begin{align*}<br>&amp;Pr\{A_t, S_{t+1},A_{t+1},\cdots,S_T|A_{t:T-1}\sim \pi,S_t\}\\<br>=&amp;\pi(A_t|S_t)p(S_{t+1}|S_t,A_t)\pi(A_{t+1}|S_{t+1})\cdots p(S_T|S_{T-1},A_{T-1})\\<br>=&amp;\prod_{k=t}^{T-1}\pi(A_k|S_k)p(S_{k+1}|S_k,A_k)<br>\end{align*}<br>其中$p$是状态转换概率，imporrance sampling计算如下：<br>$$\rho_{t:T-1}=\frac{\prod_{k=t}^{T-1} \pi(A_k|S_k)p(S_{k+1}|S_k,A_k)}{\prod_{k=t}^{T-1} b(A_k|S_k)p(S_{k+1}|S_k,A_k)}=\prod_{k=t}^{T-1}\frac{\pi(A_k|S_k)}{b(A_k|S_k}\tag{2}$$<br>因为p跟policy无关，所以可以直接消去。importance sampling ratio只和policies以及sequences有关。<br>根据behaviour policy的returns $G_t$，我们可以得到一个Expectation，即$\mathbb{E}[G_t|S_t=s]=v_b(s)$，显然，这是b的value function而不是$\pi$的value function，这个时候就用到了importance sampling，ratio $\rho_{t:T-1}$对b的returns进行转换，得到了另一个期望：<br>$$\mathbb{E}[\rho_{t:T-1}G_t|S_t=s]=v_{\pi}(s)\tag{3}$$</p><h3 id="符号定义">符号定义</h3><p>假设我们想要从policy b 中的一些episodes中估计$v_{\pi}(s)$，</p><ul><li>用$t$表示episode中的每一步，有些不同的是，$t$在不同episode之间是连续的，比如第$1$个episode有$100$个timesteps，第$2$个episode的timsteps从$101$开始。</li><li>用$J(s)$表示state $s$在不同episodes中第一次出现的$t$。</li><li>用$T(t)$表示从$t$所在那个episode的terminal timestep。</li><li>用$\left\{G_t\right\}_{t\in J(s)}$表示所有state $s$的return list。</li><li>用$\left\{\rho_{t:T(t)-1}\right\}_{t\in J(s)}$表示相应的importance ratio。</li></ul><h3 id="importance-sampling">importance sampling</h3><p>有两种importance sampling方法估计$v_{\pi}(s)$，一种是oridinary importance sampling，一种是weighted importance sampling。</p><h4 id="oridinary-importance-sampling">oridinary importance sampling</h4><p>直接对多个结果进行平均<br>$$V(s) = \frac{\sum_{t\in J(s)}\rho_{t:T(t)-1} G_t}{|J(s)|}\tag{4}$$</p><h4 id="weighted-importance-sampling">weighted importance sampling</h4><p>对多个结果进行加权平均<br>$$V(s) = \frac{\sum_{t\in J(s)}\rho_{t:T(t)-1} G_t}{\sum_{t\in J(s)}\rho_{t:T(t)-1}}\tag{5}$$</p><h4 id="异同点">异同点</h4><p>为了比较这两种importance sampling的异同，考虑state s只有一个returns的first vist MC方法，在加权平均中，ratio会约分约掉，这个returns的expectation是$v_b(s)$而不是$v_{\pi}(s)$，是一个有偏估计；而普通平均，returns的expectation还是$v_{\pi}(s)$，是一个无偏估计，但是可能会很极端，比如ratio是$10$，就说明$v_{\pi}(s)$是$v_b(s)$的$10$倍，可能与实际相差很大。<br>在fisrt visit算法中，就偏差和方差来说。普通平均的偏差是无偏的，而加权平均的偏差是有偏的（逐渐趋向$0$）。普通平均的方差是unbounded，因为ratio可以是unbounded，而加权平均对于每一个returns来说，权重最大是$1$。事实上，假定returns是bounded，即使ratios的方差是infinite，加权平均的方差也会趋于$0$。实践中，加权平均有更小的方差，通常更多的被采用。<br>在every visit算法中，普通平均和加权平均都是有偏的，随着样本的增加，偏差也趋向于$0$。在实践中，因为every visit不需要记录哪个状态是否被记录过，所以要比first visit常用。</p><h3 id="无穷大方差">无穷大方差</h3><p><img src="/2019/04/29/reinforcement-learning-an-introduction-第5章笔记/figure_5_4.png" alt="example of oridinary importance ratio"><br>考虑一个例子。只有一个non-terminal state s，两个ation，left和right，right action是deterministic transition到termination，left action有$0.9$的概率回到s，有$0.1$的概率到termination。left action回到termination会产生$+1$的reward，其他操作的reward是$0$。所有target policy策略下的episodes都会经过一些次回到state s然后到达terminal state，总的returns是$1(\gamma = 1)$。使用behaviour policy等概率选择left和right action。<br>这个例子中returns的真实期望是$1$。first visit中weighted importance sampling中return的期望是$1$，因为behaviour policy中选择right的action 在target policy中概率为$0$，不满足之前假设的条件，所以没有影响。而oridinary importance sampling的returns期望也是$1$，但是可能经过了几百万个episodes之后，也不一定收敛到$1$。<br>接下来我们证明oridinary importance sampling中returns的variance是infinite。<br>$$Var(X) = \mathbb{E}\left[(X-\bar{X})^2\right] = \mathbb{E}\left[X^2-2\bar{X}X +\bar{x}^2\right]= \mathbb{E}\left[X^2\right]-\bar{X}^2 \tag{6}$$<br>如果mean是finite，只有当random variable的平方的Expectation为infinte时variance是infinte。所以，我们需要证明：<br>$$\mathbb{E}_b\left[\left(\prod_{t=0}^{T-1}\frac{\pi(A_t|S_t)}{b(A_t|S_t)}G_0\right)^2\right] \tag{7}$$<br>是infinte的。<br>这里我们按照一个episode一个episode的进行计算。但是需要注意的是，behaviour policy可以选择right action，而target policy只有left action，当behaviour policy选择right的话，ratio是$0$。我们只需要考虑那些一直选择left action回到state s，然后通过left action到达terminal state的episodes。按照下式计算期望，注意这个和上面用oridinary important ratio估计$v_{\pi}(s)$可不一样，上面是用采样估计$v_{\pi}(s)$，这个是计算真实的$v_{\pi}(s)$的期望，不对，是它的平方的期望。<br>\begin{align*}<br>\mathbb{E}_b\left[\left( \prod_{t=0}^{T-1}\frac{\pi(A_t|S_t)}{b(A_t|S_t)}G_0\right)^2\right] = &amp; \frac{1}{2}\cdot 0.1 \left(\frac{1}{0.5}\right)^2\tag{长度为1的episode}\\<br>&amp;+\frac{1}{2}\cdot 0.9\cdot\frac{1}{2}\cdot 0.1 \left(\frac{1}{0.5}\frac{1}{0.5}\right)^2\tag{长度为2的episode}\\<br>&amp;+\frac{1}{2}\cdot 0.9\cdot \frac{1}{2} \cdot 0.9 \frac{1}{2}\cdot 0.1 \left(\frac{1}{0.5}\frac{1}{0.5}\frac{1}{0.5}\right)^2\tag{长度为3的episode}\\<br>&amp;+ \cdots\\<br>=&amp;0.1 \sum_{k=0}^{\infty}0.9^k\cdot 2^k \cdot 2\\<br>=&amp;0.2 \sum_{k=0}^{\infty}1.8^k\\<br>=&amp;\infty \tag{8}\<br>\end{align*}</p><h3 id="incremental-implementation">Incremental Implementation</h3><p>Monte Carlo prediction可以增量式实现，用episode-by-episode bias。<br>在on-policy算法中，$V_t$的估计通过直接对多个episode的$G_t$进行平均得到。<br>$$V_n(s) = \frac{G_1 + G_2 + \cdots + G_{n-1}}{n - 1} \tag{9}$$<br>其中$V_n(s)$表示在第$n$个epsisode估计的state $s$的value function，$n-1$表示采样得到的总共$n-$个episode，$G_1$表示每个episode中第一次遇到$s$时的Return。<br>在第$n+1$个episodes估计$V(s)$时：<br>\begin{align*}<br>V_{n+1}(s) &amp;= \frac{G_1 + G_2 + \cdots + G_n}{n}\\<br>nV_{n+1}(s)&amp;= G_1 + G_2 + \cdots + G_{n - 1} + G_n\tag{上式两边同时乘上n}\\<br>(n-1)V_n(s)&amp;= G_1 + G_2 + \cdots + G_{n - 1}\tag{用n-1代替n}\\<br>nV_{n+1}(s)&amp;= G_1 + G_2 + \cdots + G_{n - 1} + G_n\tag{分解V_{n+1}(s)}\\<br>&amp;= (G_1 + G_2 + \cdots + G_{n - 1}) + G_n\\<br>&amp;= (n-1)V_n(s) + G_n\\<br>\frac{nV_{n+1}(s)}{n}&amp;= \frac{(n-1)V_n(s) + G_n}{n}\tag{上式两边同时除以n}\\<br>V_{n+1}(s)&amp;= \frac{(n-1)V_n(s) + G_n}{n}\\<br>&amp; = V_n(s) +\frac{G_n-V_n(s)}{n} \tag{10}<br>\end{align*}<br>这个更新规则的一般形式如下：<br>$$NewEstimate \leftarrow OldEstimate + StepSize \left[Target - OldEstimate\right] \tag{11}$$<br>表达式$\left[Target - OldEstimate\right]$是一个estimate error，通过向&quot;Target&quot;走一步减小error。这个&quot;Target&quot;给定了更新的方向，当然也有可能是noisy，在式子$10$中，target是第$n$个episode中state s的return。式子$10$的更新规则中StepSize$\frac{1}{n}$是在变的，一般我们叫它步长或者学习率，用$\alpha$表示。<br>在off-policy算法中，odrinary importance sampling和weighted importance sampling要分开。因为odirinary importance sampling只是对ratio缩放后的不同returns做了平均，还可以使用上面的公式。而对于weighted imporatance sampling，假设一系列episodes的returns是$G_1,G_2,\cdots, G_{n-1}$，对应的权重为$W_i$（比如$W_i=\rho_{t_i:T(t_i)-1}$），有：<br>$$V_n = \frac{\sum_{k=1}^{n-1}W_kG_k}{\sum_{k=1}^{n-1}W_k} \tag{11}$$<br>用$C_n$表示前$n$个episode returns的权重和，即$C_n=\sum_{k=1}^nW_k$，$V_n$的更新规则如下：<br>\begin{align*}<br>V_{n+1}&amp;=\frac{\sum_{k=1}^{n}W_kG_k}{\sum_{k=1}^{n}W_k}\\<br>&amp;=\frac{\sum_{k=1}^{n-1}W_kG_k + W_nG_n}{\sum_{k=1}^{n}W_k}\\<br>&amp;=\frac{1}{\sum_{k=1}^{n}W_k} \cdot \left(\sum_{k=1}^{n-1}W_kG_k + W_nG_n\right)\\<br>&amp;=\frac{1}{\sum_{k=1}^{n}W_k} \cdot \left(\frac{\sum_{k=1}^{n-1}W_kG_k}{\sum_{k=1}^{n-1}W_k}(\sum_{k=1}^{n-1}W_k) + W_nG_n\right)\\<br>&amp;=\frac{1}{\sum_{k=1}^{n}W_k} \cdot \left(V_n\cdot(\sum_{k=1}^{n-1}W_k) + W_nG_n\right)\\<br>&amp;=\frac{1}{\sum_{k=1}^{n}W_k} \cdot \left(V_n\cdot(\sum_{k=1}^{n-1}W_k + W_n - W_n) + W_nG_n\right)\\<br>&amp;=\frac{1}{\sum_{k=1}^{n}W_k} \cdot \left(V_n\cdot(\sum_{k=1}^{n}W_k - W_n) + W_nG_n\right)\\<br>&amp;=\frac{1}{\sum_{k=1}^{n}W_k} \cdot \left(V_n\cdot(\sum_{k=1}^{n}W_k) + W_nG_n - W_nV_n\right)\\<br>&amp;=\frac{V_n\cdot(\sum_{k=1}^{n}W_k)}{\sum_{k=1}^{n}W_k} + \frac{W_nG_n-W_nV_n}{\sum_{k=1}^{n}W_k}\\<br>&amp;=V_n + \frac{W_n}{C_n}(G_n-V_n)\\<br>\end{align*}<br>其中$C_0=0, C_{n+1} = C_n + W_{n+1}$，事实上，在$W_k=1$的情况下，即$\pi=b$时，上面的公式就变成了on-policy的公式。接下来给出一个episode-by-episode的MC  policy evaluation incremental algorithm，使用的是weighted importance sampling。</p><h3 id="off-policy-mc-prediction-算法">Off-policy MC Prediction 算法</h3><p>算法 4 Off-policy MC prediction(policy evaluation)<br>输入: 一个任意的target policy $\pi$<br>初始化，$Q(s,a)\in \mathbb{R}, C(s,a) = 0, \forall s\in S, a\in A(s)$<br><strong>Loop</strong> forever (for each episode)<br>$\qquad$$b\leftarrow$ 任意覆盖target policy $\pi$的behaviour policy<br>$\qquad$用behaviour policy $b$生成一个episode，$S_0,A_0,R_1,\cdots, S_{T-1},A_{T-1},R_T$<br>$\qquad$$G\leftarrow 0$<br>$\qquad$$W\leftarrow 1$<br>$\qquad$<strong>FOR</strong> $t \in T-1,T-2,\cdots, 0$并且$W\neq 0$<br>$\qquad\qquad$$G\leftarrow G+\gamma R_{t+1}$<br>$\qquad\qquad$$W\leftarrow = W\cdot \frac{\pi(A_t|S_t)}{b(A_t|S_t)}$！！！原书中这个是放在最后一行的，我怎么觉得应该放在这里。。<br>$\qquad\qquad$$C(S_t, A_t)\leftarrow C(S_t, A_t)+W$<br>$\qquad\qquad$$Q(S_t, A_t)\leftarrow Q(S_t, A_t)+ \frac{W}{C(S_t,A_t)}(G_t-Q(S_t,A_t))$<br>$\qquad$<strong>END FOR</strong><br><strong>思考：这里怎么把它转换为first-visit的算法</strong></p><h2 id="off-policy-mc-control">Off-policy MC Control</h2><p>这一节给出一个off-policy的MC control算法，target policy是greedy算法，而behaviour policy是soft算法，在不同的episode中可以采用不同的behaviour policy。<br>算法 5 Off-policy MC control<br>初始化，$Q(s,a)\in \mathbb{R}, C(s,a) = 0, \forall s\in S, a\in A(s), \pi(s)\leftarrow arg max_aQ(s, a)$<br><strong>Loop</strong> forever (for each episode)<br>$\qquad$$b\leftarrow$ 任意覆盖target policy $\pi$的behaviour policy<br>$\qquad$用behaviour policy $b$生成一个episode，$S_0,A_0,R_1,\cdots, S_{T-1},A_{T-1},R_T$<br>$\qquad$$G\leftarrow 0$<br>$\qquad$$W\leftarrow 1$<br>$\qquad$<strong>for</strong> $t \in T-1,T-2,\cdots, 0$并且$W\neq 0$<br>$\qquad\qquad$$G\leftarrow G+\gamma R_{t+1}$<br>$\qquad\qquad$$C(S_t, A_t)\leftarrow C(S_t, A_t)+W$<br>$\qquad\qquad$$Q(S_t, A_t)\leftarrow Q(S_t, A_t)+ \frac{W}{C(S_t,A_t)}(G_t-Q(S_t, A_t)$<br>$\qquad\qquad\pi(s)\leftarrow arg max_aQ(S_t,a)$<br>$\qquad\qquad$<strong>if</strong> $A_t\neq\pi(S_t)$ then<br>$\qquad\qquad\qquad$break for循环<br>$\qquad\qquad$<strong>end if</strong><br>$\qquad\qquad$$W\leftarrow = W\cdot \frac{1}{b(A_t|S_t)}$这个为什么放最后一行，我能理解要进行一下if判断，但是放在这里importance ratio不就不对了吗。。<br>$\qquad$<strong>end for</strong></p><h2 id="discounting-aware-importance-sampling">Discounting-aware Importance Sampling</h2><p>这一节介绍了discounting的importance sampling，假设有$100$个steps的一个episode，$\gamma=0$，其实它的returns在第一步以后就确定了，后面的$99$步已经没有影响了，因为$\gamma=0$，这里就介绍了discount importance sampling。<br>…</p><h2 id="per-decision-importance-sampling">Per-decision Importance Sampling</h2><p>根据每一个Reward确定进行importance sampling，而不是根据每一个returns。<br>…</p><h2 id="summary">Summary</h2><p>MC相对于DP的好处</p><ol><li>model-free</li><li>sample比较容易</li><li>很容易focus在一个我们需要的subset上</li><li>不进行bootstrap</li></ol><p>在MC control算法中，估计的是action-value fucntion，因为action value function能够在不知道model dynamic的情况下改进policy。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;mc-methods&quot;&gt;MC Methods&lt;/h2&gt;
&lt;p&gt;这章主要介绍了MC算法，MC算法通过采样，估计state-value function或者action value function。为了找到最好的policy，需要让policy不断的进行探索，但是我
      
    
    </summary>
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="蒙特卡洛" scheme="http://mxxhcm.github.io/tags/%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B/"/>
    
  </entry>
  
  <entry>
    <title>ubuntu NVIDIA 驱动安装</title>
    <link href="http://mxxhcm.github.io/2019/04/26/linux-NVIDIA%E9%A9%B1%E5%8A%A8%E5%AE%89%E8%A3%85/"/>
    <id>http://mxxhcm.github.io/2019/04/26/linux-NVIDIA驱动安装/</id>
    <published>2019-04-26T13:03:02.000Z</published>
    <updated>2019-08-05T13:43:42.559Z</updated>
    
    <content type="html"><![CDATA[<h2 id="方法1-命令行安装">方法1.命令行安装</h2><h3 id="步骤">步骤</h3><p>卸载原有驱动<br>~$:sudo apt purge nvidia*<br>禁用nouveau<br>~$:sudo vim /etc/modprobe.d/blacklist.conf<br>在文件最后添加<br>blacklist nouveau<br>更新内核<br>~$:sudo update-initramfs -u<br>使用如下命令，如果没有输出，即已经关闭了nouveau<br>~$:lsmod | grep nouveau<br>关闭X service<br>~$:sudo service lightdm stop<br>接下来执行如下语句即可：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install build-essential pkg-config xserver-xorg-dev linux-headers-`uname -r` sudo apt-get install mesa-common-dev</span><br><span class="line">sudo apt-get install freeglut3-dev</span><br><span class="line">sudo chmod a+x NVIDIA-Linux-x86_64-375.66.run</span><br><span class="line">sudo sh NVIDIA-Linux-x86_64-375.66.run -no-opengl-files</span><br><span class="line">sudo reboot</span><br></pre></td></tr></table></figure><h2 id="方法2-图形界面">方法2.图形界面</h2><h2 id="方法3-apt安装">方法3.apt安装</h2><h3 id="添加apt源">添加apt源</h3><p>~$:sudo add-apt-repository ppa:graphics-drivers/ppa<br>~$:sudo apt update</p><h3 id="apt安装">apt安装</h3><p>~$:sudo ubuntu-drivers devices<br>~$:sudo ubuntu-drivers autoinstall</p><!-- ### 更新grub~$:sudo vim /etc/default/grub将"splash"改为"splash acpi_osi=linux"~$:sudo update-grub--><h2 id="安装cuda-9-0">安装cuda 9.0</h2><p>到NVIDIA官网下载cuda 9.0的runfile，然后执行<br>~$:sudo sh cuda*.run</p><h3 id="测试报错">测试报错</h3><blockquote><p>Error: unsupported compiler: 7.4.0. Use --override to override this check.</p></blockquote><p>安装gcc低版本<br>~$:sudo apt install gcc-6</p><p>从CUDA 4.1版本开始，支持gcc 4.5。gcc 4.6和4.7不受支持。<br>从CUDA 5.0版本开始，支持gcc 4.6。gcc 4.7不受支持。<br>从CUDA 6.0版本开始，支持gcc 4.7。<br>从CUDA 7.0版本开始，支持gcc 4.8，在Ubuntu 14.04和Fedora 21上支持4.9。<br>从CUDA 7.5版开始，支持gcc 4.8，在Ubuntu 14.04和Fedora 21上支持4.9。<br>从CUDA 8版本开始，Ubuntu 16.06和Fedora 23支持gcc 5.3。<br>从CUDA 9版本开始，Ubuntu 16.04，Ubuntu 17.04和Fedora 25支持gcc 6。<br>使用update-alternatives修改默认gcc版本<br>~$:sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g+±6 50<br>~$:sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-6 50</p><p>然后继续安装：<br>~$:sudo sh cuda*.run</p><p>cuda安装在/usr/local/cuda-9.0 目录下<br>卸载的话进入/usr/loca/cuda-9.0/bin 找到uninstall_cuda_9.0.pl运行卸载。</p><h3 id="import-tensorflow-报错">import tensorflow 报错</h3><blockquote><p>ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory<br>Failed to load the native TensorFlow runtime.</p></blockquote><p>配置cuda环境变量<br>在bashrc文件中加入<br>export PATH=/usr/local/cuda/bin${PATH:+:${PATH}}<br>export LD_LIBRARY_PATH=/usr/local/cuda/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}<br>export CUDA_HOME=/usr/local/cuda<br>执行<br>~$:source ~/.bashrc</p><p>继续报错<br>然后我才发现我没有装cudnn，按照参考文献[1]安装cudnn即可。<br>解压cudnn<br>~$:tar -xvf cudnn-x.x-linuz-x64-vx.x.tar.gz<br>然后执行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo cp cuda/include/cudnn.h /usr/local/cuda/include/</span><br><span class="line">sudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64/</span><br><span class="line">sudo chmod a+r /usr/local/cuda/include/cudnn.h</span><br><span class="line">sudo chmod a+r /usr/local/cuda/lib64/libcudnn*</span><br></pre></td></tr></table></figure><p>即可</p><h2 id="版本对应">版本对应</h2><h3 id="显卡">显卡</h3><p>RTX 20系列显卡，需要使用cuda 10</p><h3 id="pytorch">pytorch</h3><p>而pytorch目前不支持cuda 10.1，所以只能使用cuda 10.0。</p><h3 id="tensorflow">tensorflow</h3><p>tensorflow 13.1 – cuda 10.0  – cudnn 7.3</p><h2 id="参考文献">参考文献</h2><p>1.<a href="http://gwang-cv.github.io/2017/07/26/Faster-RCNN+Ubuntu16.04+Titan%20XP+CUDA8.0+cudnn5.0/" target="_blank" rel="noopener">http://gwang-cv.github.io/2017/07/26/Faster-RCNN+Ubuntu16.04+Titan XP+CUDA8.0+cudnn5.0/</a><br>2.<a href="https://docs.nvidia.com/deeplearning/sdk/cudnn-install/index.html#axzz4qYJp45J2" target="_blank" rel="noopener">https://docs.nvidia.com/deeplearning/sdk/cudnn-install/index.html#axzz4qYJp45J2</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;方法1-命令行安装&quot;&gt;方法1.命令行安装&lt;/h2&gt;
&lt;h3 id=&quot;步骤&quot;&gt;步骤&lt;/h3&gt;
&lt;p&gt;卸载原有驱动&lt;br&gt;
~$:sudo apt purge nvidia*&lt;br&gt;
禁用nouveau&lt;br&gt;
~$:sudo vim /etc/modprobe.d/
      
    
    </summary>
    
      <category term="linux" scheme="http://mxxhcm.github.io/categories/linux/"/>
    
    
      <category term="linux" scheme="http://mxxhcm.github.io/tags/linux/"/>
    
      <category term="ubuntu" scheme="http://mxxhcm.github.io/tags/ubuntu/"/>
    
      <category term="显卡驱动" scheme="http://mxxhcm.github.io/tags/%E6%98%BE%E5%8D%A1%E9%A9%B1%E5%8A%A8/"/>
    
  </entry>
  
  <entry>
    <title>pytorch Module.children() vs Module.modules()</title>
    <link href="http://mxxhcm.github.io/2019/04/25/pytorch-Module-children-vs-Module-modules/"/>
    <id>http://mxxhcm.github.io/2019/04/25/pytorch-Module-children-vs-Module-modules/</id>
    <published>2019-04-25T13:06:46.000Z</published>
    <updated>2019-05-08T14:16:21.716Z</updated>
    
    <content type="html"><![CDATA[<h2 id="module-modules">Module.modules()</h2><p>modules()会返回所有的模块，包括它自己。<br>如下代码所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = nn.Sequential(nn.Linear(<span class="number">5</span>, <span class="number">3</span>), nn.Sequential(nn.Linear(<span class="number">3</span>, <span class="number">2</span>)))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> module <span class="keyword">in</span> model.modules():</span><br><span class="line">    print(module)</span><br></pre></td></tr></table></figure><p>输出如下：</p><blockquote><p>Sequential(<br>(0): Linear(in_features=5, out_features=3, bias=True)<br>(1): Sequential(<br>(0): Linear(in_features=3, out_features=2, bias=True)<br>)<br>)<br>Linear(in_features=5, out_features=3, bias=True)<br>Sequential(<br>(0): Linear(in_features=3, out_features=2, bias=True)<br>)<br>Linear(in_features=3, out_features=2, bias=True)</p></blockquote><p>可以看出来，上面总共含有四个modules。</p><h2 id="module-children">Module.children()</h2><p>而children()不会返回它自己。<br>如下代码所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = nn.Sequential(nn.Linear(<span class="number">5</span>, <span class="number">3</span>), nn.Sequential(nn.Linear(<span class="number">3</span>, <span class="number">2</span>)))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> child <span class="keyword">in</span> model.children():</span><br><span class="line">    print(child)</span><br></pre></td></tr></table></figure><p>输出如下：</p><blockquote><p>Linear(in_features=5, out_features=3, bias=True)<br>Sequential(<br>(0): Linear(in_features=3, out_features=2, bias=True)<br>)</p></blockquote><p>可以看出来，上面只给出了Sequential里面的modules。</p><h3 id="完整代码">完整代码</h3><p><a href="https://github.com/mxxhcm/myown_code/blob/master/pytorch/tutorials/module_vs_children.py" target="_blank" rel="noopener">https://github.com/mxxhcm/myown_code/blob/master/pytorch/tutorials/module_vs_children.py</a></p><h2 id="参考文献">参考文献</h2><p>1.<a href="https://discuss.pytorch.org/t/module-children-vs-module-modules/4551/2" target="_blank" rel="noopener">https://discuss.pytorch.org/t/module-children-vs-module-modules/4551/2</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;module-modules&quot;&gt;Module.modules()&lt;/h2&gt;
&lt;p&gt;modules()会返回所有的模块，包括它自己。&lt;br&gt;
如下代码所示：&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td cl
      
    
    </summary>
    
      <category term="pytorch" scheme="http://mxxhcm.github.io/categories/pytorch/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="pytorch" scheme="http://mxxhcm.github.io/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>python defaultdict</title>
    <link href="http://mxxhcm.github.io/2019/04/25/python-defaultdict/"/>
    <id>http://mxxhcm.github.io/2019/04/25/python-defaultdict/</id>
    <published>2019-04-25T02:24:36.000Z</published>
    <updated>2019-05-06T16:22:27.708Z</updated>
    
    <content type="html"><![CDATA[<h2 id="使用defaultdict创建字典的值默认类型"><a href="#使用defaultdict创建字典的值默认类型" class="headerlink" title="使用defaultdict创建字典的值默认类型"></a>使用defaultdict创建字典的值默认类型</h2><h3 id="使用defaultdict创建值类型为dict的字典"><a href="#使用defaultdict创建值类型为dict的字典" class="headerlink" title="使用defaultdict创建值类型为dict的字典"></a>使用defaultdict创建值类型为dict的字典</h3><p>如下示例<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"></span><br><span class="line">ddd = defaultdict(dict)</span><br><span class="line">print(ddd)</span><br><span class="line"></span><br><span class="line">m = ddd[<span class="string">'a'</span>]</span><br><span class="line">m[<span class="string">'step'</span>] = <span class="number">1</span></span><br><span class="line">m[<span class="string">'exp'</span>] = <span class="number">3</span></span><br><span class="line">print(type(m))</span><br><span class="line">print(ddd)</span><br><span class="line"></span><br><span class="line">m = ddd[<span class="string">'b'</span>]</span><br><span class="line">m[<span class="string">'step'</span>] = <span class="number">1</span></span><br><span class="line">m[<span class="string">'exp'</span>] = <span class="number">3</span></span><br><span class="line">print(ddd)</span><br></pre></td></tr></table></figure></p><p>上述代码创建了一个dict，dict的value类型还是一个dict</p><blockquote><p>defaultdict(class ‘dict’&amp;gt , {})<br>&amp;lt class ‘dict’&amp;gt<br>defaultdict(&amp;lt class ‘dict’&amp;gt , {‘a’: {‘step’: 1, ‘exp’: 3}})<br>defaultdict(&amp;lt class ‘dict’&amp;gt , {‘a’: {‘step’: 1, ‘exp’: 3}, ‘b’: {‘step’: 1, ‘exp’: 3}})</p></blockquote><h3 id="使用defaultdict创建值类型为list的dict"><a href="#使用defaultdict创建值类型为list的dict" class="headerlink" title="使用defaultdict创建值类型为list的dict"></a>使用defaultdict创建值类型为list的dict</h3><p>如下示例<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"></span><br><span class="line">ddl = defaultdict(list)</span><br><span class="line">print(ddl)</span><br><span class="line">m = ddl[<span class="string">'a'</span>]</span><br><span class="line">print(type(m))</span><br><span class="line">m.append(<span class="number">3</span>)</span><br><span class="line">m.append(<span class="string">'hhhh'</span>)</span><br><span class="line">print(ddl)</span><br></pre></td></tr></table></figure></p><p>上述代码创建了一个dict，dict的value类型是一个list，输出如下</p><blockquote><p>defaultdict(&amp;lt class ‘list’&amp;gt , {})<br>&amp;lt class ‘list’&amp;gt<br>defaultdict(&amp;lt class ‘list’&amp;gt , {‘a’: [3, ‘hhhh’]})</p></blockquote><h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><p>点击获得<a href="https://github.com/mxxhcm/myown_code/blob/master/tools/python/defaultdict_test.py" target="_blank" rel="noopener">完整代码</a></p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>1.<a href="http://www.cnblogs.com/dancesir/p/8142775.html" target="_blank" rel="noopener">http://www.cnblogs.com/dancesir/p/8142775.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;使用defaultdict创建字典的值默认类型&quot;&gt;&lt;a href=&quot;#使用defaultdict创建字典的值默认类型&quot; class=&quot;headerlink&quot; title=&quot;使用defaultdict创建字典的值默认类型&quot;&gt;&lt;/a&gt;使用defaultdict创建字典
      
    
    </summary>
    
      <category term="python" scheme="http://mxxhcm.github.io/categories/python/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>python multiprocessing</title>
    <link href="http://mxxhcm.github.io/2019/04/23/python-multiprocessing-vs-threading/"/>
    <id>http://mxxhcm.github.io/2019/04/23/python-multiprocessing-vs-threading/</id>
    <published>2019-04-23T07:46:14.000Z</published>
    <updated>2019-10-11T05:30:51.354Z</updated>
    
    <content type="html"><![CDATA[<h2 id="multiprocessing-vs-multithread"><a href="#multiprocessing-vs-multithread" class="headerlink" title="multiprocessing vs multithread"></a>multiprocessing vs multithread</h2><p>多个threads可以在一个process中。同一个process中的所有threads共享相同的memory。而不同的processes有不同的memory areas，每一个都有自己的variables，进程之间为了通信，需要使用其他的channels，比如files, pipes和sockets等。thread比process更容易创建和管理，thread之间的交流比processes之间的交流更快。<br>这一节首先介绍一些GIL，然后介绍两个python的package，一个是threading，一个是multiprocessing。threading主要提供了多线程的实现。multiprocessing 主要提供了多进程的实现，当然也有多线程实现。</p><h2 id="GIL"><a href="#GIL" class="headerlink" title="GIL"></a>GIL</h2><p>thread有一个东西，叫做GIL(Global Interpreter Lock)，阻止同一个process中不同threads的同时运行，所以python多线程并不是多线程。举个例子，如果你有8个cores，使用8个threads，CPU的利用率不会达到800%，也不会快8倍。它会使用100%CPU，速度和原来相同，甚至会更慢，因为需要对多个threads进行调度。当然，有一些例外，如果大量的计算不是使用python运行的，而是使用一些自定义的C code进行GIL handling，就会得到你想要的性能。对于网络服务器或者GUI应用来说，大部分的事件都在等待，而不是在计算，这个时候就可以使用多个thread，相当于把他们都放在后台运行，而不需要终止相应的主线程。<br>如果想用纯python代码进行大量的CPU计算，使用threads并不能起到什么作用。使用process就没有GIL的问题，每个process有自己的GIL。这个时候需要在多线程和多进程之间做个权衡，因为进程之间的通信比线程之间通信的代价大得多。</p><h2 id="CPython的GIL实现"><a href="#CPython的GIL实现" class="headerlink" title="CPython的GIL实现"></a>CPython的GIL实现</h2><p>CPython 2.7中GIL是这样一行代码：<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">static</span> PyThread_type_lock interpreter_lock = <span class="number">0</span>; <span class="comment">/* This is the GIL */</span></span><br></pre></td></tr></table></figure></p><p>在Unix类系统中，PyThread_type_lock是标准的C lock mutex_t的别名。它的初始化方式如下：<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">void</span></span><br><span class="line">PyEval_InitThreads(<span class="keyword">void</span>)</span><br><span class="line">&#123;</span><br><span class="line">    interpreter_lock = PyThread_allocate_lock();</span><br><span class="line">    PyThread_acquire_lock(interpreter_lock);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>解释器中执行python的C代码必须持有这个lock。GIL的作用就是让你的程序足够简单：一个thread执行python代码，其他N个thread sleep或者等待I/O。或者可以等待threading.Lock或者其他同步操作。<br>那么什么时候threads进程切换呢？当一个thread 准备sleep或者进入等待I/O的时候，它释放GIL，其他thread请求GIL，执行相应的代码。这种任务叫做cooperative multitasking。还有一种是preemptive multitasking：在python2中一个thread不间断的执行1000个bytecode，或者python3中不间断的执行15 ms，然后放弃GIL让另一个thread运行。接下来举两个例子。</p><h2 id="cooperative-multithread"><a href="#cooperative-multithread" class="headerlink" title="cooperative multithread"></a>cooperative multithread</h2><p>在网络I/O中，具有很强的不确定性，当一个拥有GIL的thread请求网络I/O时，它释放GIL，这样子其他thread可以获得GIL继续执行，等到I/O完成时，该thread请求GIL继续执行。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">do_connect</span><span class="params">()</span>:</span></span><br><span class="line">    s = socket.socket()</span><br><span class="line">    s.connect((<span class="string">'python.org'</span>, <span class="number">80</span>))  <span class="comment"># drop the GIL</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">    t = threading.Thread(target=do_connect)</span><br><span class="line">    t.start()</span><br></pre></td></tr></table></figure></p><p>在上面的例子中，同一时刻只能有一个拥有GIL的thread执行python代码，但是一旦拥有GIL的thread开始connect，它就drop GIL，另一个thread可以申请GIL。但是所有的threads都可以drop GIL，也就是多个thread可以一起并行的等待sockets连接。<br>具体python在connect socket的时候是怎么drop GIL的，我们可以看一下socketmodule的c代码：<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* s.connect((host, port)) method */</span></span><br><span class="line"><span class="keyword">static</span> PyObject *</span><br><span class="line">sock_connect(PySocketSockObject *s, PyObject *addro)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">sock_addr_t</span> addrbuf;</span><br><span class="line">    <span class="keyword">int</span> addrlen;</span><br><span class="line">    <span class="keyword">int</span> res;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* convert (host, port) tuple to C address */</span></span><br><span class="line">    getsockaddrarg(s, addro, SAS2SA(&amp;addrbuf), &amp;addrlen);</span><br><span class="line"></span><br><span class="line">    Py_BEGIN_ALLOW_THREADS</span><br><span class="line">    res = connect(s-&gt;sock_fd, addr, addrlen);</span><br><span class="line">    Py_END_ALLOW_THREADS</span><br><span class="line"></span><br><span class="line">    <span class="comment">/* error handling and so on .... */</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>其中Py_BEGIN_ALLOW_THREADS宏就是drop GIL，它的定义如下：<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">PyThread_release_lock(interpreter_lock);</span><br></pre></td></tr></table></figure></p><p>同样，Py_END_ALLOW_THREADS宏是请求GIL。thread可以在这里block，等待GIL被释放，申请GIL继续执行。</p><h2 id="preemptive-multithread"><a href="#preemptive-multithread" class="headerlink" title="preemptive multithread"></a>preemptive multithread</h2><p>除了自动释放GIL外，还可以强制的释放GIL。python代码的执行有两步，第一步将python源代码编译成二进制的bytecode；第二步，python interpreter的main loop，一个叫做PyEval_EvalFrameEx()的函数，读取bytecode，并且一个一个的执行。<br>在多线程的模式下，interpreter强制周期性的drop GIL。如下所示，是thread判断是否释放GIl的代码：<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (;;) &#123;</span><br><span class="line">    <span class="keyword">if</span> (--ticker &lt; <span class="number">0</span>) &#123;</span><br><span class="line">        ticker = check_interval;</span><br><span class="line">    </span><br><span class="line">        <span class="comment">/* Give another thread a chance */</span></span><br><span class="line">        PyThread_release_lock(interpreter_lock);</span><br><span class="line">    </span><br><span class="line">        <span class="comment">/* Other threads may run now */</span></span><br><span class="line">    </span><br><span class="line">        PyThread_acquire_lock(interpreter_lock, <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    bytecode = *next_instr++;</span><br><span class="line">    <span class="keyword">switch</span> (bytecode) &#123;</span><br><span class="line">        <span class="comment">/* execute the next instruction ... */</span> </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>默认设置下是1000个bytecode。所有的threads周期性的获取GIL，然后释放。在python3下，所有thread获得15ms的GIL，而不是1000个bytecode。</p><h2 id="python的thread-safety"><a href="#python的thread-safety" class="headerlink" title="python的thread safety"></a>python的thread safety</h2><p>但是，如果买票等之类的，必须保证操作的atomic，否则就会出现问题。对于sort() operation来说，它是atomic，所以无序担心。看下面一个code snippet<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">n = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">foo</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">global</span> n</span><br><span class="line">    n += <span class="number">1</span></span><br></pre></td></tr></table></figure></p><p>我们查看foo对应的bytecode：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> dis</span><br><span class="line"></span><br><span class="line">print(dis.dis(foo))</span><br><span class="line"></span><br><span class="line"><span class="comment">#   7           0 LOAD_GLOBAL              0 (n)</span></span><br><span class="line"><span class="comment">#               2 LOAD_CONST               1 (1)</span></span><br><span class="line"><span class="comment">#               4 INPLACE_ADD</span></span><br><span class="line"><span class="comment">#               6 STORE_GLOBAL             0 (n)</span></span><br><span class="line"><span class="comment">#               8 LOAD_CONST               0 (None)</span></span><br><span class="line"><span class="comment">#              10 RETURN_VALUE</span></span><br></pre></td></tr></table></figure></p><p>可以看出，foo有6个bytecode，如果在第三个bytecode处，强制释放了GIL锁，其他thread改了n的值，等到切回这个thread的时候，就会出错。。所以，为了保证不出问题，需要手动加一个lock，保证不会在这个时候释放GIL。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"></span><br><span class="line">n = <span class="number">0</span></span><br><span class="line">lock = threading.Lock()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">foo</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">global</span> n</span><br><span class="line">    <span class="keyword">with</span> lock:</span><br><span class="line">        n += <span class="number">1</span></span><br></pre></td></tr></table></figure></p><p>当然，如果operation本身就是atomic的话，就不需要了。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">l = [<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">2</span>]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">foo</span><span class="params">()</span>:</span></span><br><span class="line">    l.sort()</span><br></pre></td></tr></table></figure></p><h2 id="threading"><a href="#threading" class="headerlink" title="threading"></a>threading</h2><p>threading是python多线程的一个package。</p><h3 id="threading-Thread"><a href="#threading-Thread" class="headerlink" title="threading.Thread"></a>threading.Thread</h3><h4 id="代码示例"><a href="#代码示例" class="headerlink" title="代码示例"></a>代码示例</h4><p><a href="thread_Thread.py">代码地址</a><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"><span class="keyword">import</span> socket</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">do_connect</span><span class="params">(website)</span>:</span></span><br><span class="line">    s = socket.socket()</span><br><span class="line">    info = s.connect((website, <span class="number">80</span>))  <span class="comment"># drop the GIL</span></span><br><span class="line">    print(type(info))</span><br><span class="line">    print(info)</span><br><span class="line">    print(os.getpid())</span><br><span class="line"></span><br><span class="line">websites = [<span class="string">'python.org'</span>, <span class="string">'baidu.com'</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(websites)):</span><br><span class="line">    t = threading.Thread(target=do_connect, args=(websites[i],))</span><br><span class="line">    t.start()</span><br></pre></td></tr></table></figure></p><h3 id="threading-Lock"><a href="#threading-Lock" class="headerlink" title="threading.Lock"></a>threading.Lock</h3><h4 id="代码示例-1"><a href="#代码示例-1" class="headerlink" title="代码示例"></a>代码示例</h4><p><a href="threading_Lock.py">代码地址</a><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line">hhhh = <span class="number">100</span></span><br><span class="line">lock = threading.Lock()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_number</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">global</span> hhhh</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">        <span class="keyword">with</span> lock:</span><br><span class="line">            hhhh += <span class="number">1</span></span><br><span class="line">            print(<span class="string">"add: "</span>, hhhh)</span><br><span class="line">            time.sleep(<span class="number">0.015</span>)</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">subtract_number</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">global</span> hhhh</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">        <span class="keyword">with</span> lock:</span><br><span class="line">            hhhh -= <span class="number">1</span></span><br><span class="line">            print(<span class="string">"subtract:"</span>, hhhh)</span><br><span class="line">            time.sleep(<span class="number">0.015</span>)</span><br><span class="line"> </span><br><span class="line">job_list = []</span><br><span class="line">job_list.append(threading.Thread(target=subtract_number, args=()))</span><br><span class="line">job_list.append(threading.Thread(target=add_number, args=()))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> job_list:</span><br><span class="line">    t.start()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> job_list:</span><br><span class="line">    t.join()</span><br><span class="line"> </span><br><span class="line">print(<span class="string">"Done"</span>)</span><br></pre></td></tr></table></figure></p><h2 id="multiprocessing"><a href="#multiprocessing" class="headerlink" title="multiprocessing"></a>multiprocessing</h2><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><div class="table-container"><table><thead><tr><th>方法</th><th>并行</th><th>是否直接阻塞</th><th>目标函数</th><th>函数返回值</th><th>适用场景</th></tr></thead><tbody><tr><td>mp.Pool.apply</td><td>否</td><td>是</td><td>只能有一个函数</td><td>函数返回值</td></tr><tr><td>mp.Pool.apply_async</td><td>是</td><td>否，调用join()进行阻塞</td><td>可以相同可以不同</td><td>返回AysncResult对象</td></tr><tr><td>mp.Pool.map</td><td>是</td><td>是</td><td>目标函数相同，参数不同</td><td>所有processes完成后直接返回有序结果</td></tr><tr><td>mp.Pool.map_async</td><td>是</td><td>否，调用join()阻塞</td><td>不知道。。</td><td>返回AysncResult对象</td></tr><tr><td>mp.Process</td><td>是</td><td>否</td><td>可以相同可以不同</td><td>无直接返回值</td><td>适用于线程数量比较小</td></tr></tbody></table></div><p>mp.Pool适用于线程数量远大于cpu数量，mp.Process适用于线程数量小于或者等于cpu数量的场景。<br>mp.Pool.apply   适用于非并行，调用apply()直接阻塞，process执行结束后直接返回结果。<br>mp.Pool.apply_async 适用于并行，异步执行，目标函数可以相同可以不同，返回AysncResult对象，因为AsyncResult对象是有序的，所以调用get得到的结果也是有序的。调用join()进行阻塞，调用get()方法获得返回结果，get()方法也是阻塞方法。<br>mp.Pool.map     适用于并行，异步，目标函数相同，参数不同。调用map()函数直接阻塞，等待所有processes完成后直接返回有序结果。<br>mp.Pool.map_async   也是调用join()和get()都能阻塞。<br>mp.Process  适用于并行，异步，目标函数可以相同可以不同，返回的结果需要借助mp.Queue()等工具，mp.Queue()存储的结果是无序的，mp.Manager()存储的结果是有序的。无序的结果可以使用特殊方法进行排序。</p><h3 id="统计cpu数量"><a href="#统计cpu数量" class="headerlink" title="统计cpu数量"></a>统计cpu数量</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cpus = mp.cpu_count()</span><br></pre></td></tr></table></figure><h3 id="实现并行的几种常用方法"><a href="#实现并行的几种常用方法" class="headerlink" title="实现并行的几种常用方法"></a>实现并行的几种常用方法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 方式1</span></span><br><span class="line">pool.apply_async</span><br><span class="line"><span class="comment"># 方式2</span></span><br><span class="line">pool.map</span><br><span class="line"><span class="comment"># 方式3</span></span><br><span class="line">mp.Process</span><br></pre></td></tr></table></figure><h3 id="retrieve并行结果"><a href="#retrieve并行结果" class="headerlink" title="retrieve并行结果"></a>retrieve并行结果</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 方式1</span></span><br><span class="line">results_obj = [pool.apply_async(f, args=(x,)) <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">3</span>)]</span><br><span class="line">results = [result_obj.get() <span class="keyword">for</span> result_obj <span class="keyword">in</span> results_obj]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 方式2</span></span><br><span class="line">results = pool.map(f, range(<span class="number">7</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 方式3</span></span><br><span class="line">output = Queue()</span><br><span class="line">pool.Process(target=f, args=(output))</span><br></pre></td></tr></table></figure><h2 id="mp-Pool"><a href="#mp-Pool" class="headerlink" title="mp.Pool"></a>mp.Pool</h2><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>指定占用的CPU核数，进程的个数可以多于CPU的核数，Pool会负责调用。如果CPU核数小于进程数，一般遵循FIFO的原则进行调用。</p><h3 id="API"><a href="#API" class="headerlink" title="API"></a>API</h3><ul><li>Pool.apply, </li><li>Pool.apply_async, </li><li>Pool.map, </li><li>Pool.map_async。</li></ul><h4 id="python-apply"><a href="#python-apply" class="headerlink" title="python apply"></a>python apply</h4><p>在老版本的python中，调用具有任意参数的function要使用apply函数，<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">apply(f, args, kwargs)</span><br></pre></td></tr></table></figure></p><p>甚至在2.7版本中还存在apply函数，但是基本上不怎么用了，3版本中已经没有了这种形式，现在都是直接使用函数名：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">f(*args, **kwargs)</span><br></pre></td></tr></table></figure></p><h4 id="mp-Pool-apply-vs-mp-Pool-apply-async"><a href="#mp-Pool-apply-vs-mp-Pool-apply-async" class="headerlink" title="mp.Pool.apply vs mp.Pool.apply_async"></a>mp.Pool.apply vs mp.Pool.apply_async</h4><p>multiprocessing.Pool中也有类似的interface。Pool.apply和python内置的apply挺像的，只不过Pool.apply会在一个单独的process执行，并且该函数会阻塞直到进程调用结束，所以Pool.apply不能异步执行。可以使用apply_async使用多个workers并行处理。<br>Pool.apply_async和apply基本一样，只不过它会在调用后立即返回一个AsyncResult对象，不用等到进程结束再返回。然后使用get()方法获得函数调用的返回值，get()方法会阻塞直到process结束。也就是说Pool.apply(func, args, kwargs)和pool.apply_async(func, args, kwargs).get()等价。Pool.apply_async可以调用很多个不同的函数。<br>Pool.apply_async返回值是无序的。</p><h4 id="mp-Pool-map-vs-mp-Pool-map-async"><a href="#mp-Pool-map-vs-mp-Pool-map-async" class="headerlink" title="mp.Pool.map vs mp.Pool.map_async"></a>mp.Pool.map vs mp.Pool.map_async</h4><p>Pool.map应用于同一个函数的不同参数，它的返回值顺序和调用顺序是一致的。Pool.map(func, iterable)和Pool.map_async(func, iterable).get()是一样的。</p><h4 id="mp-Pool-map-vs-mp-Pool-apply"><a href="#mp-Pool-map-vs-mp-Pool-apply" class="headerlink" title="mp.Pool.map vs mp.Pool.apply"></a>mp.Pool.map vs mp.Pool.apply</h4><p>Pool.apply(f, args): f函数仅仅被process pool中的一个worker执行。<br>Pool.map(f, iterable): 将iterable分割成多个单独的task，就是相当于同一个函数，给定不同的参数，每一组是一个task，然后使用pool中所有的processes执行这些taskes。所以map也能实现并行处理，而且是有序结果。</p><h4 id="mp-Pool-map-vs-mp-Pool-apply-async"><a href="#mp-Pool-map-vs-mp-Pool-apply-async" class="headerlink" title="mp.Pool.map vs mp.Pool.apply_async"></a>mp.Pool.map vs mp.Pool.apply_async</h4><p>Pool.map返回的结果是有序的；<br>Pool.apply_async返回的结果是无序的。<br>Pool.map处理相同的函数，不同的参数；</p><blockquote><p>pool.map() is a completely different kind of animal, because it distributes a bunch of arguments to the same function (asynchronously), across the pool processes, and then waits until all function calls have completed before returning the list of results.<br>Pool.apply_async处理不同的参数。</p></blockquote><h3 id="retrieve-return-value"><a href="#retrieve-return-value" class="headerlink" title="retrieve return value"></a>retrieve return value</h3><p>Pool.apply()会直接返回结果。<br>Pool.apply_async()会返回一个AsyncResult，然后使用get()方法获得结果。</p><h3 id="其他问题"><a href="#其他问题" class="headerlink" title="其他问题"></a>其他问题</h3><p>pool.map传递多个参数，或者重复参数，使用他的另一个版本，pool.starmap()<br>如下示例，<a href>代码地址</a><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Pool</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> repeat</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(string, x)</span>:</span></span><br><span class="line">    print(string)</span><br><span class="line">    <span class="keyword">return</span> x*x</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    <span class="keyword">with</span> Pool(processes=<span class="number">4</span>) <span class="keyword">as</span> pool:</span><br><span class="line">        number = <span class="number">10</span></span><br><span class="line">        s = <span class="string">"hello"</span></span><br><span class="line">        print(pool.starmap(f, zip(repeat(s), range(number))))</span><br></pre></td></tr></table></figure></p><h3 id="使用流程"><a href="#使用流程" class="headerlink" title="使用流程"></a>使用流程</h3><ol><li>创建Pool进程池，指定cpu核数<br>pool = Pool(cpu_core) </li><li>使用apply_async添加进程<br>processes = [p1, p2, p3]<br>results = []<br>for p in processes:<br> results.append(pool.apply_async(p, args=()))</li><li>关闭进程池<br>pool.close()</li><li>等待所有进程执行完毕<br>pool.join()</li><li>访问结果<br>for res in results:<br> print(res.get())</li></ol><h3 id="代码示例-2"><a href="#代码示例-2" class="headerlink" title="代码示例"></a>代码示例</h3><p><a href>代码地址</a></p><h2 id="mp-Process"><a href="#mp-Process" class="headerlink" title="mp.Process"></a>mp.Process</h2><h3 id="简介-1"><a href="#简介-1" class="headerlink" title="简介"></a>简介</h3><p>每个进程占用一个CPU核。</p><h3 id="retrieve结果"><a href="#retrieve结果" class="headerlink" title="retrieve结果"></a>retrieve结果</h3><p>使用mp.Queue()或者mp.Pipe()等对象记录结果。Queue()不保证结果的顺序和task的执行顺序一致。</p><h3 id="使用流程-1"><a href="#使用流程-1" class="headerlink" title="使用流程"></a>使用流程</h3><h3 id="代码示例-3"><a href="#代码示例-3" class="headerlink" title="代码示例"></a>代码示例</h3><p><a href="Process.py">代码地址</a></p><h2 id="mp-Pool-vs-mp-Process"><a href="#mp-Pool-vs-mp-Process" class="headerlink" title="mp.Pool vs mp.Process"></a>mp.Pool vs mp.Process</h2><ol><li>Pool会负责对cpu进行调度，即tasks数量可以远大于worker数量，一个worker占用一个cpu核。而Process的task必须小于worker，每个worker只能运行一个task。</li><li>如果执行多个task的时候，Process一定会使用多个seperate workes，但是对于Pool来说，可能会使用同一个worker去执行多个task。如下示例，p1和p2一定是两个wrokers运行两个process，而pool中，pool中有两个worker，foo可以是第一个worker也可以是第二个worker运行的process解决的，而bar也可以是这两个中任意一个worker解决的，这种情况发生在foo已经运行结束了，两个worker都是空闲的，给bar任意分配一个worker。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">foo</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bar</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">p1 = Process(target=foo, args=())</span><br><span class="line">p2 = Process(target=bar, args=())</span><br><span class="line"></span><br><span class="line">p1.start()</span><br><span class="line">p2.start()</span><br><span class="line">p1.join()</span><br><span class="line">p2.join()</span><br><span class="line"></span><br><span class="line">pool = Pool(processes=<span class="number">2</span>)             </span><br><span class="line">r1 = pool.apply_async(foo)</span><br><span class="line">r2 = pool.apply_async(bar)</span><br></pre></td></tr></table></figure></li></ol><h3 id="代码示例-4"><a href="#代码示例-4" class="headerlink" title="代码示例"></a>代码示例</h3><p><a href="Pool_Process.py">代码地址</a></p><h2 id="join方法"><a href="#join方法" class="headerlink" title="join方法"></a>join方法</h2><h3 id="简介-2"><a href="#简介-2" class="headerlink" title="简介"></a>简介</h3><p>用来阻塞当前进程，直到该进程执行完毕，再继续执行后续代码。</p><h3 id="代码示例-5"><a href="#代码示例-5" class="headerlink" title="代码示例"></a>代码示例</h3><p><a href="https://github.com/mxxhcm/myown_code/blob/master/tools/py_process_thread/mp/mp_join.py" target="_blank" rel="noopener">代码地址</a><br>可以看出来，调用join()函数的时候，会等子进程执行完之后再继续执行；而不使用join()函数的话，在子进程开始执行的时候，就会继续向后执行了。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>1.<a href="https://www.cnblogs.com/lipijin/p/3709903.html" target="_blank" rel="noopener">https://www.cnblogs.com/lipijin/p/3709903.html</a><br>2.<a href="https://www.ellicium.com/python-multiprocessing-pool-process/" target="_blank" rel="noopener">https://www.ellicium.com/python-multiprocessing-pool-process/</a><br>3.<a href="https://stackoverflow.com/questions/8533318/multiprocessing-pool-when-to-use-apply-apply-async-or-map" target="_blank" rel="noopener">https://stackoverflow.com/questions/8533318/multiprocessing-pool-when-to-use-apply-apply-async-or-map</a><mp pool apply, apply_async, map用法><br>4.<a href="https://stackoverflow.com/questions/31711378/python-multiprocessing-how-to-know-to-use-pool-or-process" target="_blank" rel="noopener">https://stackoverflow.com/questions/31711378/python-multiprocessing-how-to-know-to-use-pool-or-process</a><mp process和pool.map获得不同目标函数process的结果，对mp.process无序结果进行排序><br>5.<a href="https://stackoverflow.com/questions/18176178/python-multiprocessing-process-or-pool-for-what-i-am-doing" target="_blank" rel="noopener">https://stackoverflow.com/questions/18176178/python-multiprocessing-process-or-pool-for-what-i-am-doing</a><mp pool.apply_async, process不同函数的多process><br>6.<a href="https://stackoverflow.com/questions/10415028/how-can-i-recover-the-return-value-of-a-function-passed-to-multiprocessing-proce" target="_blank" rel="noopener">https://stackoverflow.com/questions/10415028/how-can-i-recover-the-return-value-of-a-function-passed-to-multiprocessing-proce</a>&lt;获得传递给mp Process函数返回值的方法&gt;<br>7.<a href="https://docs.python.org/3/library/multiprocessing.html#sharing-state-between-processes" target="_blank" rel="noopener">https://docs.python.org/3/library/multiprocessing.html#sharing-state-between-processes</a><br>8.<a href="https://sebastianraschka.com/Articles/2014_multiprocessing.html" target="_blank" rel="noopener">https://sebastianraschka.com/Articles/2014_multiprocessing.html</a><br>9.<a href="https://opensource.com/article/17/4/grok-gil" target="_blank" rel="noopener">https://opensource.com/article/17/4/grok-gil</a><gil解释></gil解释></mp></mp></mp></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;multiprocessing-vs-multithread&quot;&gt;&lt;a href=&quot;#multiprocessing-vs-multithread&quot; class=&quot;headerlink&quot; title=&quot;multiprocessing vs multithread&quot;&gt;
      
    
    </summary>
    
      <category term="python" scheme="http://mxxhcm.github.io/categories/python/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="Pool" scheme="http://mxxhcm.github.io/tags/Pool/"/>
    
      <category term="Process" scheme="http://mxxhcm.github.io/tags/Process/"/>
    
      <category term="multiprocessing" scheme="http://mxxhcm.github.io/tags/multiprocessing/"/>
    
      <category term="threading" scheme="http://mxxhcm.github.io/tags/threading/"/>
    
  </entry>
  
  <entry>
    <title>Asynchronous Methods for Deep Reinforcement Learning</title>
    <link href="http://mxxhcm.github.io/2019/04/19/a3c/"/>
    <id>http://mxxhcm.github.io/2019/04/19/a3c/</id>
    <published>2019-04-19T10:11:56.000Z</published>
    <updated>2019-09-27T07:27:09.429Z</updated>
    
    <content type="html"><![CDATA[<h2 id="摘要">摘要</h2><p>DQN使用experience replay buffer来稳定学习过程。本文提出一个异步框架来代替buffer，稳定学习过程。这个框架同时适用于on-policy和off-policy环境，也能应用于离散动作空间和连续的动作空间，既能训练前馈智能体，也能训练循环智能体。</p><h2 id="introduction">Introduction</h2><p>强化学习算法一般都是online的，而online学习是不稳定的，并且online更新通常都是强相关的。DQN通过引入experience replay buffer解决了这个问题，但是DQN只能应用在off policy算法上。DQN通过引入replay buffer取得了很大成功，但是replay buffer还有以下的几个缺点：</p><ul><li>在每一步交互的时候使用了更多的内存和计算资源</li><li>它只能应用在off policy的算法上，也就是说权重的更新可能会使用到很久之前的数据。</li></ul><p>这篇文章提出不使用replay buffer，而是使用异步的框架，同时在多个相同的环境中操作多个智能体（每个环境中一个智能体）并行的采集数据。这种并行性也能将智能体的数据分解成更稳定的过程（即和experience replay buffer起到了相同的作用），因为在给定的一个时间步，智能体可能会experience很多个不同的states。<br>这个框架既可以应用在on policy算法，如Sarsa，n-step methods和actor-critc等方法上，也可以应用在off policy算法如Q-learning上。</p><h2 id="异步框架">异步框架</h2><p>作者给出了一个框架，将够将on-policy search的actor-critic方法以及off-policy value-based的Q-learning方法都包括进去。<br>具体的，使用一台机器上的多CPU线程，这样子可以避免在不同机器上传递参数和梯度的消耗。然后，多个并行的actor-learner可能会探索环境的不同部分，每个actor-learner可以设置不同的exploration policy。不同的thread运行不同的exploration policy，多个actor-learner并行执行online update可能比单智能体更新在时间上更不相关。所以这里使用了不同的探索策略取代了DQN中buffer稳定学习过程的作用。<br>除了稳定学习过程之外，多个actor-learner还可以减少训练时间，此外，不使用buffer以后还可以使用on-policy的方法进行训练。</p><p><strong>总的来说，下面要介绍的四个算法，前面三个算法都使用了target network，第四个A3C算法没有使用target network。最重要的是所有四个算法都使用了多个actor-learner进行训练，并且使用累计的梯度进行更新（相当于batch的作用）。总共出现了三类参数，一类是network参数，一类是target network参数，一类是thread-specific（每个线程的参数）的参数。thread-specific参数是每个线程自己持有的，通过更新每个线程的参数更新network的参数，然后使用network的参数更新target network的参数，target network参数比network参数更新的要慢很多。<br>A3C算法的实质就是在多个线程中同步训练。分为主网络和线程中的网络，主网络不需要训练，主要用来存储和传递参数，每个线程中的网络用来训练参数。总的来说，多个线程同时训练提高了效率，另一方面，减小了数据之间的相关性，比如，线程$1$和$2$中都用主网络复制来的参数计算梯度，但是同一时刻只能有一个线程更新主网络的参数，比如线程$1$更新主网络的参数，那么线程$2$利用原来主网络参数计算的梯度会更新在线程$1$更新完之后的主网络参数上。</strong></p><h3 id="异步的one-step-q-learning">异步的one-step Q-learning</h3><ul><li>每个thread都和它自己的环境副本进行交互，在每一个时间步计算Q-learning loss的梯度。</li><li>通过使用不同的exploration策略，可以改进性能，这里实现exploration policy不同的方式就是使用$\epsilon$的不同取值实现。</li><li>使用一个共享的更新的比较缓慢的target network，就是和DQN中的target network一样。</li><li>同时也使用多个时间步上的累计梯度，和batch挺像的，这就减少了multi actor learner重写其他更新的可能性，同时也在计算效率和数据效率方面做了一个权衡。</li></ul><h4 id="伪代码">伪代码</h4><p><strong>Algorithm 1</strong> 异步的one-step Q-learning－－每个actor-learn线程的伪代码<br>用$\theta,\theta^{-}$表示全局共享参数，计数器$T=0$，<br>初始化线程时间步计数器$t\leftarrow 0$，<br>初始化target network权重$\theta^{-} \leftarrow 0$,<br>初始化network梯度$d\theta\leftarrow 0$，<br>初始化，得到初始状态$s$，<br><strong>repeat</strong><br>$\qquad$使用$\epsilon-$greedy策略采取action $a$，<br>$\qquad$接收下一个状态$s’$和reward $r$，<br>$\qquad$设置target value，$y=\begin{cases}r,&amp;for\ terminal\ s’ \\ r+\gamma max_{a’}Q(s’,a’;\theta^{-}), &amp;for\ non-terminal\ s’\end{cases}$<br>$\qquad$累计和$\theta$相关的梯度：$d\theta \leftarrow d\theta+\frac{\partial (y-Q(s,a;\theta))^2}{\partial \theta}$<br>$\qquad s\leftarrow s’$<br>$\qquad T\leftarrow T+1, t\leftarrow t+1$<br>$\qquad$<strong>if</strong> $T\ \ mod\ \ I_{target} ==0 $，那么<br>$\qquad\qquad$更新target network $\theta^{-}\leftarrow 0$<br>$\qquad$<strong>end if</strong><br>$\qquad$<strong>if</strong> $t\ \ mod\ \ I_{AsyncUpdate} ==0$或者$s$是terminal state，那么<br>$\qquad\qquad$使用$d\theta$异步更新$\theta$<br>$\qquad\qquad$将累计梯度$d\theta\leftarrow 0$<br>$\qquad$<strong>end if</strong><br><strong>until</strong> $T\ge T_{max}$</p><h3 id="异步的one-step-sarsa">异步的one-step Sarsa</h3><h4 id="概述">概述</h4><ul><li>和算法$1$很像，$Q-learning$计算target value使用$r+\gamma max_{a’}Q(s’,a’;\theta^{-})$，而Sarsa计算target value使用$r+\gamma Q(s’,a’;\theta^{-})$，即Q-learning的bahaviour policy和评估的策略是不一样的，而Sarsa的behaviour policy和评估策略是一样的。</li><li>使用target network，</li><li>同时使用多个时间步的累计梯度更新用来稳定学习过程。</li></ul><h4 id="伪代码-v2">伪代码</h4><p>和算法$1$很像。</p><h3 id="异步的n-step-q-learning">异步的n-step Q-learning</h3><h4 id="概述-v2">概述</h4><ul><li>计算$n-step$的return</li><li>在计算一次更新的时候，使用exploration policy采样到$t_{max}$步或者到terminal state。然后累加从上次更新到$t_{max}$时间步的reward。</li><li>然后计算$n-step$更新对于上次更新之后所有state-action的梯度。</li><li>使用单个时间步中的累计梯度进行更新。</li><li>使用了target network。</li></ul><h4 id="伪代码-v3">伪代码</h4><p><strong>Algorithm 2</strong> 异步的n-step Q-learning算法－－每个actor-learner线程的伪代码<br>用$\theta,\theta^{-}$表示全局共享的network参数和target network参数，用$T=0$表示全局共享计数器。<br>初始化线程步计数器$t\leftarrow 1$，<br>初始化target network参数$\theta^{-}\leftarrow \theta$<br>初始化每个线程的参数参数$\theta^{-}\leftarrow \theta$<br>初始化网络梯度$d\theta\leftarrow 0$<br><strong>repeat</strong><br>$\qquad$重置累计梯度$d\theta\leftarrow0$<br>$\qquad$同步每个线程的参数$\theta’=\theta$<br>$\qquad t_{start}=t$<br>$\qquad$得到$s_t$<br>$\qquad$<strong>repeat</strong><br>$\qquad\qquad$根据基于$Q(s_t,a;\theta’)$的$\epsilon-greedy$策略执行动作$a_t$，<br>$\qquad\qquad$接收下一个状态$s_{t+1}$和reward $r_t$，<br>$\qquad\qquad T\leftarrow T+1, t\leftarrow t+1$<br>$\qquad$ <strong>until</strong> terminal $s_t$或者$t-t_{start}==t_{max}$<br>$\qquad$设置奖励$R=\begin{cases}0,&amp;for\ terminal\ s_t\max_aQ(s_t,a;\theta^{-}), &amp;for\ non-terminal\ s_t\end{cases}$<br>$\qquad$<strong>for</strong> $i\in{t-1,\cdots,t_{start}}$ do<br>$\qquad\qquad R\leftarrow r_i+\gamma R$<br>$\qquad\qquad$累计和$\theta’$相关的梯度：$d\theta \leftarrow d\theta+\frac{\partial (R-Q(s_t,a;\theta’))^2}{\partial \theta’}$<br>$\qquad$<strong>end for</strong><br>$\qquad$使用$d\theta$异步更新$\theta$.<br>$\qquad$<strong>if</strong>$\quad T\quad mod\quad I_{target}==0$那么<br>$\qquad\qquad\theta^{-}\leftarrow \theta$<br>$\qquad$<strong>end if</strong><br><strong>until</strong> $T\gt T_{max}$</p><h3 id="异步的advantage-actor-critic">异步的advantage actor-critic</h3><h4 id="概述-v3">概述</h4><ul><li>A3C算法，是一个on-policy的actor-critic方法，使用值函数$V(s_t;\theta_v)$辅助学习policy $\pi(a_t|s_t;\theta)$，同时这里使用$n-step$的returns更新policy和value function。</li><li>每隔$t_{max}$个action更新一次或者到了terminal state更新一次。</li><li>Actor的更新方向为$\nabla_{\theta’}log\pi(a_t|s_t;\theta’)A(s_t,a_t;\theta,\theta_v)$，其中$A$是advantage function的一个估计，通过$\sum_{i=0}^{k-1} \gamma^ir_{t+i}+\gamma^kV(s_{t+k};\theta_v) - V(s_t;\theta_v)$计算。</li><li>这里同样使用并行的actor-learner和累计的梯度用来稳定学习。$\theta$和$\theta_v$在实现上通常共享参数。</li><li>添加entropy正则项鼓励exploration。包含了正则化项的的objective function的梯度为$\nabla_{\theta’}log\pi(a_t|s_t;\theta’)(R_t-V(s_t;\theta_v))+\beta\nabla_{\theta’}H(\pi(s_t;\theta’))$。这里的$R$就是上面的$\sum_{i=0}^{k-1}\gamma^ir_{t+i}+\gamma^kV(s_{t+k};\theta_v) - V(s_t;\theta_v)$。</li><li>Critic的更新方向通过最小化loss来实现，这里的loss指的是TD-error，即$\sum_{i=0}^{k-1}\gamma^ir_{t+i} + \gamma^kV(s_{t+k};\theta_v) - V(s_t;\theta_v)$。</li><li>没有使用target network。</li></ul><h4 id="伪代码-v4">伪代码</h4><p><strong>Algorithm 3</strong> A3C－－每个actor-learn线程的伪代码<br>用$\theta,\theta_v$表示全局共享参数，用$T=0$表示全局共享计数器，<br>用$\theta’,\theta’_v$表示每个线程中的参数<br>初始化线程步计数器$t\leftarrow 1$，<br><strong>repeat</strong><br>$\qquad$重置梯度$d\theta\leftarrow 0,d\theta_v\leftarrow 0$，<br>$\qquad$同步线程参数$\theta’=\theta,\theta’_v=\theta_v$<br>$\qquad t_{start}=t$<br>$\qquad$得到状态$s_t$，<br>$\qquad$<strong>repeat</strong><br>$\qquad\qquad$根据策略$\pi(a_t|s_t;\theta’)$执行动作$a_t$，<br>$\qquad\qquad$接收下一个状态$s_{t+1}$和reward $r_t$，<br>$\qquad\qquad T\leftarrow T+1, t\leftarrow t+1$<br>$\qquad$ <strong>until</strong> terminal $s_t$或者$t-t_{start}==t_{max}$<br>$\qquad$设置奖励$R=\begin{cases}0,&amp;for\ terminal\ s_t\\ V(s_t,\theta’_v), &amp;for\ non-terminal\ s_t\end{cases}$<br>$\qquad$<strong>for</strong> $i\in{t-1,\cdots,t_{start}}$ do<br>$\qquad\qquad R\leftarrow r_i+\gamma R$<br>$\qquad\qquad$累计和$\theta’$相关的梯度：$d\theta \leftarrow d\theta+\frac{\partial (y-Q(s,a;\theta))^2}{\partial \theta}$<br>$\qquad\qquad$累计和$\theta’_v$相关的梯度：$d\theta_v \leftarrow d\theta_v+\frac{\partial (R-V(s_i;\theta’_v))^2}{\partial \theta’_v}$<br>$\qquad$<strong>end for</strong><br>$\qquad$使用$d\theta$异步更新$\theta$，使用$d\theta_v$异步更新$\theta_v$.<br><strong>until</strong> $T\ge T_{max}$</p><h3 id="优化方法">优化方法</h3><p>作者尝试了三种不同的优化方法，带有momentum的SGD，带有共享statistics的RMSProp以及不带shared statistics的RMSProp。</p><h2 id="实验">实验</h2><h3 id="优化细节">优化细节</h3><p>作者在异步框架中测试了两个优化算法SGD和RMSProp，并且因为效率原因没有使用线程锁。</p><h3 id="设置">设置</h3><ul><li>Atari环境中，每个实验使用$16$个actor-learner线程。</li><li>所有方法都每隔$5$个actions更新一次，并且使用共享的RMSProp进行优化。</li><li>三个异步的value-based算法使用每隔$40000$帧更新的共享target network，</li><li>使用了DQN中action repeat of $4$.</li><li>网络架构和DQN一样</li><li>基于值的方法只有一个线性输出层，每个输出单元代表一个action的值。</li><li>actor-critic方法有两个输出层，一个softmax表示选择某一个action的概率，一个线性输出代表值函数。</li><li>所有实验使用的$\gamma=0.99$，RMSProp的衰减因子$\alpha = 0.99$。</li><li>Value-based方法采用的exploration rate $\epsilon$有三个取值$\epsilon_1,\epsilon_2,\epsilon_3$，相应的概率为$0.4,0.3,0.3$，它们的值在前$4$百万帧中从$1$退火到$0.1,0.01,0.5$。</li><li>A3C使用了entropy进行正则化，entropy项的权重为$\beta=0.01$</li><li>初始学习率从分布$LogUniform(10^{-4},10^{-2})$中进行采样，在训练过程中退火到$0$。</li></ul><h2 id="代码">代码</h2><h3 id="代码地址">代码地址</h3><p><a href="https://github.com/ikostrikov/pytorch-a3c" target="_blank" rel="noopener">https://github.com/ikostrikov/pytorch-a3c</a></p><h3 id="问题">问题</h3><p>如果直接git下来运行的话，会出问题，需要在main()下加上这样一句</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mp.set_start_method(<span class="string">"forkserver"</span>)</span><br></pre></td></tr></table></figure><p>可能是因为Unix系统默认的多进程方式是fork，这里只要不设置为fork,设置为其他两种方式spawn, forkserver都行。</p><h3 id="参考文献">参考文献</h3><p>1.<a href="https://arxiv.org/pdf/1602.01783.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1602.01783.pdf</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;摘要&quot;&gt;摘要&lt;/h2&gt;
&lt;p&gt;DQN使用experience replay buffer来稳定学习过程。本文提出一个异步框架来代替buffer，稳定学习过程。这个框架同时适用于on-policy和off-policy环境，也能应用于离散动作空间和连续的动作空间，既
      
    
    </summary>
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="A3C" scheme="http://mxxhcm.github.io/tags/A3C/"/>
    
  </entry>
  
  <entry>
    <title>linux 终端快速访问某个目录</title>
    <link href="http://mxxhcm.github.io/2019/04/15/linux-%E7%BB%88%E7%AB%AF%E5%BF%AB%E9%80%9F%E8%AE%BF%E9%97%AE%E6%9F%90%E4%B8%AA%E7%9B%AE%E5%BD%95/"/>
    <id>http://mxxhcm.github.io/2019/04/15/linux-终端快速访问某个目录/</id>
    <published>2019-04-15T10:49:57.000Z</published>
    <updated>2019-05-12T04:04:35.072Z</updated>
    
    <content type="html"><![CDATA[<h2 id="动机">动机</h2><p>在写博客的过程中，每次在终端中进入该目录，都要输好长的命令，在想着有没有什么简单的方法。后来就在网上找到了。</p><h2 id="方法">方法</h2><p>利用alias命令进行重命名<br>这里给出一个具体的例子，我的博客文件存放在/home/mxxmhh/github/blog/source/_posts下，<br>在/home/mxxmhh/.bashrc文件中添加如下一行即可(当然也可以在其他配置文件中添加)：<br>alias posts='cd /home/mxxmhh/github/blog/source/_posts’<br>然后执行<br>~\$:source /home/mxxmhh/.bashrc<br>即可。<br>接下来可在终端输入<br>~\$:posts<br>直接访问该目录。</p><h2 id="参考文献">参考文献</h2><p>1.<a href="https://www.cnblogs.com/wlsphper/p/6782625.html" target="_blank" rel="noopener">https://www.cnblogs.com/wlsphper/p/6782625.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;动机&quot;&gt;动机&lt;/h2&gt;
&lt;p&gt;在写博客的过程中，每次在终端中进入该目录，都要输好长的命令，在想着有没有什么简单的方法。后来就在网上找到了。&lt;/p&gt;
&lt;h2 id=&quot;方法&quot;&gt;方法&lt;/h2&gt;
&lt;p&gt;利用alias命令进行重命名&lt;br&gt;
这里给出一个具体的例子，我的博客
      
    
    </summary>
    
      <category term="linux" scheme="http://mxxhcm.github.io/categories/linux/"/>
    
    
      <category term="linux" scheme="http://mxxhcm.github.io/tags/linux/"/>
    
      <category term="ubuntu" scheme="http://mxxhcm.github.io/tags/ubuntu/"/>
    
  </entry>
  
  <entry>
    <title>python 类和函数的属性</title>
    <link href="http://mxxhcm.github.io/2019/04/14/python-%E7%B1%BB%E5%92%8C%E5%87%BD%E6%95%B0%E7%9A%84%E5%B1%9E%E6%80%A7/"/>
    <id>http://mxxhcm.github.io/2019/04/14/python-类和函数的属性/</id>
    <published>2019-04-14T06:49:41.000Z</published>
    <updated>2019-05-06T16:22:27.708Z</updated>
    
    <content type="html"><![CDATA[<h2 id="函数和类的默认属性"><a href="#函数和类的默认属性" class="headerlink" title="函数和类的默认属性"></a>函数和类的默认属性</h2><p>这里主要介绍类和函数的一些属性。<br><strong>dict</strong>用来描述对象的属性。对于类来说，它内部的变量就是它的数量，注意，不是它的member variable，但是对于函数来说不是。对于类来说，而对于类对象来说，输出的是整个类的属性，而<strong>dict</strong>输出的是self.variable的内容。</p><p>python中的函数有很多特殊的属性（包括自定义的函数和库函数）</p><ul><li><strong>doc</strong>  输出用户定义的关于函数的说明</li><li><strong>name</strong> 输出函数名字</li><li><strong>module</strong> 输出函数所在模块的名字</li><li><strong>dict</strong> 输出函数中的字典</li></ul><p>示例：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">myfunc</span><span class="params">()</span>:</span></span><br><span class="line">   <span class="string">'this func is to test the __doc__'</span></span><br><span class="line">   myfunc.func_attr = <span class="string">"attr"</span></span><br><span class="line">   print(<span class="string">"hhhh"</span>)</span><br><span class="line"> </span><br><span class="line">myfunc.func_attr1 = <span class="string">"first1"</span></span><br><span class="line">myfunc.func_attr2 = <span class="string">"first2"</span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">  print(myfunc.__doc__)</span><br><span class="line">  print(myfunc.__name__)</span><br><span class="line">  print(myfunc.__module__)</span><br><span class="line">  print(myfunc.__dict__)</span><br></pre></td></tr></table></figure></p><p>输出：</p><blockquote><p>this func is to test the <strong>doc</strong><br>myfunc<br><strong>main</strong><br>{‘func_attr1’: ‘first1’, ‘func_attr2’: ‘first2’}</p></blockquote><p>类也有很多特殊的属性（包括自定义的类和库中的类）</p><ul><li><strong>doc</strong>  输出用户定义的类的说明</li><li><strong>module</strong> 输出类所在模块的名字</li><li><strong>dict</strong> 输出类中的字典</li></ul><p>示例：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyClass</span>:</span></span><br><span class="line">  <span class="string">"""This is my class __doc__"""</span></span><br><span class="line">  class_name = <span class="string">"cllll"</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, test=None)</span>:</span></span><br><span class="line">     self.test = test</span><br><span class="line">  <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">  print(MyClass.__dict__)</span><br><span class="line">  print(MyClass.__doc__)</span><br><span class="line">  print(MyClass.__module__)</span><br></pre></td></tr></table></figure></p><p>输出：</p><blockquote><p>{‘<strong>module</strong>‘: ‘<strong>main</strong>‘, ‘<strong>doc</strong>‘: ‘This is my class <strong>doc</strong>‘, ‘class_name’: ‘cllll’, ‘<strong>init</strong>‘: \<function myclass.__init__ at 0x7f1349d44510\>, ‘<strong>dict</strong>‘: \<attribute '__dict__' of 'myclass' objects\>, ‘<strong>weakref</strong>‘: \<attribute '__weakref__' of 'myclass' objects\>}<br>This is my class <strong>doc</strong><br><strong>main</strong></attribute></attribute></function></p></blockquote><p>类的对象的属性</p><ul><li><strong>doc</strong>  输出用户定义的类的说明</li><li><strong>module</strong> 输出类对象所在模块的名字</li><li><strong>dict</strong> 输出类对象中的字典</li></ul><p>示例<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"> <span class="number">1</span> <span class="class"><span class="keyword">class</span> <span class="title">MyClass</span>:</span></span><br><span class="line"> <span class="number">2</span>   <span class="string">"""This is my class __doc__"""</span></span><br><span class="line"> <span class="number">3</span>   class_name = <span class="string">"cllll"</span></span><br><span class="line"> <span class="number">4</span>   <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, test=None)</span>:</span></span><br><span class="line"> <span class="number">5</span>      self.test = test</span><br><span class="line"> <span class="number">6</span>   <span class="keyword">pass</span></span><br><span class="line"> <span class="number">7</span> </span><br><span class="line"> <span class="number">8</span> <span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line"> <span class="number">9</span> </span><br><span class="line"><span class="number">10</span>   cl = MyClass()</span><br><span class="line"><span class="number">11</span>   print(cl.__dict__)</span><br><span class="line"><span class="number">12</span>   print(cl.__doc__)</span><br><span class="line"><span class="number">13</span>   print(cl.__module__)</span><br></pre></td></tr></table></figure></p><p>输出</p><blockquote><p>{‘test’: None}<br>This is my class <strong>doc</strong><br><strong>main</strong></p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;函数和类的默认属性&quot;&gt;&lt;a href=&quot;#函数和类的默认属性&quot; class=&quot;headerlink&quot; title=&quot;函数和类的默认属性&quot;&gt;&lt;/a&gt;函数和类的默认属性&lt;/h2&gt;&lt;p&gt;这里主要介绍类和函数的一些属性。&lt;br&gt;&lt;strong&gt;dict&lt;/strong&gt;用
      
    
    </summary>
    
      <category term="python" scheme="http://mxxhcm.github.io/categories/python/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>python zip和enumerate</title>
    <link href="http://mxxhcm.github.io/2019/04/13/python-zip%E5%92%8Cenumerate/"/>
    <id>http://mxxhcm.github.io/2019/04/13/python-zip和enumerate/</id>
    <published>2019-04-13T06:59:12.000Z</published>
    <updated>2019-05-06T16:22:27.708Z</updated>
    
    <content type="html"><![CDATA[<h2 id="zip-function"><a href="#zip-function" class="headerlink" title="zip function"></a>zip function</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">a = np.zeros((<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>))</span><br><span class="line">b = np.zeros((<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>))</span><br><span class="line">c = np.zeros((<span class="number">2</span>,<span class="number">2</span>,<span class="number">2</span>))</span><br><span class="line">d = zip(a,b,c)   </span><br><span class="line">print(list(d))        </span><br><span class="line">d = list(zip(a,b,c))</span><br><span class="line">e,f,g = d</span><br></pre></td></tr></table></figure><p>这里d是一个什么呢，是多个tuple，数量是min(len(a),len(b),len(c))，每一个element是一个tuple，这个tuple的内容为(a[0],b[0],c[0])，….<br>打印出list(d)是一个list，这个list的长度为min(len(a),len(b),len(c))每一个element是一个tuple，tuple的形状是((2,2),(2,2),(2,2))<br>用zip的话，就是看一下它的len，然后在第一维上对他们进行拼接，形成多个新的元组。<br>例子<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a = (<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">b = (<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">c = (<span class="number">4</span>,<span class="number">5</span>)</span><br><span class="line">d = zip(a,b,c)</span><br><span class="line">print(list(c))</span><br></pre></td></tr></table></figure></p><blockquote><p>[(2,3),(3,4),(4,5)]    </p></blockquote><p>相当于吧tuple a和tuple b分别当做一个list的一个元组，然后结合成一个新的tuple的list，</p><h2 id="enumerate-iterable-start-0"><a href="#enumerate-iterable-start-0" class="headerlink" title="enumerate(iterable, start=0)"></a>enumerate(iterable, start=0)</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">seasons = [<span class="string">'Spring'</span>, <span class="string">'Summer'</span>, <span class="string">'Fall'</span>, <span class="string">'Winter'</span>]</span><br><span class="line">print(list(enumerate(seasons)))</span><br><span class="line">print(list(enumerate(seasons, start=<span class="number">1</span>)))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> enumerate(seansons):</span><br><span class="line">   print(i)</span><br></pre></td></tr></table></figure><blockquote><p>[(0, ‘Spring’), (1, ‘Summer’), (2, ‘Fall’), (3, ‘Winter’)]<br>[(1, ‘Spring’), (2, ‘Summer’), (3, ‘Fall’), (4, ‘Winter’)]<br>(0, ‘Spring’)<br>(1, ‘Summer’)<br>(2, ‘Fall’)<br>(3, ‘Winter’)</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;zip-function&quot;&gt;&lt;a href=&quot;#zip-function&quot; class=&quot;headerlink&quot; title=&quot;zip function&quot;&gt;&lt;/a&gt;zip function&lt;/h2&gt;&lt;figure class=&quot;highlight python&quot;&gt;
      
    
    </summary>
    
      <category term="python" scheme="http://mxxhcm.github.io/categories/python/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>python time</title>
    <link href="http://mxxhcm.github.io/2019/04/13/python-time/"/>
    <id>http://mxxhcm.github.io/2019/04/13/python-time/</id>
    <published>2019-04-13T06:52:30.000Z</published>
    <updated>2019-05-06T16:22:27.708Z</updated>
    
    <content type="html"><![CDATA[<h2 id="time（time-library-datetime-library-panda-Timestamp-）"><a href="#time（time-library-datetime-library-panda-Timestamp-）" class="headerlink" title="time（time library,datetime library,panda.Timestamp()）"></a>time（time library,datetime library,panda.Timestamp()）</h2><p>import time</p><h3 id="获得当前时间"><a href="#获得当前时间" class="headerlink" title="获得当前时间"></a>获得当前时间</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">time.time()        <span class="comment">#获得当前timestamp</span></span><br></pre></td></tr></table></figure><h3 id="time-localtime-timestamp"><a href="#time-localtime-timestamp" class="headerlink" title="time.localtime(timestamp)"></a>time.localtime(timestamp)</h3><p>得到一个struct_time<br>time.struct_time(tm_year=2018….)</p><h3 id="将struct-time转换成string"><a href="#将struct-time转换成string" class="headerlink" title="将struct time转换成string"></a>将struct time转换成string</h3><blockquote><p>Convert a tuple or struct_time representing a time as returned by gmtime() or localtime() to a string as specified by the format argument.If t is not provided,the current time as returned by localtime() is used.</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line">time.strftime(format,t)        <span class="comment">#将一个struct_time表示为一个格式化字符串</span></span><br><span class="line">time.strftime(<span class="string">"%Y-%m-%d %H:%M:%S"</span>,time.localtime())</span><br></pre></td></tr></table></figure><h3 id="将一个string类型的事件转换成struct-time"><a href="#将一个string类型的事件转换成struct-time" class="headerlink" title="将一个string类型的事件转换成struct time"></a>将一个string类型的事件转换成struct time</h3><blockquote><p>Parse a string representing a time accroding to a format.The return value is a struct_time as returned by gmtime() or localtime()</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line">time.strptime(<span class="string">"a string representing a time"</span>,<span class="string">"a format"</span>)    <span class="comment">#将某个format表示的time转化为一个struct_time()</span></span><br><span class="line">time.strptime(<span class="string">"2014-02-01 00:00:00"</span>,<span class="string">"%Y-%m-%d %H:%M:%S"</span>)</span><br></pre></td></tr></table></figure><h3 id="time-mktime"><a href="#time-mktime" class="headerlink" title="time.mktime()"></a>time.mktime()</h3><p>将时间t转换成timestamp</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;time（time-library-datetime-library-panda-Timestamp-）&quot;&gt;&lt;a href=&quot;#time（time-library-datetime-library-panda-Timestamp-）&quot; class=&quot;headerl
      
    
    </summary>
    
      <category term="python" scheme="http://mxxhcm.github.io/categories/python/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>python文件和目录操作(os和shutil)</title>
    <link href="http://mxxhcm.github.io/2019/04/13/python-file-dir/"/>
    <id>http://mxxhcm.github.io/2019/04/13/python-file-dir/</id>
    <published>2019-04-13T06:51:26.000Z</published>
    <updated>2019-10-11T05:30:51.354Z</updated>
    
    <content type="html"><![CDATA[<h2 id="文件和目录操作（os库和shutil库）"><a href="#文件和目录操作（os库和shutil库）" class="headerlink" title="文件和目录操作（os库和shutil库）"></a>文件和目录操作（os库和shutil库）</h2><p>import os</p><h2 id="查看信息"><a href="#查看信息" class="headerlink" title="查看信息"></a>查看信息</h2><p>不是函数，而是属性<br>os.linesep   #列出当前平台的行终止符<br>os.name    #列出当前的平台信息</p><h2 id="环境变量"><a href="#环境变量" class="headerlink" title="环境变量"></a>环境变量</h2><p>os.getenv(key, default=None)<br>如果key存在，返回key对应的值，否则返回默认值None，也可以指定默认返回值。</p><h2 id="列出目录"><a href="#列出目录" class="headerlink" title="列出目录"></a>列出目录</h2><p>file_dir_list = os.listdir(parent_dir)    #列出某个目录下的文件和目录，默认的话为当前目录<br>parent_dir 是一个目录<br>file_dir_list是一个list</p><p>os.path.exists(pathname)    #判断pathname是否存在<br>os.path.isdir(pathname)    #判断pathname是否是目录<br>os.path.isfile(pathname)    #判断pathname是否是文件<br>os.path.isabs(pathname)    #判断pathname是否是绝对路径</p><p>os.path.basename(pathname)    # 列出pathname的dir<br>os.path.dirname(pathname)        # 列出pathname的file name<br>os.path.split(pathname)    #将pathname分为dir和filename<br>os.path.split(pathname)    #将pathname的扩展名分离出来</p><p>os.path.join(“dir_name”,”file_name”)    # 拼接两个路径</p><p>os.getcwd()    #获得当前路径<br>os.chdir(pathname)    #改变当前路径<br>os.path.expanduser(pathname)    #如果pathname中包含”~”，将其替换成/homre/user/</p><h2 id="创建和删除"><a href="#创建和删除" class="headerlink" title="创建和删除"></a>创建和删除</h2><p>os.mkdir(pathname)    #创建新目录<br>os.rmdir(pathname)    #删除目录<br>os.makedirs(“/home/mxxhcm/Documents/“)    #创建多级目录<br>os.removedirs()    #删除多个目录<br>os.remove(file_pathname)    #删除文件</p><p>os.rename(old_pathname,new_pathname)    #重命名</p><h2 id="打开文件"><a href="#打开文件" class="headerlink" title="打开文件"></a>打开文件</h2><p>对于open文件来说，共有三种模式，分别为w,a,r<br>r的话，为只读，读取一个不存在的文件，会报错<br>r+的话，为可读写，读取一个不存在的文件，会报错<br>a的话，为追加读，读取一个不存在的文件，会创建该文件<br>w的话，为写入文件，读取一个不存在的文件，会创建改文件，打开一个存在的同名文件，会删除该文件，创建一个新的文件</p><h2 id="读取文件"><a href="#读取文件" class="headerlink" title="读取文件"></a>读取文件</h2><p>fp = open(file_path_name,”r+”)</p><h3 id="read-将文件读到一个字符串中"><a href="#read-将文件读到一个字符串中" class="headerlink" title="read()将文件读到一个字符串中"></a>read()将文件读到一个字符串中</h3><p>file_str = fp.read()<br>fp.read()会返回一个字符串，包含换行符</p><h3 id="readline"><a href="#readline" class="headerlink" title="readline()"></a>readline()</h3><p>for file_str in fp:<br>    print(file_str)<br>这里的file_str是一个str类型变量</p><h3 id="readlines-将文件读到一个列表中"><a href="#readlines-将文件读到一个列表中" class="headerlink" title="readlines()将文件读到一个列表中"></a>readlines()将文件读到一个列表中</h3><p>list(fp)<br>file_list = fp.readlines()<br>filt_list是一个list变量</p><h2 id="关闭文件"><a href="#关闭文件" class="headerlink" title="关闭文件"></a>关闭文件</h2><p>fp.close()<br>或者<br>with open(file_pathname, “r”) as f:<br>    file_str = fp.read()<br>当跳出这个语句块的时候，文件已经别关闭了。</p><h2 id="复制文件"><a href="#复制文件" class="headerlink" title="复制文件"></a>复制文件</h2><p>shutil.move(‘test’,’test_move’)    # 递归的将文件或者目录移动到另一个位置。如果目标位置是一个目录，移动到这个目录里，如果目标已经存在而且不是一个目录，可能会用os.rename()重命名<br>shutil.copyfile(src,dst) #复制文件内容，metadata没有复制<br>shutil.copymode(src,dst) #copy权限。文件内容，owner和group不变。<br>shutil.copystat(src,dst)    #copy权限，各种时间以及flags位。文件内容，owner，group不变<br>shutil.copy(src,dst)    #copy file,权限为也会被copied<br>shutil.copy2(src,dst)  #和先后调用shutil.copy()和shutil.copystat()函数一样<br>shutil.copytree(src,dst,symlinks=False,ignore=None)  #递归的将str目录结构复制到dst，dst位置必须不存在，目录的权限和时间用copystat来复制，文件的赋值用copy2()来复制<br>shutil.rmtree(path[,ignore_errors[,onerror]])   #删除一个完整的目录，无论目录是否为空</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>1.<a href="https://www.zhihu.com/question/48161511/answer/445852429" target="_blank" rel="noopener">https://www.zhihu.com/question/48161511/answer/445852429</a><br>2.<a href="https://www.geeksforgeeks.org/python-os-getenv-method/" target="_blank" rel="noopener">https://www.geeksforgeeks.org/python-os-getenv-method/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;文件和目录操作（os库和shutil库）&quot;&gt;&lt;a href=&quot;#文件和目录操作（os库和shutil库）&quot; class=&quot;headerlink&quot; title=&quot;文件和目录操作（os库和shutil库）&quot;&gt;&lt;/a&gt;文件和目录操作（os库和shutil库）&lt;/h2&gt;&lt;
      
    
    </summary>
    
      <category term="python" scheme="http://mxxhcm.github.io/categories/python/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>python regex</title>
    <link href="http://mxxhcm.github.io/2019/04/13/python-regex/"/>
    <id>http://mxxhcm.github.io/2019/04/13/python-regex/</id>
    <published>2019-04-13T06:50:41.000Z</published>
    <updated>2019-06-06T07:47:18.093Z</updated>
    
    <content type="html"><![CDATA[<h2 id="regex-examples"><a href="#regex-examples" class="headerlink" title="regex examples"></a>regex examples</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> regex <span class="keyword">as</span> re</span><br><span class="line"></span><br><span class="line"><span class="comment"># 找出每一行的数字</span></span><br><span class="line">string = <span class="string">"""a9apple1234</span></span><br><span class="line"><span class="string">2banana5678</span></span><br><span class="line"><span class="string">a3coconut9012"""</span></span><br><span class="line">pattern = <span class="string">"[0-9]+"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># search</span></span><br><span class="line">result = re.search(pattern, string)</span><br><span class="line">print(type(result))</span><br><span class="line">print(result[<span class="number">0</span>])</span><br><span class="line">print(result.group(<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># match</span></span><br><span class="line"><span class="comment"># 即使设置了MULTILINE模式，也只会匹配string的开头而不是每一行的开头</span></span><br><span class="line">result = re.match(pattern, string, re.S| re.M)  </span><br><span class="line">print(type(result))</span><br><span class="line"><span class="comment"># print(result[0])</span></span><br><span class="line"><span class="comment"># print(result.group(0))</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># findall</span></span><br><span class="line">result = re.findall(pattern, string)</span><br><span class="line">print(type(result))</span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure><h2 id="语法"><a href="#语法" class="headerlink" title="语法"></a>语法</h2><p>.   匹配除了newline的任意character，如果要匹配newline，需要添加re.DOTALL flag<br>*  重复至少$0$次<br>+  重复至少$1$次<br>?   重复$0$次或者$1$次<br>{}  重复多少次，如a{3,5}表示重复$3-5$次<br>[]  匹配方括号内的内容,如[1-9]表示匹配$1-9$中任意一个<br>^   matching the start of the string<br>$   matching the end os the string<br>+,*.?    都是贪婪匹配，如果加一个?为非贪婪匹配<br>+?,*?,??    为非贪婪匹配<br>()  匹配括号内的正则表达式，表示一个group的开始和结束<br>|   或<br>\number<br>\b  匹配empty string<br>\B<br>\d  匹配数字<br>\D  匹配非数字<br>\s  匹配空白符[ \t\n\r\f\v]<br>\S  匹配非空白符<br>\w  匹配unicode<br>\W<br>\A<br>\Z</p><h2 id="模块"><a href="#模块" class="headerlink" title="模块"></a>模块</h2><ul><li>re.compile(patern, flags=0)</li><li>re.match(pattern, string, flags=0)</li><li>re.fullmatch(pattern,string,flags=0)</li><li>re.search(pattern, string, flags=0)</li><li>re.split(pattern, string, maxflit=0, flags=0)</li><li>re.findall(pattern,string,flags=0)</li><li>re.sub(pattern,repl,string,count=0,flags=0)</li><li>re.subn(pattern,repl,string,count=0,flags=0)</li></ul><h3 id="flags"><a href="#flags" class="headerlink" title="flags"></a>flags</h3><blockquote><p>flags can be re.DEBUG, re.I, re.IGNORECASE, re.L, re.LOCALE, re.M, re.MULTILINE, re.S, re.DOTALL, re.U, re.UNICODE, re.X, re.VERBOSE</p></blockquote><ul><li>re.I(re.IGNORECASE) 忽略大小写</li><li>re.L(re.LOCALE) </li><li>re.M(re.MULTILINE) 多行模式，设置以后.匹配newline。指定re.S时，’^’匹配string的开始和each line的开始(紧跟着each newline); ‘$’匹配string的结束和each line的结束($在newline之前，immediately preceding each newline)。如果不指定的话, ‘^’只匹配string的开始,’$’只匹配string的结束和immediately before the newline (if any) at the end of the string，对应inline flag (?m).</li><li>re.S(re.DOTALL)  </li><li>re.U(re.UNICODE)</li><li>re.X(re.VERBOSE)</li><li>re.DEBUG</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> regex <span class="keyword">as</span> re</span><br><span class="line"></span><br><span class="line">print(re.I)</span><br><span class="line">print(re.IGNORECASE)</span><br><span class="line">print(re.L)</span><br><span class="line">print(re.LOCALE)</span><br><span class="line">print(re.M)</span><br><span class="line">print(re.MULTILINE)</span><br><span class="line">print(re.S)</span><br><span class="line">print(re.DOTALL)</span><br><span class="line">print(re.U)</span><br><span class="line">print(re.UNICODE)</span><br><span class="line">print(re.X)</span><br><span class="line">print(re.VERBOSE)</span><br><span class="line">print(re.DEBUG)</span><br><span class="line"></span><br><span class="line">print(re.M <span class="keyword">is</span> re.MULTILINE)</span><br><span class="line">print(re.I <span class="keyword">is</span> re.IGNORECASE)</span><br></pre></td></tr></table></figure><p>re.M例子<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">text = <span class="string">"""First line.</span></span><br><span class="line"><span class="string">Second line.</span></span><br><span class="line"><span class="string">Third line."""</span></span><br><span class="line"></span><br><span class="line">pattern = <span class="string">"^.*$"</span>  <span class="comment"># 匹配从开始到结束的任何字符</span></span><br><span class="line"><span class="comment"># 默认情况下， . 不匹配newlines，所以默认情况下不会有任何匹配结果，因为$之前有newline，而.不能匹配</span></span><br><span class="line"><span class="comment"># re.search(pattern, text) is None  # Nothing matches!</span></span><br><span class="line">print(re.search(pattern, text))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果设置MULTILINE模式, $匹配每一行的结尾，这个时候第一行就满足要求了，设置MULTILINE模式后，$匹配string的结尾和每一行的结尾（each newline之前)</span></span><br><span class="line">print(re.search(pattern, text, re.M).group())</span><br><span class="line"><span class="comment"># First line.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果同时设置MULTILINE和DOTALL模式, .能够匹配newlines，所以第一行和第二行的newline都匹配了，在贪婪模式下，就匹配了整个字符串。</span></span><br><span class="line">print(re.search(pattern, text, re.M | re.S).group())</span><br><span class="line"><span class="comment"># First line.</span></span><br><span class="line"><span class="comment"># Second line.</span></span><br><span class="line"><span class="comment"># Third line.</span></span><br></pre></td></tr></table></figure></p><h3 id="re-compile-patern-flags-0"><a href="#re-compile-patern-flags-0" class="headerlink" title="re.compile(patern, flags=0)"></a>re.compile(patern, flags=0)</h3><p>将一个正则表达式语句编译成一个正则表达式对象，可以调用正则表达式的match()和search()函数进行matching。</p><blockquote><p>complie a regular expression pattern into a regular expression object,which can be used for matching using its match() and search()</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">str = <span class="string">"https://abc https://dcdf https://httpfn https://hello"</span></span><br><span class="line"><span class="keyword">import</span> regex <span class="keyword">as</span> re</span><br><span class="line"></span><br><span class="line">prog = re.compile(pattern)</span><br><span class="line">results = prog.match(string)</span><br><span class="line"><span class="comment"># 上面两行等价于下面一行</span></span><br><span class="line"></span><br><span class="line">results = re.match(pattern, string)</span><br></pre></td></tr></table></figure><h3 id="re-match-pattern-string-flags-0-or-re-fullmatch-pattern-string-flags-0"><a href="#re-match-pattern-string-flags-0-or-re-fullmatch-pattern-string-flags-0" class="headerlink" title="re.match(pattern, string, flags=0) or re.fullmatch(pattern,string,flags=0)"></a>re.match(pattern, string, flags=0) or re.fullmatch(pattern,string,flags=0)</h3><p>在给定的string开始位置进行查找，返回一个match object。<strong>即使设置了MULTILINE mode, re.match()也只会在string的开始而不是each line的每一行开始匹配。</strong></p><h3 id="re-search-pattern-string-flags-0"><a href="#re-search-pattern-string-flags-0" class="headerlink" title="re.search(pattern, string, flags=0)"></a>re.search(pattern, string, flags=0)</h3><p>在给定的string任意位置进行查找，返回一个match object。</p><blockquote><p>locat a match anywhere in string</p></blockquote><h3 id="search-vs-match"><a href="#search-vs-match" class="headerlink" title="search() vs. match()"></a>search() vs. match()</h3><p>re.macth()在string的开头查找，而re.search在string的任意位置查找，他们都返回match object对象。如果不匹配，返回None。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">import re</span><br><span class="line"></span><br><span class="line">match1 = re.match(&quot;cd&quot;, &quot;abcdef&quot;)     # match</span><br><span class="line">match2 = re.search(&quot;cd&quot;, &quot;abcdef&quot;)    # search</span><br><span class="line">print(match1)</span><br><span class="line">print(match2)</span><br><span class="line">print(match2.group(0))</span><br><span class="line"># None</span><br><span class="line"># &lt;regex.Match object; span=(2, 4), match=&apos;cd&apos;&gt;</span><br><span class="line"># cd</span><br><span class="line"></span><br><span class="line">with open(&quot;content.txt&quot;, &quot;r&quot;) as f:</span><br><span class="line">    s = f.read()</span><br><span class="line">match3 = re.match(&quot;cd&quot;, s)     # match</span><br><span class="line">match4 = re.search(&quot;cd&quot;, s)</span><br><span class="line">print(match3)</span><br><span class="line">print(match4)</span><br><span class="line"># None</span><br><span class="line"># &lt;regex.Match object; span=(4, 6), match=&apos;cd&apos;&gt;</span><br></pre></td></tr></table></figure></p><h3 id="re-findall-pattern-string-flags-0"><a href="#re-findall-pattern-string-flags-0" class="headerlink" title="re.findall(pattern,string,flags=0)"></a>re.findall(pattern,string,flags=0)</h3><p>查找字符string所有匹配pattern的字符<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> regex <span class="keyword">as</span> re</span><br><span class="line">str = <span class="string">"https://abc https://dcdf https://httpfn https://hello"</span></span><br><span class="line">p2 = <span class="string">"https.+? "</span>    <span class="comment"># pay attention to space here</span></span><br><span class="line">results = re.findall(p2,str)</span><br></pre></td></tr></table></figure></p><blockquote><p>[‘<a href="https://abc" target="_blank" rel="noopener">https://abc</a> ‘, ‘<a href="https://dcdf" target="_blank" rel="noopener">https://dcdf</a> ‘, ‘<a href="https://httpfn" target="_blank" rel="noopener">https://httpfn</a> ‘]    # pay attention to the last ,since the end of str is \n</p></blockquote><h3 id="re-split-pattern-string-maxflit-0-flags-0"><a href="#re-split-pattern-string-maxflit-0-flags-0" class="headerlink" title="re.split(pattern, string, maxflit=0, flags=0)"></a>re.split(pattern, string, maxflit=0, flags=0)</h3><p>按照patten对string进行分割<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> regex <span class="keyword">as</span> re</span><br><span class="line">str = <span class="string">"https://abc https://dcdf https://httpfn https://hello"</span></span><br><span class="line">p1 = <span class="string">" "</span></span><br><span class="line">results = re.split(p1,str)</span><br></pre></td></tr></table></figure></p><blockquote><p>[‘<a href="https://abc" target="_blank" rel="noopener">https://abc</a>‘, ‘<a href="https://dcdf" target="_blank" rel="noopener">https://dcdf</a>‘, ‘<a href="https://httpfn" target="_blank" rel="noopener">https://httpfn</a>‘, ‘<a href="https://hello" target="_blank" rel="noopener">https://hello</a>‘]</p></blockquote><h3 id="re-sub-pattern-repl-string-count-0-flags-0"><a href="#re-sub-pattern-repl-string-count-0-flags-0" class="headerlink" title="re.sub(pattern,repl,string,count=0,flags=0)"></a>re.sub(pattern,repl,string,count=0,flags=0)</h3><h3 id="re-subn-pattern-repl-string-count-0-flags-0"><a href="#re-subn-pattern-repl-string-count-0-flags-0" class="headerlink" title="re.subn(pattern,repl,string,count=0,flags=0)"></a>re.subn(pattern,repl,string,count=0,flags=0)</h3><h3 id="…"><a href="#…" class="headerlink" title="…"></a>…</h3><h2 id="正则表达式对象-regular-express-object"><a href="#正则表达式对象-regular-express-object" class="headerlink" title="正则表达式对象(regular express object)"></a>正则表达式对象(regular express object)</h2><p>class re.RegexObject<br>只有re.compile()函数会产生正则表达式对象，正则</p><blockquote><p>only re.compile() will create a direct regular express object,<br>it’s a special class which design for re.compile().<br>正则表达式对象支持下列方法和属性</p><ul><li>match(string[,pos[,endpos]])</li><li>search(string[,pos[,endpos]])</li><li>findall(string[,pos[,endpos]])</li><li>split(string,maxsplit=0)</li><li>sub()</li><li>flags</li><li>groups</li><li>groupindex</li><li>pattern</li></ul></blockquote><h3 id="match-string-pos-endpos"><a href="#match-string-pos-endpos" class="headerlink" title="match(string[,pos[,endpos]])"></a>match(string[,pos[,endpos]])</h3><h3 id="search-string-pos-endpos"><a href="#search-string-pos-endpos" class="headerlink" title="search(string[,pos[,endpos]])"></a>search(string[,pos[,endpos]])</h3><h3 id="findall-string-pos-endpos"><a href="#findall-string-pos-endpos" class="headerlink" title="findall(string[,pos[,endpos]])"></a>findall(string[,pos[,endpos]])</h3><h3 id="split-string-maxsplit-0"><a href="#split-string-maxsplit-0" class="headerlink" title="split(string,maxsplit=0)"></a>split(string,maxsplit=0)</h3><h3 id="sub"><a href="#sub" class="headerlink" title="sub()"></a>sub()</h3><h3 id="flags-1"><a href="#flags-1" class="headerlink" title="flags"></a>flags</h3><h3 id="groups"><a href="#groups" class="headerlink" title="groups"></a>groups</h3><h3 id="groupindex"><a href="#groupindex" class="headerlink" title="groupindex"></a>groupindex</h3><h3 id="pattern"><a href="#pattern" class="headerlink" title="pattern"></a>pattern</h3><h2 id="匹配对象-match-objects"><a href="#匹配对象-match-objects" class="headerlink" title="匹配对象(match objects)"></a>匹配对象(match objects)</h2><p>class re.MatchObject<br>匹配是否成功</p><blockquote><p>match objects have a boolean value of True.</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">match = re.search(pattern, string)</span><br><span class="line">if match:</span><br><span class="line">   processs(match)</span><br></pre></td></tr></table></figure><p>MatchObject支持以下方法和属性</p><ul><li>group([group1,..])</li><li>groups([default=None])</li><li>groupdict(default=None)</li><li>start([group])</li><li>end([group])</li><li>span([group])</li><li>pos</li><li>endpos</li><li>lstindex</li><li>lastgroup</li><li>re</li><li>string</li></ul><h3 id="group-group1"><a href="#group-group1" class="headerlink" title="group([group1,..])"></a>group([group1,..])</h3><p>group的话pattern需要多个()</p><h3 id="groups-default"><a href="#groups-default" class="headerlink" title="groups([default])"></a>groups([default])</h3><p>返回一个元组</p><blockquote><p>return a tuple containing all the subgroups of the match.</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">re.match(<span class="string">r"(\d+)\.(\d+)"</span>,<span class="string">"24.1632"</span>)</span><br><span class="line">m.groups()</span><br></pre></td></tr></table></figure><blockquote><p>(‘24’,’1632’)</p></blockquote><p>show default<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">m = re.match(<span class="string">r"(\d+)\.?(\d+)?"</span>, <span class="string">"24"</span>)</span><br><span class="line">m.groups()      <span class="comment"># Second group defaults to None.</span></span><br></pre></td></tr></table></figure></p><blockquote><p>(‘24’, None)</p></blockquote><p>change default to 0<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">m.groups(<span class="string">'0'</span>)  <span class="comment"># Now, the second group defaults to '0'.</span></span><br><span class="line">(<span class="string">'24'</span>, <span class="string">'0'</span>)</span><br></pre></td></tr></table></figure></p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>1.<a href="https://stackoverflow.com/questions/180986/what-is-the-difference-between-re-search-and-re-match" target="_blank" rel="noopener">https://stackoverflow.com/questions/180986/what-is-the-difference-between-re-search-and-re-match</a><br>2.<a href="https://devdocs.io/python~3.7/library/re" target="_blank" rel="noopener">https://devdocs.io/python~3.7/library/re</a><br>3.<a href="https://mail.python.org/pipermail/python-list/2014-July/674576.html" target="_blank" rel="noopener">https://mail.python.org/pipermail/python-list/2014-July/674576.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;regex-examples&quot;&gt;&lt;a href=&quot;#regex-examples&quot; class=&quot;headerlink&quot; title=&quot;regex examples&quot;&gt;&lt;/a&gt;regex examples&lt;/h2&gt;&lt;figure class=&quot;highlight 
      
    
    </summary>
    
      <category term="python" scheme="http://mxxhcm.github.io/categories/python/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="正则表达式" scheme="http://mxxhcm.github.io/tags/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/"/>
    
  </entry>
  
  <entry>
    <title>python数组初始化</title>
    <link href="http://mxxhcm.github.io/2019/04/13/python-%E6%95%B0%E7%BB%84%E5%88%9D%E5%A7%8B%E5%8C%96/"/>
    <id>http://mxxhcm.github.io/2019/04/13/python-数组初始化/</id>
    <published>2019-04-13T06:49:35.000Z</published>
    <updated>2019-06-09T03:10:21.669Z</updated>
    
    <content type="html"><![CDATA[<h2 id="array-initialize"><a href="#array-initialize" class="headerlink" title="array initialize"></a>array initialize</h2><p>array_one_dimension =  [ 0 for i in range(cols)]<br>array_multi_dimension  = [[0 for i in range(cols)] for j in range(rows)]</p><h2 id="numpy"><a href="#numpy" class="headerlink" title="numpy"></a>numpy</h2><ul><li>numpy.array()</li><li>numpy.zeros()</li><li>numpy.empty()</li></ul><p>返回np.ndarray数组</p><h3 id="np-ndarray属性"><a href="#np-ndarray属性" class="headerlink" title="np.ndarray属性"></a>np.ndarray属性</h3><p>ndarray.shape        #array的shape<br>ndarray.ndim            #array的维度<br>ndarray.size            #the number of ndarray in array<br>ndarray.dtype        #type of the number in array<br>ndarray.itemsize        #size of the element in array<br>array[array &gt; 0].size    #统计一个数组有多少个非零元素，不论array的维度是多少</p><h3 id="numpy-array"><a href="#numpy-array" class="headerlink" title="numpy.array()"></a>numpy.array()</h3><p>np.array(object,dtype=None,copy=True,order=False,subok=False,ndim=0)</p><h3 id="numpy-zeros"><a href="#numpy-zeros" class="headerlink" title="numpy.zeros()"></a>numpy.zeros()</h3><p>np.zeros(shape,dtype=float,order=’C’)</p><h3 id="numpy-empty"><a href="#numpy-empty" class="headerlink" title="numpy.empty()"></a>numpy.empty()</h3><p>np.empty(shape,dtype=float,order=’C’)</p><h3 id="numpy-random-randn-shape"><a href="#numpy-random-randn-shape" class="headerlink" title="numpy.random.randn(shape)"></a>numpy.random.randn(shape)</h3><p>np.random.randn(3,4)</p><h3 id="numpy-arange"><a href="#numpy-arange" class="headerlink" title="numpy.arange()"></a>numpy.arange()</h3><h3 id="numpy-linspace"><a href="#numpy-linspace" class="headerlink" title="numpy.linspace()"></a>numpy.linspace()</h3>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;array-initialize&quot;&gt;&lt;a href=&quot;#array-initialize&quot; class=&quot;headerlink&quot; title=&quot;array initialize&quot;&gt;&lt;/a&gt;array initialize&lt;/h2&gt;&lt;p&gt;array_one_dime
      
    
    </summary>
    
      <category term="python" scheme="http://mxxhcm.github.io/categories/python/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="numpy" scheme="http://mxxhcm.github.io/tags/numpy/"/>
    
  </entry>
  
  <entry>
    <title>python2和python3中的dict</title>
    <link href="http://mxxhcm.github.io/2019/04/13/python-dict/"/>
    <id>http://mxxhcm.github.io/2019/04/13/python-dict/</id>
    <published>2019-04-13T06:46:26.000Z</published>
    <updated>2019-05-06T16:22:27.708Z</updated>
    
    <content type="html"><![CDATA[<h2 id="python2和python3的dict"><a href="#python2和python3的dict" class="headerlink" title="python2和python3的dict"></a>python2和python3的dict</h2><h3 id="将object转换为dict"><a href="#将object转换为dict" class="headerlink" title="将object转换为dict"></a>将object转换为dict</h3><p>vars([object]) -&gt; dictionary</p><h3 id="python2-dict"><a href="#python2-dict" class="headerlink" title="python2 dict"></a>python2 dict</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">m_dict = &#123;&apos;a&apos;: 10, &apos;b&apos;: 20&#125;</span><br><span class="line"></span><br><span class="line">values = m_dict.values()</span><br><span class="line">print(type(values))</span><br><span class="line">print(values)</span><br><span class="line">print(&quot;\n&quot;)</span><br><span class="line"></span><br><span class="line">items = m_dict.items()</span><br><span class="line">print(type(items))</span><br><span class="line">print(items)</span><br><span class="line">print(&quot;\n&quot;)</span><br><span class="line"></span><br><span class="line">keys = m_dict.keys()</span><br><span class="line">print(type(keys))</span><br><span class="line">print(keys)</span><br><span class="line">print(&quot;\n&quot;)</span><br><span class="line"></span><br><span class="line">l_values = list(values)</span><br><span class="line">print(type(l_values))</span><br><span class="line">print(l_values)</span><br><span class="line"></span><br><span class="line">输出：</span><br></pre></td></tr></table></figure><h3 id="python3-dict"><a href="#python3-dict" class="headerlink" title="python3 dict"></a>python3 dict</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">m_dict = &#123;&apos;a&apos;: 10, &apos;b&apos;: 20&#125;</span><br><span class="line"></span><br><span class="line">values = m_dict.values()</span><br><span class="line">print(type(values))</span><br><span class="line">print(values) print(&quot;\n&quot;)</span><br><span class="line"></span><br><span class="line">items = m_dict.items()</span><br><span class="line">print(type(items))</span><br><span class="line">print(items)</span><br><span class="line">print(&quot;\n&quot;)</span><br><span class="line"></span><br><span class="line">keys = m_dict.keys()</span><br><span class="line">print(type(keys))</span><br><span class="line">print(keys)</span><br><span class="line">print(&quot;\n&quot;)</span><br><span class="line"></span><br><span class="line">l_values = list(values)</span><br><span class="line">print(type(l_values))</span><br><span class="line">print(l_values)</span><br></pre></td></tr></table></figure><p>输出：</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;python2和python3的dict&quot;&gt;&lt;a href=&quot;#python2和python3的dict&quot; class=&quot;headerlink&quot; title=&quot;python2和python3的dict&quot;&gt;&lt;/a&gt;python2和python3的dict&lt;/h2&gt;&lt;
      
    
    </summary>
    
      <category term="python" scheme="http://mxxhcm.github.io/categories/python/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>python中的深复制和浅复制</title>
    <link href="http://mxxhcm.github.io/2019/04/13/python-%E6%B7%B1%E5%A4%8D%E5%88%B6%E5%92%8C%E6%B5%85%E5%A4%8D%E5%88%B6/"/>
    <id>http://mxxhcm.github.io/2019/04/13/python-深复制和浅复制/</id>
    <published>2019-04-13T06:43:31.000Z</published>
    <updated>2019-05-06T16:22:27.708Z</updated>
    
    <content type="html"><![CDATA[<h2 id="简单赋值，浅拷贝，深拷贝"><a href="#简单赋值，浅拷贝，深拷贝" class="headerlink" title="简单赋值，浅拷贝，深拷贝"></a>简单赋值，浅拷贝，深拷贝</h2><h3 id="简单赋值"><a href="#简单赋值" class="headerlink" title="简单赋值"></a>简单赋值</h3><h4 id="str"><a href="#str" class="headerlink" title="str"></a>str</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = <span class="string">'hello'</span></span><br><span class="line">b = <span class="string">'hello'</span></span><br><span class="line">c = a</span><br><span class="line">print(id(a),id(b),id(c))</span><br></pre></td></tr></table></figure><blockquote><p>2432356754632  2432356754632  2432356754632</p></blockquote><p>这里打印出a，b，c的id是一样的，因为他们全是指向’hello’这个字符串在内存中的地址<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = <span class="string">'world'</span></span><br><span class="line">print(id(a),id(b),id(c))</span><br></pre></td></tr></table></figure></p><blockquote><p>2432356757376  2432356754632  2432356754632</p></blockquote><p>将a指向一个新的字符串’world’,所以变量a的地址就改变了，指向字符串’world’的地址，但是b和c还是指向字符串’hello’的地址。</p><h4 id="list"><a href="#list" class="headerlink" title="list"></a>list</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = [<span class="string">'hello'</span>]</span><br><span class="line">b = [<span class="string">'hello'</span>]</span><br><span class="line">c = a</span><br><span class="line">print(id(a),id(b),id(c))</span><br></pre></td></tr></table></figure><blockquote><p>2432356788424 2432356797064 2432356788424</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">b = [<span class="string">'world'</span>]</span><br><span class="line">print(id(a),id(b),id(c))</span><br></pre></td></tr></table></figure><blockquote><p>2432356798024 2432356797064 2432356788424</p></blockquote><h4 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h4><p>简单赋值是先给一个变量分配内存，然后把变量的地址赋值给一个变量名。<br>对于一些不可变的类型，比如str，int等，某一个值在内存中的地址是固定的，如果用赋值操作直接指向一个值的话，那么变量名指向的就是这个值在内存中地址。<br>比如a=’hello’,b=’hello’,这样a和b的id是相同的，都指向内存中hello的地址<br>对于一些可变的类型，比如list，因为他是可变的，所以如果用赋值操作指向同一个值的话，那么这几个变量的地址也不一样<br>比如a =[‘hello’],b=[‘hello’],这样a和b的id是不同的，虽然他们指向的值是一样的，</p><h3 id="浅拷贝"><a href="#浅拷贝" class="headerlink" title="浅拷贝"></a>浅拷贝</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">a = [<span class="string">'hello'</span> , [<span class="number">123</span>] ]</span><br><span class="line">b = a[:]</span><br><span class="line">a = [<span class="string">'hello'</span> , [<span class="number">123</span>] ]</span><br><span class="line">b = a[:]</span><br><span class="line">print(a,b)</span><br><span class="line">print(id(a),id(b))</span><br><span class="line">print(id(a[<span class="number">0</span>]),id(a[<span class="number">1</span>]))</span><br><span class="line">print(id(b[<span class="number">0</span>]),id(b[<span class="number">1</span>]))</span><br></pre></td></tr></table></figure><blockquote><p>[‘hello’, [123]] [‘hello’, [123]]<br>2432356775368 2432356775432 2432356754632 2432356774984<br>2432356754632 2432356774984</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;a[<span class="number">0</span>] = <span class="string">'world'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(a,b)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(id(a),id(b))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(id(a[<span class="number">0</span>]),id(a[<span class="number">1</span>]))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(id(b[<span class="number">0</span>]),id(b[<span class="number">1</span>]))</span><br></pre></td></tr></table></figure><blockquote><p>[‘world’, [123]] [‘hello’, [123]]<br>2432356775368 2432356775432<br>2432356756424 2432356774984<br>2432356754632 2432356774984</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a[<span class="number">1</span>].append(<span class="number">3</span>)</span><br><span class="line">print(a,b)</span><br><span class="line">print(id(a),id(b))</span><br><span class="line">print(id(a[<span class="number">0</span>]),id(a[<span class="number">1</span>]))</span><br><span class="line">print(id(b[<span class="number">0</span>]),id(b[<span class="number">1</span>]))</span><br></pre></td></tr></table></figure><blockquote><p>[‘world’, [123, 3]] [‘hello’, [123, 3]]<br>2432356775368 2432356775432<br>2432356756424 2432356774984<br>2432356754632 2432356774984</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">a[<span class="number">1</span>] = [<span class="number">123</span>]</span><br><span class="line">print(a,b)</span><br><span class="line">print(id(a),id(b))</span><br><span class="line">print(id(a[<span class="number">0</span>]),id(a[<span class="number">1</span>]))</span><br><span class="line">print(id(b[<span class="number">0</span>]),id(b[<span class="number">1</span>]))</span><br><span class="line">``` </span><br><span class="line">&gt; [<span class="string">'world'</span>, [<span class="number">123</span>]] [<span class="string">'hello'</span>, [<span class="number">123</span>, <span class="number">3</span>]]</span><br><span class="line"><span class="number">2432356775368</span> <span class="number">2432356775432</span></span><br><span class="line"><span class="number">2432356756424</span> <span class="number">2432356822984</span></span><br><span class="line"><span class="number">2432356754632</span> <span class="number">2432356774984</span></span><br><span class="line"></span><br><span class="line"><span class="comment">### 深拷贝</span></span><br><span class="line">``` python</span><br><span class="line"><span class="keyword">from</span> copy <span class="keyword">import</span> deepcopy</span><br><span class="line">a = [<span class="string">'hello'</span>,[<span class="number">123</span>,<span class="number">234</span>]</span><br><span class="line">b = deepcopy(a)</span><br></pre></td></tr></table></figure><p>a，b以及a，b中任何元素（除了str，int等类型）的地址都是不一样的</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;简单赋值，浅拷贝，深拷贝&quot;&gt;&lt;a href=&quot;#简单赋值，浅拷贝，深拷贝&quot; class=&quot;headerlink&quot; title=&quot;简单赋值，浅拷贝，深拷贝&quot;&gt;&lt;/a&gt;简单赋值，浅拷贝，深拷贝&lt;/h2&gt;&lt;h3 id=&quot;简单赋值&quot;&gt;&lt;a href=&quot;#简单赋值&quot; cla
      
    
    </summary>
    
      <category term="python" scheme="http://mxxhcm.github.io/categories/python/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>python special method</title>
    <link href="http://mxxhcm.github.io/2019/04/13/python-special-method/"/>
    <id>http://mxxhcm.github.io/2019/04/13/python-special-method/</id>
    <published>2019-04-13T06:41:38.000Z</published>
    <updated>2019-05-06T16:22:27.708Z</updated>
    
    <content type="html"><![CDATA[<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>print(object)就是调用了类对象object的<strong>repr</strong>()函数<br>如下代码<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Tem</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">     <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__repr__</span><span class="params">(self)</span>:</span></span><br><span class="line">     <span class="keyword">return</span> <span class="string">"tem class"</span></span><br></pre></td></tr></table></figure></p><p>声明类对象 </p><blockquote><blockquote><blockquote><p>Tem tem<br>下面两行代码的功能是一样的。<br>print(tem)<br>print(repr(tem))</p></blockquote></blockquote></blockquote><h2 id="基本的自定义方法"><a href="#基本的自定义方法" class="headerlink" title="基本的自定义方法"></a>基本的自定义方法</h2><h3 id="object-new"><a href="#object-new" class="headerlink" title="object.new"></a>object.<strong>new</strong></h3><h3 id="object-init"><a href="#object-init" class="headerlink" title="object.init"></a>object.<strong>init</strong></h3><h3 id="object-repr和object-str"><a href="#object-repr和object-str" class="headerlink" title="object.repr和object.str"></a>object.<strong>repr</strong>和object.<strong>str</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Tem</span><span class="params">(object)</span>:</span></span><br><span class="line">  <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TemStr</span><span class="params">(object)</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__str__</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">'foo'</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TemRepr</span><span class="params">(object)</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__repr__</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">'foo'</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TemStrRepr</span><span class="params">(object)</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__repr__</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">'foo'</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__str__</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">'foo_str'</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>: </span><br><span class="line">   tem = Tem() </span><br><span class="line">   print(str(tem)) </span><br><span class="line">   print(repr(tem)) </span><br><span class="line">   tem_str = TemStr() </span><br><span class="line">   print(str(tem_str)) </span><br><span class="line">   print(repr(tem_str)) </span><br><span class="line">   tem_repr = TemRepr() </span><br><span class="line">   print(str(tem_repr)) </span><br><span class="line">   print(repr(tem_repr)) </span><br><span class="line">   tem_str_repr = TemStrRepr() </span><br><span class="line">   print(str(tem_str_repr)) </span><br><span class="line">   print(repr(tem_str_repr))</span><br></pre></td></tr></table></figure><p>单独重载<strong>repr</strong>，<strong>str</strong>也会调用<strong>repr</strong>，<br>但是单独重载<strong>str</strong>,<strong>repr</strong>不会调用它。<br><strong>repr</strong>面向的是程序员，而<strong>str</strong>面向的是普通用户。它们都用来返回一个字符串，这个字符串可以是任何字符串，我觉得这个函数的目的就是将对象转化为字符串。</p><h3 id="object-bytes"><a href="#object-bytes" class="headerlink" title="object.bytes"></a>object.<strong>bytes</strong></h3><h2 id="自定义属性方法"><a href="#自定义属性方法" class="headerlink" title="自定义属性方法"></a>自定义属性方法</h2><h3 id="object-getattr-self-name"><a href="#object-getattr-self-name" class="headerlink" title="object.getattr(self, name)"></a>object.<strong>getattr</strong>(self, name)</h3><h3 id="object-setattr-self-name"><a href="#object-setattr-self-name" class="headerlink" title="object.setattr(self, name)"></a>object.<strong>setattr</strong>(self, name)</h3><h2 id="比较"><a href="#比较" class="headerlink" title="比较"></a>比较</h2><h3 id="object-eq-self-others"><a href="#object-eq-self-others" class="headerlink" title="object.eq(self, others)"></a>object.<strong>eq</strong>(self, others)</h3><h3 id="object-lt-self-others"><a href="#object-lt-self-others" class="headerlink" title="object.lt(self, others)"></a>object.<strong>lt</strong>(self, others)</h3><h3 id="object-le-self-others"><a href="#object-le-self-others" class="headerlink" title="object.le(self, others)"></a>object.<strong>le</strong>(self, others)</h3><h3 id="object-ne-self-others"><a href="#object-ne-self-others" class="headerlink" title="object.ne(self, others)"></a>object.<strong>ne</strong>(self, others)</h3><h3 id="object-gt-self-others"><a href="#object-gt-self-others" class="headerlink" title="object.gt(self, others)"></a>object.<strong>gt</strong>(self, others)</h3><h3 id="object-ge-self-others"><a href="#object-ge-self-others" class="headerlink" title="object.ge(self, others)"></a>object.<strong>ge</strong>(self, others)</h3><h2 id="特殊属性"><a href="#特殊属性" class="headerlink" title="特殊属性"></a>特殊属性</h2><h3 id="object-dict"><a href="#object-dict" class="headerlink" title="object.dict"></a>object.<strong>dict</strong></h3><h3 id="instance-class"><a href="#instance-class" class="headerlink" title="instance.class"></a>instance.<strong>class</strong></h3>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;结论&quot;&gt;&lt;a href=&quot;#结论&quot; class=&quot;headerlink&quot; title=&quot;结论&quot;&gt;&lt;/a&gt;结论&lt;/h2&gt;&lt;p&gt;print(object)就是调用了类对象object的&lt;strong&gt;repr&lt;/strong&gt;()函数&lt;br&gt;如下代码&lt;br&gt;&lt;figu
      
    
    </summary>
    
      <category term="python" scheme="http://mxxhcm.github.io/categories/python/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>reinforcement learning an introduction 第4章笔记</title>
    <link href="http://mxxhcm.github.io/2019/04/07/reinforcement-learning-an-introduction-%E7%AC%AC4%E7%AB%A0%E7%AC%94%E8%AE%B0/"/>
    <id>http://mxxhcm.github.io/2019/04/07/reinforcement-learning-an-introduction-第4章笔记/</id>
    <published>2019-04-07T15:46:17.000Z</published>
    <updated>2019-10-18T12:29:17.952Z</updated>
    
    <content type="html"><![CDATA[<h2 id="原理">原理</h2><p>Policy iteration有两种方式实现，一种是使用两个数组，一个保存原来的值，一个用来进行更新，这种方法是雅克比方法，或者叫同步的方法，因为他可以并行的进行。<br>In-place的方法是高斯赛德尔方法。就是用来解方程组的迭代法。</p><h2 id="dynamic-programming">Dynamic Programming</h2><p>DP指的是给定环境的模型，通常是一个MDP，计算智能体最优策略的一类算法。经典的DP算法应用场景有限，因为它需要环境的模型，计算量很高，但是DP的思路是很重要的。许多其他的算法都是在尽量减少计算量和对环境信息情况，尽可能获得和DP接近的性能。<br>通常我们假定环境是一个有限(finite)的MDP，也就是state, action, reward都是有限的。尽管DP可以应用于连续(continuous)的state和action space，但是只能应用在几个特殊的场景上。一个常见的做法是将连续state和action quantize(量化)，然后使用有限MDP。<br>DP关键在于使用value function寻找好的policy，在找到了满足Bellman optimal equation的optimal value function之后，可以找到optimal policy，参见<a href="https://mxxhcm.github.io/2018/12/21/reinforcement-learning-an-introduction-%E7%AC%AC3%E7%AB%A0%E7%AC%94%E8%AE%B0/">第三章推导</a>：<br>Bellman optimal equation:<br>\begin{align*}<br>v_{*}(s) &amp;= max_a\mathbb{E}\left[R_{t+1}+\gamma v_{*}(S_{t+1})|S_t=s,A_t=a\right] \\<br>&amp;= max_a \sum_{s’,r} p(s’,r|s,a){*}\left[r+\gamma v_{*}(s’)\right]  \tag{1}<br>\end{align*}</p><p>\begin{align*}<br>q_{*}(s,a) &amp;= \mathbb{E}\left[R_{t+1}+\gamma max_{a’}q_{*}(S_{t+1},a’)|S_t=s,A_t = a\right]\\<br>&amp;= \sum_{s’,r} p(s’,r|s,a) \left[r + \gamma max_a q_{*}(s’,a’)\right] \tag{2}<br>\end{align*}</p><h2 id="policy-evaluation-prediction">Policy Evaluation(Prediction)</h2><p>给定一个policy，计算state value function的过程叫做policy evaluation或者prediction problem。<br>根据$v(s)$和它的后继状态$v(s’)$之间的关系：<br>\begin{align*}<br>v_{\pi}(s) &amp;= \mathbb{E}_{\pi}[G_t|S_t = s]\\<br>&amp;= \mathbb{E}_{\pi}\left[R_{t+1}+\gamma G_{t+1}|S_t = s\right]\\<br>&amp;= \sum_a \pi(a|s)\sum_{s’}\sum_rp(s’,r|s,a) \left[r + \gamma \mathbb{E}_{\pi}\left[G_{t+1}|S_{t+1}=s’\right]\right] \tag{3}\\<br>&amp;= \sum_a \pi(a|s)\sum_{s’,r}p(s’,r|s,a) \left[r + \gamma v_{\pi}(s’) \right] \tag{4}\\<br>\end{align*}<br>只要$\gamma \lt 1$或者存在terminal state，那么$v_{\pi}$的必然存在且唯一。这个我觉得是迭代法解方程的条件。数值分析上有证明。<br>如果环境的转换概率$p$是已知的，可以列出方程组，直接求解出每个状态$s$的$v(s)$。这里采用迭代法求解，随机初始化$v_0$，使用式子$(4)$进行更新：<br>\begin{align*}<br>v_{k+1}(s) &amp;= \mathbb{E}\left[R_{t+1} + \gamma v_k(S_{t+1})\ S_t=s\right]\\<br>&amp;= \sum_a \pi(a|s)\sum_{s’,r}p(s’,r|s,a) \left[r + \gamma v_k(s’) \right] \tag{5}<br>\end{align*}<br>直到$v_k=v_{\pi}$到达fixed point，Bellman equation满足这个条件。当$k\rightarrow \infty$时收敛到$v_{\pi}$。这个算法叫做iterative policy evaluation。<br>在每一次$v_k$到$v_{k+1}$的迭代过程中，所有的$v(s)$都会被更新，$s$的旧值被后继状态$s’$的旧值加上reward替换，正如公式$(5)$中体现的那样。这个目标值被称为expected update，因为它是基于所有$s’$的期望计算出来的（利用环境的模型），而不是通过对$s’$采样计算的。<br>在实现iterative policy evaluation的时候，每一次迭代，都需要重新计算所有$s$的值。这里有一个问题，就是你在每次更新$s$的时候，使用的$s’$如果在本次迭代过程中已经被更新过了，那么是使用更新过的$s’$，还是使用没有更新的$s’$，这就和迭代法中的雅克比迭代以及高斯赛德尔迭代很像，如果使用更新后的$s’$，这里我们叫它in-place的算法，否则就不是。具体那种方法收敛的快，还是要看应用场景的，并不是in-place的就一定收敛的快，这是在数值分析上学到的。<br>下面给出in-place版本的iterative policy evation算法伪代码。<br><strong>iterative policy evation 算法</strong><br><strong>输入</strong>需要evaluation的policy $\pi$<br>给出算法的参数：阈值$\theta\gt 0$，当两次更新的差值小于这个阈值的时候，就停止迭代，随机初始化$V(s),\forall s\in S^{+}$，除了$V(terminal) = 0$。<br><strong>Loop</strong><br>$\qquad \delta \leftarrow 0$<br>$\qquad$ <strong>for</strong> each $s\in S$<br>$\qquad\qquad v\leftarrow V(s)$ （保存迭代之前的$V(s)$）<br>$\qquad\qquad V(s)\leftarrow\sum_a \pi(a|s)\sum_{s’,r}p(s’,r|s,a) \left[r + \gamma v_k(s’) \right] $<br>$\qquad\qquad \nabla \leftarrow max(\delta,|v-V(s)|)$<br>$\qquad$<strong>end for</strong><br><strong>until</strong> $\delta \lt \theta$</p><h2 id="policy-improvement">Policy Improvement</h2><p>为什么要进行policy evaluation，或者说为什么要计算value function？<br>其中一个原因是为了找到更好的policy。假设我们已经知道了一个deterministic的策略$\pi$，但是在其中一些状态，我们想要知道是不是有更好的action选择，如$a\neq \pi(s)$的时候，是不是这个改变后的策略会更好。好该怎么取评价，这个时候就可以使用值函数进行评价了，在某个状态，我们选择$a \neq \pi(s)$，在其余状态，依然遵循策略$\pi$。用公式表示为：<br>\begin{align*}<br>q_{\pi}(s,a) &amp;= \mathbb{E}\left[R_{t+1}+\gamma v_{\pi}(S_{t+1})|S_t=s,A_t = a\right]\\<br>&amp;=\sum_{s’,r}p(s’,r|s,a)\left[r+\gamma v_{\pi}(s’)\right] \tag{6}<br>\end{align*}<br>那么，这个值是是比$v(s)$要大还是要小呢？如果比$v(s)$要大，那么这个新的策略就比$\pi$要好。<br>用$\pi$和$\pi’$表示任意一对满足下式的deterministic policy：<br>$$q_{\pi}(s,\pi’(s)) \ge v_{\pi}(s) \tag{7}$$<br>那么$\pi’$至少和$\pi$一样好。可以证明，任意满足$(7)$的$s$都满足下式：<br>$$v_{\pi’}(s) \ge v_{\pi}(s) \tag{8}$$<br>对于我们提到的$\pi$和$\pi’$来说，除了在状态$s$处，$v_{\pi’}(s) = a \neq v_{\pi}(s)$，在其他状态处$\pi$和$\pi’$是一样的，都有$q_{\pi}(s,\pi’(s)) = v_{\pi}(s)$。而在状态$s$处，如果$q_{\pi}(s,a) \gt v_{\pi}(s)$，注意这里$a=\pi’(s)$，那么$\pi’$一定比$\pi$好。<br>证明：<br>\begin{align*}<br>v_{\pi}(s) &amp;\le q_{\pi}(s,\pi’(s))\\<br>&amp; = \mathbb{E}\left[R_{t+1} + \gamma v_{\pi}(S_{t+1})|S_t = s, A_t = \pi’(s) \right]\\<br>&amp; = \mathbb{E}_{\pi’}\left[R_{t+1} + \gamma v_{\pi}(S_{t+1})|S_t = s \right]\\<br>&amp; \le \mathbb{E}_{\pi’}\left[R_{t+1} + \gamma q_{\pi}(S_{t+1},\pi’(S_{t+1}))|S_t = s \right]\\<br>&amp; = \mathbb{E}_{\pi’}\left[ R_{t+1} + \gamma \mathbb{E}_{\pi’}\left[R_{t+2} +\gamma v_{\pi}(S_{t+2})|S_{t+1}, A_{t+1}=\pi’(S_{t+1})|S_t = s \right]\right]\\<br>&amp; = \mathbb{E}_{\pi’}\left[ R_{t+1} + \gamma R_{t+2} +\gamma^2 v_{\pi}(S_{t+2})|S_t = s \right]\\<br>&amp; \le \mathbb{E}_{\pi’}\left[ R_{t+1} + \gamma R_{t+2} +\gamma^2 R_{t+3}  +\gamma^3 v_{\pi}(S_{t+3})|S_t = s \right]\\<br>&amp; \le \mathbb{E}_{\pi’}\left[ R_{t+1} + \gamma R_{t+2} +\gamma^2 R_{t+3}  +\gamma^3 R_{t+4} + \cdots |S_t = s \right]\\<br>&amp;=v_{\pi’}(s)<br>\end{align*}<br>所以，在计算出一个policy的value function的时候，很容易我们就直到某个状态$s$处的变化是好还是坏。扩展到所有状态和所有action的时候，在每个state，根据$q_{\pi}(s,a)$选择处最好的action，这样就得到了一个greedy策略$\pi’$，给出如下定义：<br>\begin{align*}<br>\pi’(s’) &amp;= argmax_{a} q_{\pi}(s,a)\\<br>&amp; = argmax_{a} \mathbb{E}\left[R_{t+1} + \gamma v_{\pi}(S_{t+1} |S_t=a,A_t=a)\right] \tag{9}\\<br>&amp; = argmax_{a} \sum_{s’,r}p(s’,r|s,a)\left[r+v_{\pi}(s’) \right]<br>\end{align*}<br>可以看出来，该策略的定义一定满足式子$(7)$，所以$\pi’$比$\pi$要好或者相等，这就叫做policy improvement。当$\pi’$和$\pi$相等时，，根据式子$(9)$我们有：<br>\begin{align*}<br>v_{\pi’}(s’)&amp; = max_{a} \mathbb{E}\left[R_{t+1} + \gamma v_{\pi’}(S_{t+1} |S_t=a,A_t=a)\right] \tag{9}\\<br>&amp; = max_{a} \sum_{s’,r}p(s’,r|s,a)\left[r+v_{\pi’}(s’) \right]<br>\end{align*}<br>这和贝尔曼最优等式是一样的？？？殊途同归！！！<br>但是，需要说的一点是，目前我们假设的$\pi$和$\pi’$是deterministic，当$\pi$是stochastic情况的时候，其实也是一样的。只不过，原来我们每次选择的是使得$v_{\pi}$最大的action。对于stochastic的情况来说，输出的是每个动作的概率，可能有几个动作都能使得value function最大，那就让这几个动作的概率一样大，比如是$n$个动作，都是$\frac{1}{n}$。</p><h2 id="policy-iteration">Policy Iteration</h2><p>我们已经讲了Policy Evaluation和Policy Improvement，Evalution会计算出一个固定$\pi$的value function，Improvment会根据value function改进这个policy，然后计算出一个新的policy $\pi’$，对于新的策略，我们可以再次进行Evaluation，然后在Improvement，就这样一直迭代，对于有限的MDP，我们可以求解出最优的value function和policy。这就是Policy Iteration算法。</p><p><strong>Policy Iteration算法</strong><br><strong>1.初始化</strong><br>$V(s)\in R,\pi(s) in A(s)$<br>$\qquad$<br><strong>2.Policy Evaluation</strong><br><strong>Loop</strong><br>$\qquad\Delta\leftarrow 0 $<br>$\qquad$ <strong>For</strong> each $s\in S$<br>$\qquad\qquad v\leftarrow V(s)$<br>$\qquad\qquad V(s)\leftarrow \sum_{s’,r}p(s’,r|s,a)\left[r+\gamma V(s’)\right]$<br>$\qquad\qquad \Delta \leftarrow max(\Delta, |v-V(s)|) $<br><strong>until</strong> $\Delta \lt \theta$<br><strong>3.Policy Improvement</strong><br>$policy-stable\leftarrow true$<br><strong>For</strong> each $s \in S$<br>$\qquad old_action = \pi(s)$<br>$\qquad \pi(s) = argmax_a \sum_{s’,a’}p(s’,r|s,a)\left[r+\gamma V(s’)\right]$<br>$\qquad If\ old_action \neq \pi(s), policy-stable\leftarrow false$<br><strong>If policy-stable</strong>，停止迭代，返回$V$和$\pi$，否则回到2.Policy Evalution继续执行。</p><h2 id="value-iteration">Value Iteration</h2><p>从Policy Iteration算法中我们可以看出来，整个算法分为两步，第一步是Policy Evaluation，第二步是Policy Improvement。而每一次Policy Evaluation都要等到Value function收敛到一定程度才结束，这样子就会非常慢。一个替代的策略是我们尝试每一次Policy Evaluation只进行几步的话，一种特殊情况就是每一个Policy Evaluation只进行一步，这种就叫做Value Iteration。给出如下定义：<br>\begin{align*}<br>v_{k+1}(s) &amp;= max_a \mathbb{E}\left[R_{t+1} + \gamma v_k(S_{t+1})| S_t=s, A_t = a\right]\\<br>&amp;= max_a \sum_{s’,r}p(s’,r|s,a) \left[r+\gamma v_k(s’)\right] \tag{10}<br>\end{align*}<br>它其实就是把两个步骤给合在了一起，原来分开是：<br>\begin{align*}<br>v_{\pi}(s) &amp;= \mathbb{E}\left[R_{t+1} + \gamma v_k(S_{t+1})| S_t=s, A_t = a\right]\\<br>&amp;= \sum_{s’,r}p(s’,r|s,a) \left[r+\gamma v_k(s’)\right]\\<br>v_{\pi’}(s) &amp;= max_a \sum_{s’,r}p(s’,r|s,a) \left[r+\gamma v_{\pi}(s’)\right]\\<br>\end{align*}<br>另一种方式理解式$(10)$可以把它看成是使用贝尔曼最优等式进行迭代更新，Policy Evaluation用的是贝尔曼期望等式进行更新。下面给出完整的Value Iteration算法</p><p><strong>Value Iteration 算法</strong><br><strong>初始化</strong><br>阈值$\theta$，以及随机初始化的$V(s), s\in S^{+}$，$V(terminal)=0$。<br><strong>Loop</strong><br>$\qquad v\leftarrow V(s)$<br>$\qquad$<strong>Loop</strong> for each $s\in S$<br>$\qquad\qquad V(s) = max_a\sum_{s’,r}p(s’,r|s,a)\left[r+\gamma V(s’)\right]$<br>$\qquad\qquad\Delta \leftarrow max(Delta, |v-V(s)|)$<br><strong>until</strong> $\Delta \lt \theta$<br><strong>返回</strong> 输出一个策略$\pi\approx\pi_{*}$，这里书中说是deterministic，我觉得都可以，$\pi$也可以是stochastic的，最后得到的$\pi$满足:<br>$\pi(s) = argmax_a\sum_{s’,r}p(s’,r|s,a)\left[r+\gamma V(s’)\right]$</p><h2 id="asychronous-dynamic-programming">Asychronous Dynamic Programming</h2><p>之前介绍的这些DP方法，在每一次操作的时候，都有对所有的状态进行处理，这就很耗费资源。所以这里就产生了异步的DP算法，这类算法在更新的时候，不会使用整个的state set，而是使用部分state进行更新，其中一些state可能被访问了很多次，而另一些state一次也没有被访问过。<br>其中一种异步DP算法就是在plicy evalutaion的过程中，只使用一个state。<br>使用DP算法并不代表一定能减少计算量，他只是减少在策略没有改进之前陷入无意义的evaluation的可能。尽量选取那些重要的state用来进行更新。<br>同时，异步DP方便进行实时的交互。在使用异步DP更新的时候，同时使用一个真实场景中的agent经历进行更新。智能体的experience可以被用来确定使用哪些state进行更新，DP更新后的值也可以用来指导智能体的决策。</p><h2 id="generalized-policy-iteration">Generalized Policy Iteration</h2><p>之前介绍了三类方法，Policy Iteration,Value iteration以及Asychronous DP算法，它们都有两个过程在不断的迭代进行。一个是evaluation，一个是improvement，这类算法统一的被称为Generalized Policy Iteration(GPI)，可以根据不同的粒度进行细分。基本上所有的算法都是GPI，policy使用value function进行改进，value function朝着policy的真实值函数改进，如果value function和policy都稳定之后，那么说他们都是最优的了。<br>GPI中evalution和improvemetnt可以看成既有竞争又有合作。竞争是因为evaluation和improment的方向通常是相对的，policy改进意味着value function不适用于当前的policy,value function更新意味着policy不是greedy的。然后长期来说，他们共同作用，想要找到最优的值函数和policy。<br>GPI可以看成两个目标的交互过程，这两个目标不是正交的，改进一个目标也会使用另一个目标有所改进，直到最后，这两个交互过程使得总的目标变成最优的。</p><h2 id="efficiency-of-dynamic-programming">Efficiency of Dynamic Programming</h2><p>用$n$和$k$表示MDP的状态数和动作数，DP算法保证在多项式时间内找到最优解，即使策略的总数是$k^n$个。<br>DP比任何在policy space内搜索的算法要快上指数倍，因为policy space搜索需要检查每一个算法。Linear Programming算法也可以用来解MDP问题，在某些情况下最坏的情况还要比DP算法快，但是LP要比只适合解决state数量小的问题。而DP也能处理states很大的情况。</p><h2 id="summary">Summary</h2><ul><li>使用贝尔曼公式更新值函数，可以使用backup diagram看他们的直观表示。</li><li>基本上所有的强化学习算法都可以看成GPI(generalized policy iteraion)，先评估某个策略，然后改进这个策略，评估新的策略…这样子循环下去，直到收敛，找到一个不在变化的最优值函数和策略。<br>GPI不一定是收敛的，本章介绍的这些大多都是收敛的，但是还有一些没有被证明收敛。</li><li>可以使用异步的DP算法。</li><li>所有的DP算法都有一个属性叫做bootstrapping，即基于其他states的估计更新每一个state的值。因为每一个state value的更新都需要用到他们的successor state的估计。</li></ul><blockquote><p>They update estimates onthe basis of other estimates。</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;原理&quot;&gt;原理&lt;/h2&gt;
&lt;p&gt;Policy iteration有两种方式实现，一种是使用两个数组，一个保存原来的值，一个用来进行更新，这种方法是雅克比方法，或者叫同步的方法，因为他可以并行的进行。&lt;br&gt;
In-place的方法是高斯赛德尔方法。就是用来解方程组的
      
    
    </summary>
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="动态规划" scheme="http://mxxhcm.github.io/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"/>
    
  </entry>
  
  <entry>
    <title>reinforcement learning an introduction 第9章笔记</title>
    <link href="http://mxxhcm.github.io/2019/04/04/reinforcement-learning-an-introduction-%E7%AC%AC9%E7%AB%A0%E7%AC%94%E8%AE%B0/"/>
    <id>http://mxxhcm.github.io/2019/04/04/reinforcement-learning-an-introduction-第9章笔记/</id>
    <published>2019-04-04T02:14:08.000Z</published>
    <updated>2019-08-30T03:44:59.932Z</updated>
    
    <content type="html"><![CDATA[<h2 id="on-policy-prediction-with-approximation">On-policy Prediction with Approximation</h2><p>这一章讲的是利用on-policy的数据估计函数形式的值函数，on-policy就是说利用一个已知的policy $\pi$生成的experience来估计$v_{\pi}$。和之前讲的不同的是，前面几章讲的是表格形式的值函数，而这一章是使用参数为$\mathbf{w}\in R^d$的函数表示。即$\hat{v}(s,\mathbf{w})\approx v_{\pi}(s)$表示给定一个权值vector $\mathbf{w}$，state $s$的状态值。这个函数可以是任何形式的，可以是线性函数，也可以是神经网络，还可以是决策树。</p><h2 id="值函数估计">值函数估计</h2><p>目前这本书介绍的所有prediction方法都是更新某一个state的估计值函数向backed-up value（或者叫update target）值移动。我们用符号$s\mapsto u$表示一次更新。其中$s$是要更新的状态，$u$是$s$的估计值函数的update target。例如，Monte Carlo更新的value prediction是：$S_t \mapsto G_t$，TD(0)的update是：$S_t \mapsto R_{t+1} + \gamma \hat{v}(S_{t+1}, \mathbf{w}_t)$，$n$-step TD update是：$S_t \mapsto G_{t:t+n}$。在DP policy evaluation update中是：$s\mapsto E_{\pi}[R_{t+1}+\gamma\hat{v}(S_{t+1}, \mathbf{w}_t)| S_t =s]$，任意一个状态$s$被更新了，同时在其他真实experience中遇到的$S_t$也被更新了。</p><p>之前表格的更新太trivial了，更次更新$s$向$u$移动，其他状态的值都保持不变。现在使用函数实现更新，在状态$s$处的更新，可以一次性更新很多个其他状态的值。就像监督学习学习input和output之间的映射一样，我们可以把$s\mapsto g$的更新看做一个训练样本。这样就可以使用很多监督学习的方法学习这样一个函数。<br>但是并不是所有的方法都适用于强化学习，因为许多复杂的神经网络和统计学方法都假设训练集是静态不变的。然而强化学习中，学习是online的，即智能体不断地与环境进行交互产生新的数据，这就需要这个方法能够从不断增加的数据中高效的学习。<br>此外，强化学习通常需要function approximation能够处理target function不稳定的情况，即target function随着事件在不断的变化。比如，在基于GPI的control方法中，在$\pi$不断变化的情况下，我们想要学习出$q_{\pi}$。即使policy保持不变，如果使用booststrapping方法（DP和TD学习），训练样本的target value也在不断的改变，因为下一个state的value值在不断的改变。所以不能处理这些不稳定情况的方法有点不适合强化学习。</p><h2 id="预测目标-the-prediction-objective">预测目标(The Prediction Objective)</h2><p>表格形式的值函数最终都会收敛到真值，状态值之间也都是解耦的，即更新一个state不影响另一个state。<br>但是使用函数拟合，更新一个state的估计值就会影响很多个其他状态，并且不可能精确的估计所有states的值。假设我们的states比weights多的多，让一个state的估计更精确也意味着使得其他的state越不accurate。我们用一个state $s$上的分布,$\mu(s)\ge 0,\sum_s\mu(s)=1$代表对每个state上error的权重。然后使用$\mu(s)$对approximate value $\hat{v}(s,\mathbf{w})$和true value $v_{\pi}(s)$的squared error进行加权，得到Mean Squared Value Error，表示为$\bar{VE}$：<br>$$\bar{VE}(\mathbf{w}) = \sum_{s\in S}\mu(s)[v_{\pi}(s) - \hat{v}(s, \mathbf{w})]^2$$<br>通常情况下，$\mu(s)$是在state $s$处花费时间的百分比。在on-policy训练中，这叫做on-policy分布。在continuing tasks中，策略$\pi$下的on-policy分布是一个stationary distribution。<br>在episodic tasks中，on-policy分布有一些不同，因为它还取决于每个episodic的初始状态，用$h(s)$表示在一个episodic开始状态为$s$的概率，用$\eta(s)$表示在一个回合中，state $s$平均被访问的次数。<br>$$\eta(s) = h(s) + \sum_{\bar{s}}\eta(\bar{s})\sum_a\pi(a|\bar{s})p(s|\bar{s},a), forall\ s \in S$$<br>其中$\bar{s}$是$s$的前一个状态，$s$处的时间为以状态$s$开始的概率$h(s)$加上它由前一个状态$\bar{s}$转换过来消耗的时间。<br>列出一个方程组，可以解出来$\eta(s)$的期望值。然后进行归一化，得到：<br>$$\mu(s)=\frac{\eta{s}}{\sum_{s’}\eta{s’}}, \forall s \in S.$$<br>这是没有折扣因子的式子，如果有折扣因子的话，可以看成一种形式的</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;on-policy-prediction-with-approximation&quot;&gt;On-policy Prediction with Approximation&lt;/h2&gt;
&lt;p&gt;这一章讲的是利用on-policy的数据估计函数形式的值函数，on-policy就是说
      
    
    </summary>
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="函数近似" scheme="http://mxxhcm.github.io/tags/%E5%87%BD%E6%95%B0%E8%BF%91%E4%BC%BC/"/>
    
      <category term="on-policy" scheme="http://mxxhcm.github.io/tags/on-policy/"/>
    
      <category term="值函数" scheme="http://mxxhcm.github.io/tags/%E5%80%BC%E5%87%BD%E6%95%B0/"/>
    
  </entry>
  
  <entry>
    <title>reinforcement learning an introduction 第13章笔记.md</title>
    <link href="http://mxxhcm.github.io/2019/04/03/reinforcement-learning-an-introduction-%E7%AC%AC13%E7%AB%A0%E7%AC%94%E8%AE%B0/"/>
    <id>http://mxxhcm.github.io/2019/04/03/reinforcement-learning-an-introduction-第13章笔记/</id>
    <published>2019-04-03T01:46:49.000Z</published>
    <updated>2019-07-25T06:31:58.572Z</updated>
    
    <content type="html"><![CDATA[<h2 id="policy-gradient">Policy gradient</h2><p>这章介绍的是使用一个参数化策略(parameterized policy)直接给出action，而不用借助一个value funciton选择action。但是需要说一下的是，Policy gradient方法也可以学习一个Value function，但是value function是用来帮助学习policy parameters的，而不是用来选择action。我们用$\mathbf{\theta} \in R^{d’}$表示policy’s parameters vector，用$\pi(a|s, \mathbf{\theta}) = Pr[A_t = a|S_t = s, \mathbf{\theta}_t = \mathbf{\theta}]$表示environment在时刻$t$处于state $s$时，智能体根据参数为$\mathbf{\theta}$的策略$\pi$选择action $a$。<br>如果policy gradient方法使用了一个value function,它的权重用$\mathbf{w} \in R^d$表示，即$\hat{v}(s,\mathbf{w})$。</p><p>用$J(\mathbf{\theta})$表示policy parameters的标量performance measure。使用梯度上升(gradient ascent) 方法来最大化这个performance：<br>$$\mathbf{\theta}_{t+1} = \mathbf{\theta}_t + \alpha \widehat{\nabla J(\mathbf{\theta}_t}),\tag{1}$$<br>其中$\widehat{\nabla J(\mathbf{\theta}_t)} \in R^{d’}$是一个随机估计(stachastic estimate)，它的期望是performance measure对$\mathbf{\theta_t}$的梯度。不管它们是否使用value function，这种方法就叫做policy gradient方法。既学习policy，又学习value function的方法被称为actor-critic，其中actor指的是学到的policy，critic指的是学习到的value funciton,通常是state value function。</p><h2 id="policy估计和它的优势">policy估计和它的优势</h2><h3 id="参数化policy的条件">参数化policy的条件</h3><p>policy可以用任何方式参数化，只要$\pi(a|s,\mathbf{\theta}),\mathbf{\theta}\in R^{d’}$对于它的参数$\mathbf{\theta}$是可导的，即只要$\nabla_{\pi}(a|s,\mathbf{\theta})$（即：$\pi(a|s,\mathbf{\theta})$相对于$\mathbf{\theta}$的偏导数列向量）存在，并且$\forall s\in S, a\in A(s)$偏导数都是有限的即可。</p><h3 id="stochastic-policy">stochastic policy</h3><p>为了保证exploration，通常策略是stochastic，而不是deterministic，即$\forall s,a,\mathbf{\theta}, \pi(a|s,\mathbf{\theta})\in (0,1)$</p><h3 id="参数化方式的选择">参数化方式的选择</h3><h4 id="softmax">softmax</h4><p>对于有限且离散的action space，一个很自然的参数化方法就是对于每一个state-action对都计算一个参数化的数值偏好$h(s,a,\mathbf{\theta})\in R$。通过计算一个exponetial softmax，这个数值大的动作有更大的概率被选中：<br>$$\pi(a|s,\mathbf{\theta}) = \frac{e^{h(s,a,\mathbf{\theta} )}}{\sum_be^{h(s,b,\mathbf{\theta} )}}, \tag{2}$$<br>其中$b$是在state $s$下所有可能采取的动作，它们的概率加起来为$1$，这种方法叫做softmax in aciton preferences。</p><h4 id="nn和线性方法">NN和线性方法</h4><p>参数化还可以选择其他各种各样的方法，如AlphaGo中使用的NN，或者可以使用如下的线性方法：<br>$$h(s,a, \mathbf{\theta}) = \mathbf{\theta}^Tx(s,a), \tag{3}$$</p><h3 id="优势">优势</h3><p>和action value方法相比，policy gradient有多个优势。<br>第一个优势是使用action preferences的softmax，同时用$\epsilon-greedy$算法用$\epsilon$的概率随机选择action得到的策略可以接近一个deterministic policy。<br>而单单使用action values的方法并不会使得策略接近一个deterministic policy，但是action-value方法会逐渐收敛于它的true values，翻译成概率来表示就是在$0$和$1$之间的一个概率值。但是action preferences方法不收敛于任何值，它们产生optimal stochastic policy，如果optimal policy是deterministic，那么optimal action的preferences应该比其他所有suboptimal actions都要高。</p><p>第二个优势是使用action preferences方法得到的参数化策略可以使用任意的概率选择action。在某些问题中，最好的approximate policy可能是stochastic的，actor-value方法不能找到一个stochastic optimal policy，它总是根据action value值选出来一个值最大的action，但是这时候的结果通常不是最优的。</p><p>第三个优势是policy parameterization可能比action value parameterization更容易学习。当然，也有时候可能是action value更容易。这个要根据情况而定</p><p>第四个优势是policy parameterizaiton比较容易添加先验知识到policy中。</p><h2 id="policy-gradient理论">policy gradient理论</h2><p>除了上节说的实用优势之外，还有理论优势。policy parameterization学到关于参数的一个连续函数，action probability概率可以平滑的变化。然而$\epsilon-greedy$算法中，action-value改变以后，action probability可能变化很大。很大程度上是因为policy gradient方法的收敛性要比action value方法强的多。因为policy的连续性依赖于参数，使得policy gradient方法接近于gradient ascent。<br>这里讨论episodic情况。定义perfromance measure是episode初始状态的值。假设每一个episode，都从state $s_0$开始，定义：<br>$$J(\mathbf{\theta}) = v_{\pi_\mathbf{\theta}}(s_0), \tag{4}$$<br>其中$v_{\pi_\mathbf{\theta}}(s_0)$是由参数$\mathbf{\theta}$确定的策略$\pi_{\mathbf{\theta}}$的true value function。假设在episodic情况下，$\gamma=1$。</p><p>使用function approximation，一个需要解决的问题就是如何确保每次更新policy parameter，performance measure都有improvement。因为performence不仅仅依赖于action的选择，还取决于state的分布，然后它们都受policy parameter的影响。给定一个state，policy parameter对于actions，reward的影响，都可以相对直接的利用参数知识计算出来。但是policy parameter对于state 分布的影响是一个环境的函数，通常是不知道的。当梯度依赖于policy改变对于state分布的影响未知时，我们该如何估计performance相对于参数的梯度。</p><h3 id="episodic-case证明">Episodic case证明</h3><p>为了简化表示，用$\pi$表示参数为$\theta$的policy，所有的梯度都是相对于$\mathbf{\theta}$求的<br>\begin{align*}<br>\nabla v_{\pi}(s) &amp;= \nabla [ \sum_a \pi(a|s)q_{\pi}(s,a)], \forall s\in S \tag{5}\\<br>&amp;= \sum_a [\nabla\pi(a|s)q_{\pi}(s,a)], \forall s\in S \tag{6}\\<br>&amp;= \sum_a[\nabla\pi(a|s)q_{\pi}(s,a) + \pi(a|s)\nabla q_{\pi}(s,a)] \tag{7}\\<br>&amp;= \sum_a[\nabla\pi(a|s)q_{\pi}(s,a) + \pi(a|s)\nabla \sum_{s’,r}p(s’,r|s,a)(r+\gamma v_{\pi}(s’))] \tag{8}\\<br>&amp;= \sum_a[\nabla\pi(a|s)q_{\pi}(s,a) + \pi(a|s) \nabla \sum_{s’,r}p(s’,r|s,a)r + \pi(a|s)\nabla \sum_{s’,r}p(s’,r|s,a)\gamma v_{\pi}(s’))] \tag{9}\\<br>&amp;= \sum_a[\nabla\pi(a|s)q_{\pi}(s,a) + 0 + \pi(a|s)\sum_{s’}\gamma p(s’|s,a)\nabla v_{\pi}(s’) ] \tag{10}\\<br>&amp;= \sum_a[\nabla\pi(a|s)q_{\pi}(s,a) + 0 + \pi(a|s)\sum_{s’}\gamma p(s’|s,a)\\<br>&amp;\ \ \ \ \ \ \ \ \sum_{a’}[\nabla\pi(a’|s’)q_{\pi}(s’,a’) + \pi(a’|s’)\sum_{s’’}\gamma p(s’’|s’,a’)\nabla v_{\pi}(s’’))] ],  \tag{11}展开\\<br>&amp;= \sum_{x\in S}\sum_{k=0}^{\infty}Pr(s\rightarrow x, k,\pi)\sum_a\nabla\pi(a|x)q_{\pi}(x,a) \tag{12}<br>\end{align*}<br>第(5)式使用了$v_{\pi}(s) = \sum_a\pi(a|s)q(s,a)$进行展开。第(6)式将梯度符号放进求和里面。第(7)步使用product rule对q(s,a)求导。第(8)步利用$q_{\pi}(s, a) =\sum_{s’,r}p(s’,r|s,a)(r+v_{\pi}(s’)$ 对$q_{\pi}(s,a)$进行展开。第(9)步将(8)式进行分解。第(10)步对式(9)进行计算，因为$\sum_{s’,r}p(s’,r|s,a)r$是一个定制，求偏导之后为$0$。第(11)步对生成的$v_{\pi}(s’)$重复(5)-(10)步骤，得到式子(11)。如果对式子(11)中的$v_{\pi}(s)$一直展开，就得到了式子(12)。式子(12)中的$Pr(s\rightarrow x, k, \pi)$是在策略$\pi$下从state $s$经过$k$步转换到state $x$的概率，这里我有一个问题，就是为什么，$k$可以取到$\infty$，后来想了想，因为对第(11)步进行展开以后，可能会有重复的state，重复的意思就是从状态$s$开始，可能会多次到达某一个状态$x$，$k$就能取很多次，大不了$k=\infty$的概率为$0$就是了。</p><p>所以，对于$v_{\pi}(s_0)$，就有：<br>\begin{align*}<br>\nabla J(\mathbf{\theta}) &amp;= \nabla_{v_{\pi}}(s_0)\\<br>&amp;= \sum_{s\in S}( \sum_{k=0}^{\infty}Pr(s_0\rightarrow s,k,\pi) ) \sum_a\nabla_{\pi}(a|s)q_{\pi}(s,a)\\<br>&amp;=\sum_{s\in S}\eta(s)\sum_a \nabla_{\pi}(a|s)q_{\pi}(s,a)\\<br>&amp;=\sum_{s’\in S}\eta(s’)\sum_s\frac{\eta(s)}{\sum_{s’}\eta(s’)}\sum_a \nabla_{\pi}(a|s)q_{\pi}(s,a)\\<br>&amp;=\sum_{s’\in S}\eta(s’)\sum_s\mu(s)\sum_a \nabla_{\pi}(a|s)q_{\pi}(s,a)\\<br>&amp;\propto \sum_{s\in S}\mu(s)\sum_a\nabla\pi(a|s)q_{\pi}(s,a)<br>\end{align*}<br>最后，我们可以看出来performance对policy求导不涉及state distribution的导数。Episodic 情况下的策略梯度如下所示：<br>$$\nabla J(\mathbf{\theta})\propto \sum_{s\in S}\mu(s)\sum_aq_{\pi}(s,a)\nabla\pi(a|s,\mathbf{\theta}), \tag{13}$$<br>其中梯度是performacne指标$J$关于$\mathbf{\theta}$的偏导数列向量，$\pi$是参数$\mathbf{\theta}$对应的策略。在episodic情况下，比例常数是一个episode的平均长度，在continuing情况下，常数是$1$，实际上这个正比于就是一个等式。分布$\mu$是策略$\pi$下的on-policy分布。</p><h2 id="reinforce-monte-carlo-policy-gradient">REINFORCE: Monte Carlo Policy Gradient</h2><p>对于式子(1)，我们需要进行采样，让样本梯度的期望正比于performance measure对于$\mathbf{\theta}$的真实梯度。比例系数不需要确定，因为步长$\alpha$的大小是手动设置的。Policy gradient理论给出了一个正比于gradient的精确表达式，我们要做的就是选择采样方式，它的期望等于或者接近policy gradient理论给出的值。</p><h3 id="all-actions">all-actions</h3><p>使用随机变量的期望替换对随机变量求和的取值，我们可以将式子(13)进行如下变化：<br>\begin{align*}<br>\nabla J(\mathbf{\theta})&amp;\propto \sum_{s\in S}\mu(s)\nabla\pi(a|s,\mathbf{\theta})\sum_aq_{\pi}(s,a)\\<br>&amp;=\mathbb{E}_{\pi}\left[\nabla\pi(a|S_t,\mathbf{\theta})\sum_aq_{\pi}(S_t,a)\right]\tag{14}<br>\end{align*}<br>接下来，我们可以实例化该方法：<br>$$\mathbf{\theta}_{t+1} = \mathbf{\theta}_t+\alpha\sum_a\hat{q}(S_t,s,\mathbf{w})\nabla\pi(a|S_t,\mathbf{\theta}), \tag{15}$$<br>其中$\hat{q}$是$q_{\pi}$的估计值，这个算法被称为all-actions方法，因为它的更新涉及到了所有的action。然而，我们这里介绍的REINFORCE仅仅使用了$t$时刻的action $A_t$。。</p><h3 id="reinforce">REINFORCE</h3><p>和引入$S_t$的方法一样，使用随机变量的期望代替对与随机变量的可能取值进行求和，我们在式子(14)中引入$A_t$，<br>\begin{align*}<br>\nabla J(\mathbf{\theta}) &amp;= \mathbb{E}_{\pi}\left[\sum_aq_{\pi}(S_t,a)\nabla\pi(a|S_t,\mathbf{\theta})\right]\\<br>&amp; = \mathbb{E}_{\pi}\left[\sum_aq_{\pi}(S_t,a)\pi(a|S_t,\mathbf{\theta})\frac{\nabla\pi(a|S_t,\mathbf{\theta})}{\pi(a|S_t,\mathbf{\theta})}\right]\\<br>&amp; = \mathbb{E}_{\pi}\left[q_{\pi}(S_t,A_t)\frac{\nabla\pi(A_t|S_t,\mathbf{\theta})}{\pi(A_t|S_t,\mathbf{\theta})}\right]\\<br>\end{align*}</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;policy-gradient&quot;&gt;Policy gradient&lt;/h2&gt;
&lt;p&gt;这章介绍的是使用一个参数化策略(parameterized policy)直接给出action，而不用借助一个value funciton选择action。但是需要说一下的是，Pol
      
    
    </summary>
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="Policy Gradient" scheme="http://mxxhcm.github.io/tags/Policy-Gradient/"/>
    
  </entry>
  
  <entry>
    <title>matplotlib笔记</title>
    <link href="http://mxxhcm.github.io/2019/03/21/python-matplotlib%E7%AC%94%E8%AE%B0/"/>
    <id>http://mxxhcm.github.io/2019/03/21/python-matplotlib笔记/</id>
    <published>2019-03-21T07:29:17.000Z</published>
    <updated>2019-07-08T02:26:38.208Z</updated>
    
    <content type="html"><![CDATA[<h2 id="show"><a href="#show" class="headerlink" title="show()"></a>show()</h2><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>show()函数是一个阻塞函数，调用该函数，显示当前已经绘制的图像，然后需要手动关闭打开的图像，程序才会继续执行。</p><h3 id="代码示例"><a href="#代码示例" class="headerlink" title="代码示例"></a>代码示例</h3><p><a href="https://github.com/mxxhcm/code/blob/master/tools/matplotlib/1_show.py" target="_blank" rel="noopener">代码地址</a><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x = np.arange(<span class="number">0</span>,<span class="number">10</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">y1 = x**<span class="number">2</span></span><br><span class="line">y2 = <span class="number">2</span>*x +<span class="number">5</span></span><br><span class="line"></span><br><span class="line">plt.plot(x,y1)</span><br><span class="line">plt.savefig(<span class="string">"0_1.png"</span>)</span><br><span class="line">plt.show()  <span class="comment"># 调用show()会阻塞，然后关掉打开的图片，程序继续执行</span></span><br><span class="line"></span><br><span class="line">plt.plot(x,y2)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><h2 id="savefig"><a href="#savefig" class="headerlink" title="savefig()"></a>savefig()</h2><h3 id="介绍-1"><a href="#介绍-1" class="headerlink" title="介绍"></a>介绍</h3><p>该文件接收一个参数，作为文件保存的路径。</p><h3 id="代码示例-1"><a href="#代码示例-1" class="headerlink" title="代码示例"></a>代码示例</h3><p><a href="https://github.com/mxxhcm/code/blob/master/tools/matplotlib/2_savefig.py" target="_blank" rel="noopener">代码地址</a><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x = np.arange(<span class="number">0</span>, <span class="number">10</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">y1 = x**<span class="number">2</span></span><br><span class="line">y2 = <span class="number">2</span>*x +<span class="number">5</span></span><br><span class="line"></span><br><span class="line">plt.plot(x,y1)</span><br><span class="line">plt.savefig(<span class="string">"2.png"</span>) <span class="comment"># 保存图像，名字为2.png</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><h2 id="figure"><a href="#figure" class="headerlink" title="figure()"></a>figure()</h2><h3 id="介绍-2"><a href="#介绍-2" class="headerlink" title="介绍"></a>介绍</h3><p>figure()函数相当于生成一张画布。如果不显示调用的话，所有的图像都会绘制在默认的画布上。可以通过调用figure()函数将函数图像分开。figure()会接受几个参数，num是生成图片的序号，figsize指定图片的大小。</p><h3 id="代码示例-2"><a href="#代码示例-2" class="headerlink" title="代码示例"></a>代码示例</h3><p><a href="https://github.com/mxxhcm/code/blob/master/tools/matplotlib/3_figure.py" target="_blank" rel="noopener">代码地址</a><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x = np.arange(<span class="number">0</span>,<span class="number">10</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">y1 = x**<span class="number">2</span></span><br><span class="line">y2 = <span class="number">2</span>*x +<span class="number">5</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># figure</span></span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(x,y1)</span><br><span class="line"></span><br><span class="line">plt.figure(num=<span class="number">6</span>,figsize=(<span class="number">10</span>,<span class="number">10</span>))</span><br><span class="line">plt.plot(x,y2)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><h2 id="imshow"><a href="#imshow" class="headerlink" title="imshow()"></a>imshow()</h2><h3 id="介绍-3"><a href="#介绍-3" class="headerlink" title="介绍"></a>介绍</h3><p>该函数用来显示图像，接受一个图像矩阵。调用完该函数之后还需要调用show()函数。</p><h3 id="代码示例-3"><a href="#代码示例-3" class="headerlink" title="代码示例"></a>代码示例</h3><p><a href="https://github.com/mxxhcm/code/blob/master/tools/matplotlib/4_image.py" target="_blank" rel="noopener">代码地址</a><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">img = np.random.randint(<span class="number">0</span>, <span class="number">255</span>, [<span class="number">32</span>, <span class="number">32</span>])</span><br><span class="line">print(img.shape)</span><br><span class="line"></span><br><span class="line">plt.imshow(img)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><h2 id="subplot"><a href="#subplot" class="headerlink" title="subplot()"></a>subplot()</h2><h3 id="介绍-4"><a href="#介绍-4" class="headerlink" title="介绍"></a>介绍</h3><p>绘制$m\times n$个子图</p><h3 id="代码示例-4"><a href="#代码示例-4" class="headerlink" title="代码示例"></a>代码示例</h3><p><a href="https://github.com/mxxhcm/code/blob/master/tools/matplotlib/5_subplot.py" target="_blank" rel="noopener">代码地址</a><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x = np.arange(<span class="number">0</span>, <span class="number">10</span>, <span class="number">1</span>)</span><br><span class="line">y1 = <span class="number">2</span> * x</span><br><span class="line">y2 = <span class="number">3</span> * x</span><br><span class="line">y3 = <span class="number">4</span> * x</span><br><span class="line">y4 = <span class="number">5</span> * x</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">plt.plot(x, y1, marker=<span class="string">'s'</span>, lw=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">plt.plot(x, y2, ls=<span class="string">'-.'</span>)</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">plt.plot(x, y3, color=<span class="string">'r'</span>)</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">2</span>, <span class="number">2</span>, <span class="number">4</span>)</span><br><span class="line">plt.plot(x, y4, ms=<span class="number">10</span>, marker=<span class="string">'o'</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><h2 id="subplots"><a href="#subplots" class="headerlink" title="subplots()"></a>subplots()</h2><h3 id="介绍-5"><a href="#介绍-5" class="headerlink" title="介绍"></a>介绍</h3><p>将一张图分成$m\times n$个子图。</p><h3 id="代码示例-5"><a href="#代码示例-5" class="headerlink" title="代码示例"></a>代码示例</h3><p><a href="https://github.com/mxxhcm/code/blob/master/tools/matplotlib/6_subplots.py" target="_blank" rel="noopener">代码地址</a><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">figure,axes = plt.subplots(<span class="number">2</span>, <span class="number">3</span>, figsize=[<span class="number">40</span>,<span class="number">20</span>])</span><br><span class="line">axes = axes.flatten()</span><br><span class="line"></span><br><span class="line">x = np.arange(<span class="number">0</span>, <span class="number">20</span>) </span><br><span class="line">y1 = pow(x, <span class="number">2</span>)</span><br><span class="line">axes[<span class="number">0</span>].plot(x, y1) </span><br><span class="line"></span><br><span class="line">y5 = pow(x, <span class="number">3</span>)</span><br><span class="line">axes[<span class="number">5</span>].plot(x, y5) </span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><h2 id="ax"><a href="#ax" class="headerlink" title="ax()"></a>ax()</h2><h3 id="介绍-6"><a href="#介绍-6" class="headerlink" title="介绍"></a>介绍</h3><p>获得当前figure的坐标轴，用来绘制。</p><h3 id="代码示例-6"><a href="#代码示例-6" class="headerlink" title="代码示例"></a>代码示例</h3><p><a href="https://github.com/mxxhcm/code/blob/master/tools/matplotlib/7_axes.py" target="_blank" rel="noopener">代码地址</a><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x = np.arange(<span class="number">-3.5</span>,<span class="number">3.5</span>,<span class="number">0.5</span>)</span><br><span class="line">y1 = np.abs(<span class="number">2</span> * x)</span><br><span class="line">y2 = np.abs(x)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>,<span class="number">10</span>))</span><br><span class="line">ax = plt.gca() <span class="comment"># gca = get current axis</span></span><br><span class="line">ax.spines[<span class="string">'right'</span>].set_color(<span class="string">'none'</span>)</span><br><span class="line">ax.spines[<span class="string">'top'</span>].set_color(<span class="string">'red'</span>)</span><br><span class="line">ax.xaxis.set_ticks_position(<span class="string">"bottom"</span>)</span><br><span class="line">ax.yaxis.set_ticks_position(<span class="string">"left"</span>)</span><br><span class="line">ax.spines[<span class="string">'bottom'</span>].set_position((<span class="string">'data'</span>,<span class="number">0</span>))</span><br><span class="line">ax.spines[<span class="string">'left'</span>].set_position((<span class="string">'data'</span>,<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># both work</span></span><br><span class="line">ax.plot(x,y1,lw=<span class="number">2</span>,marker=<span class="string">'-'</span>,ms=<span class="number">8</span>)</span><br><span class="line">plt.plot(x,y2,lw=<span class="number">3</span>,marker=<span class="string">'^'</span>,ms=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># xlim and ylim</span></span><br><span class="line"><span class="comment"># ax.xlim([-3.8, 3.3])</span></span><br><span class="line"><span class="comment"># AttributeError: 'AxesSubplot' object has no attribute 'xlim'</span></span><br><span class="line">plt.xlim([<span class="number">-3.8</span>, <span class="number">3.3</span>])</span><br><span class="line">plt.ylim([<span class="number">0</span>, <span class="number">7.2</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># xlabel and ylabel</span></span><br><span class="line"><span class="comment"># ax.xlabel('x',fontsize=20)</span></span><br><span class="line"><span class="comment"># AttributeError: 'AxesSubplot' object has no attribute 'xlabel'</span></span><br><span class="line">plt.xlabel(<span class="string">'x'</span>,fontsize=<span class="number">20</span>)</span><br><span class="line">plt.ylabel(<span class="string">'y = 2x '</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># xticklabel and yticaklabel</span></span><br><span class="line"><span class="comment"># ax.xticks(x,('a','b','c','d','e','f','g','h','i','j','k','l','m','n'),fontsize=20)</span></span><br><span class="line"><span class="comment"># AttributeError: 'AxesSubplot' object has no attribute 'xticks'</span></span><br><span class="line">plt.xticks(x,(<span class="string">'a'</span>,<span class="string">'b'</span>,<span class="string">'c'</span>,<span class="string">'d'</span>,<span class="string">'e'</span>,<span class="string">'f'</span>,<span class="string">'g'</span>,<span class="string">'h'</span>,<span class="string">'i'</span>,<span class="string">'j'</span>,<span class="string">'k'</span>,<span class="string">'l'</span>,<span class="string">'m'</span>,<span class="string">'n'</span>),fontsize=<span class="number">20</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># both work</span></span><br><span class="line">ax.legend([<span class="string">'t1'</span>,<span class="string">'t2'</span>])</span><br><span class="line">plt.legend([<span class="string">'y1'</span>,<span class="string">'y2'</span>])</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><h2 id="ion-和ioff"><a href="#ion-和ioff" class="headerlink" title="ion()和ioff()"></a>ion()和ioff()</h2><h3 id="介绍-7"><a href="#介绍-7" class="headerlink" title="介绍"></a>介绍</h3><p>交互式绘图，可以在一张图上不断的更新。</p><h3 id="代码示例-7"><a href="#代码示例-7" class="headerlink" title="代码示例"></a>代码示例</h3><p><a href="https://github.com/mxxhcm/code/blob/master/tools/matplotlib/8_plt_ion_ioff.py" target="_blank" rel="noopener">代码地址</a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">count = 1</span><br><span class="line">flag = True</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">ax = plt.gca()</span><br><span class="line">x = np.arange(20)</span><br><span class="line">plt.figure()</span><br><span class="line">ax2 = plt.gca()</span><br><span class="line"></span><br><span class="line">while flag:</span><br><span class="line">    plt.ion()</span><br><span class="line">    y = pow(x[:count], 2)</span><br><span class="line">    temp = x[:count]</span><br><span class="line">    ax.plot(temp, y, linewidth=1)</span><br><span class="line">    plt.pause(1)</span><br><span class="line">    plt.ioff()</span><br><span class="line"></span><br><span class="line">    ax2.plot(x, x+count)</span><br><span class="line">    count += 1</span><br><span class="line">    if count &gt; 20:</span><br><span class="line">        break</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><h2 id="seanborn"><a href="#seanborn" class="headerlink" title="seanborn"></a>seanborn</h2><h3 id="介绍-8"><a href="#介绍-8" class="headerlink" title="介绍"></a>介绍</h3><p>对matplotlib进行了一层封装</p><h3 id="代码示例-8"><a href="#代码示例-8" class="headerlink" title="代码示例"></a>代码示例</h3><p><a href="https://github.com/mxxhcm/code/blob/master/tools/matplotlib/9_seanborn.py" target="_blank" rel="noopener">代码地址</a><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">values = np.zeros((<span class="number">21</span>,<span class="number">21</span>), dtype=np.int)</span><br><span class="line">fig, axes = plt.subplots(<span class="number">2</span>, <span class="number">3</span>, figsize=(<span class="number">40</span>,<span class="number">20</span>))</span><br><span class="line">plt.subplots_adjust(wspace=<span class="number">0.1</span>, hspace=<span class="number">0.2</span>)</span><br><span class="line">axes = axes.flatten()</span><br><span class="line"></span><br><span class="line"><span class="comment"># cmap is the paramter to specify color type, ax is the parameter to specify where to show the picture</span></span><br><span class="line"><span class="comment"># np.flipud(matrix), flip the column in the up/down direction, rows are preserved</span></span><br><span class="line">figure = sns.heatmap(np.flipud(values), cmap=<span class="string">"YlGnBu"</span>, ax=axes[<span class="number">0</span>])</span><br><span class="line">figure.set_xlabel(<span class="string">"cars at second location"</span>, fontsize=<span class="number">30</span>)</span><br><span class="line">figure.set_title(<span class="string">"policy"</span>, fontsize=<span class="number">30</span>)</span><br><span class="line">figure.set_ylabel(<span class="string">"cars at first location"</span>, fontsize=<span class="number">30</span>)</span><br><span class="line">figure.set_yticks(list(reversed(range(<span class="number">21</span>))))</span><br><span class="line"></span><br><span class="line">figure = sns.heatmap(np.flipud(values), ax=axes[<span class="number">1</span>])</span><br><span class="line">figure.set_ylabel(<span class="string">"cars at first location"</span>, fontsize=<span class="number">30</span>)</span><br><span class="line">figure.set_yticks(list(reversed(range(<span class="number">21</span>))))</span><br><span class="line">figure.set_title(<span class="string">"policy"</span>, fontsize=<span class="number">30</span>)</span><br><span class="line">figure.set_xlabel(<span class="string">"cars at second location"</span>, fontsize=<span class="number">30</span>)</span><br><span class="line"></span><br><span class="line">plt.savefig(<span class="string">"hello.pdf"</span>)</span><br><span class="line">plt.show()</span><br><span class="line">plt.close()</span><br></pre></td></tr></table></figure></p><h2 id="color"><a href="#color" class="headerlink" title="color"></a>color</h2><h3 id="介绍-9"><a href="#介绍-9" class="headerlink" title="介绍"></a>介绍</h3><p>指定线条的颜色，用color=’’实现。常见的颜色有：’b’, ‘g’, ‘r’, ‘c’, ‘m’, ‘y’, ‘k’, ‘w’。</p><h3 id="代码示例-9"><a href="#代码示例-9" class="headerlink" title="代码示例"></a>代码示例</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">color = [<span class="string">'b'</span>, <span class="string">'g'</span>, <span class="string">'r'</span>, <span class="string">'c'</span>, <span class="string">'m'</span>, <span class="string">'y'</span>, <span class="string">'k'</span>, <span class="string">'w'</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(color)):</span><br><span class="line">    x = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line">    y = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line">    plt.plot(x, y+i, color=color[i])</span><br><span class="line"></span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">plt.plot(range(<span class="number">10</span>), range(<span class="number">10</span>), color=<span class="string">'w'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h3 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h3><p>color=’w’，’w’是white，所以画出来的图你是看不到的。。。这困扰了我好久。。。。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;show&quot;&gt;&lt;a href=&quot;#show&quot; class=&quot;headerlink&quot; title=&quot;show()&quot;&gt;&lt;/a&gt;show()&lt;/h2&gt;&lt;h3 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介
      
    
    </summary>
    
      <category term="python" scheme="http://mxxhcm.github.io/categories/python/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="matplotlib" scheme="http://mxxhcm.github.io/tags/matplotlib/"/>
    
  </entry>
  
  <entry>
    <title>pandas笔记</title>
    <link href="http://mxxhcm.github.io/2019/03/18/python-pandas%E7%AC%94%E8%AE%B0/"/>
    <id>http://mxxhcm.github.io/2019/03/18/python-pandas笔记/</id>
    <published>2019-03-18T07:15:54.000Z</published>
    <updated>2019-08-16T08:59:53.844Z</updated>
    
    <content type="html"><![CDATA[<h2 id="pd-read"><a href="#pd-read" class="headerlink" title="pd.read_*()"></a>pd.read_<em>*</em>()</h2><h3 id="pd-read-csv"><a href="#pd-read-csv" class="headerlink" title="pd.read_csv()"></a>pd.read_csv()</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas</span><br><span class="line">pandas.read_csv(filepath_or_buffer, sep=<span class="string">', '</span>, delimiter=<span class="literal">None</span>, header=<span class="string">'infer'</span>, names=<span class="literal">None</span>, index_col=<span class="literal">None</span>, usecols=<span class="literal">None</span>, squeeze=<span class="literal">False</span>, prefix=<span class="literal">None</span>, mangle_dupe_cols=<span class="literal">True</span>, dtype=<span class="literal">None</span>, engine=<span class="literal">None</span>, converters=<span class="literal">None</span>, true_values=<span class="literal">None</span>, false_values=<span class="literal">None</span>, skipinitialspace=<span class="literal">False</span>, skiprows=<span class="literal">None</span>, nrows=<span class="literal">None</span>, na_values=<span class="literal">None</span>, keep_default_na=<span class="literal">True</span>, na_filter=<span class="literal">True</span>, verbose=<span class="literal">False</span>, skip_blank_lines=<span class="literal">True</span>, parse_dates=<span class="literal">False</span>, infer_datetime_format=<span class="literal">False</span>, keep_date_col=<span class="literal">False</span>, date_parser=<span class="literal">None</span>, dayfirst=<span class="literal">False</span>, iterator=<span class="literal">False</span>, chunksize=<span class="literal">None</span>, compression=<span class="string">'infer'</span>, thousands=<span class="literal">None</span>, decimal=<span class="string">b'.'</span>, lineterminator=<span class="literal">None</span>, quotechar=<span class="string">'"'</span>, quoting=<span class="number">0</span>, escapechar=<span class="literal">None</span>, comment=<span class="literal">None</span>, encoding=<span class="literal">None</span>, dialect=<span class="literal">None</span>, tupleize_cols=<span class="literal">None</span>, error_bad_lines=<span class="literal">True</span>, warn_bad_lines=<span class="literal">True</span>, skipfooter=<span class="number">0</span>, skip_footer=<span class="number">0</span>, doublequote=<span class="literal">True</span>, delim_whitespace=<span class="literal">False</span>, as_recarray=<span class="literal">None</span>, compact_ints=<span class="literal">None</span>, use_unsigned=<span class="literal">None</span>, low_memory=<span class="literal">True</span>, buffer_lines=<span class="literal">None</span>, memory_map=<span class="literal">False</span>, float_precision=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure><p>filepath_or_buffer: 文件路径，或者一个字符串，url等等<br>sep: str,分隔符，默认是’,’<br>delimiter: str,定界符，如果指定该参数，sep参数失效<br>delimiter_whitespace: boolean,指定是否吧空格作为分界符如果指定该参数，则delimiter失效<br>header: int or list of ints,指定列名字，默认是header=0,表示把第一行当做列名，如果header=[0,3,4],表示吧第0,3,4行都当做列名，真正的数据从第二行开始，如果没有列名，指定header=None<br>index_col: int or sequence or False,指定哪几列作为index，index_col=[0,1],表示用前两列的值作为一个index，去访问后面几列的值。<br>prefix: str,如果header为None的话，可以指定列名。<br>parse_dates: boolean or list of ints or names,or list of lists, or dict 如果是True，解析index，如果是list of ints，把每一个int代表的列都分别当做一个日期解析，如果是list of lists，将list中的list作为一个日期解析，如果是字典的话，将dict中key作为一个新的列名，value为这个新的列的值。<br>keep_date_col: boolean,如果parser_dates中是将多个列合并为一个日期的话，是否保留原始列<br>date_parser: function,用来解析parse_dates中给出的日期列，是自己写的函数，函数参数个数和一个日期的列数相同。</p><p>chunksize: 如果文件太大的话，分块读入<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data = pd.read_csv(<span class="string">"input.csv"</span>,chunksize=<span class="number">1000</span>)</span><br><span class="line"><span class="keyword">for</span>  i  <span class="keyword">in</span>  data:</span><br><span class="line">  ...</span><br></pre></td></tr></table></figure></p><h2 id="DataFrame"><a href="#DataFrame" class="headerlink" title="DataFrame"></a>DataFrame</h2><h3 id="声明一个DataFrame"><a href="#声明一个DataFrame" class="headerlink" title="声明一个DataFrame"></a>声明一个DataFrame</h3><p>data = pandas.DataFrame(numpy.arange(16).reshape(4,4),index=list(‘abcd’),columns=(‘wxyz’)<br>    w  x  y  z<br>a  0  1  2  3<br>b  4  5  6  7<br>c  8  9  10  11<br>d  12  13  14  15<br>index 是index列的值<br>columns 是列名</p><h3 id="访问某一列"><a href="#访问某一列" class="headerlink" title="访问某一列"></a>访问某一列</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data = pandas.DataFrame(numpy.arange(<span class="number">16</span>).reshape(<span class="number">4</span>,<span class="number">4</span>),index=list(<span class="string">'abcd'</span>),columns=(<span class="string">'wxyz'</span>)</span><br><span class="line">data[<span class="string">'w'</span>]</span><br><span class="line">data.w</span><br></pre></td></tr></table></figure><h3 id="写入某一列"><a href="#写入某一列" class="headerlink" title="写入某一列"></a>写入某一列</h3><p>只能先访问列 再访问行<br>data[‘w’] = []   # =左右两边shape必须一样<br>data[‘w’][0]  #某一列的第0行</p><h3 id="groupby"><a href="#groupby" class="headerlink" title="groupby"></a>groupby</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">data = pandas.DataFrame(np.arange(<span class="number">16</span>).reshape(<span class="number">4</span>,<span class="number">4</span>),index=list(<span class="string">'abcd'</span>),columns=(<span class="string">'wxyz'</span>))</span><br><span class="line"><span class="keyword">for</span> key,value <span class="keyword">in</span> data.groupby(<span class="string">"w"</span>):  <span class="comment"># group by 列名什么的，就是说某一列的值一样分一组</span></span><br><span class="line">  value = value.values  <span class="comment"># value是一个numpy数组</span></span><br><span class="line">  value_list = value.tolist()  <span class="comment">#将numpy数组转换为一个list</span></span><br><span class="line">  <span class="keyword">for</span> single_list <span class="keyword">in</span> value_list:</span><br><span class="line">     single_list = str(single_list)</span><br><span class="line">     ...</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;pd-read&quot;&gt;&lt;a href=&quot;#pd-read&quot; class=&quot;headerlink&quot; title=&quot;pd.read_*()&quot;&gt;&lt;/a&gt;pd.read_&lt;em&gt;*&lt;/em&gt;()&lt;/h2&gt;&lt;h3 id=&quot;pd-read-csv&quot;&gt;&lt;a href=&quot;#pd-re
      
    
    </summary>
    
      <category term="python" scheme="http://mxxhcm.github.io/categories/python/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="pandas" scheme="http://mxxhcm.github.io/tags/pandas/"/>
    
  </entry>
  
  <entry>
    <title>argparse笔记</title>
    <link href="http://mxxhcm.github.io/2019/03/18/python-argparse%E7%AC%94%E8%AE%B0/"/>
    <id>http://mxxhcm.github.io/2019/03/18/python-argparse笔记/</id>
    <published>2019-03-18T07:15:41.000Z</published>
    <updated>2019-06-26T03:27:59.464Z</updated>
    
    <content type="html"><![CDATA[<h2 id="简单的例子"><a href="#简单的例子" class="headerlink" title="简单的例子"></a>简单的例子</h2><h3 id="创建一个parser"><a href="#创建一个parser" class="headerlink" title="创建一个parser"></a>创建一个parser</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">parser = argparse.ArgumentParser(description=<span class="string">'Process Intergers'</span>)</span><br></pre></td></tr></table></figure><h3 id="添加参数"><a href="#添加参数" class="headerlink" title="添加参数"></a>添加参数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">parser.add_argument(,,)</span><br></pre></td></tr></table></figure><h3 id="解析参数"><a href="#解析参数" class="headerlink" title="解析参数"></a>解析参数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">arglist = parser.parse_args()</span><br></pre></td></tr></table></figure><h3 id="代码示例"><a href="#代码示例" class="headerlink" title="代码示例"></a>代码示例</h3><p>完整代码如下<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_args</span><span class="params">()</span>:</span></span><br><span class="line">    parser = argparse.ArgumentParser(<span class="string">"input parameters"</span>)</span><br><span class="line">    parser.add_argument(<span class="string">"--batch_size"</span>, type=int, default=<span class="number">32</span>)</span><br><span class="line">    parser.add_argument(<span class="string">"--episodes"</span>, type=int, default=<span class="number">1</span>)</span><br><span class="line">    parser.add_argument(<span class="string">"--lr"</span>, type=float, default=<span class="number">0.01</span>)</span><br><span class="line">    parser.add_argument(<span class="string">"--momentum"</span>, type=float, default=<span class="number">0.9</span>)</span><br><span class="line">    args_list = parser.parse_args()</span><br><span class="line">    <span class="keyword">return</span> args_list</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(args_list)</span>:</span></span><br><span class="line">print(args_list.batch_size)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    args_list = parse_args()</span><br><span class="line">    main(args_list)</span><br></pre></td></tr></table></figure></p><h2 id="ArgumentParser-objects"><a href="#ArgumentParser-objects" class="headerlink" title="ArgumentParser objects"></a>ArgumentParser objects</h2><blockquote><p>The ArgumentParser object will hold all the information necessary to parse the command line into python data types</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">argparse</span>.<span class="title">ArgumentParser</span><span class="params">(</span></span></span><br><span class="line"><span class="class"><span class="params">prog=None,</span></span></span><br><span class="line"><span class="class"><span class="params">usage=None,</span></span></span><br><span class="line"><span class="class"><span class="params">description=None,</span></span></span><br><span class="line"><span class="class"><span class="params">epilog=None,</span></span></span><br><span class="line"><span class="class"><span class="params">parents=[],</span></span></span><br><span class="line"><span class="class"><span class="params">formatter_class=argparse.HelpFormatter,</span></span></span><br><span class="line"><span class="class"><span class="params">prefix_chars=<span class="string">'-'</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">fromfile_prefix_chars=None,</span></span></span><br><span class="line"><span class="class"><span class="params">argument_default=None,</span></span></span><br><span class="line"><span class="class"><span class="params">conflict_handler=<span class="string">'error'</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">add_help=True</span></span></span><br><span class="line"><span class="class"><span class="params">)</span></span></span><br></pre></td></tr></table></figure><h3 id="创建一个名为test-py的程序如下"><a href="#创建一个名为test-py的程序如下" class="headerlink" title="创建一个名为test.py的程序如下"></a>创建一个名为test.py的程序如下</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">args = parser.parse_args()</span><br></pre></td></tr></table></figure><p>~#:python test.py -h</p><blockquote><p>usage: test.py [-h]<br>optional arguments:<br>  -h, —help  show this help message and exit</p></blockquote><h3 id="prog参数"><a href="#prog参数" class="headerlink" title="prog参数"></a>prog参数</h3><p>设置显示程序的名称</p><h4 id="直接使用默认显示的程序名"><a href="#直接使用默认显示的程序名" class="headerlink" title="直接使用默认显示的程序名"></a>直接使用默认显示的程序名</h4><p>~#:python test.py -h</p><blockquote><p>usage: test.py [-h]<br>optional arguments:<br>  -h, —help  show this help message and exit</p></blockquote><h4 id="使用prog参数进行设置"><a href="#使用prog参数进行设置" class="headerlink" title="使用prog参数进行设置"></a>使用prog参数进行设置</h4><p>修改test.py的程序如下<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser(prog=<span class="string">"mytest"</span>)</span><br><span class="line">args = parser.parse_args()</span><br></pre></td></tr></table></figure></p><p>~#:python test.py -h</p><blockquote><p>usage: mytest [-h]<br> optional arguments:<br>  -h, —help  show this help message and exit</p></blockquote><p>usage后的名称变为我们prog参数指定的名称</p><h3 id="usage"><a href="#usage" class="headerlink" title="usage"></a>usage</h3><h4 id="使用默认的usage"><a href="#使用默认的usage" class="headerlink" title="使用默认的usage"></a>使用默认的usage</h4><h4 id="使用指定的usage"><a href="#使用指定的usage" class="headerlink" title="使用指定的usage"></a>使用指定的usage</h4><h3 id="description"><a href="#description" class="headerlink" title="description"></a>description</h3><h4 id="使用默认的description"><a href="#使用默认的description" class="headerlink" title="使用默认的description"></a>使用默认的description</h4><h4 id="使用指定的description"><a href="#使用指定的description" class="headerlink" title="使用指定的description"></a>使用指定的description</h4><h3 id="epilog"><a href="#epilog" class="headerlink" title="epilog"></a>epilog</h3><h4 id="使用默认的epilog"><a href="#使用默认的epilog" class="headerlink" title="使用默认的epilog"></a>使用默认的epilog</h4><h4 id="使用指定的epilog"><a href="#使用指定的epilog" class="headerlink" title="使用指定的epilog"></a>使用指定的epilog</h4><h3 id="parents"><a href="#parents" class="headerlink" title="parents"></a>parents</h3><h3 id="formatter-class"><a href="#formatter-class" class="headerlink" title="formatter_class"></a>formatter_class</h3><h3 id="prefix-chars"><a href="#prefix-chars" class="headerlink" title="prefix_chars"></a>prefix_chars</h3><p>指定其他的prefix，默认的是-，比如可以指定可选参数的前缀为+</p><h3 id="fromfile-prefix-chars"><a href="#fromfile-prefix-chars" class="headerlink" title="fromfile_prefix_chars"></a>fromfile_prefix_chars</h3><h3 id="argument-default"><a href="#argument-default" class="headerlink" title="argument_default"></a>argument_default</h3><h3 id="conflict-handler"><a href="#conflict-handler" class="headerlink" title="conflict_handler"></a>conflict_handler</h3><p>将conflict_handler设置为resolve就可以防止override原来older arguments<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser(conflict_handler=<span class="string">'resolve'</span>)</span><br><span class="line">parser.add_argument(<span class="string">'--foo'</span>,<span class="string">'-f'</span>,help=<span class="string">"old help"</span>)</span><br><span class="line">parser.add_argument(<span class="string">'-f'</span>,help=<span class="string">"new_help"</span>)</span><br><span class="line">parser.print_help()</span><br></pre></td></tr></table></figure></p><h3 id="add-help"><a href="#add-help" class="headerlink" title="add_help"></a>add_help</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser(add_help=<span class="literal">False</span>)</span><br><span class="line">parser.print_help()</span><br></pre></td></tr></table></figure><p>输出</p><blockquote><p>usage: [-h]<br> optional arguments:<br>  -h, —help  show this help message and exit<br>将add_help设置为false</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser(add_help=<span class="literal">False</span>)</span><br><span class="line">parser.print_help()</span><br></pre></td></tr></table></figure><p>输出</p><blockquote><p>usage: </p></blockquote><h2 id="The-add-argument-method"><a href="#The-add-argument-method" class="headerlink" title="The add_argument() method"></a>The add_argument() method</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">ArgumentParser.add_argument(</span><br><span class="line">name <span class="keyword">or</span> flags...</span><br><span class="line">[,action],</span><br><span class="line">[,nargs],</span><br><span class="line">[,const],</span><br><span class="line">[,default],</span><br><span class="line">[,type],</span><br><span class="line">[,choices],</span><br><span class="line">[,required],</span><br><span class="line">[,help],</span><br><span class="line">[,metavar],</span><br><span class="line">[,dest]</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">'-f'</span>,<span class="string">'-foo'</span>,<span class="string">'-a'</span>, defaults=, type=, help=)</span><br><span class="line">parser.add_argument(<span class="string">'hello'</span>)</span><br><span class="line">parser.add_argument(<span class="string">'hi'</span>)</span><br><span class="line">args = parser.parse_args([<span class="string">'Hello'</span>,<span class="string">'-f'</span>,<span class="string">'123'</span>,<span class="string">'Hi'</span>])</span><br><span class="line">print(args)</span><br></pre></td></tr></table></figure><h3 id="name-or-flags"><a href="#name-or-flags" class="headerlink" title="name or flags"></a>name or flags</h3><h4 id="添加可选参数"><a href="#添加可选参数" class="headerlink" title="添加可选参数"></a>添加可选参数</h4><p>parser.add_argument(‘-f’, ‘—foo’, ‘-fooo’)</p><h4 id="添加必选参数"><a href="#添加必选参数" class="headerlink" title="添加必选参数"></a>添加必选参数</h4><p>parser.add_argument(‘bar’)</p><h4 id="调用parse-args"><a href="#调用parse-args" class="headerlink" title="调用parse_args()"></a>调用parse_args()</h4><p>当parse_args()函数被调用的时候，可选参数会被-prefix所识别，剩下的参数会被分配给必选参数的位置。如下代码中，’3’对应的就是’hello’的参数，’this is hi’对应的就是’hi’的参数，而’123’是’-f’的参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">'-f'</span>,<span class="string">'-foo'</span>,<span class="string">'-a'</span>)</span><br><span class="line">parser.add_argument(<span class="string">'hello'</span>, type=int)</span><br><span class="line">parser.add_argument(<span class="string">'hi'</span>)</span><br><span class="line">args = parser.parse_args([<span class="string">'3'</span>,<span class="string">'-f'</span>,<span class="string">'123'</span>,<span class="string">'this is hi'</span>])</span><br><span class="line">print(args)</span><br></pre></td></tr></table></figure><p>输出</p><blockquote><p>Namespace(f=’123’, hello=’Hello’, hi=’Hi’)</p></blockquote><h3 id="action"><a href="#action" class="headerlink" title="action"></a>action</h3><h4 id="store-the-default-action"><a href="#store-the-default-action" class="headerlink" title="store,the default action"></a>store,the default action</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">'--foo'</span>)</span><br><span class="line">args = parser.parse_args([<span class="string">'--foo'</span>,<span class="string">'1'</span>])</span><br><span class="line">print(args)</span><br></pre></td></tr></table></figure><p>输出</p><blockquote><p>Namespace(foo=’1’)</p></blockquote><h4 id="store-const"><a href="#store-const" class="headerlink" title="store_const"></a>store_const</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">'--foo'</span>, action=<span class="string">'store_const'</span>, const=<span class="number">42</span>)</span><br><span class="line">args = parser.parse_args([<span class="string">'--foo'</span>)</span><br><span class="line">print(args)</span><br></pre></td></tr></table></figure><p>输出</p><blockquote><p>Namespace(foo=42)</p></blockquote><h4 id="store-true-and-store-false"><a href="#store-true-and-store-false" class="headerlink" title="store_true and store_false"></a>store_true and store_false</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">'--foo'</span>, action=<span class="string">'store_true'</span>)</span><br><span class="line">parser.add_argument(<span class="string">'--bar'</span>, action=<span class="string">'store_false'</span>)</span><br><span class="line">parser.add_argument(<span class="string">'--baz'</span>, action=<span class="string">'store_false'</span>)</span><br><span class="line">args = parser.parse_args(<span class="string">'--foo --bar'</span>.split())</span><br><span class="line">print(args)</span><br></pre></td></tr></table></figure><p>输出</p><blockquote><p>Namespace(bar=False, baz=True, foo=True)</p></blockquote><p>这里为什么是这样呢，因为默认存储的都是True，当你调用—bar,—foo参数时，会执行action操作，会把action指定的动作执行</p><h4 id="d-append"><a href="#d-append" class="headerlink" title="d.append"></a>d.append</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">'--foo'</span>, action=<span class="string">'append'</span>)</span><br><span class="line">args = parser.parse_args(<span class="string">'--foo 1 --foo 2 --foo 3'</span>.split())</span><br><span class="line">print(args)</span><br></pre></td></tr></table></figure><p>输出</p><blockquote><p>Namespace(foo=[‘1’, ‘2’, ‘3’])</p></blockquote><h4 id="append-const"><a href="#append-const" class="headerlink" title="append_const"></a>append_const</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">'--str'</span>, action=<span class="string">'append_const'</span>,const=str)</span><br><span class="line">parser.add_argument(<span class="string">'--int'</span>, action=<span class="string">'append_const'</span>,const=int)</span><br><span class="line">args = parser.parse_args(<span class="string">'--str --int'</span>.split())</span><br><span class="line">print(args)</span><br></pre></td></tr></table></figure><p>输出</p><blockquote><p>Namespace(int=[<class 'int'>], str=[<class 'str'>])</class></class></p></blockquote><h4 id="count"><a href="#count" class="headerlink" title="count"></a>count</h4><p>统计一个keyword argument出现了多少次<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">'--co'</span>, <span class="string">'-c'</span>,action=<span class="string">'count'</span>)</span><br><span class="line">args = parser.parse_args([<span class="string">'-ccc'</span>])</span><br><span class="line">print(args)</span><br></pre></td></tr></table></figure></p><p>输出</p><blockquote><p>Namespace(co=3)</p></blockquote><h4 id="help"><a href="#help" class="headerlink" title="help"></a>help</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">args = parser.parse_args(<span class="string">'--help'</span>.split())</span><br><span class="line">print(args)</span><br></pre></td></tr></table></figure><p>输出，如果是交互式环境的话，会退出python</p><blockquote><p>usage: [-h]</p><p>optional arguments:<br>  -h, —help  show this help message and exit</p></blockquote><h4 id="version"><a href="#version" class="headerlink" title="version"></a>version</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">'--version'</span>, action=<span class="string">'version'</span>,version=<span class="string">'version 3'</span>)</span><br><span class="line">args = parser.parse_args(<span class="string">'--version'</span>.split())</span><br><span class="line">print(args)</span><br></pre></td></tr></table></figure><p>输出,如果是交互式环境的话，会退出python</p><blockquote><p>version 3</p></blockquote><h3 id="nargs-指定参数个数"><a href="#nargs-指定参数个数" class="headerlink" title="nargs 指定参数个数"></a>nargs 指定参数个数</h3><h4 id="N"><a href="#N" class="headerlink" title="N"></a>N</h4><p>如果是可选参数的话，或者不指定这个参数，或者必须指定N个参数<br>如果是必选参数的话，必须指定N个参数，不能多也不能少，也不能为0个<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">'--foo'</span>,nargs=<span class="number">3</span>)</span><br><span class="line">parser.add_argument(<span class="string">'bar'</span>,nargs=<span class="number">4</span>)</span><br><span class="line">args = parser.parse_args(<span class="string">'bar 3 4 5'</span>.split())</span><br><span class="line">print(args)</span><br></pre></td></tr></table></figure></p><p>输出</p><blockquote><p>Namespace(bar=[‘bar’, ‘3’, ‘4’, ‘5’], foo=None)</p></blockquote><h4 id><a href="#" class="headerlink" title="?"></a>?</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">'--foo'</span>,nargs=<span class="string">'?'</span>,const=<span class="string">'c'</span>,default=<span class="string">'d'</span>)</span><br><span class="line">parser.add_argument(<span class="string">'bar'</span>,nargs=<span class="string">'?'</span>,default=<span class="string">'d'</span>)</span><br><span class="line">args = parser.parse_args(<span class="string">'3'</span>.split())</span><br><span class="line">print(args)</span><br><span class="line">args = parser.parse_args(<span class="string">'3 --foo'</span>.split())</span><br><span class="line">print(args)</span><br></pre></td></tr></table></figure><p>输出</p><blockquote><p>Namespace(bar=’3’, foo=’d’)<br>Namespace(bar=’3’, foo=’c’)</p></blockquote><p>如果显式指定可选参数，但是不给它参数，那么如果有const的话，就会显示const的值，否则就会显示None</p><h4 id="-1"><a href="#-1" class="headerlink" title="*"></a>*</h4><p>nargs设置为*的话，不能直接用const=’’来设置const参数，需要使用其他方式<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">'--foo'</span>,nargs=<span class="string">'*'</span>,default=<span class="string">'d'</span>)</span><br><span class="line">parser.add_argument(<span class="string">'bar'</span>,nargs=<span class="string">'*'</span>,default=<span class="string">'d'</span>)</span><br><span class="line">args = parser.parse_args(<span class="string">'3 --foo 3 4'</span>.split())</span><br><span class="line">print(args)</span><br></pre></td></tr></table></figure></p><p>输出</p><blockquote><p>Namespace(bar=[‘3’], foo=[‘3’, ‘4’])</p></blockquote><h4 id="-2"><a href="#-2" class="headerlink" title="+"></a>+</h4><p>nargs设置为+，参数个数必须大于等于1<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">'--foo'</span>,nargs=<span class="string">'+'</span>,default=<span class="string">'d'</span>)</span><br><span class="line">parser.add_argument(<span class="string">'bar'</span>,nargs=<span class="string">'+'</span>,default=<span class="string">'d'</span>)</span><br><span class="line">args = parser.parse_args(<span class="string">'3 3'</span>.split())</span><br><span class="line">print(args)</span><br><span class="line">args = parser.parse_args(<span class="string">'--foo 3'</span>.split())</span><br><span class="line">print(args)</span><br></pre></td></tr></table></figure></p><p>输出</p><blockquote><p>Namespace(bar=[‘3’], foo=’d’)<br>Namespace(bar=[‘3’], foo=[‘3’])</p></blockquote><h3 id="const"><a href="#const" class="headerlink" title="const"></a>const</h3><h4 id="action-’’store-const”-or-action-”append-const”"><a href="#action-’’store-const”-or-action-”append-const”" class="headerlink" title="action=’’store_const” or action=”append_const”"></a>action=’’store_const” or action=”append_const”</h4><p>the examples are in the action<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">'--foo'</span>, action=<span class="string">'store_const'</span>, const=<span class="number">42</span>)</span><br><span class="line">args = parser.parse_args([<span class="string">'--foo'</span>)</span><br><span class="line">print(args)</span><br></pre></td></tr></table></figure></p><p>输出</p><blockquote><p>Namespace(foo=42)</p></blockquote><h4 id="like-f-or-—foo-and-nargs-’-’"><a href="#like-f-or-—foo-and-nargs-’-’" class="headerlink" title="like -f or —foo and nargs=’?’"></a>like -f or —foo and nargs=’?’</h4><p>the examples are the same as examples in the nargs=’?’<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">'--foo'</span>,nargs=<span class="string">'?'</span>,const=<span class="string">'c'</span>,default=<span class="string">'d'</span>)</span><br><span class="line">parser.add_argument(<span class="string">'bar'</span>,nargs=<span class="string">'?'</span>,default=<span class="string">'d'</span>)</span><br><span class="line">args = parser.parse_args(<span class="string">'3'</span>.split())</span><br><span class="line">print(args)</span><br><span class="line">args = parser.parse_args(<span class="string">'3 --foo'</span>.split())</span><br><span class="line">print(args)</span><br></pre></td></tr></table></figure></p><p>输出</p><blockquote><p>Namespace(bar=’3’, foo=’d’)<br>Namespace(bar=’3’, foo=’c’)<br>如果显式指定可选参数，但是不给它参数，那么如果有const的话，就会显示const的值，否则就会显示None</p></blockquote><h3 id="default"><a href="#default" class="headerlink" title="default"></a>default</h3><p>default对于可选参数来说，是有用的，当可选参数没有在command line中显示出来时被使用，但是对于必选参数来说，只有nargs=?或者*才能起作用。</p><h4 id="对于可选参数"><a href="#对于可选参数" class="headerlink" title="对于可选参数"></a>对于可选参数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">'--foo'</span>,default=<span class="number">43</span>)</span><br><span class="line">args = parser.parse_args([])</span><br><span class="line">print(args)</span><br><span class="line">args = parser.parse_args(<span class="string">'--foo 3'</span>.split())</span><br><span class="line">print(args)</span><br></pre></td></tr></table></figure><p>输出</p><blockquote><p>Namespace(foo=’43’)<br>Namespace(foo=’3’)</p></blockquote><h4 id="对于必选参数"><a href="#对于必选参数" class="headerlink" title="对于必选参数"></a>对于必选参数</h4><p>对于nargs=‘+’是会出错<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(&apos;bar&apos;,nargs=&apos;+&apos;,default=&apos;d&apos;)</span><br><span class="line">args = parser.parse_args([])</span><br><span class="line">print(args)</span><br></pre></td></tr></table></figure></p><p>输出</p><blockquote><p>usage: [-h] bar [bar …]<br>: error: the following arguments are required: bar</p></blockquote><p>对于nargs=‘*’或者nargs=’?’就行了<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">'bar'</span>,nargs=<span class="string">'?'</span>,default=<span class="string">'d'</span>)</span><br><span class="line">args = parser.parse_args([])</span><br><span class="line">print(args)</span><br></pre></td></tr></table></figure></p><p>输出</p><blockquote><p>Namespace(bar=’d’)</p></blockquote><h3 id="type"><a href="#type" class="headerlink" title="type"></a>type</h3><p>将输入的字符串参数转换为你想要的参数类型<br>对于文件类型来说，这个文件必须在当前目录存在。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">'--door'</span>,type=int)</span><br><span class="line">parser.add_argument(<span class="string">'filename'</span>,type=file)</span><br><span class="line">parser.parse_args([<span class="string">'--door'</span>,<span class="string">'3'</span>,<span class="string">'hello.txt'</span>])</span><br></pre></td></tr></table></figure></p><p>输出</p><blockquote><p>Namespace(door=3)<br>这里的door就是int类型的</p></blockquote><h3 id="choices"><a href="#choices" class="headerlink" title="choices"></a>choices</h3><p>输入的参数必须在choices这个范围中，否则就会报错<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParse()</span><br><span class="line">parser.add_argument(<span class="string">'--door'</span>,type=int,choices=range(<span class="number">1</span>,<span class="number">9</span>))</span><br><span class="line">parser.parse_args([<span class="string">'--door'</span>,<span class="string">'3'</span>])</span><br></pre></td></tr></table></figure></p><p>输出</p><blockquote><p>Namespace(door=3)</p></blockquote><h3 id="required"><a href="#required" class="headerlink" title="required"></a>required</h3><p>如果将required设置为True的话，那么这个可选参数必须要设置的<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">'-f'</span>, <span class="string">'--foo-bar'</span>, <span class="string">'--foo'</span>,required=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure></p><h3 id="help-1"><a href="#help-1" class="headerlink" title="help"></a>help</h3><p>help可以设置某个参数的简要介绍。<br>使用help=argparse.SUPRESS可以在help界面中不显示这个参数的介绍<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">'-f'</span>, <span class="string">'--foo-bar'</span>, <span class="string">'--foo'</span>,help=<span class="string">'fool you '</span>)</span><br><span class="line">parser.add_argument(<span class="string">'-xs'</span>, <span class="string">'--y'</span>,help=argparse.SUPPRESS)</span><br><span class="line">parser.print_help()</span><br></pre></td></tr></table></figure></p><p>输出</p><blockquote><p>usage: [-h] [-f FOO_BAR]</p><p>optional arguments:<br>  -h, —help            show this help message and exit<br>  -f FOO_BAR, —foo-bar FOO_BAR, —foo FOO_BAR<br>                        fool you</p></blockquote><h3 id="dest"><a href="#dest" class="headerlink" title="dest"></a>dest</h3><p>dest就是在help输出时显示的optional和positional参数后跟的名字（没有指定metavar时）<br>如下,dest就是FOO<br>-foo FOO</p><h4 id="positional-argument"><a href="#positional-argument" class="headerlink" title="positional argument"></a>positional argument</h4><p>dest is normally supplied as the first argument to add_argument()</p><h4 id="可选参数"><a href="#可选参数" class="headerlink" title="可选参数"></a>可选参数</h4><p>对于optional argument选择，—参数最长的一个作为dest，如果没有最长的，选择第一个出现的，如果没有—参数名，选择-参数。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">'-f'</span>, <span class="string">'--foo-bar'</span>, <span class="string">'--foo'</span>)</span><br><span class="line">parser.add_argument(<span class="string">'-xs'</span>, <span class="string">'--y'</span>)</span><br><span class="line">args = parser.parse_args(<span class="string">'-f 1 -xs 2'</span>.split())</span><br><span class="line">print(args)</span><br><span class="line">args = parser.parse_args(<span class="string">'--foo 1 --y 2'</span>.split())</span><br><span class="line">print(args)</span><br><span class="line">parser.print_help()</span><br></pre></td></tr></table></figure></p><p>输出</p><blockquote><p>Namespace(foo_bar=’1’, y=’2’)<br>Namespace(foo_bar=’1’, y=’2’)<br>usage: [-h] [-f FOO_BAR] [-xs Y]</p><p>optional arguments:<br>  -h, —help            show this help message and exit<br>  -f FOO_BAR, —foo-bar FOO_BAR, —foo FOO_BAR<br>  -xs Y, —y Y</p></blockquote><h3 id="metavar"><a href="#metavar" class="headerlink" title="metavar"></a>metavar</h3><p>如果指定metavar变量名的话，那么help输出的postional和positional参数后跟的名字就是metavar的名字而不是dest的名字</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">'-f'</span>, <span class="string">'--foo-bar'</span>, <span class="string">'--foo'</span>,metavar=<span class="string">"FOO"</span>)</span><br><span class="line">parser.add_argument(<span class="string">'-xs'</span>, <span class="string">'--y'</span>,metavar=<span class="string">'XY'</span>)</span><br><span class="line">args = parser.parse_args(<span class="string">'-f 1 -xs 2'</span>.split())</span><br><span class="line">print(args)</span><br><span class="line">args = parser.parse_args(<span class="string">'--foo 1 --y 2'</span>.split())</span><br><span class="line">print(args)</span><br><span class="line">parser.print_help()</span><br></pre></td></tr></table></figure><p>输出</p><blockquote><p>Namespace(foo_bar=’1’, y=’2’)<br>Namespace(foo_bar=’1’, y=’2’)<br>usage: [-h] [-f FOO] [-xs XY]</p><p>optional arguments:<br>  -h, —help            show this help message and exit<br>  -f FOO, —foo-bar FOO, —foo FOO<br>  -xs XY, —y XY</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;简单的例子&quot;&gt;&lt;a href=&quot;#简单的例子&quot; class=&quot;headerlink&quot; title=&quot;简单的例子&quot;&gt;&lt;/a&gt;简单的例子&lt;/h2&gt;&lt;h3 id=&quot;创建一个parser&quot;&gt;&lt;a href=&quot;#创建一个parser&quot; class=&quot;headerlink&quot; 
      
    
    </summary>
    
      <category term="python" scheme="http://mxxhcm.github.io/categories/python/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="argparse" scheme="http://mxxhcm.github.io/tags/argparse/"/>
    
  </entry>
  
  <entry>
    <title>numpy笔记</title>
    <link href="http://mxxhcm.github.io/2019/03/18/python-numpy%E7%AC%94%E8%AE%B0/"/>
    <id>http://mxxhcm.github.io/2019/03/18/python-numpy笔记/</id>
    <published>2019-03-18T07:15:29.000Z</published>
    <updated>2019-10-21T06:51:19.712Z</updated>
    
    <content type="html"><![CDATA[<h2 id="numpy-ndarray"><a href="#numpy-ndarray" class="headerlink" title="numpy.ndarray"></a>numpy.ndarray</h2><h3 id="attribute-of-the-np-ndarray"><a href="#attribute-of-the-np-ndarray" class="headerlink" title="attribute of the np.ndarray"></a>attribute of the np.ndarray</h3><p>ndarray.shape        #array的shape<br>ndarray.ndim            #array的维度<br>ndarray.size            #the number of ndarray in array<br>ndarray.dtype        #type of the number in array，dtype可以是’S’,int等<br>ndarray.itemsize        #size of the element in array<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array[array&gt;<span class="number">0</span>].size    <span class="comment">#统计一个数组有多少个非零元素，不论array的维度是多少</span></span><br></pre></td></tr></table></figure></p><h3 id="改变数组数据类型"><a href="#改变数组数据类型" class="headerlink" title="改变数组数据类型"></a>改变数组数据类型</h3><p>将整形数组改为字符型<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = numpy.zeros((<span class="number">3</span>,<span class="number">4</span>),dtype=<span class="string">'i'</span>)</span><br><span class="line">a.astype(<span class="string">'S'</span>)</span><br></pre></td></tr></table></figure></p><h3 id="将numpy转为list"><a href="#将numpy转为list" class="headerlink" title="将numpy转为list"></a>将numpy转为list</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">a = np.zeros((<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>))</span><br><span class="line">b = a.tolist()</span><br><span class="line">print(b)</span><br><span class="line">print(len(b))</span><br><span class="line">print(len(b[<span class="number">0</span>]))</span><br><span class="line"><span class="comment"># [[[0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0]], [[0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0]], [[0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0]]]</span></span><br><span class="line"><span class="comment"># 3</span></span><br><span class="line"><span class="comment"># 4</span></span><br></pre></td></tr></table></figure><h3 id="reshape"><a href="#reshape" class="headerlink" title="reshape"></a>reshape</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">a = np.zeros((<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>))</span><br><span class="line">a.reshape(<span class="number">-1</span>, <span class="number">1</span>)</span><br></pre></td></tr></table></figure><h3 id="flatten"><a href="#flatten" class="headerlink" title="flatten"></a>flatten</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">a = np.zeros((<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>))</span><br><span class="line">a.flatten()</span><br></pre></td></tr></table></figure><h2 id="numpy数组初始化"><a href="#numpy数组初始化" class="headerlink" title="numpy数组初始化"></a>numpy数组初始化</h2><ul><li>numpy.array()</li><li>numpy.zeros()</li><li>numpy.empty()</li><li>numpy.random()</li></ul><h3 id="numpy-array"><a href="#numpy-array" class="headerlink" title="numpy.array()"></a>numpy.array()</h3><h4 id="API"><a href="#API" class="headerlink" title="API"></a>API</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">np.array(</span><br><span class="line">    object,</span><br><span class="line">    dtype=<span class="literal">None</span>,</span><br><span class="line">    copy=<span class="literal">True</span>,</span><br><span class="line">    order=<span class="literal">False</span>,</span><br><span class="line">    subok=<span class="literal">False</span>,</span><br><span class="line">    ndim=<span class="number">0</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="numpy-zeros"><a href="#numpy-zeros" class="headerlink" title="numpy.zeros()"></a>numpy.zeros()</h3><h4 id="API-1"><a href="#API-1" class="headerlink" title="API"></a>API</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">np.zeros(</span><br><span class="line">    shape,</span><br><span class="line">    dtype=float,</span><br><span class="line">    order=<span class="string">'C'</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h4 id="代码示例"><a href="#代码示例" class="headerlink" title="代码示例"></a>代码示例</h4><p><a href>代码地址</a><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">np.zeros((<span class="number">3</span>, <span class="number">4</span>),dtype=<span class="string">'i'</span>)</span><br><span class="line">``` </span><br><span class="line"></span><br><span class="line"><span class="comment">### numpy.empty()</span></span><br><span class="line"><span class="comment">#### API</span></span><br><span class="line">``` python</span><br><span class="line">np.empty(</span><br><span class="line">    shape,</span><br><span class="line">    dtype=float,</span><br><span class="line">    order=<span class="string">'C'</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure></p><h4 id="代码示例-1"><a href="#代码示例-1" class="headerlink" title="代码示例"></a>代码示例</h4><p><a href>代码地址</a><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.empty((<span class="number">3</span>, <span class="number">4</span>),dtype=<span class="string">'f'</span>)</span><br></pre></td></tr></table></figure></p><h3 id="numpy-random"><a href="#numpy-random" class="headerlink" title="numpy.random"></a>numpy.random</h3><h4 id="numpy-random-randn"><a href="#numpy-random-randn" class="headerlink" title="numpy.random.randn()"></a>numpy.random.randn()</h4><p>返回标准正态分布的一个样本<br>numpy.random.randn(d0, d1, …, dn)<br>例子<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.random.randn(<span class="number">3</span>,<span class="number">4</span>)</span><br></pre></td></tr></table></figure></p><blockquote><p>array([[ 0.47203644, -0.0869761 , -1.02814481, -0.45945482],<br>       [ 0.34586502, -0.63121119,  0.35510786,  0.82975136],<br>       [-2.00253326, -0.63773715, -0.82700167,  1.80724647]])</p></blockquote><h4 id="numpy-random-rand"><a href="#numpy-random-rand" class="headerlink" title="numpy.random.rand()"></a>numpy.random.rand()</h4><p>创建一个给定shape的数组，从区间[0,1)上的均匀分布中随机采样</p><blockquote><p>create an array of the given shape and populate it with random samples from a uniform disctribution over [0,1)</p></blockquote><p>numpy.random.rand(d0,d1,…,dn)<br>例子<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.random.rand(<span class="number">3</span>,<span class="number">4</span>)</span><br></pre></td></tr></table></figure></p><h4 id="numpy-random-random"><a href="#numpy-random-random" class="headerlink" title="numpy.random.random()"></a>numpy.random.random()</h4><p>返回区间[0.0, 1.0)之间的随机浮点数</p><blockquote><p>return random floats in the half-open interval [0.0,1.0)</p></blockquote><p>numpy.random.random(size=None)<br>例子<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.random.random((<span class="number">3</span>,<span class="number">4</span>))</span><br></pre></td></tr></table></figure></p><h5 id="Note"><a href="#Note" class="headerlink" title="Note"></a>Note</h5><p>注意，random.random()和random.rand()实现的功能都是一样的，就是输入的参数不同。见参考文献[1]。</p><h4 id="numpy-random-ranf"><a href="#numpy-random-ranf" class="headerlink" title="numpy.random.ranf()"></a>numpy.random.ranf()</h4><p>我觉得它和random.random()没啥区别</p><h4 id="numpy-random-randint"><a href="#numpy-random-randint" class="headerlink" title="numpy.random.randint()"></a>numpy.random.randint()</h4><blockquote><p>return random integers from low(inclusive) to high(exclusive),[low,high) if high is None,then results are from [0,low)</p></blockquote><p>numpy.random.randint(low,high=None,size=None,dtype=’l’)<br>例子<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">np.random.randint(<span class="number">3</span>,size=[<span class="number">3</span>,<span class="number">4</span>])</span><br><span class="line">np.random.randint(<span class="number">4</span>,<span class="number">6</span>,size=[<span class="number">6</span>,<span class="number">2</span>])</span><br></pre></td></tr></table></figure></p><h4 id="numpy-random-RandomState"><a href="#numpy-random-RandomState" class="headerlink" title="numpy.random.RandomState()"></a>numpy.random.RandomState()</h4><blockquote><p>class numpy.random.RandomState(seed=None)</p></blockquote><p>这是一个类，给定一个种子，它接下来产生的一系列随机数都是固定的。每次需要重新产生随机数的时候，就重置种子。<br>通过一个例子来看：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">rdm = np.randrom.RandomState()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">   rdm.seed(<span class="number">3</span>)</span><br><span class="line">   print(rdm.rand())</span><br><span class="line">   print(rdm.rand())</span><br><span class="line">   print(rdm.rand())</span><br><span class="line">    print(<span class="string">"\n"</span>)</span><br><span class="line"><span class="comment"># 0.9670298390136767</span></span><br><span class="line"><span class="comment"># 0.5472322491757223</span></span><br><span class="line"><span class="comment"># 0.9726843599648843</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 0.9670298390136767</span></span><br><span class="line"><span class="comment"># 0.5472322491757223</span></span><br><span class="line"><span class="comment"># 0.9726843599648843</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 0.9670298390136767</span></span><br><span class="line"><span class="comment"># 0.5472322491757223</span></span><br><span class="line"><span class="comment"># 0.9726843599648843</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 0.9670298390136767</span></span><br><span class="line"><span class="comment"># 0.5472322491757223</span></span><br><span class="line"><span class="comment"># 0.9726843599648843</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 0.9670298390136767</span></span><br><span class="line"><span class="comment"># 0.5472322491757223</span></span><br><span class="line"><span class="comment"># 0.9726843599648843</span></span><br></pre></td></tr></table></figure></p><h3 id="创建bool类型数组"><a href="#创建bool类型数组" class="headerlink" title="创建bool类型数组"></a>创建bool类型数组</h3><p>np.ones([2, 2], dtype=bool)<br>np.zeros([2, 2], dtype=bool)</p><h3 id="others"><a href="#others" class="headerlink" title="others"></a>others</h3><h4 id="numpy-arange"><a href="#numpy-arange" class="headerlink" title="numpy.arange()"></a>numpy.arange()</h4><h4 id="numpy-linspace"><a href="#numpy-linspace" class="headerlink" title="numpy.linspace()"></a>numpy.linspace()</h4><h2 id="np-random-binomial"><a href="#np-random-binomial" class="headerlink" title="np.random.binomial"></a>np.random.binomial</h2><h3 id="API-2"><a href="#API-2" class="headerlink" title="API"></a>API</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">numpy.random.binomial(</span><br><span class="line">n, </span><br><span class="line">p, </span><br><span class="line">size=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>二项分布，共有三个参数，前两个是必选参数，第三个是可选参数。$n$是实验的个数，比如同时扔三枚硬币，这里就是$n=3$,$p$是为$1$的概率。$size$是总共进行多少次实验。<br>返回值是在每次试验中，trival成功的个数。如果是一个scalar，代表$size=1$，如果是一个list，代表$size\gt 1$。</p><h3 id="代码示例-2"><a href="#代码示例-2" class="headerlink" title="代码示例"></a>代码示例</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    rand = np.random.binomial(<span class="number">2</span>, <span class="number">0.9</span>)</span><br><span class="line">    print(rand)</span><br><span class="line"><span class="comment"># 可以看成扔2个硬币，每个硬币正面向上的概率是0.9,最后有几个硬币正面向上。</span></span><br><span class="line"><span class="comment"># 2</span></span><br><span class="line"><span class="comment"># 2</span></span><br><span class="line"><span class="comment"># 2</span></span><br><span class="line"><span class="comment"># 2</span></span><br><span class="line"><span class="comment"># 2</span></span><br><span class="line"><span class="comment"># 2</span></span><br><span class="line"><span class="comment"># 2</span></span><br><span class="line"><span class="comment"># 1</span></span><br><span class="line"><span class="comment"># 1</span></span><br><span class="line"><span class="comment"># 2</span></span><br><span class="line"></span><br><span class="line">rand = np.random.binomial(<span class="number">3</span>, <span class="number">0.9</span>, <span class="number">5</span>)</span><br><span class="line">print(rand)</span><br><span class="line"><span class="comment"># 可以看成扔3个硬币，每个硬币正面向上的概率是0.9,最后有几个硬币正面向上。一共进行5次实验。</span></span><br><span class="line"><span class="comment"># [2 2 3 3 2]</span></span><br></pre></td></tr></table></figure><h2 id="np-random-choice"><a href="#np-random-choice" class="headerlink" title="np.random.choice"></a>np.random.choice</h2><h3 id="API-3"><a href="#API-3" class="headerlink" title="API"></a>API</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">numpy.random.choice(</span><br><span class="line">    a,  <span class="comment"># 1d array或者int，如果是一个数组，从其中生成样本；如果是一个整数，从np.arange(a)中生成样本</span></span><br><span class="line">    size=<span class="literal">None</span>,  <span class="comment"># output shape，比如是(m, n, k)的话，总共要m*n*k个样本，默认是None,返回一个样本。</span></span><br><span class="line">    replace=<span class="literal">True</span>,   <span class="comment"># 是否使用replacement，设置为False的话所有元素不重复。</span></span><br><span class="line">    p=<span class="literal">None</span>  <span class="comment"># 概率分布，相加必须等于1，默认是从一个均匀分布中采样。</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="代码示例-3"><a href="#代码示例-3" class="headerlink" title="代码示例"></a>代码示例</h3><p><a href>代码地址</a><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">a0 = np.random.choice([<span class="number">8</span>, <span class="number">9</span>, <span class="number">-1</span>, <span class="number">2</span>, <span class="number">0</span>], <span class="number">3</span>)</span><br><span class="line">print(a0)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 从np.arange(5)从使用均匀分布采样一个shape为4的样本</span></span><br><span class="line">a1 = np.random.choice(<span class="number">5</span>, <span class="number">4</span>)</span><br><span class="line">print(a1)</span><br><span class="line"></span><br><span class="line">a2 = np.random.choice(<span class="number">5</span>, <span class="number">8</span>, p=[<span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.5</span>, <span class="number">0.2</span>, <span class="number">0</span>])</span><br><span class="line">print(a2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># replace 设置为False，相当于np.random.permutation()</span></span><br><span class="line">a3 = np.random.choice([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">8</span>, <span class="number">9</span>], <span class="number">5</span>, replace=<span class="literal">False</span>)</span><br><span class="line">print(a3)</span><br></pre></td></tr></table></figure></p><h2 id="np-random-permutation"><a href="#np-random-permutation" class="headerlink" title="np.random.permutation"></a>np.random.permutation</h2><h3 id="API-4"><a href="#API-4" class="headerlink" title="API"></a>API</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">np.random.permutation(</span><br><span class="line">    x   <span class="comment"># int或者array，如果是int，置换np.arange(x)。如果是array，make a copy，随机打乱元素。</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>对输入序列进行排列组合，如果输入是多维的话，只会在第一维重新排列。</p><h3 id="代码示例-4"><a href="#代码示例-4" class="headerlink" title="代码示例"></a>代码示例</h3><p><a href>代码地址</a><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">a1 = np.random.permutation(<span class="number">9</span>)</span><br><span class="line">print(a1)</span><br><span class="line"></span><br><span class="line">a2 = np.random.permutation([<span class="number">1</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">9</span>, <span class="number">8</span>])</span><br><span class="line">print(a2)</span><br><span class="line"></span><br><span class="line">a3 = np.random.permutation(np.arange(<span class="number">9</span>).reshape(<span class="number">3</span>, <span class="number">3</span>))</span><br><span class="line">print(a3)</span><br></pre></td></tr></table></figure></p><h2 id="np-random-normal"><a href="#np-random-normal" class="headerlink" title="np.random.normal"></a>np.random.normal</h2><h3 id="API-5"><a href="#API-5" class="headerlink" title="API"></a>API</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">numpy.random.normal(loc=0.0, scale=1.0, size=None)  </span><br><span class="line">loc:float，正态分布的均值，对应着整个分布的center</span><br><span class="line">scale:float，正态分布的标准差，对应于分布的宽度，scale越大越矮胖，scale越小，越瘦高</span><br><span class="line">size:int or tuple of ints，输出的shape，默认为None，只输出一个值</span><br><span class="line">np.random.randn(size)相当于np.random.normal(loc=0, scale=1, size)</span><br></pre></td></tr></table></figure><h2 id="np-argsort"><a href="#np-argsort" class="headerlink" title="np.argsort"></a>np.argsort</h2><h3 id="API-6"><a href="#API-6" class="headerlink" title="API"></a>API</h3><p>numpy.argsort(a, axis=-1, kind=’quicksort’, order=None)<br>axis:对哪个axis进行排序，默认是-1</p><h3 id="功能"><a href="#功能" class="headerlink" title="功能"></a>功能</h3><p>将数组排序后（默认是从小到大排序），返回排序后的数组在原数组中的位置。</p><p>参考文献<br>1.<a href="https://stackoverflow.com/questions/47231852/np-random-rand-vs-np-random-random" target="_blank" rel="noopener">https://stackoverflow.com/questions/47231852/np-random-rand-vs-np-random-random</a><br>2.<a href="https://stackoverflow.com/questions/21174961/how-to-create-a-numpy-array-of-all-true-or-all-false" target="_blank" rel="noopener">https://stackoverflow.com/questions/21174961/how-to-create-a-numpy-array-of-all-true-or-all-false</a><br>3.<a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.choice.html" target="_blank" rel="noopener">https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.choice.html</a><br>4.<a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.permutation.html" target="_blank" rel="noopener">https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.permutation.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;numpy-ndarray&quot;&gt;&lt;a href=&quot;#numpy-ndarray&quot; class=&quot;headerlink&quot; title=&quot;numpy.ndarray&quot;&gt;&lt;/a&gt;numpy.ndarray&lt;/h2&gt;&lt;h3 id=&quot;attribute-of-the-np-n
      
    
    </summary>
    
      <category term="python" scheme="http://mxxhcm.github.io/categories/python/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="numpy" scheme="http://mxxhcm.github.io/tags/numpy/"/>
    
  </entry>
  
  <entry>
    <title>h5py笔记</title>
    <link href="http://mxxhcm.github.io/2019/03/18/python-hdf5%E7%AC%94%E8%AE%B0/"/>
    <id>http://mxxhcm.github.io/2019/03/18/python-hdf5笔记/</id>
    <published>2019-03-18T07:12:03.000Z</published>
    <updated>2019-06-13T02:06:17.974Z</updated>
    
    <content type="html"><![CDATA[<h2 id="python包安装"><a href="#python包安装" class="headerlink" title="python包安装"></a>python包安装</h2><p>~$:pip install h5py</p><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><h3 id="创建和打开h5py文件"><a href="#创建和打开h5py文件" class="headerlink" title="创建和打开h5py文件"></a>创建和打开h5py文件</h3><p>f = h5py.File(“pathname”,”w”)<br>w     create file, truncate if exist<br>w- or x  create file,fail if exists<br>r         readonly, file must be exist r+        read/write,file must be exist<br>a        read/write if exists,create othrewise (default)</p><h3 id="删除一个dataset或者group"><a href="#删除一个dataset或者group" class="headerlink" title="删除一个dataset或者group"></a>删除一个dataset或者group</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">del</span> group[<span class="string">"dataset_name/group_name"</span>]</span><br></pre></td></tr></table></figure><h2 id="dataset"><a href="#dataset" class="headerlink" title="dataset"></a>dataset</h2><h3 id="什么是dataset"><a href="#什么是dataset" class="headerlink" title="什么是dataset"></a>什么是dataset</h3><p>datasets和numpy arrays挺像的</p><h3 id="创建一个dataset"><a href="#创建一个dataset" class="headerlink" title="创建一个dataset"></a>创建一个dataset</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">f = h5py.File(<span class="string">"pathname"</span>,<span class="string">"w"</span>)</span><br><span class="line">f.create_dataset(<span class="string">"dataset_name"</span>, (<span class="number">10</span>,), dtype=<span class="string">'i'</span>)</span><br><span class="line">f.create_dataset(<span class="string">"dataset_name"</span>, (<span class="number">10</span>,), dtype=<span class="string">'c'</span>)</span><br></pre></td></tr></table></figure><p>第一个参数是dataset的名字, 第二个参数是dataset的shape, dtype参数是dataset中元素的类型。</p><h3 id="如何访问一个dataset"><a href="#如何访问一个dataset" class="headerlink" title="如何访问一个dataset"></a>如何访问一个dataset</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dataset = f[<span class="string">"dataset_name"</span>]                           <span class="comment"># acess like a python dict</span></span><br><span class="line">dataset = f.create_dateset(<span class="string">"dataset_name"</span>)  <span class="comment"># or create a new dataset</span></span><br></pre></td></tr></table></figure><h3 id="dataset的属性"><a href="#dataset的属性" class="headerlink" title="dataset的属性"></a>dataset的属性</h3><p>dataset.name        #输出dataset的名字<br>dataset.tdype        #输出dataset中elements的type<br>dataset.shape        #输出dataset的shape<br>dataset.value<br>dataset doesn’t hava attrs like keys,values,items,etc..</p><h3 id="给h5py-dataset复制numpy-array"><a href="#给h5py-dataset复制numpy-array" class="headerlink" title="给h5py dataset复制numpy array"></a>给h5py dataset复制numpy array</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">array = np.zero((<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">h[<span class="string">'array'</span>] = array        <span class="comment"># in h5py file, you need't to explicit declare the shape of array, just assign it an object of numpy array</span></span><br></pre></td></tr></table></figure><h2 id="group"><a href="#group" class="headerlink" title="group"></a>group</h2><h3 id="什么是group"><a href="#什么是group" class="headerlink" title="什么是group"></a>什么是group</h3><p>group和字典挺像的</p><h3 id="创建一个group"><a href="#创建一个group" class="headerlink" title="创建一个group"></a>创建一个group</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">group = f.create_group(<span class="string">"group_name"</span>)    <span class="comment">#在f下创建一个group</span></span><br><span class="line">group.create_group(<span class="string">"group_name"</span>)        <span class="comment">#在group下创建一个group</span></span><br><span class="line">group.create_dataset(<span class="string">"dataset_name"</span>)    <span class="comment">#在group下创建一个dataset</span></span><br></pre></td></tr></table></figure><h3 id="访问一个group-the-same-as-dataset"><a href="#访问一个group-the-same-as-dataset" class="headerlink" title="访问一个group(the same as dataset)"></a>访问一个group(the same as dataset)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">group = f[<span class="string">"group_name"</span>]                           <span class="comment"># acess like a python dict</span></span><br><span class="line">group = f.create_dateset(<span class="string">"group_name"</span>)  <span class="comment"># or create a new group</span></span><br></pre></td></tr></table></figure><h3 id="group的属性和方法"><a href="#group的属性和方法" class="headerlink" title="group的属性和方法"></a>group的属性和方法</h3><p>group.name        #输出group的名字<br>以下内容分为python2和python3版本</p><h4 id="python-2-版本"><a href="#python-2-版本" class="headerlink" title="python 2 版本"></a>python 2 版本</h4><p>group.values()    #输出group的value<br>group.keys()        #输出gorup的keys<br>group.items()    #输出group中所有的item，包含group和dataste</p><h4 id="python-3-版本"><a href="#python-3-版本" class="headerlink" title="python 3 版本"></a>python 3 版本</h4><p>list(group.keys())<br>list(group.values())<br>list(group.items())</p><h2 id="属性"><a href="#属性" class="headerlink" title="属性"></a>属性</h2><h3 id="设置dataset属性"><a href="#设置dataset属性" class="headerlink" title="设置dataset属性"></a>设置dataset属性</h3><p>dataset.attrs[“attr_name”]=”attr_value”    #设置attr<br>print(dataset.attrs[“attr_name”])                #访问attr</p><h3 id="设置group属性"><a href="#设置group属性" class="headerlink" title="设置group属性"></a>设置group属性</h3><p>group.attrs[“attr_name”]=”attr_value”    #设置attr<br>print(group.attrs[“attr_name”])                #访问attr</p><h2 id="numpy-and-h5py"><a href="#numpy-and-h5py" class="headerlink" title="numpy and h5py"></a>numpy and h5py</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">f = h5py.File(pathname,<span class="string">"r"</span>)</span><br><span class="line"></span><br><span class="line">data = f[<span class="string">'data'</span>]    <span class="comment"># type 是dataset</span></span><br><span class="line">data = f[<span class="string">'data'</span>][:] <span class="comment">#type是numpy ndarray</span></span><br><span class="line">f.close()</span><br></pre></td></tr></table></figure><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>1.<a href="http://docs.h5py.org/en/latest/index.html" target="_blank" rel="noopener">http://docs.h5py.org/en/latest/index.html</a><br>2.<a href="https://stackoverflow.com/questions/31037088/discovering-keys-using-h5py-in-python3" target="_blank" rel="noopener">https://stackoverflow.com/questions/31037088/discovering-keys-using-h5py-in-python3</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;python包安装&quot;&gt;&lt;a href=&quot;#python包安装&quot; class=&quot;headerlink&quot; title=&quot;python包安装&quot;&gt;&lt;/a&gt;python包安装&lt;/h2&gt;&lt;p&gt;~$:pip install h5py&lt;/p&gt;
&lt;h2 id=&quot;简介&quot;&gt;&lt;a hre
      
    
    </summary>
    
      <category term="python" scheme="http://mxxhcm.github.io/categories/python/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="h5py" scheme="http://mxxhcm.github.io/tags/h5py/"/>
    
  </entry>
  
  <entry>
    <title>神经网络-激活函数</title>
    <link href="http://mxxhcm.github.io/2019/03/14/activation/"/>
    <id>http://mxxhcm.github.io/2019/03/14/activation/</id>
    <published>2019-03-14T03:45:46.000Z</published>
    <updated>2019-05-06T16:22:27.712Z</updated>
    
    <content type="html"><![CDATA[<h2 id="激活函数的一些问题">激活函数的一些问题</h2><h3 id="为什么要使用non-linear激活函数不使用linear激活函数？">为什么要使用non-linear激活函数不使用linear激活函数？</h3><p><img src="/2019/03/14/activation/fnn.png" alt="fnn"><br>给定一个如图所示的前馈神经网络。有一个输入层，一个隐藏层，一个输出层。输入是$2$维的，有$4$个隐藏单元，输出是$2$维的。<br>则：$ \hat{f}(x) = \sigma(w_1x+b_1)w_2 + b_2$<br>这里$\sigma$是一个线性的激活函数，不妨设$\sigma(x) = x$。<br>那么就有：<br>\begin{align*}<br>\hat{f}(x) &amp;= \sigma(w_1x+b_1)w_2 + b_2\<br>&amp;= (w_1x+b_1)w_2 + b_2\<br>&amp;= w_1w_2x + w_2b1 + b_2\<br>&amp;= (w_1w_2) x + (w_2b1 + b_2)\<br>&amp;= w’ x + b’<br>\end{align*}<br>因此，当使用线性激活函数的时候，我们可以把一个多层感知机模型化简成一个线性模型。当使用线性激活函数时，增加网络的深度没有用，使用线性激活函数的十层感知机和一层感知机没有区别，并不能增加网络的表达能力。因为任意两个仿射函数的组合还是仿射函数。</p><h3 id="为什么relu激活函数是non-linear的？">为什么ReLU激活函数是non-linear的？</h3><p>ReLU的数学表达形式如下：<br>$$g(x) = max(0, x)$$<br>首先考虑一下什么是linear function,什么是non-linear function。在微积分上，平面内的任意一条直线是线性函数，否则就是非线性函数。<br>考虑这样一个例子，输入数据的维度为$1$，输出数据的维度也为$1$，用$g(ax+b)$表示ReLU激活函数。如果我们使用两个隐藏单元，那么$h_1(x) = g(x)+g(-x)$可以用来表示$f(x)=|x|$，而函数$|x|$是一个非线性函数，函数图像如下所示。<br><img src="/2019/03/14/activation/absolute.png" alt="f(x)=|x|"><br>我们还可以用ReLU逼近二次函数$f(x) = x^2$，如使用函数$h_2(x) = g(x) + g(-x) + g(2x-2) + g(2x+2)$逼近二次函数，对应的图像如下。<br><img src="/2019/03/14/activation/quadratic.png" alt="h_2(x)"><br>使用的项越多，最后近似出来的图像也就和我们要逼近的二次函数越像。<br>同理，可以使用ReLU激活函数去逼近任意非线性函数。</p><h3 id="为什么relu比sigmod还有tanh激活函数要好？">为什么ReLU比sigmod还有tanh激活函数要好？</h3><p>ReLU收敛的更快，因为梯度更大。<br>当CNN的层数越来越深的时候，实验表明，使用ReLU的CNN要比使用sigmod或者tanh的CNN训练的更容易，更快收敛。<br>为什么会这样，目前有两种理论，见参考文献[4]。<br>第一个，$tanh(x)$有梯度消散问题(vanishing gradient)。当$x$趋向于$\pm\infty$时，$tanh(x)$的导数趋向于$0$。如下图所示。</p><blockquote><p>Vanishing gradients occur when lower layers of a DNN have gradients of nearly 0 because higher layer units are nearly saturated at -1 or 1, the asymptotes of the tanh function. Such vanishing gradients cause slow optimization convergence, and in some cases the final trained network converges to a poor local minimum.</p></blockquote><blockquote><p>One way ReLUs improve neural networks is by speeding up training. The gradient computation is very simple (either 0 or 1 depending on the sign of x). Also, the computational step of a ReLU is easy: any negative elements are set to 0.0 – no exponentials, no multiplication or division operations.</p></blockquote><p><img src="/2019/03/14/activation/tanh.png" alt="tanh(x)"><br>ReLU是non-saturating nonlinearity的激活函数，sigmod和tanh是saturating nonlinearity激活函数，会将输出挤压到一个区间内。</p><blockquote><p>f是non-saturating 当且仅当$|lim_{z\rightarrow -\infty} f(z)| \rightarrow + \infty$或者$|lim_{z\rightarrow +\infty} f(z)| \rightarrow + \infty$</p></blockquote><p>tanh和sigmod将输入都挤压在某一个很小的区间内，比如(0,1)，输入发生很大的变化，经过激活函数以后变化很小，经过好几层之后，基本上就没有差别了。而当网络很深的时候，反向传播主要集中在后几层，而输入层附近的权值没办法好好学习。而对于ReLU来说，任意深度的神经网络，都不存在梯度消失。</p><p>第二种理论是说有一些定理能够证明，在某些假设条件下，局部最小就是全局最小。如果使用sigmod或者tanh激活函数的时候，这些假设不能成立，而使用ReLU的话，这些条件就会成立。</p><h3 id="为什么发生了梯度消失以后训练结构很差？">为什么发生了梯度消失以后训练结构很差？</h3><p>我的想法是，</p><h2 id="参考文献">参考文献</h2><p>1.<a href="https://stats.stackexchange.com/a/391971" target="_blank" rel="noopener">https://stats.stackexchange.com/a/391971</a><br>2.<a href="https://stats.stackexchange.com/a/299933" target="_blank" rel="noopener">https://stats.stackexchange.com/a/299933</a><br>3.<a href="https://stats.stackexchange.com/a/141978" target="_blank" rel="noopener">https://stats.stackexchange.com/a/141978</a><br>4.<a href="https://stats.stackexchange.com/a/335972" target="_blank" rel="noopener">https://stats.stackexchange.com/a/335972</a><br>5.<a href="https://stats.stackexchange.com/a/174438" target="_blank" rel="noopener">https://stats.stackexchange.com/a/174438</a><br>6.<a href="https://stats.stackexchange.com/questions/391968/relu-vs-a-linear-activation-function" target="_blank" rel="noopener">https://stats.stackexchange.com/questions/391968/relu-vs-a-linear-activation-function</a><br>7.<a href="https://stats.stackexchange.com/questions/141960/why-are-rectified-linear-units-considered-non-linear" target="_blank" rel="noopener">https://stats.stackexchange.com/questions/141960/why-are-rectified-linear-units-considered-non-linear</a><br>8.<a href="https://stats.stackexchange.com/questions/299915/how-does-the-rectified-linear-unit-relu-activation-function-produce-non-linear" target="_blank" rel="noopener">https://stats.stackexchange.com/questions/299915/how-does-the-rectified-linear-unit-relu-activation-function-produce-non-linear</a><br>9.<a href="https://stats.stackexchange.com/questions/226923/why-do-we-use-relu-in-neural-networks-and-how-do-we-use-it/226927#226927" target="_blank" rel="noopener">https://stats.stackexchange.com/questions/226923/why-do-we-use-relu-in-neural-networks-and-how-do-we-use-it/226927#226927</a><br>10.<a href="https://www.zhihu.com/question/264163033" target="_blank" rel="noopener">https://www.zhihu.com/question/264163033</a><br>11.<a href="http://ai.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf" target="_blank" rel="noopener">http://ai.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;激活函数的一些问题&quot;&gt;激活函数的一些问题&lt;/h2&gt;
&lt;h3 id=&quot;为什么要使用non-linear激活函数不使用linear激活函数？&quot;&gt;为什么要使用non-linear激活函数不使用linear激活函数？&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/2019/03/
      
    
    </summary>
    
      <category term="机器学习" scheme="http://mxxhcm.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://mxxhcm.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="深度学习" scheme="http://mxxhcm.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="激活函数" scheme="http://mxxhcm.github.io/tags/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/"/>
    
      <category term="神经网络" scheme="http://mxxhcm.github.io/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="ReLU" scheme="http://mxxhcm.github.io/tags/ReLU/"/>
    
      <category term="tanh" scheme="http://mxxhcm.github.io/tags/tanh/"/>
    
      <category term="sigmod" scheme="http://mxxhcm.github.io/tags/sigmod/"/>
    
  </entry>
  
  <entry>
    <title>python 常见问题（不定期更新）</title>
    <link href="http://mxxhcm.github.io/2019/03/13/python-%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/"/>
    <id>http://mxxhcm.github.io/2019/03/13/python-常见问题/</id>
    <published>2019-03-13T02:40:03.000Z</published>
    <updated>2019-10-11T05:30:51.354Z</updated>
    
    <content type="html"><![CDATA[<h2 id="问题1-‘dict-values’-object-does-not-support-indexing’"><a href="#问题1-‘dict-values’-object-does-not-support-indexing’" class="headerlink" title="问题1-‘dict_values’ object does not support indexing’"></a>问题1-‘dict_values’ object does not support indexing’</h2><p>参考文献[1,2,3]</p><h3 id="报错"><a href="#报错" class="headerlink" title="报错"></a>报错</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&apos;dict_values&apos; object does not support indexing&apos;</span><br></pre></td></tr></table></figure><h3 id="原因"><a href="#原因" class="headerlink" title="原因"></a>原因</h3><p>The objects returned by dict.keys(), dict.values() and dict.items() are view objects. They provide a dynamic view on the dictionary’s entries, which means that when the dictionary changes, the view reflects these changes.<br>python3 中调用字典对象的一些函数，返回值是view objects。如果要转换为list的话，需要使用list()强制转换。<br>而python2的返回值直接就是list。</p><h3 id="代码示例"><a href="#代码示例" class="headerlink" title="代码示例"></a>代码示例</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">m_dict = &#123;<span class="string">'a'</span>: <span class="number">10</span>, <span class="string">'b'</span>: <span class="number">20</span>&#125;</span><br><span class="line">values = m_dict.values()</span><br><span class="line">print(type(values))</span><br><span class="line">print(values)</span><br><span class="line">print(<span class="string">"\n"</span>)</span><br><span class="line"></span><br><span class="line">items = m_dict.items()</span><br><span class="line">print(type(items))</span><br><span class="line">print(items)</span><br><span class="line">print(<span class="string">"\n"</span>)</span><br><span class="line"></span><br><span class="line">keys = m_dict.keys()</span><br><span class="line">print(type(keys))</span><br><span class="line">print(keys)</span><br><span class="line">print(<span class="string">"\n"</span>)</span><br></pre></td></tr></table></figure><p>如果使用python3执行以上代码，输出结果如下所示：</p><blockquote><p>class ‘dict_values’<br>dict_values([10, 20])<br>class ‘dict_items’<br>dict_items([(‘a’, 10), (‘b’, 20)])<br>class ‘dict_keys’<br>dict_keys([‘a’, ‘b’])</p></blockquote><p>如果使用python2执行以上代码，输出结果如下所示：</p><blockquote><p>type ‘list’<br>[10, 20]<br>type ‘list’<br>[(‘a’, 10), (‘b’, 20)]<br>type ‘list’<br>[‘a’, ‘b’]</p></blockquote><h2 id="问题2-‘TimeLimit’-object-has-no-attribute-‘ale’"><a href="#问题2-‘TimeLimit’-object-has-no-attribute-‘ale’" class="headerlink" title="问题2-‘TimeLimit’ object has no attribute ‘ale’"></a>问题2-‘TimeLimit’ object has no attribute ‘ale’</h2><p>参考文献[4,5,6]</p><h3 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h3><p>运行github clone 下来的<a href="https://github.com/devsisters/DQN-tensorflow" target="_blank" rel="noopener">DQN-tensorflow</a>，报错:</p><blockquote><p>AttributeError: ‘TimeLimit’ object has no attribute ‘ale’.</p></blockquote><h3 id="原因-1"><a href="#原因-1" class="headerlink" title="原因"></a>原因</h3><p>是因为gym版本原因，在gym 0.7版本中，可以使用env.ale.lives()访问ale属性，但是0.8版本以及以上，就没有了该属性，可以在系列函数中添加如下修改：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, config)</span>:</span></span><br><span class="line">    self.step_info = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_step</span><span class="params">(self, action)</span>:</span></span><br><span class="line">    self._screen, self.reward, self.terminal, self.step_info = self.env.step(action)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lives</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> self.step_info <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> self.step_info[<span class="string">'ale.lives'</span>]</span><br></pre></td></tr></table></figure></p><h3 id="ale属性是什么"><a href="#ale属性是什么" class="headerlink" title="ale属性是什么"></a>ale属性是什么</h3><p>我看官方文档也没有看清楚，但是我觉得就是生命值是否没有了</p><blockquote><p>info (dict): diagnostic information useful for debugging. It can sometimes be useful for learning (for example, it might contain the raw probabilities behind the environment’s last state change). However, official evaluations of your agent are not allowed to use this for learning.</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> gym</span><br><span class="line">env = gym.make(<span class="string">'CartPole-v0'</span>)</span><br><span class="line"><span class="keyword">for</span> i_episode <span class="keyword">in</span> range(<span class="number">20</span>):</span><br><span class="line">    observation = env.reset()</span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">        env.render()</span><br><span class="line">        print(observation)</span><br><span class="line">        action = env.action_space.sample()</span><br><span class="line">        observation, reward, done, info = env.step(action)</span><br><span class="line">        <span class="keyword">if</span> done:</span><br><span class="line">            print(<span class="string">"Episode finished after &#123;&#125; timesteps"</span>.format(t+<span class="number">1</span>))</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">env.close()</span><br></pre></td></tr></table></figure><h2 id="问题3-cannot-import-name"><a href="#问题3-cannot-import-name" class="headerlink" title="问题3-cannot import name ***"></a>问题3-cannot import name ***</h2><p>参考文献[7]</p><h3 id="报错-1"><a href="#报错-1" class="headerlink" title="报错"></a>报错</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cannot import name tqdm</span><br></pre></td></tr></table></figure><h3 id="问题原因"><a href="#问题原因" class="headerlink" title="问题原因"></a>问题原因</h3><p>谷歌了半天，没有发现原因，然后百度了一下，发现了原因，看来还是自己太菜了。。<br>因为自己起的文件名就叫tqdm，然后就和库中的tqdm冲突了，这也太蠢了吧。。。</p><h2 id="问题4-linux下python执行shell脚本输出重定向"><a href="#问题4-linux下python执行shell脚本输出重定向" class="headerlink" title="问题4-linux下python执行shell脚本输出重定向"></a>问题4-linux下python执行shell脚本输出重定向</h2><p><a href="https://mxxhcm.github.io/2019/06/03/linux-python调用shell脚本并将输出重定向到文件/">详细介绍</a></p><h2 id="问题4-ImportError-No-module-named-conda-cli’"><a href="#问题4-ImportError-No-module-named-conda-cli’" class="headerlink" title="问题4-ImportError: No module named conda.cli’"></a>问题4-ImportError: No module named conda.cli’</h2><h3 id="问题描述-1"><a href="#问题描述-1" class="headerlink" title="问题描述"></a>问题描述</h3><p>anaconda的python版本是3.7，执行了conda install python=3.6之后，运行conda命令出错。报错如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from conda.cli import main </span><br><span class="line">ModuleNotFoundError: No module named &apos;conda&apos;</span><br></pre></td></tr></table></figure></p><h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><p>找到anaconda安装包，加一个-u参数，如下所示。重新安装anaconda自带的package，自己安装的包不会丢失。<br>~$:sh xxx.sh -u</p><h2 id="问题5-python-pip使用国内源"><a href="#问题5-python-pip使用国内源" class="headerlink" title="问题5-python-pip使用国内源"></a>问题5-python-pip使用国内源</h2><h3 id="暂时使用国内pip源"><a href="#暂时使用国内pip源" class="headerlink" title="暂时使用国内pip源"></a>暂时使用国内pip源</h3><p>使用清华源<br>~\$:pip install -i <a href="https://pypi.tuna.tsinghua.edu.cn/simple" target="_blank" rel="noopener">https://pypi.tuna.tsinghua.edu.cn/simple</a> package-name<br>使用阿里源<br>~\$:pip install -i <a href="https://mirrors.aliyun.com/pypi/simple" target="_blank" rel="noopener">https://mirrors.aliyun.com/pypi/simple</a> package-name</p><h3 id="将国内pip源设为默认"><a href="#将国内pip源设为默认" class="headerlink" title="将国内pip源设为默认"></a>将国内pip源设为默认</h3><p>~\$:pip install pip -U<br>~\$:pip config set global.index-url <a href="https://pypi.tuna.tsinghua.edu.cn/simple" target="_blank" rel="noopener">https://pypi.tuna.tsinghua.edu.cn/simple</a><br>~\$:pip config set global.timeout 60</p><blockquote><p>Writing to /home/username/.config/pip/pip.conf</p></blockquote><h4 id="查看pip配置文件"><a href="#查看pip配置文件" class="headerlink" title="查看pip配置文件"></a>查看pip配置文件</h4><p>~\$:find / -name pip.conf<br>我的是在/home/username/.config/pip/pip.conf</p><h2 id="问题6-ImportError-lib-x86-64-linux-gnu-libc-so-6-version-GLIBC-2-28-not-found"><a href="#问题6-ImportError-lib-x86-64-linux-gnu-libc-so-6-version-GLIBC-2-28-not-found" class="headerlink" title="问题6-ImportError: /lib/x86_64-linux-gnu/libc.so.6: version GLIBC_2.28 not found"></a>问题6-ImportError: /lib/x86_64-linux-gnu/libc.so.6: version GLIBC_2.28 not found</h2><h3 id="问题描述-2"><a href="#问题描述-2" class="headerlink" title="问题描述"></a>问题描述</h3><p>安装roboschool之后，出现ImportError。报错如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ImportError: /lib/x86_64-linux-gnu/libc.so.6: version `GLIBC_2.28&apos; not found (required by /usr/local/lib/python3.6/dist-packages/roboschool/.libs/libQt5Core.so.5)</span><br></pre></td></tr></table></figure></p><h3 id="解决方案-1"><a href="#解决方案-1" class="headerlink" title="解决方案"></a>解决方案</h3><p>在roboschool上找到一个issue，说从1.0.49版本退回到1.0.48即可。我退回之后，又出现以下错误：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ImportError: libpcre16.so.3: cannot open shared object file: No such file or directory</span><br></pre></td></tr></table></figure></p><p>安装相应的库即可。完整的命令如下<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">~$:pip install roboschool==1.0.48</span><br><span class="line">~$:sudo apt install libpcre3-dev</span><br></pre></td></tr></table></figure></p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>1.<a href="https://www.cnblogs.com/timxgb/p/8905290.html" target="_blank" rel="noopener">https://www.cnblogs.com/timxgb/p/8905290.html</a><br>2.<a href="https://docs.python.org/3/library/stdtypes.html#dictionary-view-objects" target="_blank" rel="noopener">https://docs.python.org/3/library/stdtypes.html#dictionary-view-objects</a><br>3.<a href="https://stackoverflow.com/questions/43663206/typeerror-unsupported-operand-types-for-dict-values-and-int" target="_blank" rel="noopener">https://stackoverflow.com/questions/43663206/typeerror-unsupported-operand-types-for-dict-values-and-int</a><br>4.<a href="https://github.com/devsisters/DQN-tensorflow/issues/29" target="_blank" rel="noopener">https://github.com/devsisters/DQN-tensorflow/issues/29</a><br>5.<a href="https://gym.openai.com/docs" target="_blank" rel="noopener">https://gym.openai.com/docs</a><br>6.<a href="https://github.com/openai/baselines/issues/42" target="_blank" rel="noopener">https://github.com/openai/baselines/issues/42</a><br>7.<a href="https://blog.csdn.net/m0_37561765/article/details/78714603" target="_blank" rel="noopener">https://blog.csdn.net/m0_37561765/article/details/78714603</a><br>8.<a href="https://blog.csdn.net/u014432608/article/details/79066813" target="_blank" rel="noopener">https://blog.csdn.net/u014432608/article/details/79066813</a><br>9.<a href="https://mirrors.tuna.tsinghua.edu.cn/help/pypi/" target="_blank" rel="noopener">https://mirrors.tuna.tsinghua.edu.cn/help/pypi/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;问题1-‘dict-values’-object-does-not-support-indexing’&quot;&gt;&lt;a href=&quot;#问题1-‘dict-values’-object-does-not-support-indexing’&quot; class=&quot;headerlin
      
    
    </summary>
    
      <category term="python" scheme="http://mxxhcm.github.io/categories/python/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="gym" scheme="http://mxxhcm.github.io/tags/gym/"/>
    
      <category term="常见问题" scheme="http://mxxhcm.github.io/tags/%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/"/>
    
      <category term="pip源" scheme="http://mxxhcm.github.io/tags/pip%E6%BA%90/"/>
    
  </entry>
  
  <entry>
    <title>tensorflow 常见问题（不定期更新）</title>
    <link href="http://mxxhcm.github.io/2019/03/07/tensorflow-problems/"/>
    <id>http://mxxhcm.github.io/2019/03/07/tensorflow-problems/</id>
    <published>2019-03-07T06:51:01.000Z</published>
    <updated>2019-07-18T12:27:16.349Z</updated>
    
    <content type="html"><![CDATA[<h2 id="问题1-the-value-of-a-feed-cannot-be-a-tf-tensor-object">问题1-The value of a feed cannot be a tf.Tensor object</h2><h3 id="报错">报错</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">TypeError: The value of a feed cannot be a tf.Tensor object</span><br></pre></td></tr></table></figure><h3 id="问题原因">问题原因</h3><p>sess.run(op, feed_dict={})中的feed value不能是tf.Tensor类型。</p><h3 id="解决方法">解决方法</h3><p>sess.run(train, feed_dict={x:images, y:labels}的输入不能是tensor，可以使用sess.run(tensor)得到numpy.array形式的数据再喂给feed_dict。</p><blockquote><p>Once you have launched a sess, you can use your_tensor.eval(session=sess) or sess.run(your_tensor) to get you feed tensor into the format of numpy.array and then feed it to your placeholder.</p></blockquote><h2 id="问题2-could-not-create-cudnn-handle-cudnn-status-internal-error">问题2-Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR</h2><h3 id="配置">配置</h3><p>环境配置如下：</p><ul><li>Ubuntu 18.04</li><li>CUDA 10.0</li><li>CuDNN 7.4.2</li><li>Python3.7.3</li><li>Tensorflow 1.13.1</li><li>Nvidia Drivers 430.09</li><li>RTX2070</li></ul><h3 id="报错-v2">报错</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">2019-05-12 14:45:59.355405: E tensorflow/stream_executor/cuda/cuda_dnn.cc:334] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR</span><br><span class="line">2019-05-12 14:45:59.357698: E tensorflow/stream_executor/cuda/cuda_dnn.cc:334] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR</span><br><span class="line">...</span><br></pre></td></tr></table></figure><h3 id="问题原因-v2">问题原因</h3><p>GPU不够用了。</p><h3 id="解决方法-v2">解决方法</h3><p>在代码中添加下面几句：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">config = tf.ConfigProto()</span><br><span class="line">config.gpu_options.allow_growth = <span class="literal">True</span></span><br><span class="line">session = InteractiveSession(config=config)</span><br></pre></td></tr></table></figure><h2 id="问题3-libcublas-so-10-0-cannot-open-shared-object-file-no-such-file-or-directory">问题3-libcublas.so.10.0: cannot open shared object file: No such file or directory</h2><p>在命令行或者pycharm中import tensorflow报错</p><h3 id="报错-v3">报错</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ImportError: libcublas.so.10.0: cannot open shared object file: No such file or directory</span><br><span class="line">Failed to load the native TensorFlow runtime.</span><br></pre></td></tr></table></figure><h3 id="问题原因-v3">问题原因</h3><p>没有配置CUDA环境变量</p><h3 id="解决方法-v3">解决方法</h3><h4 id="命令行中">命令行中</h4><p>在.bashrc文件中加入下列语句：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export PATH=/usr/local/cuda/bin$&#123;PATH:+:$&#123;PATH&#125;&#125;</span><br><span class="line">export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$&#123;LD_LIBRARY_PATH:+:$&#123;LD_LIBRARY_PATH&#125;&#125;</span><br><span class="line">export CUDA_HOME=/usr/local/cuda</span><br></pre></td></tr></table></figure><h4 id="pycharm中">pycharm中</h4><h5 id="方法1-这种方法我没有实验成功-不知道为什么">方法1（这种方法我没有实验成功，不知道为什么）</h5><p>在左上角选中<br>File&gt;&gt;Settings&gt;&gt;Build.Execution,Deployment&gt;&gt;Console&gt;&gt;Python Console<br>在Environment下的Environment variables中添加<br>LD_LIBRARY_PATH=/usr/local/cuda/lib64:${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}即可。</p><h5 id="方法2">方法2</h5><p>修改完.bashrc文件后从终端中运行pycharm。</p><h2 id="问题4-dlerror-libcupti-so-10-0-cannot-open-shared-object-file-no-such-file-or-directory">问题4-dlerror: libcupti.so.10.0: cannot open shared object file: No such file or directory</h2><p>执行mnist_with_summary代码时报错</p><h3 id="报错-v4">报错</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">I tensorflow/stream_executor/dso_loader.cc:142] Couldn&apos;t open CUDA library libcupti.so.10.0. LD_LIBRARY_PATH: /usr/local/cuda/lib64:</span><br><span class="line">2019-05-13 23:04:10.620149: F tensorflow/stream_executor/lib/statusor.cc:34] Attempting to fetch value instead of handling error Failed precondition: could not dlopen DSO: libcupti.so.10.0; dlerror: libcupti.so.10.0: cannot open shared object file: No such file or directory</span><br><span class="line">Aborted (core dumped)</span><br></pre></td></tr></table></figure><h3 id="问题问题问题问题问题问题问题问题问题原因">问题问题问题问题问题问题问题问题问题原因</h3><p>libcupti.so.10.0包没找到</p><h3 id="解决方法-v4">解决方法</h3><p>执行以下命令，找到相关的依赖包：<br>~$:find /usr/local/cuda/ -name libcupti.so.10.0<br>输出如下：</p><blockquote><p>/usr/local/cuda/extras/CUPTI/lib64/libcupti.so.10.0</p></blockquote><p>然后修改~/.bashrc文件中相应的环境变量:<br>export LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64/😒{LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}<br>重新运行即可。</p><h2 id="问题5-unhashable-type-list">问题5-unhashable type: ‘list’</h2><p>sess.run(op, feed_dict={})中feed的数据中包含有list的时候会报错。</p><h3 id="报错-v5">报错</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">TypeError: unhashable type: &apos;list&apos;</span><br></pre></td></tr></table></figure><h3 id="问题原因-v4">问题原因</h3><p>feed_dict中不能的value不能是list。</p><h3 id="解决方法-v5">解决方法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">feed_dict = &#123;</span><br><span class="line">               placeholder : value </span><br><span class="line">                  <span class="keyword">for</span> placeholder, value <span class="keyword">in</span> zip(placeholder_list, inputs_list))</span><br><span class="line">            &#125;</span><br></pre></td></tr></table></figure><h3 id="代码示例">代码示例</h3><p><a href="https://github.com/mxxhcm/code/blob/master/tf/ops/tf_placeholder_list.py" target="_blank" rel="noopener">代码地址</a></p><h2 id="问题6-attempting-to-use-uninitialized-value">问题6-Attempting to use uninitialized value</h2><p>tf.Session()和tf.InteractiveSession()混用问题。</p><h3 id="报错-v6">报错</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensorflow.python.framework.errors_impl.FailedPreconditionError: Attempting to use uninitialized value prediction/l1/w</span><br><span class="line"> [[&#123;&#123;node prediction/l1/w/read&#125;&#125;]]</span><br><span class="line"> [[&#123;&#123;node prediction/LogSoftmax&#125;&#125;]]</span><br></pre></td></tr></table></figure><h3 id="问题原因-v5">问题原因</h3><p>声明了如下session:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sess = tf.Session()</span><br><span class="line">sess.run(tf.global_variables_initializer())</span><br></pre></td></tr></table></figure><p>在接下来的代码中，因为我声明的是tf.Session()，使用了op.eval()函数，这种用法是tf.InteractiveSession的用法，所以就相当于没有初始化。<br>result = op.eval(feed_dict={})<br>然后就报了未初始化的错误。<br>把代码改成：<br>result = sess.run([op], feeed_dct={})<br>即可，即上下文使用的session应该一致。</p><h3 id="解决方案">解决方案</h3><p>使用统一的session类型</p><h2 id="问题7-setting-an-array-element-with-a-sequence">问题7-setting an array element with a sequence</h2><p>feed_dict键值对中中值必须是numpy.ndarray，不能是其他类型。</p><h3 id="报错-v7">报错</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">value error setting an array element with a sequence,</span><br></pre></td></tr></table></figure><h3 id="问题原因-v6">问题原因</h3><p>feed_dict中key-value的value必须是numpy.ndarray，不能是其他类型，尤其不能是tf.Variable。</p><h3 id="解决方法-v6">解决方法</h3><p>检查sess.run(op, feed_dict={})中的feed_dict，确保他们的类型，不能是tf.Variable()类型的对象，需要是numpy.ndarray。</p><h2 id="问题8-访问tf-variable-的值">问题8-访问tf.Variable()的值</h2><p>如何获得tf.Variable()对象的值</p><h3 id="解决方法-v7">解决方法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">x = tf.Varialbe([<span class="number">1.0</span>, <span class="number">2.0</span>])</span><br><span class="line">sess = tf.Session()</span><br><span class="line">sess.run(tf.global_variables_initializer())</span><br><span class="line">value = sess.run(x)</span><br></pre></td></tr></table></figure><p>或者</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">x = tf.Varialbe([1.0, 2.0])</span><br><span class="line">sess = tf.InteractiveSession()</span><br><span class="line">sess.run(tf.global_variables_initializer())</span><br><span class="line">x.eval()</span><br></pre></td></tr></table></figure><h2 id="问题9-can-not-convert-a-ndarray-into-a-tensor-or-operation">问题9-Can not convert a ndarray into a Tensor or Operation</h2><h3 id="报错-v8">报错</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Can not convert a ndarray into a Tensor or Operation.</span><br></pre></td></tr></table></figure><h3 id="问题原因-v7">问题原因</h3><p>原因是sess.run()前后参数名重了，比如outputs = sess.run(outputs)，outputs本来是自己定义的一个op，但是sess.run(outputs)之后outputs就成了一个变量，就把定义的outputs op覆盖了。</p><h3 id="解决方法-v8">解决方法</h3><p>换个变量名字就行</p><h2 id="问题10-本地使用gpu-server的tensorboard">问题10-本地使用gpu server的tensorboard</h2><h3 id="问题描述">问题描述</h3><p>在gpu server跑的实验结果，然后summary的记录也在server上，但是又没办法可视化，只好在本地可视化。</p><h3 id="解决方法-v9">解决方法</h3><p>使用ssh进行映射好了。</p><h4 id="本机设置">本机设置</h4><p>~$:ssh -L 12345:10.1.114.50:6006 <a href="mailto:mxxmhh@127.0.0.1" target="_blank" rel="noopener">mxxmhh@127.0.0.1</a><br>将本机的12345端口映射到10.1.114.50的6006端口，中间服务器使用的是本机。<br>或者可以使用10.1.114.50作为中间服务器。<br>~$:ssh -L 12345:10.1.114.50:6006 <a href="mailto:liuchi@10.1.114.50" target="_blank" rel="noopener">liuchi@10.1.114.50</a><br>或者可以使用如下方法：<br>~$:ssh -L 12345:127.0.0.1:6006 <a href="mailto:liuchi@10.1.114.50" target="_blank" rel="noopener">liuchi@10.1.114.50</a><br>从这个方法中，可以看出127.0.0.1这个ip是中间服务器可以访问的ip。<br>以上三种方法中，-L后的端口号12345可以随意设置，只要不冲突即可。</p><h4 id="服务端设置">服务端设置</h4><p>然后在服务端运行以下命令：<br>~$:tensorboard --logdir logdir -port 6006<br>这个端口号也是可以任意设置的，不冲突即可。</p><h4 id="运行">运行</h4><p>然后在本机访问<br><a href="https://127.0.0.1:12345" target="_blank" rel="noopener">https://127.0.0.1:12345</a>即可。</p><h2 id="问题11-每一步summary一个list的每一个元素">问题11-每一步summary一个list的每一个元素</h2><h3 id="问题原因-v8">问题原因</h3><p>有一个tf list的placeholder，但是每一步只能生成其中的一个元素，所以怎么样summary中其中的某一个？</p><h3 id="解决方法-v10">解决方法</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">number = 3</span><br><span class="line">x_ph_list = []</span><br><span class="line">for i in range(number):</span><br><span class="line">    x_ph_list.append(tf.placeholder(tf.float32, shape=None))</span><br><span class="line"></span><br><span class="line">x_summary_list = []</span><br><span class="line">for i in range(number):</span><br><span class="line">    x_summary_list.append(tf.summary.scalar("x%s" % i, x_ph_list[i]))</span><br><span class="line"></span><br><span class="line">writer = tf.summary.FileWriter("./tf_summary/scalar_list_summary/sep")</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    scope = 10</span><br><span class="line">    inputs = np.arange(scope*number)</span><br><span class="line">    inputs = inputs.reshape(scope, number)</span><br><span class="line">    # inputs = np.random.randn(scope, number)</span><br><span class="line">    for i in range(scope):</span><br><span class="line">        for j in range(number):</span><br><span class="line">            out, xj_s = sess.run([x_ph_list[j], x_summary_list[j]], feed_dict=&#123;x_ph_list[j]: inputs[i][j]&#125;)</span><br><span class="line">            writer.add_summary(xj_s, global_step=i)</span><br></pre></td></tr></table></figure><h2 id="问题12-for-value-in-summary-value-attributeerror-list-object-has-no-attribute-value">问题12- for value in summary.value: AttributeError: ‘list’ object has no attribute ‘value’</h2><h3 id="问题描述-v2">问题描述</h3><p>writer.add_summary时报错</p><h3 id="报错-v9">报错</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">File &quot;/home/mxxmhh/anaconda3/lib/python3.7/site-packages/tensorflow/python/summary/writer/writer.py&quot;, line 127, in add_summary</span><br><span class="line">    for value in summary.value:</span><br><span class="line">AttributeError: &apos;list&apos; object has no attribute &apos;value&apos;</span><br></pre></td></tr></table></figure><h3 id="问题原因-v9">问题原因</h3><p>执行以下代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">s_ = sess.run([loss_summary], feed_dict=&#123;p_losses_ph: inputs1, q_losses_ph: inputs2&#125;)</span><br><span class="line">writer.add_summary(s_, global_step=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>因为[loss_summary]加了方括号，就把它当成了一个list。。返回值也是list，就报错了</p><h3 id="解决方法-v11">解决方法</h3><ul><li>方法1，在等号左边加一个逗号，取出list中的值</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">s_, = sess.run([loss_summary], feed_dict=&#123;p_losses_ph: inputs1, q_losses_ph: inputs2&#125;)</span><br></pre></td></tr></table></figure><ul><li>方法2，去掉loss_summary外面的中括号。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">s_ = sess.run(loss_summary, feed_dict=&#123;p_losses_ph: inputs1, q_losses_ph: inputs2&#125;)</span><br></pre></td></tr></table></figure><h2 id="问题13-tf-get-default-session-always-returns-none-type">问题13- tf.get_default_session() always returns None type:</h2><h3 id="问题描述-v3">问题描述</h3><p>调用tf.get_default_session()时，返回的是None</p><h3 id="报错-v10">报错</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">    tf.get_default_session().run(y)</span><br><span class="line">AttributeError: &apos;NoneType&apos; object has no attribute &apos;run&apos;</span><br></pre></td></tr></table></figure><h3 id="问题原因-v10">问题原因</h3><p>只有在设定default session之后，才能使用tf.get_default_session()获得当前的默认session，在我们写代码的时候，一般会按照下面的方式写：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    some operations</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure><p>这种情况下已经把tf.Session()生成的session当做了默认session，但是如果仅仅使用以下代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">sess =  tf.Session()</span><br><span class="line">sess.run(some operations)</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>是没有把tf.Session()当成默认session的，即只有在with block内，才会将这个session当做默认session。</p><h3 id="解决方案-v2">解决方案</h3><h2 id="参考文献">参考文献</h2><p>1.<a href="https://github.com/tensorflow/tensorflow/issues/4842" target="_blank" rel="noopener">https://github.com/tensorflow/tensorflow/issues/4842</a><br>2.<a href="https://github.com/tensorflow/tensorflow/issues/24496" target="_blank" rel="noopener">https://github.com/tensorflow/tensorflow/issues/24496</a><br>3.<a href="https://github.com/tensorflow/tensorflow/issues/9530" target="_blank" rel="noopener">https://github.com/tensorflow/tensorflow/issues/9530</a><br>4.<a href="https://stackoverflow.com/questions/51128427/how-to-feed-list-of-values-to-a-placeholder-list-in-tensorflow" target="_blank" rel="noopener">https://stackoverflow.com/questions/51128427/how-to-feed-list-of-values-to-a-placeholder-list-in-tensorflow</a><br>5.<a href="https://github.com/tensorflow/tensorflow/issues/11897" target="_blank" rel="noopener">https://github.com/tensorflow/tensorflow/issues/11897</a><br>6.<a href="https://stackoverflow.com/questions/34156639/tensorflow-python-valueerror-setting-an-array-element-with-a-sequence-in-t" target="_blank" rel="noopener">https://stackoverflow.com/questions/34156639/tensorflow-python-valueerror-setting-an-array-element-with-a-sequence-in-t</a><br>7.<a href="https://stackoverflow.com/questions/33679382/how-do-i-get-the-current-value-of-a-variable" target="_blank" rel="noopener">https://stackoverflow.com/questions/33679382/how-do-i-get-the-current-value-of-a-variable</a><br>8.<a href="https://blog.csdn.net/michael__corleone/article/details/79007425" target="_blank" rel="noopener">https://blog.csdn.net/michael__corleone/article/details/79007425</a><br>9.<a href="https://stackoverflow.com/questions/47721792/tensorflow-tf-get-default-session-after-sess-tf-session-is-none" target="_blank" rel="noopener">https://stackoverflow.com/questions/47721792/tensorflow-tf-get-default-session-after-sess-tf-session-is-none</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;问题1-the-value-of-a-feed-cannot-be-a-tf-tensor-object&quot;&gt;问题1-The value of a feed cannot be a tf.Tensor object&lt;/h2&gt;
&lt;h3 id=&quot;报错&quot;&gt;报错&lt;/h3&gt;

      
    
    </summary>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/categories/tensorflow/"/>
    
    
      <category term="机器学习" scheme="http://mxxhcm.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="tensorflow" scheme="http://mxxhcm.github.io/tags/tensorflow/"/>
    
      <category term="深度学习" scheme="http://mxxhcm.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="常见问题" scheme="http://mxxhcm.github.io/tags/%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>linux-查看python package的安装位置</title>
    <link href="http://mxxhcm.github.io/2019/03/04/linux-%E6%9F%A5%E7%9C%8Bpython-package%E7%9A%84%E5%AE%89%E8%A3%85%E4%BD%8D%E7%BD%AE/"/>
    <id>http://mxxhcm.github.io/2019/03/04/linux-查看python-package的安装位置/</id>
    <published>2019-03-04T06:52:16.000Z</published>
    <updated>2019-05-12T04:03:26.986Z</updated>
    
    <content type="html"><![CDATA[<p>使用pip install package-name之后，不知道该包存在了哪个路径下。<br>可以再次使用pip install package-name，这时候就会给出该包存放在哪个路径下。</p><h2 id="参考文献">参考文献</h2><ol><li><a href="https://blog.csdn.net/weixin_41712059/article/details/82940516" target="_blank" rel="noopener">https://blog.csdn.net/weixin_41712059/article/details/82940516</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;使用pip install package-name之后，不知道该包存在了哪个路径下。&lt;br&gt;
可以再次使用pip install package-name，这时候就会给出该包存放在哪个路径下。&lt;/p&gt;
&lt;h2 id=&quot;参考文献&quot;&gt;参考文献&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a
      
    
    </summary>
    
      <category term="linux" scheme="http://mxxhcm.github.io/categories/linux/"/>
    
    
      <category term="python" scheme="http://mxxhcm.github.io/tags/python/"/>
    
      <category term="技巧" scheme="http://mxxhcm.github.io/tags/%E6%8A%80%E5%B7%A7/"/>
    
  </entry>
  
  <entry>
    <title>shadowsocks服务端以及客户端配置</title>
    <link href="http://mxxhcm.github.io/2019/03/04/linux-shadowsocks%E6%9C%8D%E5%8A%A1%E7%AB%AF%E4%BB%A5%E5%8F%8A%E5%AE%A2%E6%88%B7%E7%AB%AF%E9%85%8D%E7%BD%AE/"/>
    <id>http://mxxhcm.github.io/2019/03/04/linux-shadowsocks服务端以及客户端配置/</id>
    <published>2019-03-04T05:03:57.000Z</published>
    <updated>2019-06-19T03:39:56.289Z</updated>
    
    <content type="html"><![CDATA[<h2 id="服务器端配置">服务器端配置</h2><p>首先需要有一个VPS账号，vultr,digitalocean,搬瓦工等等都行。<br>首先到下面两个网站检测22端口是否开启，如果关闭的话，vps换个ip把。。<br><a href="http://tool.chinaz.com/port" target="_blank" rel="noopener">http://tool.chinaz.com/port</a><br><a href="https://www.yougetsignal.com/tools/open-ports/" target="_blank" rel="noopener">https://www.yougetsignal.com/tools/open-ports/</a></p><h3 id="启用bbr加速">启用BBR加速</h3><p>~#:apt update<br>~#:apt upgrade<br>~#:echo “net.core.default_qdisc=fq” &gt;&gt; /etc/sysctl.conf<br>~#:echo “net.ipv4.tcp_congestion_control=bbr” &gt;&gt; /etc/sysctl.conf<br>~#:sysctl -p<br>上述命令就完成了BBR加速，执行以下命令验证：<br>~#:lsmod |grep bbr<br>看到输出包含tcp_bbr就说明已经成功了。</p><h3 id="搭建shadowsocks-server">搭建shadowsocks server</h3><h4 id="安装shadowsocks-server">安装shadowsocks server</h4><p>~#:apt install python-pip<br>~#:pip install shadowsocks<br>需要说一下的是，shadowsocks目前还不支持python3.5及以上版本，上次我把/usr/bin/python指向了python3.6，就是系统默认的python指向了python3.6，然后就gg了。一定要使用Python 2.6,2.7,3.3,3.4中的一个版本才能使用。。</p><h4 id="创建shadowsocks配置文件">创建shadowsocks配置文件</h4><p>如果你的VPS支持ipv6的话，那么可以开多进程分别运行ipv4和ipv6的shadowsocks server。本地只有ipv4的话，可以用本地ipv4访问ipv6，从而访问byr等网站，但是六维空间对此做了屏蔽。如果本地有ipv6的话，还可以用本地的ipv6访问ipv6实现校园网不走ipv4流量。</p><h5 id="ipv4配置">ipv4配置</h5><p>~#:vim /etc/shadowsocks_v4.json<br>配置文件如下</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line"><span class="attr">"server"</span>:<span class="string">"0.0.0.0"</span>,</span><br><span class="line"><span class="attr">"server_port"</span>:<span class="string">"你的端口号"</span>,</span><br><span class="line"><span class="attr">"local_address"</span>:<span class="string">"127.0.0.1"</span>,</span><br><span class="line"><span class="attr">"local_port"</span>:<span class="number">1080</span>,</span><br><span class="line"><span class="attr">"password"</span>:<span class="string">"你的密码"</span>,</span><br><span class="line"><span class="attr">"timeout"</span>:<span class="number">600</span>,</span><br><span class="line"><span class="attr">"method"</span>:<span class="string">"aes-256-cfb"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="ipv6配置">ipv6配置</h5><p>~#:vim /etc/shadowsocks_v6.json<br>配置文件如下</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line"><span class="attr">"server"</span>:<span class="string">"::"</span>,</span><br><span class="line"><span class="attr">"server_port"</span>:<span class="string">"你的端口号"</span>,</span><br><span class="line"><span class="attr">"local_address"</span>:<span class="string">"127.0.0.1"</span>,</span><br><span class="line"><span class="attr">"local_port"</span>:<span class="number">1080</span>,</span><br><span class="line"><span class="attr">"password"</span>:<span class="string">"你的密码"</span>,</span><br><span class="line"><span class="attr">"timeout"</span>:<span class="number">600</span>,</span><br><span class="line"><span class="attr">"method"</span>:<span class="string">"aes-256-cfb"</span></span><br><span class="line">&#125;</span><br><span class="line">``` </span><br><span class="line">注意这两个文件的server_port一定要不同，以及双引号必须是英文引号。</span><br><span class="line">##### 1.2.2.3.手动运行shadowsocks server</span><br><span class="line">~#:ssserver -c /etc/shadowsock_v4.json -d start --pid-file ss1.pid</span><br><span class="line">~#:ssserver -c /etc/shadowsock_v6.json -d start --pid-file ss2.pid</span><br><span class="line">注意这里要给两条命令分配不同的进程号。</span><br><span class="line"></span><br><span class="line">### 设置shadowsocks server开机自启</span><br><span class="line">如果重启服务器的话，就需要重新手动执行上述命令，这里我们可以把它写成开机自启脚本。</span><br><span class="line">~#:vim /etc/init.d/shadowsocks_v4</span><br><span class="line">内容如下：</span><br><span class="line">``` shell</span><br><span class="line">#!/bin/sh</span><br><span class="line">### BEGIN INIT INFO</span><br><span class="line"># Provides:          apache2</span><br><span class="line"># Required-Start:    $local_fs $remote_fs $network $syslog</span><br><span class="line"># Required-Stop:     $local_fs $remote_fs $network $syslog</span><br><span class="line"># Default-Start:     2 3 4 5</span><br><span class="line"># Default-Stop:      0 1 6</span><br><span class="line"># Short-Description: apache2 service</span><br><span class="line"># Description:       apache2 service daemon</span><br><span class="line">### END INIT INFO</span><br><span class="line">start()&#123;</span><br><span class="line">  ssserver -c /etc/shadowsocks_v4.json -d start --pid-file ss2.pid</span><br><span class="line">&#125;</span><br><span class="line">stop()&#123;</span><br><span class="line">  ssserver -c /etc/shadowsocks_v4.json -d stop --pid-file ss2.pid</span><br><span class="line">&#125;</span><br><span class="line">case "$1" in</span><br><span class="line">start)</span><br><span class="line">  start</span><br><span class="line">  ;;</span><br><span class="line">stop)</span><br><span class="line">  stop</span><br><span class="line">  ;;</span><br><span class="line">restart)</span><br><span class="line">  stop</span><br><span class="line">  start</span><br><span class="line">  ;;</span><br><span class="line">*)</span><br><span class="line">  echo "Uasage: $0 &#123;start|reload|stop&#125;$"</span><br><span class="line">  exit 1</span><br><span class="line">  ;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure><p>~#:vim /etc/init.d/shadowsocks_v6<br>内容如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span>!/bin/sh</span><br><span class="line"><span class="meta">#</span>## BEGIN INIT INFO</span><br><span class="line"><span class="meta">#</span> Provides:          apache2</span><br><span class="line"><span class="meta">#</span> Required-Start:    $local_fs $remote_fs $network $syslog</span><br><span class="line"><span class="meta">#</span> Required-Stop:     $local_fs $remote_fs $network $syslog</span><br><span class="line"><span class="meta">#</span> Default-Start:     2 3 4 5</span><br><span class="line"><span class="meta">#</span> Default-Stop:      0 1 6</span><br><span class="line"><span class="meta">#</span> Short-Description: apache2 service</span><br><span class="line"><span class="meta">#</span> Description:       apache2 service daemon</span><br><span class="line"><span class="meta">#</span>## END INIT INFO</span><br><span class="line">start()&#123;</span><br><span class="line">  ssserver -c /etc/shadowsocks_v6.json -d start --pid-file ss1.pid</span><br><span class="line">&#125;</span><br><span class="line">stop()&#123;</span><br><span class="line">  ssserver -c /etc/shadowsocks_v6.json -d stop --pid-file ss1.pid</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line">case "$1" in</span><br><span class="line">start)</span><br><span class="line">  start</span><br><span class="line">  ;;</span><br><span class="line">stop)</span><br><span class="line">  stop</span><br><span class="line">  ;;</span><br><span class="line">restart)</span><br><span class="line">  stop</span><br><span class="line">  start</span><br><span class="line">  ;;</span><br><span class="line">*)</span><br><span class="line">  echo "Uasage: $0 &#123;start|reload|stop&#125;$"</span><br><span class="line">  exit 1</span><br><span class="line">  ;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure><p>然后执行下列命令即可：<br>~#:chmod a+x /etc/init.d/shadowsocks_v4<br>~#:chmod a+x /etc/init.d/shadowsocks_v6<br>~#:update-rc.d shadowsocks_v4 defaults<br>~#:update-rc.d shadowsocks_v6 defaults</p><p>至此，服务器端配置完成。</p><h2 id="服务端自动配置脚本">服务端自动配置脚本</h2><p><a href="https://github.com/mxxhcm/code/tree/master/shell/ss" target="_blank" rel="noopener">地址</a><br>首先将该文件中所有文件复制到vps上，然后执行<br>~#:sh install_ss_server.sh<br>即可</p><h3 id="补充说明">补充说明</h3><p>该文件夹共包含五个文件<br>shadowsocks_v4.json为ipv4 ss配置文件，可根据自己的需要修改端口号和密码<br>shadowsocks_v6.json为ipv6 ss配置文件，可根据自己的需要修改端口号和密码<br>shadowsocks_v4为ipv4 ss自启动文件，无需修改<br>shadowsocks_v6为ipv6 ss自启动文件，无需修改<br>install_ss_server.sh为安装脚本，该脚本同时配置ipv4和ipv6 ss server。可根据自己需要自行选择。</p><h2 id="客户端配置">客户端配置</h2><h3 id="windows客户端配置">Windows客户端配置</h3><h4 id="安装shadowsock客户端">安装shadowsock客户端</h4><p>到该网址 <a href="https://github.com/shadowsocks/shadowsocks-windows/releases" target="_blank" rel="noopener">https://github.com/shadowsocks/shadowsocks-windows/releases</a> 下载相应的windows客户端程序。<br>然后配置服务器即可～</p><h3 id="linux客户端配置">Linux客户端配置</h3><h4 id="安装shadowsocks程序">安装shadowsocks程序</h4><p>~$:sudo pip install shadowsocks</p><h4 id="运行shadowsocks客户端程序">运行shadowsocks客户端程序</h4><p>~$:sudo vim /etc/shadowsocks.json<br>填入以下配置文件<br>{<br>“server”:“填上自己的shadowsocks server ip地址”,<br>“server_port”:“8888”,//填上自己的shadowsocks server 端口&quot;<br>“local_port”:1080,<br>“password”:“mxxhcm150929”,<br>“timeout”:600,<br>“method”:“aes-256-cfb”<br>}</p><p>接下来可以执行以下命令运行shadowsocks客户端：<br>~$:sudo sslocal -c /etc/shadowsocks.json<br>然后报错：</p><blockquote><p>INFO: loading config from /etc/shadowsocks.json<br>2019-03-04 14:37:49 INFO     loading libcrypto from libcrypto.so.1.1<br>Traceback (most recent call last):<br>File “/usr/local/bin/sslocal”, line 11, in <module><br>load_entry_point(‘shadowsocks==2.8.2’, ‘console_scripts’, ‘sslocal’)()<br>File “/usr/local/lib/python2.7/dist-packages/shadowsocks/local.py”, line 39, in main<br>config = shell.get_config(True)<br>File “/usr/local/lib/python2.7/dist-packages/shadowsocks/shell.py”, line 262, in get_config<br>check_config(config, is_local)<br>File “/usr/local/lib/python2.7/dist-packages/shadowsocks/shell.py”, line 124, in check_config<br>encrypt.try_cipher(config[‘password’], config[‘method’])<br>File “/usr/local/lib/python2.7/dist-packages/shadowsocks/encrypt.py”, line 44, in try_cipher<br>Encryptor(key, method)<br>File “/usr/local/lib/python2.7/dist-packages/shadowsocks/encrypt.py”, line 83, in <strong>init</strong><br>random_string(self._method_info[1]))<br>File “/usr/local/lib/python2.7/dist-packages/shadowsocks/encrypt.py”, line 109, in get_cipher<br>return m[2](method, key, iv, op)<br>File “/usr/local/lib/python2.7/dist-packages/shadowsocks/crypto/openssl.py”, line 76, in <strong>init</strong><br>load_openssl()<br>File “/usr/local/lib/python2.7/dist-packages/shadowsocks/crypto/openssl.py”, line 52, in load_openssl<br>libcrypto.EVP_CIPHER_CTX_cleanup.argtypes = (c_void_p,)<br>File “/usr/lib/python2.7/ctypes/<strong>init</strong>.py”, line 379, in <strong>getattr</strong><br>func = self.<strong>getitem</strong>(name)<br>File “/usr/lib/python2.7/ctypes/<strong>init</strong>.py”, line 384, in <strong>getitem</strong><br>func = self._FuncPtr((name_or_ordinal, self))<br>AttributeError: /usr/lib/x86_64-linux-gnu/libcrypto.so.1.1: undefined symbol: EVP_CIPHER_CTX_cleanup</module></p></blockquote><p>按照参考文献4的做法，是在openssl 1.1.0版本中放弃了EVP_CIPHER_CTX_cleanup函数</p><blockquote><p>EVP_CIPHER_CTX was made opaque in OpenSSL 1.1.0. As a result, EVP_CIPHER_CTX_reset() appeared and EVP_CIPHER_CTX_cleanup() disappeared.<br>EVP_CIPHER_CTX_init() remains as an alias for EVP_CIPHER_CTX_reset().</p></blockquote><p>将openssl库中的EVP_CIPHER_CTX_cleanup改为EVP_CIPHER_CTX_reset即可。<br>再次执行以下命令，查看shadowsocks安装位置<br>~#:pip install shadowsocks<br>Requirement already satisfied: shadowsocks in /usr/local/lib/python2.7/dist-packages<br>~#:cd /usr/local/lib/python2.7/dist-packages/shadowsocks<br>~#:vim crypto/openssl.py<br>搜索cleanup，将其替换为reset<br>具体位置在第52行libcrypto.EVP_CIPHER_CTX_cleanup.argtypes = (c_void_p,)和第111行libcrypto.EVP_CIPHER_CTX_cleanup(self._ctx)</p><h4 id="手动运行后台挂起">手动运行后台挂起</h4><p>将所有的log重定向到~/.log/sslocal.log文件中<br>~$:mkdir ~/.log<br>~$:touch ~/.log/ss-local.log<br>~$:nohup sslocal -c /etc/shadowsocks_v6.json &lt;/dev/null &amp;&gt;&gt;~/.log/ss-local.log &amp;</p><h4 id="开机自启shadowsocks-client">开机自启shadowsocks client</h4><p>但是这样子的话，每次开机都要重新运行上述命令，太麻烦了。可以写个开机自启脚本。执行以下命令：<br>~$:sudo vim /etc/init.d/shadowsocks<br>内容为以下shell脚本</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span>!/bin/sh</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span>## BEGIN INIT INFO</span><br><span class="line"><span class="meta">#</span> Provides:          shadowsocks local</span><br><span class="line"><span class="meta">#</span> Required-Start:    $local_fs $remote_fs $network $syslog</span><br><span class="line"><span class="meta">#</span> Required-Stop:     $local_fs $remote_fs $network $syslog</span><br><span class="line"><span class="meta">#</span> Default-Start:     2 3 4 5</span><br><span class="line"><span class="meta">#</span> Default-Stop:      0 1 6</span><br><span class="line"><span class="meta">#</span> Short-Description: shadowsocks service</span><br><span class="line"><span class="meta">#</span> Description:       shadowsocks service daemon</span><br><span class="line"><span class="meta">#</span>## END INIT INFO</span><br><span class="line">start()&#123;</span><br><span class="line">　　  sslocal -c /etc/shadowsocks.json -d start</span><br><span class="line">&#125;</span><br><span class="line">stop()&#123;</span><br><span class="line">　　  sslocal -c /etc/shadowsocks.json -d stop</span><br><span class="line">&#125;</span><br><span class="line">case “$1” in</span><br><span class="line">start)</span><br><span class="line">　　　start</span><br><span class="line">　　　;;</span><br><span class="line">stop)</span><br><span class="line">　　　stop</span><br><span class="line">　　　;;</span><br><span class="line">reload)</span><br><span class="line">　　　stop</span><br><span class="line">　　　start</span><br><span class="line">　　　;;</span><br><span class="line">\*)</span><br><span class="line">　　　echo “Usage: $0 &#123;start|reload|stop&#125;”</span><br><span class="line">　　　exit 1</span><br><span class="line">　　　;;</span><br><span class="line">esac</span><br></pre></td></tr></table></figure><p>然后执行以下命令即可：<br>~$:sudo chomod a+x /etc/init.d/shadowsocks<br>~$:sudo update_rc.d shadowsocks defaults<br>上述命令执行完成以后，进行测试<br>~$:sudo service shadosowcks start</p><h4 id="配置代理">配置代理</h4><p>上一步的目的是建立了shadowsocks服务的本地客户端，socks5流量会走该通道，但是浏览器的网页的流量是https的，我们需要配置相应的代理，将https流量转换为socks5流量，走ss客户端到达ss服务端。当然，也可以把其他各种流量，如tcp,udp等各种流量都转换为socks5流量，这个可以通过全局代理实现，也可以通过添加特定的代理规则实现。</p><h5 id="配置全局代理">配置全局代理</h5><p>如下图所示，添加ubuntu socks5系统代理：</p><p>然后就可以成功上网了。</p><h5 id="使用switchyomega配置chrome代理">使用SwitchyOmega配置chrome代理</h5><p>首先到 <a href="https://github.com/FelisCatus/SwitchyOmega/releases" target="_blank" rel="noopener">https://github.com/FelisCatus/SwitchyOmega/releases</a> 下载SyitchyOmega.crx。然后在chrome的地址栏输入chrome://extensions，将刚才下载的插件拖进去。<br>然后在浏览器右上角就有了这个插件，接下来配置插件。如下图：<br><img src="https:" alt="mxx"><br>直接配置proxy，添加如图所示的规则，这样chrome打开的所有网站都是走代理的。</p><h4 id="使用privoxy让terminal走socks5">使用privoxy让terminal走socks5</h4><p>~$:sudo apt install privoxy<br>~$:sudo vim /etc/privoxy/config<br>取消下列行的注释，或者添加相应条目<br>forward-socks5 / 127.0.0.1:1080 . # SOCKS5代理地址<br>listen-address 127.0.0.1:8118     # HTTP代理地址<br>forward 10.*.*.*/ .               # 内网地址不走代理<br>forward .abc.com/ .             # 指定域名不走代理<br>重启privoxy服务<br>~$:sudo service privoxy restart<br>在bashrc中添加如下环境变量<br>export http_proxy=&quot;<a href="http://127.0.0.1:8118" target="_blank" rel="noopener">http://127.0.0.1:8118</a>&quot;<br>export https_proxy=“<a href="http://127.0.0.1:8118" target="_blank" rel="noopener">http://127.0.0.1:8118</a>”</p><p>~$:source ~/.bashrc<br>~$:curl.gs</p><h2 id="参考文献">参考文献</h2><ol><li><a href="http://godjose.com/2017/06/14/new-article/" target="_blank" rel="noopener">http://godjose.com/2017/06/14/new-article/</a></li><li><a href="https://www.polarxiong.com/archives/%E6%90%AD%E5%BB%BAipv6-VPN-%E8%AE%A9ipv4%E4%B8%8Aipv6-%E4%B8%8B%E8%BD%BD%E9%80%9F%E5%BA%A6%E6%8F%90%E5%8D%87%E5%88%B0100M.html" target="_blank" rel="noopener">https://www.polarxiong.com/archives/搭建ipv6-VPN-让ipv4上ipv6-下载速度提升到100M.html</a></li><li><a href="https://blog.csdn.net/li1914309758/article/details/86510127" target="_blank" rel="noopener">https://blog.csdn.net/li1914309758/article/details/86510127</a></li><li><a href="https://blog.csdn.net/blackfrog_unique/article/details/60320737" target="_blank" rel="noopener">https://blog.csdn.net/blackfrog_unique/article/details/60320737</a></li><li><a href="https://blog.csdn.net/qq_31851531/article/details/78410146" target="_blank" rel="noopener">https://blog.csdn.net/qq_31851531/article/details/78410146</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;服务器端配置&quot;&gt;服务器端配置&lt;/h2&gt;
&lt;p&gt;首先需要有一个VPS账号，vultr,digitalocean,搬瓦工等等都行。&lt;br&gt;
首先到下面两个网站检测22端口是否开启，如果关闭的话，vps换个ip把。。&lt;br&gt;
&lt;a href=&quot;http://tool.c
      
    
    </summary>
    
      <category term="linux" scheme="http://mxxhcm.github.io/categories/linux/"/>
    
    
      <category term="工具" scheme="http://mxxhcm.github.io/tags/%E5%B7%A5%E5%85%B7/"/>
    
      <category term="linux" scheme="http://mxxhcm.github.io/tags/linux/"/>
    
      <category term="shadowsocks" scheme="http://mxxhcm.github.io/tags/shadowsocks/"/>
    
  </entry>
  
  <entry>
    <title>Bayesian Networks</title>
    <link href="http://mxxhcm.github.io/2019/01/06/bayesian-networks/"/>
    <id>http://mxxhcm.github.io/2019/01/06/bayesian-networks/</id>
    <published>2019-01-06T06:32:55.000Z</published>
    <updated>2019-08-30T06:31:44.078Z</updated>
    
    <content type="html"><![CDATA[<h2 id="介绍">介绍</h2><p>贝叶斯网络是一个有向无环图(directed acyclic graphs)，它用节点代表随机变量，用边代表变量之间的依赖关系。</p><h2 id="意义">意义</h2><p>贝叶斯网络可以用来表示任意的联合分布。</p><h2 id="推理">推理</h2><p>贝叶斯网络的一个基本任务就是求后验概率。<br>在AI这本书中，贝叶斯网络中的变量被分为了证据变量(evidence variable)，隐变量(hidden variable)和查询变量(query variable)。<br>而在PRML这本书中，贝叶斯网络中的变量被分为了观测变量(observed variable)和隐变量(latent variable,hidden variable)。</p><p>具体的可以看另外两篇笔记有详细的记录。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;介绍&quot;&gt;介绍&lt;/h2&gt;
&lt;p&gt;贝叶斯网络是一个有向无环图(directed acyclic graphs)，它用节点代表随机变量，用边代表变量之间的依赖关系。&lt;/p&gt;
&lt;h2 id=&quot;意义&quot;&gt;意义&lt;/h2&gt;
&lt;p&gt;贝叶斯网络可以用来表示任意的联合分布。&lt;/p&gt;
&lt;
      
    
    </summary>
    
      <category term="机器学习" scheme="http://mxxhcm.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://mxxhcm.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="监督学习" scheme="http://mxxhcm.github.io/tags/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="人工智能" scheme="http://mxxhcm.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
      <category term="概率图模型" scheme="http://mxxhcm.github.io/tags/%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/"/>
    
      <category term="贝叶斯网络" scheme="http://mxxhcm.github.io/tags/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BD%91%E7%BB%9C/"/>
    
      <category term="模式识别" scheme="http://mxxhcm.github.io/tags/%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB/"/>
    
      <category term="有向图" scheme="http://mxxhcm.github.io/tags/%E6%9C%89%E5%90%91%E5%9B%BE/"/>
    
  </entry>
  
  <entry>
    <title>singular value decomposition（奇异值分解）</title>
    <link href="http://mxxhcm.github.io/2019/01/03/linear-algebra-singular-value-decomposition/"/>
    <id>http://mxxhcm.github.io/2019/01/03/linear-algebra-singular-value-decomposition/</id>
    <published>2019-01-03T07:19:54.000Z</published>
    <updated>2019-09-09T08:53:25.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="特征值分解-eigen-value-decomposition">特征值分解(eigen value decomposition)</h2><p>要谈奇异值分解，首先要从特征值分解(eigen value decomposition, EVD)谈起。<br>矩阵的作用有三个：一个是旋转，一个是拉伸，一个是平移，都是线性操作。如果一个$n\times n$方阵$A$对某个向量$x$只产生拉伸变换，而不产生旋转和平移变换，那么这个向量就称为方阵$A$的特征向量(eigenvector)，对应的伸缩比例叫做特征值(eigenvalue)，即满足等式$Ax = \lambda x$。其中$A$是方阵，$x$是方阵$A$的一个特征向量，$\lambda$是方阵$A$对应特征向量$x$的特征值。<br>假设$S$是由方阵$A$的$n$个线性无关的特征向量构成的方阵，$\Lambda$是方阵$A$的$n$个特征值构成的对角矩阵，则$A=S\Lambda S^{-1}$，这个过程叫做对角化过程。<br>证明：<br>因为$Ax_1 = \lambda_1 x_1,\cdots,Ax_n = \lambda_n x_n$,<br>所以<br>\begin{align*}AS &amp;= A\begin{bmatrix}x_1&amp; \cdots&amp;x_n\end{bmatrix}\\<br>&amp;=\begin{bmatrix} \lambda_1x_1&amp;\cdots&amp;\lambda x_n\end{bmatrix}\\<br>&amp;= \begin{bmatrix}x_1&amp; \cdots&amp;x_n\end{bmatrix} \begin{bmatrix}\lambda_1&amp; &amp; &amp;\\&amp;\lambda_2&amp;&amp;\\&amp;&amp;\cdots&amp;\\&amp;&amp;&amp;\lambda_n\end{bmatrix}\<br>&amp;= S\Lambda<br>\end{align*}<br>所以$AS=S\Lambda, A=S\Lambda S^{-1}, S^{-1}AS=\Lambda$。<br>若方阵$A$为对称矩阵，矩阵$A$的特征向量是正交的，将其单位化为$Q$，则$A=Q\Lambda Q^T$，这个过程就叫做特征值分解。</p><h2 id="奇异值分解-singular-value-decomposition">奇异值分解(singular value decomposition)</h2><p>特征值分解是一个非常好的分解，因为它能把一个方阵分解称两类非常好的矩阵，一个是正交阵，一个是对角阵，这些矩阵都便于进行各种计算，但是它对于原始矩阵的要求太严格了，必须要求矩阵是对称正定矩阵，这是一个很苛刻的条件。所以就产生了奇异值分解，奇异值分解可以看作特征值分解在$m\times n$维矩阵上的推广。对于对称正定矩阵来说，有特征值，对于其他一般矩阵，有奇异值。</p><p>奇异值分解可以看作将一组正交基映射到另一组正交基的变换。普通矩阵$A$不是对称正定矩阵，但是$AA^T $和$A^TA $一定是对称矩阵，且至少是半正定的。从对$A^TA $进行特征值分解开始，$A^T A=V\Sigma_1V^T $，$V$是一组正交的单位化特征向量${v_1,\cdots,v_n}$，则$Av_1,\cdots,Av_n$也是正交的。<br>证明：<br>\begin{align*}Av_1\cdot Av_2 &amp;=(Av_1)^T Av_2\\<br>&amp;=v_1^T A^T Av_2\\<br>&amp;=v_1^T \lambda v_2\\<br>&amp;=\lambda v_1^T v_2\\<br>&amp;=0<br>\end{align*}<br>所以$Av_1,Av_2$是正交的，同理可得$Av_1,\cdots,Av_n$都是正交的。<br>而：<br>\begin{align*}<br>Av_i\cdot Av_i &amp;= v_i^T A^T Av_i\\<br>&amp;=v_i \lambda v_i\\<br>&amp;=\lambda v_i^2\\<br>&amp;=\lambda<br>\end{align*}<br>将$Av_i$单位化为$u_i$，得$u_i = \frac{Av_i}{|Av_i|} = \frac{Av_i}{\sqrt{\lambda_i}}$，所以$Av_i = \sqrt{\lambda_i}u_i$。<br>将向量组${v_1,\cdots,v_r}$扩充到$R^n $中的标准正交基${v_1,\cdots,v_n}$，将向量组${u_1,\cdots,u_r}$扩充到$R^n $中的标准正交基${u_1,\cdots,u_n}$，则$AV = U\Sigma$，$A=U\sigma V^T $。</p><p>事实上，奇异值分解可以看作将行空间的一组正交基加上零空间的一组基映射到列空间的一组正交基加上左零空间的一组基的变换。对一矩阵$A,A\in \mathbb{R}^{m\times n} $，若$r(A)=r$，取行空间的一组特殊正交基${v_1,\cdots,v_r}$，当矩阵$A$作用到这组基上，会得到另一组正交基${u_1,\cdots,u_r}$，即$Av_i = \sigma_iu_i$。<br>矩阵表示是：<br>\begin{align*}<br>AV &amp;= A\begin{bmatrix}v_1&amp;\cdots&amp;v_r\end{bmatrix}\\<br>&amp;= \begin{bmatrix}\sigma_1u_1 &amp; \cdots &amp; \sigma_ru_r\end{bmatrix}\\<br>&amp;= \begin{bmatrix}u_1&amp;u_2&amp;\cdots&amp;u_r\end{bmatrix}\begin{bmatrix}\sigma_1&amp;&amp;&amp;\\&amp;\sigma_2&amp;&amp;\\&amp;&amp;\cdots&amp;\\&amp;&amp;&amp;\sigma_n\end{bmatrix}\\<br>&amp;=U\Sigma<br>\end{align*}<br>其中$A\in \mathbb{R}^{m\times n}, V\in \mathbb{R}^{n\times r},U\in \mathbb{R}^{m\times r}, \Sigma \in \mathbb{R}^{r\times n}$。<br>当有零空间的时候，行空间的一组基是$r$维，加上零空间的$n-r$维，构成$R^n $空间中的一组标准正交基。列空间的一组基也是$r$维的，加上左零空间的$m-r$维，构成$R^m $空间的一组标准正交基。零空间中的向量在对角矩阵$\Sigma$中体现为$0$，<br>则$A=U\Sigma V^{-1} $，$V$是正交的，所以$A=U\Sigma V^T $，其中$V\in \mathbb{R}^{n\times n}, U\in \mathbb{R}^{m\times m}, \Sigma \in \mathbb{R}^{m\times n}$。</p><p>$A=U\Sigma V^T $,<br>$A^T = V\Sigma^T U^T $,<br>$AA^T = U\Sigma V^T V\Sigma^T U^T $,<br>$A^T A = V\Sigma^T U^T U\Sigma V^T $<br>对$A A^T $和$A^T A$作特征值分解，则$A A^T = U\Sigma_1U^T $,$A^T A=V\Sigma_2V^T $，所以对$AA^T $作特征值分解求出来的$U$和对$A^T A$作特征值分解求出来的$V$就是对$A$作奇异值分解求出来的$U$和$V$，$AA^T $和$A^T A$作特征值分解求出来的$\Sigma$的非零值是相等的，都是对$A$作奇异值分解的$\Sigma$的平方。</p><h3 id="a-t-a-和-aa-t-的非零特征值是相等的">$A^T A$和$AA^T $的非零特征值是相等的</h3><p>证明：对于任意的$m\times n$矩阵$A$，$A^T A$和$AA^T $的非零特征值相同的。 设$A^T A$的特征值为$\lambda_i$，对应的特征向量为$v_i$，即$A^T Av_i = \lambda_i v_i$。<br>则$AA^T Av_i = A\lambda_iv_i = \lambda_i Av_i$。<br>所以$AA^T $的特征值为$\lambda_i$，对应的特征向量为$Av_i$。<br>因此$A^T A$和$AA^T $的非零特征值相等。</p><h3 id="几何意义">几何意义</h3><p>对于任意一个矩阵，找到其行空间(加上零空间)的一组正交向量，使得该矩阵作用在该向量序列上得到的新的向量序列保持两两正交。奇异值的几何意义就是这组变化后的新的向量序列的长度。</p><h3 id="物理意义">物理意义</h3><p>奇异值往往对应着矩阵隐含的重要信息，且重要性和奇异值大小正相关。每个矩阵都可以表示为一系列秩为$1$的“小矩阵”的和，而奇异值则衡量了这些秩一矩阵对$A$的权重。<br>奇异值分解的物理意义可以通过图像压缩表现出来。给定一张$m\times n$像素的照片$A$，用奇异值分解将矩阵分解为若干个秩一矩阵之和，即：<br>\begin{align*}<br>A&amp;=\sigma_1 u_1v_1^T +\sigma_2 u_2v_2^T +\cdots+\sigma_r u_rv_r^T\\<br>&amp;= \begin{bmatrix}u_1&amp;u_2&amp;\cdots&amp;u_r\end{bmatrix}\begin{bmatrix}\sigma_1&amp;&amp;&amp;\&amp;\sigma_2&amp;&amp;\&amp;&amp;\cdots&amp;\&amp;&amp;&amp;\sigma_n\end{bmatrix}\begin{bmatrix}v_1<sup>T\v_2</sup>T\ \vdots\v_r^T\end{bmatrix}\\<br>&amp;=U\Sigma V^T<br>\end{align*}</p><p>这个也叫部分奇异值分解。其中$V\in R^{r\times n}, U\in R^{m\times r}, \Sigma \in R^{r\times r}$。因为不含有零空间和左零空间的基，如果加上零空间的$n-r$维和左零空间的$m-r$维，就是奇异值分解。<br>较大的奇异值保存了图片的主要信息，特别小的奇异值有时可能是噪声，或者对于图片的整体信息不是特别重要。做图像压缩的时候，可以只取一部分较大的奇异值，比如取前八个奇异值作为压缩后的图片：<br>$$A = \sigma_1 u_1v_1^T +\sigma_2 u_2v_2^T + \cdots + \sigma_8 u_8v_8^T$$<br>现实中常用的做法有两个：</p><ol><li>保留矩阵中$90%$的信息：将奇异值平方和累加到总值的%90%为止。</li><li>当矩阵有上万个奇异值的时候，取前面的$2000$或者$3000$个奇异值。。</li></ol><h2 id="参考文献-references">参考文献(references)</h2><p>1.Gilbert Strang, MIT Open course：Linear Algebra<br>2.<a href="https://www.cnblogs.com/pinard/p/6251584.html" target="_blank" rel="noopener">https://www.cnblogs.com/pinard/p/6251584.html</a><br>3.<a href="http://www.ams.org/publicoutreach/feature-column/fcarc-svd" target="_blank" rel="noopener">http://www.ams.org/publicoutreach/feature-column/fcarc-svd</a><br>4.<a href="https://www.zhihu.com/question/22237507/answer/53804902" target="_blank" rel="noopener">https://www.zhihu.com/question/22237507/answer/53804902</a><br>5.<a href="http://charleshm.github.io/2016/03/Singularly-Valuable-Decomposition/" target="_blank" rel="noopener">http://charleshm.github.io/2016/03/Singularly-Valuable-Decomposition/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;特征值分解-eigen-value-decomposition&quot;&gt;特征值分解(eigen value decomposition)&lt;/h2&gt;
&lt;p&gt;要谈奇异值分解，首先要从特征值分解(eigen value decomposition, EVD)谈起。&lt;br&gt;
矩
      
    
    </summary>
    
      <category term="线性代数" scheme="http://mxxhcm.github.io/categories/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/"/>
    
    
      <category term="线性代数" scheme="http://mxxhcm.github.io/tags/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/"/>
    
      <category term="奇异值分解" scheme="http://mxxhcm.github.io/tags/%E5%A5%87%E5%BC%82%E5%80%BC%E5%88%86%E8%A7%A3/"/>
    
      <category term="特征值分解" scheme="http://mxxhcm.github.io/tags/%E7%89%B9%E5%BE%81%E5%80%BC%E5%88%86%E8%A7%A3/"/>
    
  </entry>
  
  <entry>
    <title>convex optimization chapter 2 Convex sets</title>
    <link href="http://mxxhcm.github.io/2018/12/24/convex-optimization-chapter-2-Convex-sets/"/>
    <id>http://mxxhcm.github.io/2018/12/24/convex-optimization-chapter-2-Convex-sets/</id>
    <published>2018-12-24T08:28:45.000Z</published>
    <updated>2019-09-09T08:25:28.782Z</updated>
    
    <content type="html"><![CDATA[<h2 id="仿射集-affine-sets-和凸集-convex-sets">仿射集(affine sets)和凸集(convex sets)</h2><h3 id="直线-line-和线段-line-segmens">直线(line)和线段(line segmens)</h3><p>假设$x_1,x_2 \in \mathbb{R}^n $是n维空间中不重合$(x_1 \ne x_2)$的两点，给定：<br>$$y = \theta x_1 + (1 - \theta)x_2,$$<br>当$\theta\in R$时，$y$是经过点$x_1$和点$x_2$的直线。当$\theta=1$时，$y=x_1$,当$\theta=0$时，$y=x_2$。当$\theta\in[0,1]$时，$y$是$x_1$和$x_2$之间的线段(line segment)。 把$y$改写成如下形式： $$y = x_2 + \theta(x_1 - x_2)$$，可以给出另一种解释，$y$是点$x_2$和方向$x_1 - x_2$(从$x_2$到$x_1$的方向)乘上一个缩放因子$\theta$的和。<br>如下图所示，可以将y看成$\theta$的函数。<br><img src="https://ws1.sinaimg.cn/large/006wtfMEly1fyhy7m4llij30mz0alwep.jpg" alt="line_line-segment"></p><h3 id="仿射集-affine-sets">仿射集(affine sets)</h3><h4 id="仿射集的定义">仿射集的定义</h4><p>给定一个集合$C\subset \mathbb{R}^n $,如果经过$C$中任意两个不同点的直线仍然在$C$中，那么$C$就是一个仿射集。即，对于任意$x_1,x_2\in C$和$\theta\in R$，都有$\theta x_1 + (1 - \theta)x_2 \in C$。换句话说，给定线性组合的系数和为$1$，$C$中任意两点的线性组合仍然在$C$中，我们就称这样的集合是仿射的(affine)。</p><h4 id="仿射组合-affine-combination">仿射组合(affine combination)</h4><p>我们可以把两个点的线性组合推广到多个点的线性组合，这里称它为仿射组合。<br>仿射组合的定义：给定$\theta_1+\cdots+\theta_k = 1$,则$\theta_1 x_1 + \cdots + \theta_k x_k$是点$x_1,\cdots,x_k$的仿射组合(affine combination)。<br>根据仿射集的定义，一个仿射集(affine set)包含集合中任意两个点的仿射（线性）组合，那么可以推导出仿射集包含集合中任意点（大于等于两个）的仿射组合，即：如果$C$是一个仿射集，$x_1,\cdots,x_k\in C$,且$\theta_1 x_1 + \cdots + \theta_k x_k = 1$,那么点$\theta_1 x_1 + \cdots + \theta_k x_k$仍然属于$C$。</p><h4 id="仿射集的子空间-subspce">仿射集的子空间(subspce)</h4><p>如果$C$是一个仿射集，$x_0 \in C$,那么集合<br>$$V = C - x_0 = {x - x_0\big|x \in C}$$<br>是一个子空间(subspace),因为$V$是加法封闭和数乘封闭的。<br>证明：<br>假设$v_1, v_2 \in V$，并且$\alpha,\beta \in R$。<br>要证明V是一个子空间，那么只需要证明$\alpha v_1 + \beta v_2 \in V$即可。<br>因为$v_1, v_2 \in V$，则$v_1+x_0, v_2+x_0 \in C$。<br>而$x_0 \in C$，所以有<br>$$\alpha(v_1+x_0) + \beta(v_2+x_0) + (1 - \alpha - \beta)x_0 \in C$$<br>即：<br>\begin{align*}<br>\alpha v_1 + \beta v_2 + (\alpha + \beta + 1 - \alpha - \beta)x_0 &amp;\in C\\<br>\alpha v_1 + \beta v_2 + x_0 &amp;\in C<br>\end{align*}<br>所以$\alpha v_1 + \beta v_2 \in V$。<br>所以，仿射集$C$可以写成：<br>$$C = V + x_0 = { v + x_0\big| v \in V},$$<br>即，一个子空间加上一个偏移(offset)。而与仿射集$C$相关的子空间$V$与$x_0$的选择无关，即$x_0$可以为$C$中任意一点。</p><h4 id="示例">示例</h4><p>线性方程组的解。一个线性方程组的解可以表示为一个仿射集:$C={x\big|Ax = b}$,其中 $A\in \mathbb{R}^{m \times n}, b \in \mathbb{R}^m $。<br>证明：<br>设$x_1, x_2 \in C$,即$Ax_1 = b, Ax_2 = b$。对于任意$\theta \in R$,有:<br>\begin{align*}<br>A(\theta x_1 + (1-\theta x_2) &amp;= \theta Ax_1 + (1-\theta)Ax_2\\<br>&amp;= \theta b + (1 - \theta) b\\<br>&amp;= b \end{align*}<br>所以线性方程组的解是一个仿射组合：$\theta x_1 + (1 - \theta) x_2$，这个仿射组合在集合$C$中，所以线性方程组的解集$C$是一个仿射集。<br>和该仿射集$C$相关的子空间$V$是$A$的零空间(nullspace)。因为仿射集$C$中的任意点都是方程$Ax = b$的解，而$V = C - x_0 = {x - x_0\big|x \in C}$，有$Ax = b, Ax_0 = b$，则$Ax - Ax_0 = A(x - x_0) = b - b = 0$，所以$V$是$A$的零空间。</p><h4 id="仿射包-affine-hull">仿射包(affine hull)</h4><p>给定集合$C\subset \mathbb{R}^n $，集合中点的仿射组合称为集合$C$的仿射包(affine hull),表示为$aff C$:<br>$aff C = {\theta_1 x_1 + \cdots + \theta_k x_k\big| x_1,\cdots,x_k \in C, \theta_1 + \cdots + \theta_k = 1}$<br>集合$C$可以是任意集合。仿射包是包含集合$C$的最小仿射集（一个集合的仿射包只有一个，是不变的）。即如果$S$是任意仿射集，满足$C\subset S$，那么有$aff C \subset S$。或者说仿射包是所有包含集合$C$的仿射集的交集。</p><h3 id="仿射纬度-affine-dimension-和相对内部-relative-interior">仿射纬度(affine dimension)和相对内部(relative interior)</h3><h4 id="拓扑-topology">拓扑(topology)</h4><p>拓扑(topology)，开集(open sets),闭集(close sets),内部(interior),边界(boundary),闭包(closure),邻域(neighbood),相对内部(relative interior)<br>同一个集合可以有很多个不同的拓扑。</p><h5 id="定义">定义</h5><p>给定一个集合$X$,$\tau$是$X$的一系列子集，如果$\tau$满足以下条件：</p><ol><li>空集(empty set)和全集X都是$\tau$的元素;</li><li>$\tau$中任意元素的并集(union)仍然是$\tau$的元素;</li><li>$\tau$中任意有限多个元素的交集(intersection)仍然是$\tau$中的元素。</li></ol><p>则称$\tau$是集合$X$上的一个拓扑。<br>如果$\tau$是$X$上的一个拓扑，那么$(X,\tau)$对称为一个拓扑空间(topological space)。<br>如果$X$的一个子集在$\tau$中，这个子集被称为开集(open set)。<br>如果$X$的一个子集的补集是在$\tau$中，那么这个子集是闭集(closed set)。<br>$X$的子集可能是开集，闭集，或者都是，都不是。<br>空集和全集是开集，也是闭集（定义）。</p><h5 id="示例-v2">示例</h5><ol><li>给定集合$X={1,2,3,4}$, 集合$\tau = { {},{1,2,3,4} }$就是$X$上的一个拓扑。</li><li>给定集合$X={1,2,3,4}$, 集合$\tau = { {},{1}, {3,4},{1,3,4},{1,2,3,4} }$就是$X$上的另一个拓扑。</li><li>给定集合$X={1,2,3,4}$, $X$的幂集(power set)也是$X$上的另一个拓扑。</li></ol><p><strong>通常如果不说的话，默认是在欧式空间(1维，2维,…,n维欧式空间)的拓扑，即欧式拓扑。以下讲的一些概念是在欧式空间的拓扑（通常拓扑）上的定义和一般拓扑直观上可能不太一样，但实际上意义是相同的。</strong></p><h4 id="epsilon-disc-或-epsilon-邻域">$\epsilon-disc$或$\epsilon$邻域</h4><h5 id="定义-v2">定义</h5><p>给定$x\in \mathbb{R}^n $以及$\epsilon\gt 0$，集合<br>$$D(x,\epsilon) = {y\in \mathbb{R}^n \big|d(x,y) \lt \epsilon}$$<br>称为关于$x$的$\epsilon-disc$或者$\epsilon$邻域(neighbood)或者$\epsilon$球(ball)。即所有离点$x$距离小于$\epsilon$的点$y$的集合。</p><h4 id="开集-open-sets">开集(open sets)</h4><h5 id="定义-v3">定义</h5><p><strong>给定集合$A\subset \mathbb{R}^n $，对于$A$中的所有元素，即$\forall x\in A$，都存在$\epsilon \gt 0$使得$D(x,\epsilon)\subset A$，那么就称该集合是开的。</strong><br>即集合$A$中所有元素的$spsilon$邻域都还在集合$A$中（定理$1$）。<br><strong>注意：必须满足$\epsilon \gt 0$</strong></p><h5 id="定理">定理</h5><h6 id="定理-1-epsilon-邻域是开集">定理$1$ $epsilon$邻域是开集</h6><ul><li>在$\mathbb{R}^n $中，对于一个$\epsilon \gt 0, x\in \mathbb{R}^n $,那么集合$x$的$\epsilon$邻域$D(x,\epsilon)$是开的，给定一个$\epsilon$，能找到一个更小的$epsilon$邻域。</li></ul><h6 id="定理-2">定理$2$</h6><ul><li>$\mathbb{R}^n $中有限个开子集的交集是$\mathbb{R}^n $的开子集。</li><li>$\mathbb{R}^n $中任意个开子集的并集是$\mathbb{R}^n $的开子集。</li></ul><p><strong>注意：任意开集的交可能不是开集，一个点不是开集，但是它是所有包含它的开集的交。</strong></p><h5 id="示例-v3">示例</h5><p><img src="/2018/12/24/convex-optimization-chapter-2-Convex-sets/unit_circle.png" alt="unit_circle"></p><ol><li>$\mathbb{R}^2 $中的不包含边界的球是开的，如图。</li><li>考虑一个$\mathbb{R}^1 $中的开区间，如$(0,1)$，它是一个开集，但是如果把它放在二维欧式空间中(是x轴上的一个线段)，它不是开的，不满足定义，所以开集是必须针对于某一个给定的集合$X$。</li><li>$\mathbb{R}^2 $上的包含边界的单位圆$X = {x\in \mathbb{R}^2 \big||x|\le 1}$不是开的。因为边界上的点$x$不满足$\epsilon \gt 0, D(x,\epsilon) \subset X$。</li><li>集合$S={(x,y) \in \mathbb{R}^2 \big|0 \lt x \lt 1}$是开集。对于每个点$(x,y)\in S$,我们可以画出半径$r = min{x,1-x}$的邻域并且其全部含于$S$，所以$S$是开集。</li><li>集合$S={(x,y) \in \mathbb{R}^2 \big|0 \lt x \le 1}$不是开集。因为点$(1,0) \in S$的邻域包含点$(x,0)$,其中$x\gt 1$。</li></ol><h4 id="内部-interior">内部(interior)</h4><h5 id="定义-v4">定义</h5><p><strong>给定集合$A\subset \mathbb{R}^n $,点$x \in A$，如果有一个开集$U$使得$x \in U\subset A$,那么该点就称为$A$的一个内点。或者说对于$x\in A$，有一个$\epsilon \gt 0$使得$D(x,\epsilon)\subset A$。$A$的所有内点组成的集合叫做$A$的内部(interior)，记做$int(A)$。</strong></p><h5 id="属性">属性</h5><ol><li>集合内部可能是空的，单点的内部就是空的。</li><li>单位圆的内部是不包含边界的单位圆。</li><li>事实上$A$的内部是$A$所有开子集的并，由开集的定理得$A$的内部是开的，且$A$的内部是$A$的最大的开集。</li><li>当且仅当$A$的内部等于$A$的时候，$A$是开集（$A$可能是闭集）。</li><li>只需要寻找集合内$\epsilon$邻域还在这个集合内的点即可。</li></ol><h5 id="示例-v4">示例</h5><ol><li>给定集合$S={(x,y)\in \mathbb{R}^2 \big| 0 \lt x \le 1}$，$int(S) = {(x,y)\big|0 \lt x \lt 1}$。因为区间$(0,1)$中的点都满足它们的$\epsilon$邻域在$S$中。</li><li>$int(A) \cup int(B) \ne int(A\cup B)$。在实数轴上，$A=[0,1],B=[1,2]$，那么$int(A) = (0,1),int(B) = (1,2)$，所以$int(A) \cup int(B) = (0,1)\cup (1,2) = (0,2)\backslash {1}$，而$int(A\cup B) = int[0,2] = (0,2)$。</li></ol><h4 id="闭集-closed-set">闭集(closed set)</h4><h5 id="定义-v5">定义</h5><p><strong>对于$\mathbb{R}^n $中的集合$B$，如果它在$\mathbb{R}^n $的补（即集合$\mathbb{R}^n \backslash B$）是开集，那么它是闭集。</strong><br>单点是闭集。含有边界的单位圆组成的集合是闭集，因为它的补集不包含边界。一个集合可能既不是开集也不是闭集。例如，在一维欧几里得空间，半开半闭区间（如$(0,1]$）既不是开集也不是闭集。</p><h5 id="定理-v2">定理</h5><ol><li>$\mathbb{R}^n $中有限个闭子集的并是闭集。</li><li>$\mathbb{R}^n $中任意个闭子集的交是闭集。</li></ol><p>这个定理是从开集的定理中得出的，在对开集取补变成闭集时候，并与交相互变换即可。</p><h5 id="示例-v5">示例</h5><ol><li>给定集合$S={(x,y) \in \mathbb{R}^2 \big| 0 \lt x \le 1, 0 \lt y \lt 1}$，$S$不是闭集。因为目标区域的下边界不在S中。</li><li>给定集合$S={(x,y) \in \mathbb{R}^2 \big| x^2 +y^2 \le 1}$，$S$是闭集，因为它的闭集是$\mathbb{R}^2 $中的开集。</li><li>$\mathbb{R}^n $中任何有限集是闭集。因为单点是闭集，有限集可以看成很多个单点的并，由定理$1$可以得出。</li></ol><h4 id="聚点-accumulation-point">聚点(accumulation point)</h4><h5 id="定义-v6">定义</h5><p>对于点$x\in \mathbb{R}^n $，如果包含$x$的每个开集$U$包含不同于$x$但依然属于集合$A$中的点，那么就称$x$是$A$的一个聚点(accumulation points)，也叫聚类点(cluster points)。**注意这里是包含集合$A$中的点，而不是全部是集合$A$中的点，所以集合的聚点不一定必须在集合中。**如，在一维欧式空间中，单点集合没有聚点，开区间$(0,1)$的聚点是$[0,1]$，${0,1}$不在区间内，但是是聚点。<br>此外，$x$是聚类点等价于：对于每个$\epsilon \gt 0$，$D(x,\epsilon)$包含$A$中的某点$y$且$y\ne x$。</p><h5 id="定理-v3">定理</h5><p>当且仅当集合$S$的所有聚点属于$S$时，$S\subset \mathbb{R}^n $是闭集。</p><h5 id="示例-v6">示例</h5><ol><li>给定集合$S={x\in R\big|x\in [0,1]且x是有理数}$，$S$的聚点为$[0,1]$中所有点。任何不属于$[0,1]$的点都不是聚点，因为这类点有一个包含它的$\epsilon$邻域与$[0,1]$不相交。</li><li>给定集合$S={(x,y)\in \mathbb{R}^2 \big| 0 \le  x\le 1\ or\ \ x = 2}$, 它的聚点是它本身，因为它是闭集。</li><li>给定集合$S={(x,y)\in \mathbb{R}^2 \big|y \lt x^2 + 1}$，S的聚点为集合${(x,y)\in \mathbb{R}^2 \big|y \le x^2 + 1}$，</li></ol><h4 id="闭包-closure">闭包(closure)</h4><h5 id="定义-v7">定义</h5><p>给定集合$A\subset \mathbb{R}^n $,集合$A$的闭包$cl(A)$定义成所有包含$A$的闭集的交，所以$cl(A)$是一个闭集。定价的定义是给定集合$A$，包含$A$的最小闭集叫做这个集合$X$的闭包(closure)，用$cl(A)$或者${\overline{A}}$表示。</p><h5 id="定理-v4">定理</h5><p>给定$A\subset \mathbb{R}^n $，那么$cl(A)$由$A$和$A$的所有聚点组成。</p><h5 id="示例-v7">示例</h5><ol><li>$R$中$S=[0,1)\cup {2}$的闭包是$[0,1]$和${2}$。$S$的聚点是$[0,1]$，再并上$S$得到$S$的闭包是$[0,1]\cup{2}$。</li><li>对于任意$S\subset \mathbb{R}^n $，$\mathbb{R}^n \backslash cl(S)$是开集。因为$cl(S)$是闭集，所以它的补集是开集。</li><li>$cl(A\cap B) \ne cl(A)\cap cl(B)$。比如$A=(0,1),B(1,2),cl(A)=[0,1],cl(B)=[1,2]$,$A\cap B = \varnothing$,$cl(A\cap B) = \varnothing$,而$cl(A)\cap cl(B) = {1}$。</li></ol><h4 id="边界-boundary">边界(boundary)</h4><h5 id="定义-v8">定义</h5><p>对于$\mathbb{R}^n $中的集合$A$，边界定义为集合：<br>$bd(A) = cl(A)\cap cl(\mathbb{R}^n \backslash A)$<br>即集合$A$的补集的闭包和$A$的闭包的交集，所以$bd(A)$是闭集。$bd(A)$是$A$与$\mathbb{R}^n \backslash A$之间的边界。</p><h5 id="定理-v5">定理</h5><p>给定$A\subset \mathbb{R}^n $，当且仅当对于每个$\epsilon \gt 0$，$D(x,\epsilon)$包含$A$与$\mathbb{R}^n \backslash A$的点，$x\in bd(A)$。</p><h5 id="示例-v8">示例</h5><ol><li>给定集合$S={x\in R\big|x\in [0,1],x是有理数}$，$bd(S) = [0,1]$。因为对于任意$\epsilon \gt 0, x\in [0,1],D(x,\epsilon) = (x-\epsilon, x+\epsilon)$包含有理数和无理数，即x是有理数和无理数之间的边界。</li><li>给定$x\in bd(S)$，$x$不一定是聚点。给定集合$S = {0} \subset R$，$bd(S) = {0}$，但是单点没有聚点。</li><li>给定集合$S={(x,y)\in \mathbb{R}^2 \big| x^2 -y^2 \gt 1 }$，$bd(S)={(x,y)\big|x^2 - y^2 = 1}$。</li></ol><h4 id="仿射维度-affine-dimension">仿射维度(affine dimension)</h4><h5 id="定义-v9">定义</h5><p>给定一个仿射集$C$，仿射维度是它的仿射包的维度。<br>仿射维度和其他维度的定义不总是相同的，具体可以看以下的示例。</p><h5 id="示例-v9">示例</h5><p>给定一个二维欧几里得空间的单位圆，${x\in C\big|x_1^2 +x_2^2 =1}$。它的仿射包是整个$\mathbb{R}^2$，所以二维平面的单位圆仿射维度是$2$。但是在很多定义中，二维平面的单位圆的维度是$1$。</p><h4 id="相对内部-relative-interior">相对内部(relative interior)</h4><p>给定一个集合$C\subset \mathbb{R}^n $，它的仿射维度可能小于$n$，这个时候仿射集$aff\ C \ne \mathbb{R}^n $。</p><h5 id="定义-v10">定义</h5><p>给定集合$C$，相对内部的定义如下：<br>$relint\ C = {x\in C\big|(B(x,r)\cup aff\ C) \subset C, \exists \ r \gt 0}.$<br>就是集合$C$内所有$\epsilon$球在$C$的仿射集内的点的集合。<br>其中$B(x,r)={y \big|\Vert y- x\Vert \le r}$，是以$x$为中心，以$r$为半径的圆。这里的范数可以是任何范数，它们定义的相对内部是相同的。</p><h5 id="示例-v10">示例</h5><p>给定一个$\mathbb{R}^3 $空间中$(x_1,x_2)$平面上的正方形，$C={x\in \mathbb{R}^3 \big|-1 \le x_1 \le 1, -1\le x_2 \le 1, x_3 = 0}$。它的仿射包是$(x_1,x_2)$平面，$aff\ C = {x\in \mathbb{R}^3 \big|x_3=0}$。$C$的内部是空的，但是相对内部是：<br>$relint\ C = {x \in \mathbb{R}^3 \big|-1 \le x_1 \le 1, -1\le x_2 \le 1,x_3=0}$。</p><h4 id="相对边界-relative-boundary">相对边界(relative boundary)</h4><h5 id="定义-v11">定义</h5><p>给定集合$C$，相对边界(relative boundary)定义为$cl\ C \backslash relint\ C$，其中$cl\ C$是集合$C$的闭包(closure)。</p><h5 id="示例-v11">示例</h5><p>对于上例（相对内部的示例）来说，它的边界(boundary)是它本身。它的相对内部是边框，${x\in \mathbb{R}^3 \big|max{|x_1|,|x_2|}=1,x_3=0}$。</p><h3 id="凸集-convex-sets">凸集(convex sets)</h3><h4 id="凸集定义">凸集定义</h4><p>给定一个集合$C$，如果集合$C$中经过任意两点的线段仍然在$C$中，这个集合就是一个凸集。<br>给定$\forall x_1,x_2 \in C, 0 \le \theta \le 1$，那么我们有$\theta x_1 + (1-\theta)x_2 \in C$。<br>每一个仿射集都是凸的，因为它包含经过任意两个不同点的直线，所以肯定就包含过那两个点的线段。</p><h4 id="凸组合-convex-combination">凸组合(convex combination)</h4><p>给定$k$个点$x_1,x_2,\cdots,x_k$，如果具有$\theta_1 x_1 + \cdots, \theta_k x_k$形式且满足$\theta_1 + \cdots + \theta_k=1, \theta_i \ge 0,i=1,\cdots,k$,那么就称这是$x_1,\cdots,x_k$的一个凸组合。<br>当且仅当一个集合包含其中所有点的凸组合，这个集合是一个凸集。点的一个凸组合可以看成点的混合或者加权，$\theta_i$是第$i$个点$x_i$的权重。<br>凸组合可以推广到无限维求和，积分，概率分布等等。假设$\theta_1,\theta_2,\cdots$满足：<br>$$\theta_i \le 0, i = 1,2,\cdots, \sum_{i=1}^{\infty}\theta_i = 1$$<br>并且$x_1,x_2,\cdots \in C$，$C\subset \mathbb{R}^n $是凸的，如果(series)$\sum_{i=1}^{\infty} \theta_i x_i$收敛，那么$\sum_{i=1}^{\infty} \theta_i x_i \in C$。<br>更一般的，假设概率分布$p$，$\mathbb{R}^n \rightarrow R$满足$p(x)\le 0 for\ all\ x\in C, \int_{C}p(x)dx = 1$,其中$C\subset \mathbb{R}^n $是凸的，如果$\int_{C}p(x)xdx$存在的话，那么$\int_{C}p(x)xdx\in C$。</p><h4 id="凸包-convex-hull">凸包(convex hull)</h4><p>给定一个集合$C$，凸包的定义为包含集合$C$中所有点的凸组合的结合，记为$conv\ C$，公式如下：<br>$conv\ C = {\theta_1 x_1 + \cdots + \theta_k x_k\big|x_i \in C, \theta_i \ge 0, i = 1,\cdots,k,\theta_1 +\cdots + \theta_k = 1}$<br>任意集合都是有凸包的。一个集合的凸包总是凸的。集合$C$的凸包是包含集合$C$的最小凸集。如果集合$B$是任意包含$C$的凸集，那么$conv\ C \subset B$。</p><h4 id="示例-v12">示例</h4><p><img src="/2018/12/24/convex-optimization-chapter-2-Convex-sets/figure2_2.png" alt="figure 2.2"><br><img src="/2018/12/24/convex-optimization-chapter-2-Convex-sets/figure2_3.png" alt="figure 2.3"></p><h3 id="锥-cones">锥(cones)</h3><h4 id="锥-cones-和凸锥-convex-cones-的定义">锥(cones)和凸锥(convex cones)的定义</h4><p>给定集合$C$，如果$\forall x \in C, \theta \ge 0$，都有$\theta x\in C$，这样的集合就称为一个锥(cone)，或者非负同质(nonnegative homogeneour)。<br>一个集合$C$如果既是锥又是凸的，那这个集合是一个凸锥(convex cone)，即：$\forall x_1,x_2 \in C, \theta_1,\theta_2 \ge 0$,那么有$\theta_1 x_1+\theta_2 x_2 \in C$。几何上可以看成经过顶点为原点，两条边分别经过点$x_1$和$x_2$的$2$维扇形。</p><h4 id="锥组合-conic-combination">锥组合(conic combination)</h4><p>给定$k$个点$x_1,x_2,\cdots,x_k$，如果具有$\theta_1 x_1 + \cdots, \theta_k x_k$形式且满足$\theta_i \ge 0,i=1,\cdots,k$,那么就称这是$x_1,\cdots,x_k$的一个锥组合(conic combination)或者非负线性组合(nonnegative combination)。<br>给定集合$C$是凸锥，那么集合$C$中任意点$x_i$的锥组合仍然在集合$C$中。反过来，当且仅当集合$C$包含它的任意元素的凸组合时，这个集合是一个凸锥(convex cone)。</p><h4 id="锥包-conic-hull">锥包(conic hull)</h4><p>给定集合$C$，它的锥包(conic hull)是集合$C$中所有点的锥组合。即：<br>$conic\ C = {\theta_1 x_1 + \cdots + \theta_k x_k\big|x_i \in C, \theta_i \ge 0, i = 1,\cdots,k}$<br>集合$C$的锥包是包含集合$C$的最小凸锥。</p><h4 id="示例-v13">示例</h4><p><img src="/2018/12/24/convex-optimization-chapter-2-Convex-sets/figure2_4.png" alt="figure 2.4"><br><img src="/2018/12/24/convex-optimization-chapter-2-Convex-sets/figure2_5.png" alt="figure 2.5"></p><h3 id="一些想法">一些想法</h3><p>在我自己看来，在几何上<br>仿射集可以看成是集合中任意两个点的直线的集合。<br>凸集可以看成是集合中任意两个点的线段的集合，因为直线一定包含线段，所以仿射集一定是凸集。<br>锥集可以看成是集合中任意一个点和原点构成的射线的集合，锥集不一定是连续的（两条射线也是锥集），所以锥集不一定是凸集。<br>而凸锥既是凸集又是锥集。<br>我在stackexchange看到这样一句话觉得说的挺好的。</p><blockquote><p>What basically distinguishes the definitions of convex, affine and cone, is the domain of the coefficients and the constraints that relate them.</p></blockquote><p>区别凸集，仿射和锥的是系数的取值范围和一些其他限制。仿射集要求$\theta_1+\cdots+\theta_k = 1$，凸集要求$\theta_1 +\cdots +\theta_k = 1, 0\le \theta \le 1$，锥的要求是$\theta \ge 0$，凸锥的要求是$\theta_i \ge 0,i=1,\cdots,k$。<br>仿射集不是凸集的子集，凸集也不是仿射集的子集。所有仿射集的集合是所有凸集的集合的子集，一个仿射集是一个凸集。</p><h2 id="示例-v14">示例</h2><ul><li>$\emptyset$，单点(single point)${x_0}$，整个$\mathbb{R}^n $空间都是$\mathbb{R}^n $中的仿射子集，所以也是凸集，点不一定是凸锥（在原点熵是凸锥），空集是凸锥，$\mathbb{R}^n $维空间也是凸锥。<strong>根据定义证明。</strong></li><li>任意一条直线都是仿射的，所以是凸集。如果经过原点，它是一个子空间，也就是一个凸锥，否则不是。</li><li>任意一条线段都是凸集，不是仿射集，当它退化成一点的时候，它是仿射的，线段不是凸锥。</li><li>一条射线${x_0 + \theta v\big| \theta \ge 0}$是凸的，但是不是仿射的，当$x_0=0$时，它是凸锥。</li><li>任意子空间都是仿射的，也是凸锥，所以是凸的。</li></ul><p>补充最后一条，任意子空间都是仿射的，也是凸锥。<br>如果$V$是一个子空间，那么$V$中任意两个向量的线性组合还在$V$中。即如果$x_1,x_2\in V$，对于$\theta_1,\theta_2 \in R$，都有$\theta_1 x_1 + \theta_2 x_2 \in V$。正如前面说的，子空间是加法和数乘封闭的。<br>而根据仿射集的定义，如果$x_1,x_2$在一个仿射集$C$中，那么对于$\theta_1+\theta_2 = 1$，都有$\theta_1 x_1 + \theta_2 x_2 \in C$。我们可以看出来，如果取子空间中线性组合的系数和为$1$，那么就成了仿射集。如果取子空间中的系数$\theta_1,\theta_2 \in R_+$,那么就成了锥，如果同时满足$\theta_1+\theta_2 = 1$，那么就成凸锥。那么如果加上这些限制条件，即取子空间中线性组合的系数和为$1$，或者取子空间中的系数$\theta_1,\theta_2 \in R_+$,同时满足$\theta_1+\theta_2 = 1$。<br>事实上，子空间要求的条件比仿射集和凸锥的条件要更严格。仿射集和凸锥只要求在系数$\theta_i$满足相应的条件时,有$\theta_1 x_1 + \theta_2 x_2 \in \mathbb{R}^n $；而子空间要求的是在系数$\theta_i$取任意值的时候，都有$\theta_1 x_1 + \theta_2 x_2 \in \mathbb{R}^n $，所以子空间一定是仿射集，也一定是凸锥。（拿二维的举个例子，给定$x_1$和$x_2$，仿射集可以看成是$\theta_1$的函数，因为$\theta_2=1-\theta_1$，而子空间可以看成$\theta_1$和$\theta_2$的函数，一个是一元函数，一个是二元函数）</p><h3 id="超平面-hyperplane-和半空间-halfspace">超平面(hyperplane)和半空间(halfspace)</h3><p>超平面是一个仿射集，也是凸集，但不一定是锥集(过原点才是锥集，也是一个子空间)。<br>闭的半空间是一个凸集，不是仿射集。</p><h4 id="超平面-hyperplane">超平面(hyperplane)</h4><p>超平面通常具有以下形式：<br>$${x\big|a^T x=b},$$<br>其中$a\in \mathbb{R}^n ,a\ne 0,b\in R$，它其实是一个平凡(nontrivial)线性方程组的解，因此也是一个仿射集。几何上，超平面可以解释为和一个给定向量$a$具有相同内积(inner product)的点集，或者说是法向量为$a$的一个超平面。常数$b$是超平面和原点之间的距离(offset)。<br>几何意义可以被表示成如下形式：<br>$${x\big|a^T (x-x_0) = 0},$$<br>其中$x_0$是超平面上的一点，即满足$a^T x_0=0$。可以被表示成如下形式：<br>$${x\big|a^T (x-x_0)=0} = x_0+a^{\perp},$$<br>其中$a^{\perp} $是$a$的正交补，即所有与$a$正交的向量的集合，满足$a^{\perp} ={v\big|a^T v=0}$。所以，超平面的几何解释可以看做一个偏移(原点到这个超平面的距离)加上所有垂直于一个特定向量$a$(正交向量)的向量，即这些垂直于$a$的向量构成了一个过原点的超平面，再加上这个偏移量就是我们要的超平面。几何表示如下图所示。<br><img src="/2018/12/24/convex-optimization-chapter-2-Convex-sets/figure2_6.png" alt="figure 2.6"></p><h4 id="半空间-halfspace">半空间(halfspace)</h4><p>一个超平面将$\mathbb{R}^n $划分为两个半空间(halfspaces)，一个是闭(closed)半空间，一个是开半空间。闭的半空间可以表示成${x\big|a^T x\le b}$，其中$a\ne 0$，半空间是凸的，但不是仿射的。下图便是一个闭的半空间。<br><img src="/2018/12/24/convex-optimization-chapter-2-Convex-sets/figure2_7.png" alt="figure 2.7"><br>这个半空间也可以写成：<br>$${x\big|a^T (x-x_0)\le 0},$$<br>其中$x_0$是划分两个半空间的超平面上的一点，即满足$a^T x_0=b$。一个几何解释是：半空间由一个偏移$x_0$加上所有和一个特定向量$a$(超平面的外(outward)法向量)成钝角(obtuse)或者直角(right)的所有向量组成。<br>这个半空间的边界是超平面${x\big|a^T x=b}$。这个半空间${x\big|a^T x\le b}$的内部是${x\big|a^T x\lt b}$，也被称为一个开半平面。</p><h3 id="欧几里得球-euclidean-ball-和椭球-ellipsoid">欧几里得球(Euclidean ball)和椭球(ellipsoid)</h3><h4 id="欧几里得球">欧几里得球</h4><p>$\mathbb{R}^n $空间中的欧几里得球或者叫球，有如下的形式：<br>$$B(x_r,r = {x\big|\Vert x-x_c\Vert_2\le r}={x \big|(x-x_c)^T (x-x_c)\le \mathbb{R}^2 },$$<br>其中$r\gt 0$,$\Vert \cdot\Vert_2$是欧几里得范数(第二范数)，即$\Vert u\Vert_2=(u^T u)^{\frac{1}{2}} $。向量$x_c$是球心，标量$r$是半径。$B(x_c,r)$包含所有和圆心$x_c$距离小于$r$的球。<br>欧几里得球的另一种表示形式是：<br>$$B(x_c,r)={x_c + ru\big| \Vert u \Vert_2 \le 1},$$<br>一个欧几里得球是凸集，如果$\Vert x_1-x_c\Vert_2 \le r,\Vert x_2-x_c\Vert_2\le r, 0\le\theta\le1$，那么：<br>\begin{align*}<br>\Vert\theta x_1 + (1-\theta)x_2 - x_c\Vert_2 &amp;= \Vert\theta(x_1-x_c)+(1-\theta)(x_2-x_c)\Vert_2\\<br>&amp;\le\theta\Vert x_1-x_c\Vert_2 + (1-\theta)\Vert x_2 - x_c \Vert_2\\<br>&amp;\le r<br>\end{align*}<br>用其次性和三角不等式可证明</p><h4 id="椭球">椭球</h4><p>另一类凸集是椭球，它们有如下的形式：<br>$$\varepsilon ={x\big|(x-x_c)^T P^{-1} (x-x_c) \le 1},$$<br>其中$P=P^T \succ 0$即$P$是对称和正定的。向量$x_c\in \mathbb{R}^n $是椭球的中心。矩阵$P$决定了椭球从$x_c$向各个方向扩展的距离。椭球$\varepsilon$的半轴由矩阵$P$的特征值$\lambda_i$算出，$\sqrt{\lambda_i}$，球是$P=\mathbb{R}^2 I$的椭球。<br><strong>这里这种表示形式为什么要用$P^{-1} $？</strong><br>椭球的另一种表示是：<br>$$\varepsilon = {x_c + Au\big| \Vert u \Vert_2 \le 1},$$<br>其中$A$是一个非奇异方阵。假设$A$是对称正定的，取$A=P^{\frac{1}{2}} $，这种表示就和上面的表示是一样的。第一次看到这种表示的时候，我在想，椭球的边界上有无数个点，一个方阵$A$是怎么实现对这无数个操作的，后来和球做了对比，发现自己一直都想错了，这无数个点是通过范数实现的而不是通过矩阵$A$实现的，到球心距离为$\Vert u\Vert_2\le 1$的点有无数个，$A$对这无数个点的坐标都做了仿射变换，将一个球变换成了椭球，特殊情况下就是球。当矩阵$A$是对称半正定但是是奇异的时候，这个情况下称为退化椭球(degenerate ellipsoid)，它的仿射维度和矩阵$A$的秩(rank)是相同的。退化椭球也是凸的。</p><h3 id="范数球-norm-ball-和范数锥-norm-cone">范数球(norm ball)和范数锥(norm cone)</h3><h4 id="范数球-norm-ball">范数球(norm ball)</h4><h5 id="定义-v12">定义</h5><p>$\Vert \cdot\Vert$是$\mathbb{R}^n $上的范数。一个范数球(norm ball)可以看成一个以$x_c$为中心，以$r$为半径的集合，但是这个$r$可以是任何范数，即${x\big|\Vert x-x_c \Vert \le r}$，它是凸的。</p><h5 id="示例-v15">示例</h5><p>我们常见的球是二范数（欧几里得范数）对应的范数球。</p><h4 id="范数锥">范数锥</h4><h5 id="定义-v13">定义</h5><p>和范数相关的范数锥是集合：$C = {(x,t)\big|\Vert x\Vert \le t} \subset \mathbb{R}^{n+1} $，它也是凸锥。</p><h5 id="示例-v16">示例</h5><p><img src="/2018/12/24/convex-optimization-chapter-2-Convex-sets/figure2_10.png" alt="figure 2.10"><br>二阶锥(second-order cone)是欧几里得范数对应的范数锥，如图所示，其表达式为：<br>\begin{align*}<br>C &amp;={(x,t)\in \mathbb{R}^{n+1} \big| \Vert x\Vert_2 \le t}\\<br>&amp;= \left{ \begin{bmatrix}x\\t\end{bmatrix} \big| \begin{bmatrix}x\\t\end{bmatrix}^T \begin{bmatrix}I&amp;0\\ 0&amp;-1\end{bmatrix} \begin{bmatrix}x\\t\end{bmatrix}\le 0, t \gt 0 \right}<br>\end{align*}<br>这个二阶锥也被称为二次锥(quadratic cone)，因为它是通过一个二次不等式定义的，也被叫做Lorentz cone或者冰激凌锥(ice-cream cone)。</p><h4 id="范数锥和范数球的区别">范数锥和范数球的区别</h4><p>范数球是所有点到圆心$x_c$的范数小于一个距离$r$。<br>范数锥是很多直线组成的锥。</p><h3 id="多面体-polyhedra">多面体(polyhedra)</h3><h4 id="定义-v14">定义</h4><p>多面体(polyhedron)是有限个线性不等式或者线性方程组的解集的集合：<br>$P = {x\big|a_j^T x\le b_j, j=1,\cdots,m,c_j^T x=d_j,j=1,\cdots,p}$<br>多面体因此也是有限个半空间或者超平面的交集。仿射集(如，子空间，超平面，直线)，射线，线段，半空间等等都是多面体，多面体也是凸集。有界的polyhedron有时也被称为polytope，一些作者会把它们两个反过来叫。<br>上式的紧凑(compact)表示是：<br>$$P={x\big|Ax\preceq b, Cx=d}$$<br>其中$A=\begin{bmatrix}a_1^T \\ \vdots\\ a_m^T \end{bmatrix},C=\begin{bmatrix}c_1^T \\ \vdots\\c_p^T \end{bmatrix}$，$\preceq$表示$\mathbb{R}^m $空间中的向量不等式(vector ineuqalitied)或者分量大小的不等式，$u\preceq v$代表着$u_i\le v_i, i=1,\cdots,m$。</p><h5 id="simplexes">simplexes</h5><p>simplexes是另一类很重要的多面体。假设$\mathbb{R}^n $空间中的$k+1$个点是仿射独立(affinely independent)，意味着$v_1-v_0, \cdots,v_k-v_0$是线性独立的。由$k+1$个仿射独立的点确定的simplex是：<br>$$C = conv{v_0,\cdots,v_k} = {\theta_0v_0+\cdots+\theta_kv_k\big| \theta \succeq 0, \mathcal{1}\theta=1 },$$<br>其中$\mathcal{1}$是全为$1$的列向量。这个simplex的仿射维度是$k$，所以它也叫$\mathbb{R}^n $空间中的$k$维simplex。为什么仿射维度是$k$，我的理解是simplex是凸集，而凸集不是子空间，凸集去掉其中任意一个元素才是子空间，所以就是$k$维而不是$k+1$维。<br>为了将simplex表达成一个紧凑形式的多面体。定义$y=(\theta_1,\cdots,\theta_k)$和$B=[v_1-v_0\ \cdots\ v_k-v_0]\in \mathbb{R}^{n\times k} $，当且仅当存在$y\succeq 0, \mathcal{1}^T y\le 1$，$x=v_0+By$有$x\in C$，<strong>疑问，这里为什么变成了$\mathcal{1}^T y\le 1$，难道是因为少了个$v_0$吗</strong>。点$v_0,\cdots,v_k$表明矩阵$B$的秩为$k$。因此存在一个非奇异矩阵$A=(A_1,A_2)\in \mathbb{R}^{n\times n} $使得：<br>$$AB = \begin{bmatrix}A_1\\A_2\end{bmatrix}B= \begin{bmatrix}I\\0\end{bmatrix}.$$<br>对$x = v_0+By$同时左乘$A$，得到：<br>$$A_1x = A_1v_0+y, A_2x=A_xv_0.$$<br>从中我们可以看出如果$A_2x=A_2v_0$，且向量$y=A_1x-A_1v_0$满足$y\succeq 0, \mathcal{1}^T y\le1$时，$x\in C$。换句话说，当且仅当$x$满足以下等式和不等式时：<br>$$A_2x = A_2v_0,A_1x\succeq A_1v_0, \mathcal{1}A_1x\le1+\mathcal{1}^T A_1v_0,$$<br>有$x\in C$。</p><h5 id="多面体的凸包描述">多面体的凸包描述</h5><p>一个有限集合${v_1,\cdots,v_k}$的凸包是：<br>$$conv{v_1,\cdots,v_k} = {\theta_1 v_1 +\cdots +\theta_k v_k\big| \theta \succeq 0, \mathcal{1}^T \theta = 1}.$$<br>这个集合是一个多面体，并且有界。但是它（除了simplex）不容易化成多面体的紧凑表示，即不等式和等式的集合。<br>一个一般化的凸包描述是：<br>$${\theta_1 v_1 +\cdots +\theta_k v_k\big| \theta_1+\cdots + \theta_m = 1,\theta_i \ge 0,i=1,\cdots,k}.$$<br>其中$m\le k$，它可以看做是点$v_1,\cdots,v_m$的凸包加上点$v_{m+1},\cdots,v_{k}$的锥包。这个集合定义了一个多面体，反过来，任意一个多面体可以看做凸包加上锥包。<br>一个多面体如何表示是很有技巧的。比如一个$\mathbb{R}^n $空间上的无穷范数单位球$C$：<br>$$C={x\big|\ |x_i|\le 1,i = 1,\cdots,n}.$$<br>集合$C$可以被表示成$2n$个线性不等式$\pm e_i^T x\le 1$，其中$e_i$是第$i$个单位向量。然而用凸包形式描述这个集合需要用至少$2^n $个点：<br>$$C = conv{v_{1},\cdots,v_{2^n }},$$<br>其中$v_{1},\cdots,v_{2n}$是$2^n $个向量，每个向量的元素都是$1$或$-1$。因此凸包描述和不等式描述有很大差异，尤其是$n$很大的时候。<br>这里为什么是$2^n $个点呢？因为是无穷范数构成的单位圆，在数轴上是区间$[-1,1]$，在$\mathbb{R}^2 $是正方形${(x,)\big|-1 \le x\le 1,-1\le y\le 1}$，对应的四个点是${(1,1),(1,-1),(-1,1),(-1,-1)}$，而在$\mathbb{R}^3 $是立方体${(x,y,z)\big|-1 \le x\le 1,-1\le y\le 1, -1\le z\le 1}$，对应的是立方体的八个顶点${(1,1,1),(1,1,-1),(1,-1,1),(1,-1,-1),(-1,1,1),(-1,1,-1),(-1,-1,1),(-1,-1,-1)}$。</p><h4 id="示例-v17">示例</h4><ol><li>如图所示，是五个半平面的交集定义的多面体。<br><img src="/2018/12/24/convex-optimization-chapter-2-Convex-sets/figure2_11.png" alt="figure 2.11"></li><li>非负象限(nonnegative orthant)是非负点的集合，即：<br>$$R_{+}^n = {x\in \mathbb{R}^n \big| x_i\ge 0, i = 1,\cdots,n} = {x\in \mathbb{R}^n \big| x\succeq 0}.$$<br>非负象限是一个多面体，也是一个锥，所以也叫多面体锥(polyhedral cone)，也叫非负象限锥。</li><li>一个1维的simplex是一条线段。一个二维的simplex是一个三角形（包含它的内部）。一个三维的simple是一个四面体(tetrahedron)。</li><li>由$\mathbb{R}^n $中的零向量和单位向量确定的simplex是$n$维unit simplex。它是向量集合：<br>$$x\succeq 0, \mathcal{1}^T x \le 1.$$</li><li>由$\mathbb{R}^n $中的单位向量确定的simplex是$n-1$维probability simplex。它是向量集合：<br>$$x\succeq 0, \mathcal{1}^T x = 1.$$<br>Probability simplex是中的向量可以看成具有$n$个元素的集合的概率分布，$x_i$解释为第$i$个元素的概率。</li></ol><h3 id="半正定锥-positive-sefidefinite-cone">半正定锥(positive sefidefinite cone)</h3><h4 id="定义-v15">定义</h4><p>用$S^n $表示$n\times n$的对称矩阵：$S^n ={X\in \mathbb{R}^{n\times n} \big| X = X^T }$，$S^n $是一个$n(n+1)/2$维基的向量空间。比如，三维空间中对称矩阵的一组基是：<br>$$\begin{bmatrix}1&amp;0&amp;0\\0&amp;0&amp;0\\0&amp;0&amp;0\end{bmatrix}\begin{bmatrix}0&amp;0&amp;0\\1&amp;0&amp;0\\0&amp;0&amp;0\end{bmatrix}\begin{bmatrix}0&amp;0&amp;0\\0&amp;1&amp;0\\0&amp;0&amp;0\end{bmatrix}\begin{bmatrix}0&amp;0&amp;0\\0&amp;0&amp;0\\1&amp;0&amp;0\end{bmatrix}\begin{bmatrix}0&amp;0&amp;0\\0&amp;0&amp;0\\0&amp;1&amp;0\end{bmatrix}\begin{bmatrix}0&amp;0&amp;0\\0&amp;0&amp;0\\0&amp;0&amp;1\end{bmatrix}.$$<br>用$S_{+}^n $表示半正定的对称矩阵集合：<br>$$S_{+}^n = {X\in S^n \big| X\succeq 0},$$<br>用$S_{++}^n $表示正定的对称矩阵集合：<br>$$S_{+}^n = {X\in S^n \big| X\succ 0},$$<br>集合$S_{+}^n $是凸锥：如果$\theta_1,\theta_2 \ge 0$且$A,B\in S_{+}^n $，那么$\theta_1 A+\theta_{2} B\in S_{+}^n $。这个可以直接从依靠半正定的定义来证明，如果$A,B\in S_{+}^n ,\theta_1,\theta_2\ge 0$，(<strong>这里原书中用的是$A,B\succeq 0$,我觉得应该是写错了吧</strong>)，对任意$\forall x \in \mathbb{R}^n $，都有：<br>$$x^T (\theta_1A+\theta_2B)x = \theta_1x^T Ax + \theta_2x^T Bx.$$</p><h4 id="示例-v18">示例</h4><p>对于$S^2 $空间中的半正定锥，有<br>$$X=\begin{bmatrix}x&amp;y\\y&amp;z\end{bmatrix}\in S_{+}^2 \Leftrightarrow x\ge 0,z\ge 0, xz\ge y^2 $$<br>这个锥的边界如下图所示。<br><img src="/2018/12/24/convex-optimization-chapter-2-Convex-sets/figure2_12.png" alt="figure 2.12"></p><h3 id="常见的几种锥">常见的几种锥</h3><p>范数锥，非负象限锥，半正定锥，它们都过原点。<br>想想对应的图像是什么样的。<br>范数锥和非负象限锥图像还好理解一些，非负象限锥是$\mathbb{R}^n $空间所有非负半轴围成的锥，范数锥的边界像一个沙漏，但是是无限延伸的。半正定锥怎么理解，还没有太好的类比。</p><h2 id="保凸运算-operations-that-preserve-convexity">保凸运算(operations that preserve convexity)</h2><p>这一小节介绍的是一些保留集合凸性，或者从一些集合中构造凸集的运算。这些运算和simplex形成了凸集的积分去确定或者建立集合的凸性。</p><h3 id="集合交-intersection">集合交(intersection)</h3><p>凸集求交集可以保留凸性：如果$S_1$和$S_2$是凸集，那么$S_1\cup S_2$是凸集。扩展到无限个集合就是：如果$\forall \alpha \in A,S_{\alpha}$都是凸的，那么$\cup_{\alpha\in A S_{\alpha}}$是凸的</p><h3 id="仿射函数-affine-functions">仿射函数(affine functions)</h3><h3 id="线性分式-linear-fractional-和视角函数-perspective-functions">线性分式(linear-fractional)和视角函数(perspective functions)</h3><h4 id="线性分式-linear-fractional">线性分式(linear-fractional)</h4><h4 id="视角函数-perspective-functions">视角函数(perspective functions)</h4><h2 id="广义不等式-generalized-inequalities">广义不等式（Generalized inequalities)</h2><h3 id="真锥-proper-cones-和广义不等式-generalized-inequalities">真锥(Proper cones)和广义不等式（Generalized inequalities)</h3><h3 id="最小-minimum-和最小元素-minimal-elemetns">最小(Minimum)和最小元素(minimal elemetns)</h3><h2 id="separating和supporting-hyperplanes">Separating和supporting hyperplanes</h2><h3 id="separating-hyperplane-theorem">Separating hyperplane theorem</h3><h3 id="supporting-hyperplanes">Supporting Hyperplanes</h3><h2 id="对偶锥-dual-cones-和广义不等式-generalized-inequalities">对偶锥(dual cones)和广义不等式(generalized inequalities)</h2><h3 id="none"></h3><h2 id="符号定义">符号定义</h2><p>$\preceq$表示$\mathbb{R}^m $空间中的向量不等式(vector ineuqalitied)或者element-wise的不等式，$u\preceq v$代表着$u_i\le v_i, i=1,\cdots,m$。</p><h2 id="参考文献">参考文献</h2><p>1.stephen boyd. Convex optimization<br>2.<a href="https://en.wikipedia.org/wiki/Topology" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Topology</a><br>3.<a href="https://en.wikipedia.org/wiki/Topological_space" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Topological_space</a><br>4.<a href="https://en.wikipedia.org/wiki/Power_set" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Power_set</a><br>5.<a href="https://en.wikipedia.org/wiki/Open_set" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Open_set</a><br>6.<a href="https://en.wikipedia.org/wiki/Closed_set" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Closed_set</a><br>7.<a href="https://en.wikipedia.org/wiki/Clopen_set" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Clopen_set</a><br>8.<a href="https://en.wikipedia.org/wiki/Interior_(topology)" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Interior_(topology)</a><br>9.<a href="https://en.wikipedia.org/wiki/Closure_(topology)" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Closure_(topology)</a><br>10.<a href="https://en.wikipedia.org/wiki/Boundary_(topology)" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Boundary_(topology)</a><br>11.<a href="https://en.wikipedia.org/wiki/Ball_(mathematics)" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Ball_(mathematics)</a><br>12.<a href="https://blog.csdn.net/u010182633/article/details/53792588" target="_blank" rel="noopener">https://blog.csdn.net/u010182633/article/details/53792588</a><br>13.<a href="https://blog.csdn.net/u010182633/article/details/53819910" target="_blank" rel="noopener">https://blog.csdn.net/u010182633/article/details/53819910</a><br>14.<a href="https://blog.csdn.net/u010182633/article/details/53983642" target="_blank" rel="noopener">https://blog.csdn.net/u010182633/article/details/53983642</a><br>15.<a href="https://blog.csdn.net/u010182633/article/details/53997843" target="_blank" rel="noopener">https://blog.csdn.net/u010182633/article/details/53997843</a><br>16.<a href="https://blog.csdn.net/u010182633/article/details/54093987" target="_blank" rel="noopener">https://blog.csdn.net/u010182633/article/details/54093987</a><br>17.<a href="https://blog.csdn.net/u010182633/article/details/54139896" target="_blank" rel="noopener">https://blog.csdn.net/u010182633/article/details/54139896</a><br>18.<a href="https://math.stackexchange.com/questions/1168898/why-is-any-subspace-a-convex-cone" target="_blank" rel="noopener">https://math.stackexchange.com/questions/1168898/why-is-any-subspace-a-convex-cone</a><br>19.<a href="https://www.zhihu.com/question/22799760/answer/139753685" target="_blank" rel="noopener">https://www.zhihu.com/question/22799760/answer/139753685</a><br>20.<a href="https://www.zhihu.com/question/22799760/answer/34282205" target="_blank" rel="noopener">https://www.zhihu.com/question/22799760/answer/34282205</a><br>21.<a href="https://www.zhihu.com/question/22799760/answer/137768096" target="_blank" rel="noopener">https://www.zhihu.com/question/22799760/answer/137768096</a><br>22.<a href="https://en.wikipedia.org/wiki/Positive-definite_matrix" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Positive-definite_matrix</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;仿射集-affine-sets-和凸集-convex-sets&quot;&gt;仿射集(affine sets)和凸集(convex sets)&lt;/h2&gt;
&lt;h3 id=&quot;直线-line-和线段-line-segmens&quot;&gt;直线(line)和线段(line segmens)&lt;/
      
    
    </summary>
    
      <category term="凸优化" scheme="http://mxxhcm.github.io/categories/%E5%87%B8%E4%BC%98%E5%8C%96/"/>
    
    
      <category term="凸优化" scheme="http://mxxhcm.github.io/tags/%E5%87%B8%E4%BC%98%E5%8C%96/"/>
    
      <category term="convex sets" scheme="http://mxxhcm.github.io/tags/convex-sets/"/>
    
      <category term="affine sets" scheme="http://mxxhcm.github.io/tags/affine-sets/"/>
    
      <category term="cones" scheme="http://mxxhcm.github.io/tags/cones/"/>
    
      <category term="convex cones" scheme="http://mxxhcm.github.io/tags/convex-cones/"/>
    
      <category term="convex optimizaitons" scheme="http://mxxhcm.github.io/tags/convex-optimizaitons/"/>
    
      <category term="锥" scheme="http://mxxhcm.github.io/tags/%E9%94%A5/"/>
    
      <category term="凸锥" scheme="http://mxxhcm.github.io/tags/%E5%87%B8%E9%94%A5/"/>
    
      <category term="凸集" scheme="http://mxxhcm.github.io/tags/%E5%87%B8%E9%9B%86/"/>
    
      <category term="仿射集" scheme="http://mxxhcm.github.io/tags/%E4%BB%BF%E5%B0%84%E9%9B%86/"/>
    
  </entry>
  
  <entry>
    <title>convex optimization chapter 1 Introduction</title>
    <link href="http://mxxhcm.github.io/2018/12/22/convex-optimization-chapter-1-Introduction/"/>
    <id>http://mxxhcm.github.io/2018/12/22/convex-optimization-chapter-1-Introduction/</id>
    <published>2018-12-22T05:44:11.000Z</published>
    <updated>2019-09-04T12:51:00.437Z</updated>
    
    <content type="html"><![CDATA[<h2 id="数学优化-mathematical-optimization">数学优化(mathematical optimization)</h2><h3 id="定义">定义</h3><p>一个数学优化问题（或者称为优化问题）通常有如下的形式：<br>\begin{align*}<br>&amp;minimize \quad f_0(x)\\<br>&amp;subject \ to \quad f_i(x) \le b_i, i = 1,\cdots,m.<br>\end{align*}<br>其中$x = (x_1, \cdots, x_m)$被称为优化变量(optimization variables), 或者决策变量(decision variables)。 $f_0(x):\mathbb{R}^n \rightarrow \mathbb{R}$是目标函数(object function), $f_i(x):\mathbb{R}^n \rightarrow \mathbb{R},i =1,\cdots,m$是约束函数(constraint functions)。 常量(constraints) $b_1,\cdots,b_m$是约束的限界(limits)或者边界(bounds), $b_i$可以为$0$，这个可以通过移项构造出新的$f_i(x)$实现。如果向量$x$使得目标函数取得最小的值，并且满足所有的约束条件，那么这个向量被称为最优解$x^{*} $。</p><h4 id="线性优化-linear-program">线性优化(linear program)</h4><p>目标函数和约束函数$f_0,\cdots,f_m$是线性的, 它们满足不等式：<br>$$f_i(\alpha x+\beta y) = \alpha f_i(x) + \beta f_i(y)$$<br>对于所有的$x,y \in \mathbb{R}^n $和所有的$\alpha, \beta \in\mathbb{R}$。<br>线性优化是凸优化的一个特殊形式, 它的目标函数和约束函数都是线性的等式。</p><h4 id="非线性问题-non-linear-problem">非线性问题(non-linear problem)</h4><p>如果优化问题不是线性的，就是非线性问题。只要目标函数或者约束函数至少有一个不是线性的，那么这个优化问题就是非线性优化问题。</p><h4 id="凸问题-convex-problem">凸问题(convex problem)</h4><p>凸问题是目标函数和约束函数都是凸的的优化问题，它们满足：<br>$$f_i(\alpha x + \beta y) \le \alpha f_i(x) + \beta f_i(y)$$<br>对于所有的$x,y \in \mathbb{r}^n $和所有的$\alpha, \beta \in \mathbb{r}$且$\alpha + \beta = 1, \alpha \ge 0, \beta \ge 0$。<br>凸性比线性的范围更广，不等式取代了更加严格的等式，不等式只有在$\alpha$和$\beta$取一些特定值时才成立。凸优化和线性问题以及非线性问题都有交集，它是线性问题的超集(superset)，是非线性问题的子集(subset)。技术上来说，nonlinear problem包括convex optimization(除了linear programming), 可以用来描述不确定是非凸的问题。<br>Nonlinear program &gt; convex problem &gt; linear problem</p><h3 id="示例">示例</h3><h4 id="组合优化-portfolio-optimization">组合优化(portfolio optimization)</h4><p>变量：不同资产的投资数量<br>约束：预算，每个资产最大/最小投资数量，至少要得到的回报<br>目标：所有的风险，获利的变化</p><h4 id="电子设备的元件大小-device-sizing-in-electronic-circuits">电子设备的元件大小(device sizing in electronic circuits)</h4><p>变量：元件的宽度和长度<br>约束：生产工艺的炼制，时间要求，面积等<br>目标：节约能耗</p><h4 id="数据拟合-data-fitting">数据拟合(data fitting)</h4><p>变量：模型参数<br>约束：先验知识，参数约束条件<br>目标：错误率</p><h3 id="优化问题求解-solving-optimization-problems">优化问题求解(solving optimization problems)</h3><p>所有的问题都是优化问题。<br>绝大部分优化问题我们解不出来。</p><h4 id="一般的优化问题-general-optimization-problem">一般的优化问题(general optimization problem)</h4><ul><li>很难解出来。</li><li>做一些compromise，比如要很长时间才能解出来，或者并不总能找到解。</li></ul><h4 id="一些例外-some-exceptions">一些例外(some exceptions)</h4><ul><li>最小二乘问题(least-squares problems)</li><li>线性规划问题(linear programming problems)</li><li>凸优化问题(convex optimization problems)</li></ul><h2 id="最小二乘-least-squares-和线性规划-linear-programming">最小二乘(least-squares)和线性规划(linear programming)</h2><p>least-squares和linear programming是凸优化问题中最有名的两个子问题。</p><h3 id="最小二乘问题-least-squares-problems">最小二乘问题(least-squares problems)</h3><p>最小二乘问题是一个无约束的优化问题，它的目标函数是项$a_i^T x-b_i$的平方和。<br>\begin{align*}<br>minimize \quad f_0(x) &amp;= {||Ax-b||}^2_2 \\<br>&amp;=\sum_{i=1}^k (a_i^T x-b_i)^2<br>\end{align*}</p><h4 id="求解-solving-least-squares-problems">求解(solving least-squares problems)</h4><ul><li>最小二乘问题的解可以转换为求线性方程组$(A^T A)x = A^T b$的解。线性代数上我们学过该方程组的解析解为$x=(A^T A)^{-1} A^T b$。</li><li>时间复杂度是$n^2 k = n*k*n+n*k+n*n*n, (k &gt; n)$(转置，求逆，矩阵乘法)。</li><li>该问题具有可靠且高效的求解算法。</li><li>是一个很成熟的算法</li></ul><h4 id="应用-using-least-squares">应用(using least-squares)</h4><p>很容易就可以看出来一个问题是最小二乘问题，我们只需要验证目标函数是不是二次函数，以及对应的二次型是不是正定的即可。</p><h5 id="加权最小二乘-weighted-least-squares">加权最小二乘(weighted least-squares)</h5><p>加权最小二乘形式如下:<br>$$\sum_{i=1}^k \omega_i(a_i^T x-b_i)^2 ,$$<br>其中$\omega_1,\cdots,\omega_k$是正的，被最小化。 这里选出权重$\omega$来体现不同项$a_i^T x-b_i$的比重, 或者仅仅用来影响结果。</p><h5 id="正则化-regularization">正则化(regularization)</h5><p>目标函数中被加入了额外项, 形式如下：<br>$$\sum_{i=1}^k (a_i^T x-b_i)^2 + \rho \sum_{i=1}^n x_i^2 ,$$<br>正则项是用来惩罚大的$x$, 求出一个仅仅最小化第一个求和项的不出来的好结果。合理的选择参数$\rho$在原始的目标函数和正则化项之间做一个trade-off, 使得$\sum_{k=1}^i (a_i^T - b_i)^2 $和$\rho \sum_{k=1}^n  x_i^2 $都很小。<br>正则化项和加权最小二乘会在第六章中讲到，它们的统计解释在第七章给出。</p><h3 id="线性规划-linear-programming">线性规划(linear programming)</h3><p>线性规划问题装目标函数和约束函数都是线性的：<br>\begin{align*}<br>&amp;minimize \quad c^T x\\<br>&amp;subject \ to \quad a_i^T \le b_i, i = 1, \cdots, m.<br>\end{align*}<br>其中向量$c,a_1,\cdots,a_m \in \mathbb{R}^n $, 和标量$b_1,\cdots, b_m \in \mathbb{R}$是指定目标函数和约束函数条件的参数。</p><h4 id="求解线性规划-solving-linear-programs">求解线性规划(solving linear programs)</h4><ul><li>除了一个特例，没有解析解公式(和least-squares不同)；</li><li>有可靠且高效的算法实现；</li><li>时间复杂度是$O(n^2 m)$, m是约束条件的个数, m是维度$；</li><li>是一个成熟的方法。</li></ul><h4 id="应用-using-linear-programs">应用(using linear programs)</h4><p>一些应用直接使用线性规划的标准形式,或者其中一个标准形式。在很多时候，原始的优化问题没有一个标准的线性规划形式，但是可以被转化为等价的线性规划形式。比如切米雪夫近似问题(Chebyshev approximation problem)。它的形式如下：<br>$$minimize \quad max_{i=1,\cdots,k}|a_i^T x-b_i|$$<br>其中$x\in \mathbb{R}^n $是变量，$a_1,\cdots,a_k \in \mathbb{R}^n , b_1,\cdots,b_k \in \mathbb{R}$是实例化的问题参数,和least-squares相似的是，它们的目标函数都是项$a^T_i x-b_i$。不同之处在于，least-squares用的是该项的平方和作为目标函数，而Chebyshev approximation中用的是绝对值的最大值。Chebyshev approximation problem的目标函数是不可导的(max operation), least-squares problem的目标函数是二次的(quadratic), 因此可导的(differentiable)。</p><h2 id="凸优化-convex-optimization">凸优化(Convex optimization)</h2><p>凸优化问题是优化问题的一种,它的目标函数和优化函数都是凸的。<br>具有以下形式的问题是一种凸优化问题：<br>\begin{align*}<br>&amp;minimize \quad f_0(x)\\<br>&amp;subject \ to \quad f_i(x) \le b_i, i = 1,\cdots,m.<br>\end{align*}<br>其中函数$f_0,\cdots,f_m:\mathbb{R}^n \rightarrow \mathbb{R}$是凸的(convex), 如满足<br>$$f_i(\alpha x+ \beta y) \le \alpha f_i(x) + \beta f_i(y)$$<br>对于所有的$x,y \in \mathbb{R}^n $和所有的$\alpha, \beta \in \mathbb{R}$且$\alpha + \beta = 1, \alpha \ge 0, \beta \ge 0$。<br>或者：<br>$$f_i(\theta x+ (1-\theta) y) \le \theta f_i(x) + (1 - \theta) f_i(y)$$<br>其中$\theta \in [0,1]$。<br>课上有人问这里为$\theta$是0和1, 有没有什么物理意义，Stephen Boyd回答说这是定义，就是这么定义的。<br>The least-squares和linear programming problem都是convex optimization problem的特殊形式。线性函数(linear functions)也是convex，它们正处在边界上，它们的曲率(curvature)为0。一种方式是用正曲率去描述凸性。</p><h3 id="凸优化求解-solving-convex-optimization-problems">凸优化求解(solving convex optimization problems)</h3><ul><li>没有解析解；</li><li>有可靠且有效的算法；</li><li>时间复杂度正比于$max{n^3 , n^2 m,F},$F$是评估$f$和计算一阶导数和二阶导数的时间；</li><li>有成熟的方法，如interior-point methods。</li></ul><h3 id="凸优化的应用-using-convex-optimization">凸优化的应用(using convex optimization)</h3><p>将实际问题形式化称凸优化问题。</p><h2 id="非线性优化-nonlinear-optimization">非线性优化(Nonlinear optimization)</h2><h3 id="非线性优化">非线性优化</h3><p>非线性优化用来描述目标函数和约束函数都是非线性函数(但不是凸的)优化问题。因为凸优化问题包括least-squares和linear programming, 它们是线性的。刚开始给出的优化问题就是非线性优化问题，目前没有有效的方法解该问题。目前有一些方法来解决一般的非线性问题，但是都做了一些compromise。</p><h4 id="局部优化-local-optimization">局部优化(local optimization)</h4><p>局部优化是非线性优化的一种解法，compromise是寻找局部最优点，而不是全局最优点，在可行解附近最小化目标函数，不保证能得到一个最小的目标值。<br>局部优化需要随机初始化一个初值，这个初值很关键，很大程度的影响了局部解得到的目标值, 也就是说是一个初值敏感的算法。关于初始值和全局最优值距离有多远并没有很多有用的信息。局部优化对于算法的参数值很敏感，需要根据具体问题去具体调整。<br>使用局部优化的方法比解least-squares problems, linear program, convex optimization problem更有技巧性，因为它牵扯到算法的选择，算法参数的选择，以及初值的选取。</p><h4 id="全局优化-global-optimization">全局优化(global optimization)</h4><p>全局优化也是非线性优化的一种解法, 在全局优化中，优化目标的全局最优解被找到， compromise是效率。</p><h4 id="凸优化问题在非凸优化问题中的应用-role-of-convex-optimization-in-nonconvex-problems">凸优化问题在非凸优化问题中的应用(role of convex optimization in nonconvex problems)</h4><h5 id="初始化局部优化-initialization-for-local-optimization">初始化局部优化(initialization for local optimization)</h5><h5 id="用于非凸优化的凸的启发式搜索-convex-heuristics-for-nonconvex-optimization">用于非凸优化的凸的启发式搜索(convex heuristics for nonconvex optimization)</h5><h5 id="全局最优的边界-bounds-for-global-optimization">全局最优的边界(bounds for global optimization)</h5><h2 id="大纲-outline">大纲(outline)</h2><h3 id="理论-part-one-theory">理论(part one: Theory)</h3><p>第一部分是理论，给出一些概念和定义，第一章是Introduction, 第二章和第三章分别介绍凸集(convex set)和凸函数(convex function), 第四章介绍凸优化问题， 第五章引入拉格朗日对偶性。</p><h3 id="应用-part-two-applications">应用(part two: Applications)</h3><p>第二部分主要给出凸优化在一些领域的应用，如概率论与数理统计，经济学，计算几何以及数据拟合等领域。<br>凸优化如何应用在实践中。</p><h3 id="算法-part-three-algorithms">算法(part three: Algorithms)</h3><p>第三部分给出了凸优化的数值解法，如牛顿法(Newton’s algorithm)和内点法(interior-point)。<br>第三部分有三章，分别包含了无约束优化，等式约束优化和不等式约束优化。章节之间是递进的，解一个问题被分解为解一系列简单问题。二次优化问题(包括，如least-squares)是最底层的基石，它可以通过线性方程组精确求解。牛顿法，在第十章和第十一章介绍到，是下个层次，无约束问题或者等式约束问题被转化成一系列二次优化问题的求解。第十一章介绍了内点法，是最顶层, 这些方法将不等式约束问题转化为一系列无约束或者等式约束的问题。</p><h2 id="参考文献">参考文献</h2><p>1.stephen boyd. Convex optimization</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;数学优化-mathematical-optimization&quot;&gt;数学优化(mathematical optimization)&lt;/h2&gt;
&lt;h3 id=&quot;定义&quot;&gt;定义&lt;/h3&gt;
&lt;p&gt;一个数学优化问题（或者称为优化问题）通常有如下的形式：&lt;br&gt;
\begin{a
      
    
    </summary>
    
      <category term="凸优化" scheme="http://mxxhcm.github.io/categories/%E5%87%B8%E4%BC%98%E5%8C%96/"/>
    
    
      <category term="凸优化" scheme="http://mxxhcm.github.io/tags/%E5%87%B8%E4%BC%98%E5%8C%96/"/>
    
      <category term="convex optimization" scheme="http://mxxhcm.github.io/tags/convex-optimization/"/>
    
  </entry>
  
  <entry>
    <title>reinforcement learning an introduction 第3章笔记</title>
    <link href="http://mxxhcm.github.io/2018/12/21/reinforcement-learning-an-introduction-%E7%AC%AC3%E7%AB%A0%E7%AC%94%E8%AE%B0/"/>
    <id>http://mxxhcm.github.io/2018/12/21/reinforcement-learning-an-introduction-第3章笔记/</id>
    <published>2018-12-21T07:13:38.000Z</published>
    <updated>2019-10-04T04:03:17.473Z</updated>
    
    <content type="html"><![CDATA[<h2 id="马尔科夫过程-markov-process-马尔科夫链-markov-chain">马尔科夫过程(markov process)、马尔科夫链(markov chain)</h2><p>马尔科夫过程或者马尔科夫链(markov chain)是一个tuple $\lt S,P\gt$,其中S是一个有限(或者无限)的状态集合,P是状态转移矩阵(transition probability matrix)或马尔科夫矩阵(markov matrix),$P_{ss’}= P[S_{t+1} = s’|S_t = s]$.</p><h2 id="马尔科夫奖励过程-markov-reward-process">马尔科夫奖励过程(markov reward process)</h2><p>马尔科夫奖励过程是一个tuple $\lt S,P,R,\gamma\gt$,和马尔科夫过程相比，它多了一个奖励R，R和某个具体的状态相关，MRP中的reward只和state有关,和action无关。<br>S是一个(有限)状态的集合。<br>P是一个状态转移概率矩阵。<br>R是一个奖励函数$R = \mathbb{E}[R_{t+1}|S_t = s]$, <strong>这里为什么是t+1时刻的reward?这仅仅是一个约定，为了描述RL问题中涉及到的observation，action，reward比较方便。这里可以理解为离开这个状态才能获得奖励而不是进入这个状态即获得奖励。如果改成$R_t$也是可以的，这时可以理解为进入这个状态获得的奖励。</strong><br>$\gamma$称为折扣因子(discount factor), $\gamma \epsilon [0,1]$.<strong>为什么引入$\gamma$，David Silver的公开课中提到了四个原因:(1)数学上便于计算回报(return)；(2)避免陷入无限循环；(3)长远利益具有一定的不确定性；(4)符合人类对眼前利益的追求。</strong></p><h3 id="奖励-reward">奖励(reward)</h3><p>每个状态s在一个时刻t立即可得到一个reward,reward的值需要由环境给出,这个值可正可负。目前的强化学习算法中reward都是人为设置的。</p><h3 id="回报-return">回报(return)</h3><p>回报是累积的未来的reward,其计算公式如下:<br>$$G_t = R_{t+1} + R_{t+2} + … = \sum_{k=0}^{\infty} {\gamma^k R_{t+k+1}} \tag{1}$$<br>它是一个马尔科夫链上从t时刻开始往后所有奖励的有衰减(带折扣因子)的总和。</p><h3 id="值函数-value-function">值函数(value function)</h3><p>值函数是回报(return)的期望(expected return), 一个MRP过程中某一状态的value function为从该状态开始的markov charin return的期望，即$v(s) = \mathbb{E}[G_t|S_t=s]$.<br>MRP的value function和MDP的value function是不同的, MRP的value function是对于state而言的，而MDP的value function是针对tuple $\lt$state, action$\gt$的。<br>这里为什么要取期望,因为policy是stotastic的情况时，在每个state时，采取每个action都是可能的，都有一定的概率，next state也是不确定的了，所以value funciton是一个随机变量，因此就引入期望来刻画随机变量的性质。<br>为什么在当前state就知道下一时刻的state了?对于有界的RL问题来说，return是在一个回合结束时候计算的；对于无界的RL问题来说，由于有衰减系数，只要reward有界，return就可以计算出来。</p><h3 id="马尔科夫奖励过程的贝尔曼方程-bellman-equation-for-mrp">马尔科夫奖励过程的贝尔曼方程(bellman equation for MRP)</h3><p>\begin{align*}<br>v(s) &amp;= \mathbb{E}[G_t|S_t = s]\\<br>&amp;= \mathbb{E}[R_{t+1} + \gamma R_{t+2} + … | S_t = s]\\<br>&amp;= \mathbb{E}[R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3} + …|S_t = s]\\<br>&amp;= \mathbb{E}[R_{t+1} + \gamma G_{t+1} |S_t = s]\\<br>&amp;= \mathbb{E}[R_{t+1} + \gamma v(S_{t+1})|S_t = s]\\<br>v(s) &amp;= \mathbb{E}[R_{t+1} + \gamma v(S_{t+1})|S_t = s]<br>\end{align*}<br>v(s)由两部分组成，一部分是immediate reward的期望(expectation)，$\mathbb{E}[R_{t+1}]$, 只与当前时刻state有关；另一部分是下一时刻state的value function的expectation。如果用s’表示s状态下一时刻的state，那么bellman equation可以写成：<br>$$v(s) = R_s + \gamma \sum_{s’ \epsilon S} P_{ss’}v(s’)$$<br>我们最终的目的是通过迭代使得t轮迭代时的v(s)和第t+1轮迭代时的v(s)相等。将其写成矩阵形式为：<br>$$v_t = R + \gamma P v_{t+1}$$<br>$$(v_1,v_2,…,v_n)^T = (R_1,R_2,…,R_n)^T + \gamma \begin{bmatrix}P_{11}&amp;P_{12}&amp;…&amp;P_{1n}\\P_{21}&amp;P_{22}&amp;…&amp;P_{2n}\\&amp;&amp;…&amp;\\P_{n1}&amp;P_{n2}&amp;…&amp;P_{nn}\end{bmatrix} (v_1,v_2,…,v_n)^T $$<br>MRP的Bellman方程组是线性的，可以直接求解:<br>\begin{align*}<br>v &amp;= R + \gamma Pv\\<br>(1-\gamma P) &amp;= R\\<br>v &amp;= (1 - \gamma P)^{-1} R<br>\end{align*}<br>可以直接解方程，但是复杂度为$O(n^3)$，对于大的MRP方程组不适用，可以通过迭代法求解，常用的迭代法有动态规划,蒙特卡洛算法和时序差分算法等求解(动态规划是迭代法吗？）</p><h2 id="马尔科夫决策过程-markov-decision-process">马尔科夫决策过程(markov decision process)</h2><p>马尔科夫决策过程，比markov reward process多了一个A,它也是一个tuple $\lt S,A,P,R,\gamma\gt$, 在MRP中奖励R仅仅和状态S相关，在MDP中奖励R和概率P对应的是某个状态S和某个动作A的组合。<br>\begin{align*}<br>P_{ss’}^a &amp;= P[S_{t+1} = s’ | S_t = s, A_t = a]\\<br>R_s^a &amp;= \mathbb{E}[R_{t+1} | S_t = s, A_t = a]<br>\end{align*}<br>这里的reward不仅仅与state相关，而是与tuple $\lt state，action\gt$相关。</p><h3 id="回报">回报</h3><p>MDP中的$G_t$和式子$(1)$的$G_t$是一样的，将$G_t$写成和后继时刻相关的形式如下：<br>\begin{align*}<br>G_t &amp;= R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \gamma^3 R_{t+4} + …\\<br>&amp;= R_{t+1} + \gamma (R_{t+2} + \gamma^1 R_{t+3} + \gamma^2 R_{t+4} + …)\\<br>&amp;= R_{t+1} + \gamma G_{t+1} \tag{2}<br>\end{align*}<br>这里引入$\gamma$之后，即使是在continuing情况下，只要$G_t$是非零常数，$G_t$也可以通过等比数列求和公式进行计算，即:<br>$$G_t = \sum_{k=1}^{\infty} \gamma^k = \frac{1}{1-\gamma} \tag{3}$$</p><h3 id="策略-policy">策略(policy)</h3><p>策略$\pi$的定义:给定状态时采取各个动作的概率分布。<br>$$\pi(a|s) = P[A_t = a | S_t = a] \tag{4}$$</p><h3 id="值函数-value-function-v2">值函数(value function)</h3><p>这里给出的是值函数的定义，就是这么定义的。<br>MDP的值函数有两种，状态值函数(state value function)和动作值函数(action value function), 这两种值函数的含义其实是一样的，也可以相互转换。具体来说, 值函数定义为给定一个policy $\pi$，得到的回报的期望(expected return)。<br>一个MDP的状态s对应的值函数(state value function) $v_{\pi}(s)$是从状态s开始采取策略$\pi$得到的回报的期望。<br>\begin{align*}<br>v_{\pi}(s) &amp;= \mathbb{E}_{\pi}[G_t|S_t = s]\\<br>&amp;=\mathbb{E}_{\pi}[\sum_{k=0}^{\infty} \gamma^{k} R_{t+k+1}|S_t=s] \tag{5}<br>\end{align*}<br>这里的$G_t$是式子(2)中的回报。<br>一个MDP过程中动作值函数(action value function) $q_{\pi}(s,a)$是从状态s开始,采取action a，采取策略$\pi$得到的回报的期望。<br>&lt;action value function $q_{\pi}(s,a)$ is the expected return starting from states, taking action a, and then following policy \pi.&gt;<br>\begin{align*}<br>q_{\pi}(s,a) &amp;= \mathbb{E}_{\pi}\left[G_t | S_t = s, A_t = a\right]\\<br>&amp;= \mathbb{E}_{\pi}\left[\sum_{k=0}^{\infty} \gamma^{k} R_{t+k+1}|S_t=s, A_t=a\right] \tag{6}<br>\end{align*}</p><h4 id="状态值函数-state-value-function">状态值函数(state value function)</h4><p>\begin{align*}<br>v_{\pi}(s) &amp;= \sum_{a \epsilon A} \pi(a|s) q_{\pi} (s,a) \tag{7}\\<br>v_{\pi}(s) &amp;= \sum_a \pi(a|s)\sum_{s’,r}p(s’,r|s,a) \left[r + \gamma v_{\pi}(s’) \right] \tag{8}\\<br>\end{align*}<br>式子$(7)$是$v(s)$和$q(s,a)$的关系，式子$(8)$是$v(s)$和它的后继状态$v(s’)$的关系。<br>式子$(8)$的推导如下：<br>\begin{align*}<br>v_{\pi}(s) &amp;= \mathbb{E}_{\pi}[G_t|S_t = s]\\<br>&amp;= \mathbb{E}_{\pi}\left[R_{t+1}+\gamma G_{t+1}|S_t = s\right]\\<br>&amp;= \sum_a \pi(a|s)\sum_{s’}\sum_rp(s’,r|s,a) \left[r + \gamma \mathbb{E}_{\pi}\left[G_{t+1}|S_{t+1}=s’\right]\right]\\<br>&amp;= \sum_a \pi(a|s)\sum_{s’,r}p(s’,r|s,a) \left[r + \gamma v_{\pi}(s’) \right]\\<br>\end{align*}</p><h4 id="动作值函数-action-value-function">动作值函数(action value function)</h4><p>\begin{align*}<br>q_{\pi}(s,a) &amp;= \sum_{s’}\sum_r p(s’,r|s,a)(r + \gamma  v_{\pi}(s’)) \tag{9}\\<br>q_{\pi}(s,a) &amp;= \sum_{s’}\sum_r p(s’,r|s,a)(r + \gamma  \sum_{a’}\pi(a’|s’)q(s’,a’)) \tag{10}\\<br>\end{align*}<br>式子$(9)$是$q(s,a)$和$v(s)$的关系，式子$(10)$是$q(s,a)$和它的后继状态$q(s’,a’)$的关系。<br>以上都是针对MDP来说的，在MDP中，给定policy $\pi$下，状态s下选择a的action value function，$q_{\pi}(s,a)$类似MRP里面的v(s)，而MDP中的v(s)是要考虑在state s下采率各个action后的情况。</p><h3 id="贝尔曼期望方程-bellmam-expectation-equation">贝尔曼期望方程(Bellmam expectation equation)</h3><p>\begin{align*}<br>v_{\pi}(s) &amp;= \mathbb{E}_{\pi}[R_{t+1} + \gamma v_{\pi}(S_{t+1})|S_t = s] \tag{11}\\<br>v_{\pi}(s) &amp;= \mathbb{E}_{\pi}\left[q_{\pi}(S_t,A_t)|S_t=s,A_t=a\right]\tag{12}\\<br>q_{\pi}(s,a)&amp;= \mathbb{E}_{\pi}\left[R_{t+1} + \gamma v_{\pi}(S_{t+1}) |S_t=s,A_t=a\right]\tag{13}\\<br>q_{\pi}(s,a) &amp;= \mathbb{E}_{\pi}[R_{t+1} + \gamma q_{\pi}(S_{t+1},A_{t+1}) | S_t = s, A_t = a] \tag{14}<br>\end{align*}</p><h4 id="矩阵形式">矩阵形式</h4><p>\begin{align*}<br>v_{\pi} &amp;= R^{\pi} + \gamma P^{\pi} v_{\pi}\\<br>v_{\pi} &amp;= (I-\gamma P^{\pi} )^{-1} R^{\pi}<br>\end{align*}</p><h2 id="最优策程的求解-how-to-find-optimal-policy">最优策程的求解(how to find optimal policy)</h2><h3 id="最优价值函数-optimal-value-function">最优价值函数(optimal value function)</h3><p>$v_{*} = \max_{\pi}v_{\pi}(s)$,从所有策略产生的state value function中，选取使得state s的价值最大的函数<br>$q_{*}(s,a) = \max_{\pi} q_{\pi}(s,a)$,从所有策略产生的action value function中，选取使$\lt s,a\gt$价值最大的函数<br>当我们得到了optimal value function，也就知道了每个state的最优价值，便认为这个MDP被解决了</p><h3 id="最优策略-optimal-policy">最优策略(optimal policy)</h3><p>对于每一个state s，在policy $\pi$下的value 大于在policy $\pi’$的value， 就称策略$\pi$优于策略$\pi’$， $\pi \ge \pi’$ if $v_{\pi}(s) \ge v_{\pi’}(s)$, 对于任意s都成立<br>对于任何MDP，都满足以下条件：</p><ol><li>都存在一个optimal policy，它比其他策略好或者至少相等；</li><li>所有的optimal policy的optimal value function是相同的；</li><li>所有的optimal policy 都有相同的 action value function.</li></ol><h3 id="寻找最优策略">寻找最优策略</h3><p>寻找optimal policy可以通过寻找optimal action value function来实现：<br>$${\pi}_{*}(a|s) =<br>\begin{cases}1, &amp;if\quad a = \arg\max\ q_{*}(s,a)\\0, &amp;otherwise\end{cases}$$</p><h3 id="贝尔曼最优方程-bellman-optimal-equation">贝尔曼最优方程(bellman optimal equation)</h3><p>*号表示最优的策略。</p><h4 id="最优状态值函数-state-value-function">最优状态值函数(state value function)</h4><p>\begin{align*}<br>v_{*}(s) &amp;= \max_a q_{*}(s,a)\\<br>&amp;= \max_a\mathbb{E}_{\pi_{*}}\left[G_t|S_t=s,A_t=a\right]\\<br>&amp;= \max_a\mathbb{E}_{\pi_{*}}\left[R_{t+1}+\gamma G_t|S_t=s,A_t=a\right]\\<br>&amp;= \max_a\mathbb{E}\left[R_{t+1} +\gamma v_{*}(S_{t+1})|S_t=s,A_t=a\right]\\<br>&amp;= \max_a \left[\sum_{s’,r} p(s’,r|s,a)(r+\gamma v_{*}(s’) )\right] \tag{15}\\<br>\end{align*}</p><h4 id="最优动作值函数-action-value-function">最优动作值函数(action value function)</h4><p>\begin{align*}<br>q_{*}(s,a) &amp;= \sum_{s’,r} p(s’,r|s,a) (r + \gamma v_{*}(s’))\\<br>&amp;= \sum_{s’,r} p(s’,r|s,a) (r + \gamma \max_{a’} q_{*}(s’,a’))\\<br>&amp;=\mathbb{E}\left[R_{t+1}+\gamma \max_{a’}q_{*}(S_{t+1},a’)|S_t=s,A_t=a \right]\tag{16}\\<br>\end{align*}</p><h3 id="贝尔曼最优方程的求解-solution-to-bellman-optimal-equation">贝尔曼最优方程的求解(solution to Bellman optimal equation)</h3><p>Bellman equation和Bellman optimal equation相比，一个是对于给定的策略，求其对应的value function,是对一个策略的估计，而bellman optimal equation是要寻找最优策略，通过对action value function进行贪心。<br>Bellman最优方程是非线性的，没有固定的解决方案，只能通过迭代法来解决，如Policy iteration，value iteration，Q-learning，Sarsa等。</p><h2 id="参考文献">参考文献</h2><p>1.<a href="http://incompleteideas.net/book/the-book-2nd.html" target="_blank" rel="noopener">http://incompleteideas.net/book/the-book-2nd.html</a><br>2.<a href="https://www.bilibili.com/video/av32149008/?p=2" target="_blank" rel="noopener">https://www.bilibili.com/video/av32149008/?p=2</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;马尔科夫过程-markov-process-马尔科夫链-markov-chain&quot;&gt;马尔科夫过程(markov process)、马尔科夫链(markov chain)&lt;/h2&gt;
&lt;p&gt;马尔科夫过程或者马尔科夫链(markov chain)是一个tuple $\l
      
    
    </summary>
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="强化学习" scheme="http://mxxhcm.github.io/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="MDP" scheme="http://mxxhcm.github.io/tags/MDP/"/>
    
      <category term="MRP" scheme="http://mxxhcm.github.io/tags/MRP/"/>
    
      <category term="Bellman Equation" scheme="http://mxxhcm.github.io/tags/Bellman-Equation/"/>
    
  </entry>
  
  <entry>
    <title>linux 常见问题（不定期更新）</title>
    <link href="http://mxxhcm.github.io/2018/12/20/linux-%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/"/>
    <id>http://mxxhcm.github.io/2018/12/20/linux-常见问题/</id>
    <published>2018-12-20T12:30:34.000Z</published>
    <updated>2019-06-04T02:37:22.309Z</updated>
    
    <content type="html"><![CDATA[<h2 id="问题1-undefined-reference-to-pthread-create-in-linux">问题1 Undefined reference to pthread_create in Linux</h2><p>在阅读自然语言处理的一篇论文时，读到了bype pair encoding(bpe)算法。在github找到了一个实现<a href="https://github.com/glample/fastBPE" target="_blank" rel="noopener">fastBPE</a>, 算法是用C++写的，在编译的过程中遇到了问题&quot;Undefined reference to pthread_create in Linux&quot;,</p><h3 id="terminal下解决方案">terminal下解决方案</h3><p>查阅资料了解到pthread不是Linux操作系统默认的库函数，所以需要在编译的时候将pthread链接该库函数，后来在看fastBPE的文档时发现文档中已经有说明:<br>Compile with:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">g++ -std=c++11 -pthread -O3 fast.cc -o fast</span><br></pre></td></tr></table></figure><h3 id="codeblocks下解决方案">codeblocks下解决方案</h3><p>上面给出的方案是使用gcc在terminal进行编译时加入静态库，但是对于不习惯在命令行使用gdb进行调试的人来说没有用。<br>在codeblocks中，如果要链接静态库,找到Settings --&gt; Compiler… --&gt; Linker settings，点击add，添加相应的库函数即可。</p><h2 id="参考文献">参考文献</h2><p>1:<a href="https://stackoverflow.com/questions/1662909/undefined-reference-to-pthread-create-in-linux" target="_blank" rel="noopener">https://stackoverflow.com/questions/1662909/undefined-reference-to-pthread-create-in-linux</a><br>2:<a href="https://blog.csdn.net/zhaoyue007101/article/details/7705753" target="_blank" rel="noopener">https://blog.csdn.net/zhaoyue007101/article/details/7705753</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;问题1-undefined-reference-to-pthread-create-in-linux&quot;&gt;问题1 Undefined reference to pthread_create in Linux&lt;/h2&gt;
&lt;p&gt;在阅读自然语言处理的一篇论文时，读到了by
      
    
    </summary>
    
      <category term="linux" scheme="http://mxxhcm.github.io/categories/linux/"/>
    
    
      <category term="gcc" scheme="http://mxxhcm.github.io/tags/gcc/"/>
    
      <category term="linux" scheme="http://mxxhcm.github.io/tags/linux/"/>
    
      <category term="常见问题" scheme="http://mxxhcm.github.io/tags/%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/"/>
    
      <category term="codeblocks" scheme="http://mxxhcm.github.io/tags/codeblocks/"/>
    
  </entry>
  
  <entry>
    <title>classfication</title>
    <link href="http://mxxhcm.github.io/2018/10/21/classfication/"/>
    <id>http://mxxhcm.github.io/2018/10/21/classfication/</id>
    <published>2018-10-21T10:47:44.000Z</published>
    <updated>2019-10-21T14:53:18.789Z</updated>
    
    <content type="html"><![CDATA[<h2 id="classfication">Classfication</h2><h2 id="lda">LDA</h2><h2 id="logistic-regression">Logistic Regression</h2><h3 id="logistic-function">Logistic function</h3><p>$$ S(x) = \frac{1}{1+e^{x} }$$<br>如下图所示：<br><img src="/2018/10/21/classfication/logistic_function.png" alt="logistic_func"><br>它的取值在$[0,1]$之间。<br>logistic regression的目标函数是：<br>$$h(x) = \frac{1}{1+e<sup>{-\theta</sup>T x} 3}$$<br>其中$x$是输入，$\theta$是要求的参数。</p><h3 id="思路">思路</h3><p>Logistic regression利用logistic function进行分类，给出一个输入，经过参数$\theta$的变换，输出一个$[0,1]$之间的值，如果大于$0.5$，把它分为一类，小于$0.5$，分为另一类。这个$0.5$只是一个例子，可以根据不同的需求选择不同的值。<br>$\theta^T x$相当于给出了一个非线性的决策边界。</p><h3 id="cost-function">Cost function</h3><p>$$J(\theta) = -\log L(\theta) = -\sum_{i=1}^m (y(i)\log h(x^{(i)}) + (1-y<sup>{(i)})\log(1-h(x</sup>{(i)} )) )$$<br>给出两种方式推导logistic regression的cost function</p><h4 id="maximum-likelyhood-estimation">Maximum likelyhood estimation</h4><p>通过极大似然估计推导得到的，当是两个类别的分类时，即$0$或者$1$，有：<br>$$P(y=1|x,\theta) = h(x)$$<br>$$P(y=0|x,\theta) = 1- h(x)$$<br>服从二项分布，写成一个式子是：<br>$$P(y|x,\theta) = h(x)^y (1-h(x))^{1-y}$$<br>其中$y$取值只有$0$和$1$。<br>有了$y$的表达式，我们就可以使用最大似然估计进行求解了：<br>$$L(\theta) = \prod_{i=1}^m (h(x<sup>{(i)})</sup>{y(i)}(1-h(x^{(i)} ))<sup>{(1-y</sup>{(i)})}$$<br>似然函数要求最大化，即求使得$m$个observation出现概率最大的$\theta$，<br>损失函数是用来衡量损失的，令损失函数取负的对数似然，然后最小化loss也就是最大化似然函数了：<br>$$J(\theta) = -\log L(\theta) = -\sum_{i=1}^m (y(i)\log h(x^{(i)}) + (1-y<sup>{(i)})\log(1-h(x</sup>{(i)} )) )$$</p><h4 id="cross-entropy">Cross-entropy</h4><p>对于$k$类问题，写出交叉熵公式如下所示：<br>$$J(\theta) = -\frac{1}{n}\left[\sum_{i=1}^m \sum_k y_k^{(i)} \log h(x_k^{(i)} ) \right]$$<br>当$k=2$时：<br>$$J(\theta) = -\frac{1}{n}\left[\sum_{i=1}^m  y^{(i)} \log h(x^{(i)} ) + (1-y^{(i)}) \log (1-h(x^{(i)} ))\right]$$</p><h3 id="梯度下降">梯度下降</h3><p>$$J(\theta) = -\log L(\theta) = -\sum_{i=1}^m \left[y(i)\log h(x^{(i)}) + (1-y<sup>{(i)})\log(1-h(x</sup>{(i)} )) \right]$$</p><p>\begin{align*}<br>\nabla J &amp; =  -\sum_{i=1}^m \left[ y(i)\frac{1}{h(x^{(i)})}\nabla h(x^{(i)}) - (1-y<sup>{(i)})\frac{1}{\log(1-h(x</sup>{(i)} ))}\nabla\log(1-h(x^{(i)} ))\right]<br>&amp;=-\sum_{i=1}^m  (h(x^{(i)}) - y^{(i)}) x^{(i)}<br>\end{align*}</p><h2 id="参考文献">参考文献</h2><p>1.<a href="https://blog.csdn.net/jk123vip/article/details/80591619" target="_blank" rel="noopener">https://blog.csdn.net/jk123vip/article/details/80591619</a><br>2.<a href="https://zhuanlan.zhihu.com/p/28408516" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/28408516</a><br>3.<a href="https://www.cnblogs.com/pinard/p/6029432.html" target="_blank" rel="noopener">https://www.cnblogs.com/pinard/p/6029432.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;classfication&quot;&gt;Classfication&lt;/h2&gt;
&lt;h2 id=&quot;lda&quot;&gt;LDA&lt;/h2&gt;
&lt;h2 id=&quot;logistic-regression&quot;&gt;Logistic Regression&lt;/h2&gt;
&lt;h3 id=&quot;logistic-funct
      
    
    </summary>
    
      <category term="机器学习" scheme="http://mxxhcm.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://mxxhcm.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="classfication" scheme="http://mxxhcm.github.io/tags/classfication/"/>
    
      <category term="分类" scheme="http://mxxhcm.github.io/tags/%E5%88%86%E7%B1%BB/"/>
    
  </entry>
  
</feed>

<!DOCTYPE html>












  


<html class="theme-next pisces use-motion" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
































<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=6.6.0" rel="stylesheet" type="text/css">



  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.jpg?v=6.6.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.jpg?v=6.6.0">










<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '6.6.0',
    sidebar: {"position":"left","display":"always","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="常见问题RuntimeError: CUDNN_STATUS_ARCH_MISMATCHCUDNN doesn’t support CUDA arch 2.1 cards.CUDNN requires Compute Capability 3.0, at least.意思是GPU的加速能力不够，CUDNN只支持CUDA Capability 3.0以上的GPU加速，实验室主机是GT620的显卡，2">
<meta name="keywords" content="机器学习,python,深度学习,pytorch">
<meta property="og:type" content="article">
<meta property="og:title" content="pytorch踩坑（不定期更新）">
<meta property="og:url" content="http://mxxhcm.github.io/2019/03/13/pytorch踩坑（不定期更新）/index.html">
<meta property="og:site_name" content="mxxhcm&#39;s blog">
<meta property="og:description" content="常见问题RuntimeError: CUDNN_STATUS_ARCH_MISMATCHCUDNN doesn’t support CUDA arch 2.1 cards.CUDNN requires Compute Capability 3.0, at least.意思是GPU的加速能力不够，CUDNN只支持CUDA Capability 3.0以上的GPU加速，实验室主机是GT620的显卡，2">
<meta property="og:locale" content="zh-CN">
<meta property="og:updated_time" content="2019-04-23T11:23:32.346Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="pytorch踩坑（不定期更新）">
<meta name="twitter:description" content="常见问题RuntimeError: CUDNN_STATUS_ARCH_MISMATCHCUDNN doesn’t support CUDA arch 2.1 cards.CUDNN requires Compute Capability 3.0, at least.意思是GPU的加速能力不够，CUDNN只支持CUDA Capability 3.0以上的GPU加速，实验室主机是GT620的显卡，2">



  <link rel="alternate" href="/atom.xml" title="mxxhcm's blog" type="application/atom+xml">




  <link rel="canonical" href="http://mxxhcm.github.io/2019/03/13/pytorch踩坑（不定期更新）/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>pytorch踩坑（不定期更新） | mxxhcm's blog</title>
  












  <noscript>
  <style>
    .use-motion .motion-element,
    .use-motion .brand,
    .use-motion .menu-item,
    .sidebar-inner,
    .use-motion .post-block,
    .use-motion .pagination,
    .use-motion .comments,
    .use-motion .post-header,
    .use-motion .post-body,
    .use-motion .collection-title { opacity: initial; }

    .use-motion .logo,
    .use-motion .site-title,
    .use-motion .site-subtitle {
      opacity: initial;
      top: initial;
    }

    .use-motion .logo-line-before i { left: initial; }
    .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">mxxhcm's blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
    
      
    

    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>标签</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">

    
    
    
      
    

    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>分类</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>归档</a>

  </li>

      
      
    </ul>
  

  

  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
            

          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://mxxhcm.github.io/2019/03/13/pytorch踩坑（不定期更新）/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="马晓鑫爱马荟荟">
      <meta itemprop="description" content="记录硕士三年自己的积累">
      <meta itemprop="image" content="/images/favicon.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="mxxhcm's blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">pytorch踩坑（不定期更新）

              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-03-13 15:10:22" itemprop="dateCreated datePublished" datetime="2019-03-13T15:10:22+08:00">2019-03-13</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-04-23 19:23:32" itemprop="dateModified" datetime="2019-04-23T19:23:32+08:00">2019-04-23</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/python/" itemprop="url" rel="index"><span itemprop="name">python</span></a></span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/03/13/pytorch踩坑（不定期更新）/#comments" itemprop="discussionUrl">
                  <span class="post-meta-item-text">评论数：</span> <span class="post-comments-count gitment-comments-count" data-xid="/2019/03/13/pytorch踩坑（不定期更新）/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h2 id="常见问题"><a href="#常见问题" class="headerlink" title="常见问题"></a>常见问题</h2><h3 id="RuntimeError-CUDNN-STATUS-ARCH-MISMATCH"><a href="#RuntimeError-CUDNN-STATUS-ARCH-MISMATCH" class="headerlink" title="RuntimeError: CUDNN_STATUS_ARCH_MISMATCH"></a>RuntimeError: CUDNN_STATUS_ARCH_MISMATCH</h3><p>CUDNN doesn’t support CUDA arch 2.1 cards.<br>CUDNN requires Compute Capability 3.0, at least.<br>意思是GPU的加速能力不够，CUDNN只支持CUDA Capability 3.0以上的GPU加速，实验室主机是GT620的显卡，2.1的加速能力。<br>GPU对应的capability: <a href="https://developer.nvidia.com/cuda-gpus" target="_blank" rel="noopener">https://developer.nvidia.com/cuda-gpus</a><br>所以，对于不能使用cudnn对cuda加速的显卡，我们可以设置cudnn加速为False，这个默认是为True的<br>torch.backends.cudnn.enabled=False<br>但是，由于显卡版本为2.1，太老了，没有二进制版本。所以，还是会报其他错误，因此，就别使用cpu进行加速啦。</p>
<h3 id="查看cuda版本"><a href="#查看cuda版本" class="headerlink" title="查看cuda版本"></a>查看cuda版本</h3><p>~#:nvcc —version</p>
<h2 id="神经网络参数初始化"><a href="#神经网络参数初始化" class="headerlink" title="神经网络参数初始化"></a>神经网络参数初始化</h2><h3 id="方法-1-Model-apply-fn"><a href="#方法-1-Model-apply-fn" class="headerlink" title="方法$1$.Model.apply(fn)"></a>方法$1$.Model.apply(fn)</h3><p><a href="https://github.com/mxxhcm/myown_code/blob/master/pytorch/tutorials/initialize/apply.py" target="_blank" rel="noopener">示例</a>如下<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_weights</span><span class="params">(m)</span>:</span></span><br><span class="line">  print(m)</span><br><span class="line">  <span class="keyword">if</span> type(m) == nn.Linear:</span><br><span class="line">    m.weight.data.fill_(<span class="number">1.0</span>)</span><br><span class="line">    print(m.weight)</span><br><span class="line"></span><br><span class="line">net = nn.Sequential(nn.Linear(<span class="number">2</span>, <span class="number">2</span>), nn.Linear(<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">net.apply(init_weights)</span><br></pre></td></tr></table></figure></p>
<p>输出结果如下：</p>
<blockquote>
<p>Linear(in_features=2, out_features=2, bias=True)<br>Parameter containing:<br>tensor([[1., 1.],<br>        [1., 1.]], requires_grad=True)<br>Linear(in_features=2, out_features=2, bias=True)<br>Parameter containing:<br>tensor([[1., 1.],<br>        [1., 1.]], requires_grad=True)<br>Sequential(<br>  (0): Linear(in_features=2, out_features=2, bias=True)<br>  (1): Linear(in_features=2, out_features=2, bias=True)<br>)<br>Linear(in_features=2, out_features=2, bias=True)<br>Linear(in_features=2, out_features=2, bias=True)</p>
</blockquote>
<p>其中最后两行为net对象调用self.children()函数返回的模块，就是模型中所有网络的参数。事实上，调用net.apply(fn)函数，会对self.children()中的所有模块应用fn函数，</p>
<h2 id="torch"><a href="#torch" class="headerlink" title="torch"></a>torch</h2><h3 id="torch-cat"><a href="#torch-cat" class="headerlink" title="torch.cat"></a>torch.cat</h3><h4 id="函数原型"><a href="#函数原型" class="headerlink" title="函数原型"></a>函数原型</h4><p>将多个tensor在某一个维度上（默认是第0维）拼接到一起（除了拼接的维度上，其他维度的shape必须一定），最后返回一个tensor。<br>torch.cat(tensors, dim=0, out=None) → Tensor</p>
<blockquote>
<p>Concatenates the given sequence of seq tensors in the given dimension. All tensors must either have the same shape (except in the concatenating dimension) or be empty.</p>
</blockquote>
<h4 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h4><p>tensors (sequence of Tensors) – 任意类型相同python序列或者tensor<br>dim (int, optional) - 在第几个维度上进行拼接(只有在拼接的维度上可以不同，其余维度必须相同。<br>out (Tensor, optional) – 输出的tensor</p>
<h4 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x1 = torch.randn(<span class="number">3</span>, <span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line">x2 = torch.randn(<span class="number">3</span>, <span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line">x = torch.cat([x1, x2], <span class="number">1</span>)</span><br><span class="line">print(x.size())</span><br></pre></td></tr></table></figure>
<p>输出如下：</p>
<blockquote>
<p>torch.Size([3, 5, 4])</p>
</blockquote>
<h3 id="img"><a href="#img" class="headerlink" title="img"></a>img</h3><p>torch中图像的shape是(‘RGB’,width, height)，而numpy和matplotlib中都是(width, height, ‘RGB’)<br>matplotlib.pyplot.imshow()需要的参数是图像矩阵，如果矩阵中是整数，那么它的值需要在区间[0,255]之内，如果是浮点数，需要在[0,1]之间。</p>
<blockquote>
<p>Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).</p>
</blockquote>
<h2 id="torch-autograd-torch-autograd-Variable和torch-autograd-Function"><a href="#torch-autograd-torch-autograd-Variable和torch-autograd-Function" class="headerlink" title="torch.autograd(torch.autograd.Variable和torch.autograd.Function)"></a>torch.autograd(torch.autograd.Variable和torch.autograd.Function)</h2><h3 id="Variable-class-torch-autograd-Variable"><a href="#Variable-class-torch-autograd-Variable" class="headerlink" title="Variable(class torch.autograd.Variable)"></a>Variable(class torch.autograd.Variable)</h3><h4 id="声明一个tensor"><a href="#声明一个tensor" class="headerlink" title="声明一个tensor"></a>声明一个tensor</h4><p>torch.zeros,torch.ones,torch.rand,torch.Tensor<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">torch.zeros(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">torch.ones(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">torch.rand(<span class="number">2</span>,<span class="number">4</span>)</span><br><span class="line">torch.randn(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">torch.randn(<span class="number">2</span>,<span class="number">3</span>).type()</span><br><span class="line">torch.Tensor(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">torch.Tensor(<span class="number">3</span>,<span class="number">4</span>).uniform_(<span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line">torch.Tensor(<span class="number">3</span>,<span class="number">4</span>).normal_(<span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line">torch.Tensor(<span class="number">3</span>,<span class="number">4</span>).fill_(<span class="number">5</span>)</span><br><span class="line">torch.arange(<span class="number">1</span>,<span class="number">3</span>,<span class="number">0.4</span>)</span><br></pre></td></tr></table></figure></p>
<h4 id="tensor的各种操作"><a href="#tensor的各种操作" class="headerlink" title="tensor的各种操作"></a>tensor的各种操作</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">a = torch.ones(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">b = torch.ones(<span class="number">2</span>,<span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<h5 id="加操作"><a href="#加操作" class="headerlink" title="加操作"></a>加操作</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">print(a+b)                <span class="comment">#方法1</span></span><br><span class="line">c = torch.add(a,b)    <span class="comment">#方法2</span></span><br><span class="line">torch.add(a,b,result)    <span class="comment">#方法3</span></span><br><span class="line">a.add(b)                    <span class="comment">#方法4,将a加上b，且a不变</span></span><br><span class="line">a.add_(b)                <span class="comment">#方法5,将a加上b并将其赋值给a</span></span><br></pre></td></tr></table></figure>
<h5 id="转置操作"><a href="#转置操作" class="headerlink" title="转置操作"></a>转置操作</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(a.t())               <span class="comment"># 打印出tensor a的转置</span></span><br><span class="line">print(a.t_())                 <span class="comment">#将tensor a 转置，并将其赋值给a</span></span><br></pre></td></tr></table></figure>
<h5 id="求最大行和列"><a href="#求最大行和列" class="headerlink" title="求最大行和列"></a>求最大行和列</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.max(tensor,dim)</span><br><span class="line">np.max(array,dim)</span><br></pre></td></tr></table></figure>
<h5 id="和relu功能比较类似。"><a href="#和relu功能比较类似。" class="headerlink" title="和relu功能比较类似。"></a>和relu功能比较类似。</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.clamp(tensor, min, max,out=<span class="keyword">None</span>)</span><br><span class="line">np.maximun(x1, x2)  <span class="comment"># x1 and x2 must hava the same shape</span></span><br></pre></td></tr></table></figure>
<h4 id="tensor和numpy转化"><a href="#tensor和numpy转化" class="headerlink" title="tensor和numpy转化"></a>tensor和numpy转化</h4><h5 id="convert-tensor-to-numpy"><a href="#convert-tensor-to-numpy" class="headerlink" title="convert tensor to numpy"></a>convert tensor to numpy</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = torch.ones(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">b = a.numpy()</span><br></pre></td></tr></table></figure>
<h5 id="convert-numpy-to-tensor"><a href="#convert-numpy-to-tensor" class="headerlink" title="convert numpy to tensor"></a>convert numpy to tensor</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a =  numpy.ones(<span class="number">4</span>,<span class="number">3</span>)</span><br><span class="line">b = torch.from_numpy(a)</span><br></pre></td></tr></table></figure>
<h4 id="Variable和Tensor"><a href="#Variable和Tensor" class="headerlink" title="Variable和Tensor"></a>Variable和Tensor</h4><p>Variable<br>图1.Variable</p>
<h5 id="属性"><a href="#属性" class="headerlink" title="属性"></a>属性</h5><p>如图1,Variable wrap a Tensor,and it has six attributes,data,grad,requies_grad,volatile,is_leaf and grad_fn.We can acess the raw tensor through .data operation, we can accumualte gradients w.r.t this Variable into .grad,.Finally , creator attribute will tell us how the Variable were created,we can acess the creator attibute by .grad_fn,if the Variable was created by the user,then the grad_fn is None,else it will show us which Function created the Variable.<br>if the grad_fn is None,we call them graph leaves<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Variable.shape  <span class="comment">#查看Variable的size</span></span><br><span class="line">Variable.size()</span><br></pre></td></tr></table></figure></p>
<h5 id="parameters"><a href="#parameters" class="headerlink" title="parameters"></a>parameters</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.autograd.Variable(data,requires_grad=<span class="keyword">False</span>,volatile=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure>
<p>requires_grad : indicate whether the backward() will ever need to be called</p>
<h5 id="backward"><a href="#backward" class="headerlink" title="backward"></a>backward</h5><p>backward(gradient=None,retain_graph=None,create_graph=None,retain_variables=None)<br>如果Variable是一个scalar output，我们不需要指定gradient，但是如果Variable不是一个scalar，而是有多个element，我们就需要根据output指定一下gradient，gradient的type可以是tensor也可以是Variable，里面的值为梯度的求值比例，例如<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = Variable(torch.Tensor([<span class="number">3</span>,<span class="number">6</span>,<span class="number">4</span>]),requires_grad=<span class="keyword">True</span>)</span><br><span class="line">y = Variable(torch.Tensor([<span class="number">5</span>,<span class="number">3</span>,<span class="number">6</span>]),requires_grad=<span class="keyword">True</span>)</span><br><span class="line">z = x+y</span><br><span class="line">z.backward(gradient=torch.Tensor([<span class="number">0.1</span>,<span class="number">1</span>,<span class="number">10</span>]))</span><br></pre></td></tr></table></figure></p>
<p>这里[0.1,1,10]分别表示的是对正常梯度分别乘上$0.1,1,10$，然后将他们累积在leaves Variable上<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">detach()    <span class="comment">#</span></span><br><span class="line">detach_()</span><br><span class="line">register_hook()</span><br><span class="line">register_grad()</span><br></pre></td></tr></table></figure></p>
<h3 id="Function-class-torch-autograd-Funtion"><a href="#Function-class-torch-autograd-Funtion" class="headerlink" title="Function(class torch.autograd.Funtion)"></a>Function(class torch.autograd.Funtion)</h3><h4 id="用法"><a href="#用法" class="headerlink" title="用法"></a>用法</h4><p>Function一般只定义一个操作，并且它无法保存参数，一般适用于激活函数，pooling等，它需要定义三个方法，<strong>init</strong>(),forward(),backward()（这个需要自己定义怎么求导）<br>Model保存了参数，适合定义一层，如线性层(Linear layer),卷积层(conv layer),也适合定义一个网络。<br>和Model的区别，model只需要定义<strong>init()</strong>,foward()方法，backward()不需要我们定义，它可以由自动求导机制计算。</p>
<p>Function定义只是一个函数，forward和backward都只与这个Function的输入和输出有关</p>
<h4 id="functions"><a href="#functions" class="headerlink" title="functions"></a>functions</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyReLU</span><span class="params">(torch.autograd.Function)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    We can implement our own custom autograd Functions by subclassing</span></span><br><span class="line"><span class="string">    torch.autograd.Function and implementing the forward and backward passes</span></span><br><span class="line"><span class="string">    which operate on Tensors.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        In the forward pass we receive a Tensor containing the input and return a</span></span><br><span class="line"><span class="string">        Tensor containing the output. You can cache arbitrary Tensors for use in the</span></span><br><span class="line"><span class="string">        backward pass using the save_for_backward method.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.save_for_backward(input)</span><br><span class="line">        <span class="keyword">return</span> input.clamp(min=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(self, grad_output)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        In the backward pass we receive a Tensor containing the gradient of the loss</span></span><br><span class="line"><span class="string">        with respect to the output, and we need to compute the gradient of the loss</span></span><br><span class="line"><span class="string">        with respect to the input.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        input, = self.saved_tensors</span><br><span class="line">        grad_input = grad_output.clone()</span><br><span class="line">        grad_input[input &lt; <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">        <span class="keyword">return</span> grad_input</span><br><span class="line"></span><br><span class="line">dtype = torch.FloatTensor</span><br><span class="line"><span class="comment"># dtype = torch.cuda.FloatTensor # Uncomment this to run on GPU</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># N is batch size; D_in is input dimension;</span></span><br><span class="line"><span class="comment"># H is hidden dimension; D_out is output dimension.</span></span><br><span class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create random Tensors to hold input and outputs, and wrap them in Variables.</span></span><br><span class="line">x = Variable(torch.randn(N, D_in).type(dtype), requires_grad=<span class="keyword">False</span>)</span><br><span class="line">y = Variable(torch.randn(N, D_out).type(dtype), requires_grad=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create random Tensors for weights, and wrap them in Variables.</span></span><br><span class="line">w1 = Variable(torch.randn(D_in, H).type(dtype), requires_grad=<span class="keyword">True</span>)</span><br><span class="line">w2 = Variable(torch.randn(H, D_out).type(dtype), requires_grad=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">1e-6</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</span><br><span class="line">    <span class="comment"># Construct an instance of our MyReLU class to use in our network</span></span><br><span class="line">    relu = MyReLU()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Forward pass: compute predicted y using operations on Variables; we compute</span></span><br><span class="line">    <span class="comment"># ReLU using our custom autograd operation.</span></span><br><span class="line">    y_pred = relu(x.mm(w1)).mm(w2)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute and print loss</span></span><br><span class="line">    loss = (y_pred - y).pow(<span class="number">2</span>).sum()</span><br><span class="line">    print(t, loss.data[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Use autograd to compute the backward pass.</span></span><br><span class="line">    loss.backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update weights using gradient descent</span></span><br><span class="line">    w1.data -= learning_rate * w1.grad.data</span><br><span class="line">    w2.data -= learning_rate * w2.grad.data</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Manually zero the gradients after updating weights</span></span><br><span class="line">    w1.grad.data.zero_()</span><br><span class="line">    w2.grad.data.zero_()</span><br></pre></td></tr></table></figure>
<h2 id="torch-nn"><a href="#torch-nn" class="headerlink" title="torch.nn"></a>torch.nn</h2><h3 id="Container"><a href="#Container" class="headerlink" title="Container"></a>Container</h3><h4 id="Module"><a href="#Module" class="headerlink" title="Module"></a>Module</h4><h4 id="Sequential"><a href="#Sequential" class="headerlink" title="Sequential"></a>Sequential</h4><h3 id="Convolution-Layers"><a href="#Convolution-Layers" class="headerlink" title="Convolution Layers"></a>Convolution Layers</h3><h4 id="torch-nn-Conv2d"><a href="#torch-nn-Conv2d" class="headerlink" title="torch.nn.Conv2d"></a>torch.nn.Conv2d</h4><h5 id="类声明"><a href="#类声明" class="headerlink" title="类声明"></a>类声明</h5><p>应用2维卷积到输入信号中。<br>torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)</p>
<blockquote>
<p>Applies a 2D convolution over an input signal composed of several input planes.</p>
</blockquote>
<h5 id="参数声明"><a href="#参数声明" class="headerlink" title="参数声明"></a>参数声明</h5><p>in_channels (int) – 输入图像的通道<br>out_channels (int) – 卷积产生的输出通道数（也就是有几个kernel）<br>kernel_size (int or tuple) – kernel的大小<br>stride (int or tuple, optional) – 卷积的步长，默认为$1$<br>padding (int or tuple, optional) – 向输入数据的各边添加Zero-padding的数量，默认为$0$<br>dilation (int or tuple, optional) – Spacing between kernel elements. Default: 1<br>groups (int, optional) – Number of blocked connections from input channels to output channels. Default: 1<br>bias (bool, optional) – If True, adds a learnable bias to the output</p>
<h5 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h5><h6 id="例子1"><a href="#例子1" class="headerlink" title="例子1"></a>例子1</h6><p>用$6$个$5\times 5$的filter处理维度为$32\times 32\times 1$的图像。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">model = torch.nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">input = torch.randn(<span class="number">16</span>, <span class="number">1</span>, <span class="number">32</span>, <span class="number">32</span>)</span><br><span class="line">output = model(input)</span><br><span class="line">print(output.size())</span><br></pre></td></tr></table></figure></p>
<p>输出是：</p>
<blockquote>
<p>torch.Size([16, 6, 28, 28])</p>
</blockquote>
<h6 id="例子2"><a href="#例子2" class="headerlink" title="例子2"></a>例子2</h6><p>stride和padding<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"></span><br><span class="line">inputs = Variable(torch.randn(<span class="number">64</span>,<span class="number">3</span>,<span class="number">32</span>,<span class="number">32</span>))</span><br><span class="line"></span><br><span class="line">m1 = nn.Conv2d(<span class="number">3</span>,<span class="number">16</span>,<span class="number">3</span>)</span><br><span class="line">print(m1)</span><br><span class="line">output1 = m1(inputs)</span><br><span class="line">print(output1.size())</span><br><span class="line"></span><br><span class="line">m2 = nn.Conv2d(<span class="number">3</span>, <span class="number">16</span>, <span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">print(m2)</span><br><span class="line">output2 = m2(inputs)</span><br><span class="line">print(output2.size())</span><br></pre></td></tr></table></figure></p>
<p>输出</p>
<blockquote>
<p>Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))<br>torch.Size([64, 16, 30, 30])<br>Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))<br>torch.Size([64, 16, 32, 32])</p>
</blockquote>
<h3 id="Pooling-Layers"><a href="#Pooling-Layers" class="headerlink" title="Pooling Layers"></a>Pooling Layers</h3><h4 id="MaxPool2dd"><a href="#MaxPool2dd" class="headerlink" title="MaxPool2dd"></a>MaxPool2dd</h4><p>MaxPool2d这个layer stride默认是和kernel_size相同的<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torch import nn</span><br><span class="line">from torch.autograd import Variable</span><br><span class="line"></span><br><span class="line"># maxpool2d</span><br><span class="line"># class torch.nn.MaxPool2d(kernel_size,stride=None,padding=0,dilation=1,return_indices=False,ceil_mode=False)</span><br><span class="line"></span><br><span class="line">print(&quot;input:&quot;)</span><br><span class="line">input = Variable(torch.randn(30,20,32,32))</span><br><span class="line">print(input.size())</span><br><span class="line"></span><br><span class="line">m2 = nn.MaxPool2d(5)</span><br><span class="line">print(m2)</span><br><span class="line"></span><br><span class="line">for param in m2.parameters():</span><br><span class="line">  print(param)</span><br><span class="line"></span><br><span class="line">print(m2.state_dict().keys())</span><br><span class="line"></span><br><span class="line">output = m2(input)</span><br><span class="line">print(&quot;output:&quot;)</span><br><span class="line">print(output.size())</span><br></pre></td></tr></table></figure></p>
<p>输出</p>
<blockquote>
<p>input:<br>torch.Size([30, 20, 32, 32])<br>MaxPool2d (size=(5, 5), stride=(5, 5), dilation=(1, 1))<br>[]<br>output:<br>torch.Size([30, 20, 6, 6])</p>
</blockquote>
<h3 id="Padding-Layers"><a href="#Padding-Layers" class="headerlink" title="Padding Layers"></a>Padding Layers</h3><h3 id="Linear-layers"><a href="#Linear-layers" class="headerlink" title="Linear layers"></a>Linear layers</h3><h3 id="Dropout-layers"><a href="#Dropout-layers" class="headerlink" title="Dropout layers"></a>Dropout layers</h3><h4 id="Drop2D"><a href="#Drop2D" class="headerlink" title="Drop2D"></a>Drop2D</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line">m = nn.Dropout2d(<span class="number">0.3</span>)</span><br><span class="line">print(m)</span><br><span class="line">inputs = torch.randn(<span class="number">1</span>,<span class="number">28</span>,<span class="number">28</span>)</span><br><span class="line">outputs = m(inputs)</span><br><span class="line">print(outputs)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<blockquote>
<p>Dropout2d(p=0.3)<br>([[[ 0.8535,  1.0314,  2.7904,  1.2136,  2.7561, -2.0429,  0.0772,<br>     -1.9372, -0.0864, -1.4132, -0.1648,  0.2403,  0.5727,  0.8102,<br>      0.4544,  0.1414,  0.1547, -0.9266, -0.6033,  0.5813, -1.3541,<br>     -0.0536,  0.9574,  0.0554,  0.8368,  0.7633, -0.3377, -1.4293],<br>    [ 0.0000,  0.0000, -0.0000, -0.0000,  0.0000, -0.0000,  0.0000,<br>      0.0000,  0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000,<br>      0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000,<br>     -0.0000,  0.0000,  0.0000, -0.0000, -0.0000, -0.0000, -0.0000],<br>      …<br>    [ 0.6452, -0.6455,  0.2370,  0.1088, -0.5421, -0.5120, -2.2915,<br>      0.2061,  1.6384,  2.2276,  2.4022,  0.2033,  0.6984,  0.1254,<br>      1.1627,  1.0699, -2.1868,  1.1293, -0.7030,  0.0454, -1.5428,<br>      -2.4052, -0.3204, -1.5984,  0.1282,  0.2127, -2.3506, -2.2395]]])</p>
</blockquote>
<p>会发现输出的数组中有很多被置为$0$了。</p>
<h3 id="Loss-function"><a href="#Loss-function" class="headerlink" title="Loss function"></a>Loss function</h3><h2 id="torch-nn-functional"><a href="#torch-nn-functional" class="headerlink" title="torch.nn.functional"></a>torch.nn.functional</h2><h3 id="convoludion-functions"><a href="#convoludion-functions" class="headerlink" title="convoludion functions"></a>convoludion functions</h3><h4 id="conv2d"><a href="#conv2d" class="headerlink" title="conv2d"></a>conv2d</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torch.nn as nn</span><br><span class="line">import torch.nn.functional as F</span><br><span class="line">from torch.autograd import Variable</span><br><span class="line"></span><br><span class="line">inputs = Variable(torch.randn(64,3,32,32))</span><br><span class="line"></span><br><span class="line">filters1 = Variable(torch.randn(16,3,3,3))</span><br><span class="line">output1 = F.conv2d(inputs,filters1)</span><br><span class="line">print(output1.size())</span><br><span class="line"></span><br><span class="line">filters2 = Variable(torch.randn(16,3,3,3))</span><br><span class="line">output2 = F.conv2d(inputs,filters2,padding=1)</span><br><span class="line">print(output2.size())</span><br></pre></td></tr></table></figure>
<p>输出</p>
<blockquote>
<p>torch.Size([64, 16, 30, 30])<br>torch.Size([64, 16, 32, 32])</p>
</blockquote>
<h3 id="relu-functions"><a href="#relu-functions" class="headerlink" title="relu functions"></a>relu functions</h3><h3 id="pooling-functions"><a href="#pooling-functions" class="headerlink" title="pooling functions"></a>pooling functions</h3><h3 id="dropout-functions"><a href="#dropout-functions" class="headerlink" title="dropout functions"></a>dropout functions</h3><h4 id="例子-1"><a href="#例子-1" class="headerlink" title="例子"></a>例子</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"> </span><br><span class="line">x = torch.randn(<span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line">y = F.dropout(x, <span class="number">0.5</span>, <span class="keyword">True</span>)</span><br><span class="line">y = F.dropout2d(x, <span class="number">0.5</span>)</span><br><span class="line"> </span><br><span class="line">print(y)</span><br></pre></td></tr></table></figure>
<p>注意$9$中说的问题，不过可能已经被改正了，注意一些就是了。</p>
<h3 id="linear-functions"><a href="#linear-functions" class="headerlink" title="linear functions"></a>linear functions</h3><h3 id="loss-functions"><a href="#loss-functions" class="headerlink" title="loss functions"></a>loss functions</h3><h2 id="torch-optim"><a href="#torch-optim" class="headerlink" title="torch.optim"></a>torch.optim</h2><h2 id="torchvision"><a href="#torchvision" class="headerlink" title="torchvision"></a>torchvision</h2><h3 id="torchvision-datasets"><a href="#torchvision-datasets" class="headerlink" title="torchvision.datasets"></a>torchvision.datasets</h3><p>torchvision提供了很多数据集<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line">print(torchvision.datasets.__all__)</span><br></pre></td></tr></table></figure></p>
<blockquote>
<p>(‘LSUN’, ‘LSUNClass’, ‘ImageFolder’, ‘DatasetFolder’, ‘FakeData’, ‘CocoCaptions’,     ‘CocoDetection’, ‘CIFAR10’, ‘CIFAR100’, ‘EMNIST’, ‘FashionMNIST’, ‘MNIST’, ‘STL10’,     ‘SVHN’, ‘PhotoTour’, ‘SEMEION’, ‘Omniglot’)</p>
</blockquote>
<h3 id="CIFAR10"><a href="#CIFAR10" class="headerlink" title="CIFAR10"></a>CIFAR10</h3><h4 id="原型"><a href="#原型" class="headerlink" title="原型"></a>原型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">calss torchvision.datasets.CIFAR10(root, train=<span class="keyword">True</span>, transform=<span class="keyword">None</span>, target_transform=<span class="keyword">None</span>, download=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure>
<h4 id="参数-1"><a href="#参数-1" class="headerlink" title="参数"></a>参数</h4><p>root (string) – cifar-10-batches-py的存放目录或者download设置为True时将会存放的目录。<br>train (bool, optional) – 设置为True的时候, 从training set创建dataset, 否则从test set创建dataset.<br>transform (callable, optional) – 输入是一个 PIL image，返回一个transformed的版本。如，transforms.RandomCrop<br>target_transform (callable, optional) – A function/transform that takes in the target and transforms it.<br>download (bool, optional) – If true, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again.</p>
<h4 id="例子-2"><a href="#例子-2" class="headerlink" title="例子"></a>例子</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line">trainset = torchvision.datasets.CIFAR100(root=<span class="string">"./datasets"</span>, train=<span class="keyword">True</span>, transform=    <span class="keyword">None</span>, download=<span class="keyword">True</span>)</span><br><span class="line">testset = torchvision.datasets.CIFAR100(root=<span class="string">"./datasets"</span>, train=<span class="keyword">False</span>, transform=    <span class="keyword">None</span>, download=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure>
<h3 id="torchvision-models"><a href="#torchvision-models" class="headerlink" title="torchvision.models"></a>torchvision.models</h3><p>模型</p>
<h3 id="torchvision-transforms"><a href="#torchvision-transforms" class="headerlink" title="torchvision.transforms"></a>torchvision.transforms</h3><p>transform</p>
<h3 id="torchvision-utils"><a href="#torchvision-utils" class="headerlink" title="torchvision.utils"></a>torchvision.utils</h3><p>一些工具包</p>
<h2 id="torch-utils-data"><a href="#torch-utils-data" class="headerlink" title="torch.utils.data"></a>torch.utils.data</h2><h3 id="Dataloader"><a href="#Dataloader" class="headerlink" title="Dataloader"></a>Dataloader</h3><h4 id="原型-1"><a href="#原型-1" class="headerlink" title="原型"></a>原型</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">torch</span>.<span class="title">utils</span>.<span class="title">data</span>.<span class="title">DataLoader</span><span class="params">(dataset, batch_size=<span class="number">1</span>, shuffle=False, sampler=None, batch_sampler=None, num_workers=<span class="number">0</span>, collate_fn=&lt;function default_collate&gt;, pin_memory=False, drop_last=False, timeout=<span class="number">0</span>, worker_init_fn=None)</span></span></span><br></pre></td></tr></table></figure>
<h4 id="参数-2"><a href="#参数-2" class="headerlink" title="参数"></a>参数</h4><p>dataset (Dataset) – 从哪加载数据<br>batch_size (int, optional) – batch大小 (default: 1).<br>shuffle (bool, optional) – 每个epoch的数据是否打乱 (default: False).<br>sampler (Sampler, optional) – 定义采样策略。如果指定这个参数, shuffle必须是False.<br>batch_sampler (Sampler, optional) – like sampler, but returns a batch of indices at a time. Mutually exclusive with batch_size, shuffle, sampler, and drop_last.<br>num_workers (int, optional) – 多少个子进程用来进行数据加载。0代表使用主进程加载数据 (default: 0)<br>collate_fn (callable, optional) – merges a list of samples to form a mini-batch.<br>pin_memory (bool, optional) – If True, the data loader will copy tensors into CUDA pinned memory before returning them.<br>drop_last (bool, optional) – set to True to drop the last incomplete batch, if the dataset size is not divisible by the batch size. If False and the size of dataset is not divisible by the batch size, then the last batch will be smaller. (default: False)<br>timeout (numeric, optional) – if positive, the timeout value for collecting a batch from workers. Should always be non-negative. (default: 0)<br>worker_init_fn (callable, optional) – If not None, this will be called on each worker subprocess with the worker id (an int in [0, num_workers - 1]) as input, after seeding and before data loading. (default: None)</p>
<h4 id="例子-3"><a href="#例子-3" class="headerlink" title="例子"></a>例子</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line">dataset = torchvision.datasets.CIFAR100(root=<span class="string">'./data'</span>, train=<span class="keyword">True</span>, download=<span class="keyword">True</span>, transform=<span class="keyword">None</span>)</span><br><span class="line">train_loader = torch.utils.data.DataLoader(dataset, bath_size=<span class="number">16</span>, shuffle=<span class="keyword">False</span>, num_worker=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<h4 id="如何访问DataLoader返回值"><a href="#如何访问DataLoader返回值" class="headerlink" title="如何访问DataLoader返回值"></a>如何访问DataLoader返回值</h4><p>train_loader不是整数，所以不能用range，这里用enumerate()，i是<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i, data <span class="keyword">in</span> enumerate(train_loader):</span><br><span class="line">    images, labels = data</span><br></pre></td></tr></table></figure></p>
<h2 id="torch-distributed"><a href="#torch-distributed" class="headerlink" title="torch.distributed"></a>torch.distributed</h2><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>1.<a href="https://pytorch.org/docs/stable/torch.html" target="_blank" rel="noopener">https://pytorch.org/docs/stable/torch.html</a><br>2.<a href="https://pytorch.org/docs/stable/nn.html" target="_blank" rel="noopener">https://pytorch.org/docs/stable/nn.html</a><br>3.<a href="http://pytorch.org/tutorials/beginner/pytorch_with_examples.html" target="_blank" rel="noopener">http://pytorch.org/tutorials/beginner/pytorch_with_examples.html</a><br>4.<a href="https://discuss.pytorch.org/t/distributed-model-parallelism/10377" target="_blank" rel="noopener">https://discuss.pytorch.org/t/distributed-model-parallelism/10377</a><br>5.<a href="https://ptorch.com/news/40.html" target="_blank" rel="noopener">https://ptorch.com/news/40.html</a><br>6.<a href="https://discuss.pytorch.org/t/distributed-data-parallel-freezes-without-error-message/8009" target="_blank" rel="noopener">https://discuss.pytorch.org/t/distributed-data-parallel-freezes-without-error-message/8009</a><br>7.<a href="https://discuss.pytorch.org/t/runtimeerror-cudnn-status-arch-mismatch/3580" target="_blank" rel="noopener">https://discuss.pytorch.org/t/runtimeerror-cudnn-status-arch-mismatch/3580</a><br>8.<a href="https://discuss.pytorch.org/t/error-when-using-cudnn/577/7" target="_blank" rel="noopener">https://discuss.pytorch.org/t/error-when-using-cudnn/577/7</a><br>9.<a href="https://www.zhihu.com/question/67209417" target="_blank" rel="noopener">https://www.zhihu.com/question/67209417</a></p>

      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/机器学习/" rel="tag"># 机器学习</a>
          
            <a href="/tags/python/" rel="tag"># python</a>
          
            <a href="/tags/深度学习/" rel="tag"># 深度学习</a>
          
            <a href="/tags/pytorch/" rel="tag"># pytorch</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/03/13/dict-values-object-does-not-support-indexing/" rel="next" title="'dict_values' object does not support indexing">
                <i class="fa fa-chevron-left"></i> 'dict_values' object does not support indexing
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/03/13/CNN/" rel="prev" title="CNN">
                CNN <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>


  </div>


          </div>
          

  
    <div class="comments" id="comments">
      
        <div id="gitment-container"></div>
      
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/favicon.jpg" alt="马晓鑫爱马荟荟">
            
              <p class="site-author-name" itemprop="name">马晓鑫爱马荟荟</p>
              <p class="site-description motion-element" itemprop="description">记录硕士三年自己的积累</p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">62</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  <a href="/categories/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">9</span>
                    <span class="site-state-item-name">分类</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  <a href="/tags/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">103</span>
                    <span class="site-state-item-name">标签</span>
                  </a>
                </div>
              
            </nav>
          

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="https://github.com/mxxhcm" title="GitHub &rarr; https://github.com/mxxhcm" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="mailto:mxxhcm@gmail.com" title="E-Mail &rarr; mailto:mxxhcm@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                </span>
              
            </div>
          

          

          
          

          
            
          
          

        </div>
      </div>

      
      <!--noindex-->
        <div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#常见问题"><span class="nav-number">1.</span> <span class="nav-text">常见问题</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#RuntimeError-CUDNN-STATUS-ARCH-MISMATCH"><span class="nav-number">1.1.</span> <span class="nav-text">RuntimeError: CUDNN_STATUS_ARCH_MISMATCH</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#查看cuda版本"><span class="nav-number">1.2.</span> <span class="nav-text">查看cuda版本</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#神经网络参数初始化"><span class="nav-number">2.</span> <span class="nav-text">神经网络参数初始化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#方法-1-Model-apply-fn"><span class="nav-number">2.1.</span> <span class="nav-text">方法$1$.Model.apply(fn)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#torch"><span class="nav-number">3.</span> <span class="nav-text">torch</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#torch-cat"><span class="nav-number">3.1.</span> <span class="nav-text">torch.cat</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#函数原型"><span class="nav-number">3.1.1.</span> <span class="nav-text">函数原型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#参数"><span class="nav-number">3.1.2.</span> <span class="nav-text">参数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#例子"><span class="nav-number">3.1.3.</span> <span class="nav-text">例子</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#img"><span class="nav-number">3.2.</span> <span class="nav-text">img</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#torch-autograd-torch-autograd-Variable和torch-autograd-Function"><span class="nav-number">4.</span> <span class="nav-text">torch.autograd(torch.autograd.Variable和torch.autograd.Function)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Variable-class-torch-autograd-Variable"><span class="nav-number">4.1.</span> <span class="nav-text">Variable(class torch.autograd.Variable)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#声明一个tensor"><span class="nav-number">4.1.1.</span> <span class="nav-text">声明一个tensor</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#tensor的各种操作"><span class="nav-number">4.1.2.</span> <span class="nav-text">tensor的各种操作</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#加操作"><span class="nav-number">4.1.2.1.</span> <span class="nav-text">加操作</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#转置操作"><span class="nav-number">4.1.2.2.</span> <span class="nav-text">转置操作</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#求最大行和列"><span class="nav-number">4.1.2.3.</span> <span class="nav-text">求最大行和列</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#和relu功能比较类似。"><span class="nav-number">4.1.2.4.</span> <span class="nav-text">和relu功能比较类似。</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#tensor和numpy转化"><span class="nav-number">4.1.3.</span> <span class="nav-text">tensor和numpy转化</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#convert-tensor-to-numpy"><span class="nav-number">4.1.3.1.</span> <span class="nav-text">convert tensor to numpy</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#convert-numpy-to-tensor"><span class="nav-number">4.1.3.2.</span> <span class="nav-text">convert numpy to tensor</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Variable和Tensor"><span class="nav-number">4.1.4.</span> <span class="nav-text">Variable和Tensor</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#属性"><span class="nav-number">4.1.4.1.</span> <span class="nav-text">属性</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#parameters"><span class="nav-number">4.1.4.2.</span> <span class="nav-text">parameters</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#backward"><span class="nav-number">4.1.4.3.</span> <span class="nav-text">backward</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Function-class-torch-autograd-Funtion"><span class="nav-number">4.2.</span> <span class="nav-text">Function(class torch.autograd.Funtion)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#用法"><span class="nav-number">4.2.1.</span> <span class="nav-text">用法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#functions"><span class="nav-number">4.2.2.</span> <span class="nav-text">functions</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#torch-nn"><span class="nav-number">5.</span> <span class="nav-text">torch.nn</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Container"><span class="nav-number">5.1.</span> <span class="nav-text">Container</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Module"><span class="nav-number">5.1.1.</span> <span class="nav-text">Module</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Sequential"><span class="nav-number">5.1.2.</span> <span class="nav-text">Sequential</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Convolution-Layers"><span class="nav-number">5.2.</span> <span class="nav-text">Convolution Layers</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#torch-nn-Conv2d"><span class="nav-number">5.2.1.</span> <span class="nav-text">torch.nn.Conv2d</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#类声明"><span class="nav-number">5.2.1.1.</span> <span class="nav-text">类声明</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#参数声明"><span class="nav-number">5.2.1.2.</span> <span class="nav-text">参数声明</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#示例"><span class="nav-number">5.2.1.3.</span> <span class="nav-text">示例</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#例子1"><span class="nav-number">5.2.1.3.1.</span> <span class="nav-text">例子1</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#例子2"><span class="nav-number">5.2.1.3.2.</span> <span class="nav-text">例子2</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Pooling-Layers"><span class="nav-number">5.3.</span> <span class="nav-text">Pooling Layers</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#MaxPool2dd"><span class="nav-number">5.3.1.</span> <span class="nav-text">MaxPool2dd</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Padding-Layers"><span class="nav-number">5.4.</span> <span class="nav-text">Padding Layers</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Linear-layers"><span class="nav-number">5.5.</span> <span class="nav-text">Linear layers</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Dropout-layers"><span class="nav-number">5.6.</span> <span class="nav-text">Dropout layers</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Drop2D"><span class="nav-number">5.6.1.</span> <span class="nav-text">Drop2D</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Loss-function"><span class="nav-number">5.7.</span> <span class="nav-text">Loss function</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#torch-nn-functional"><span class="nav-number">6.</span> <span class="nav-text">torch.nn.functional</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#convoludion-functions"><span class="nav-number">6.1.</span> <span class="nav-text">convoludion functions</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#conv2d"><span class="nav-number">6.1.1.</span> <span class="nav-text">conv2d</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#relu-functions"><span class="nav-number">6.2.</span> <span class="nav-text">relu functions</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#pooling-functions"><span class="nav-number">6.3.</span> <span class="nav-text">pooling functions</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#dropout-functions"><span class="nav-number">6.4.</span> <span class="nav-text">dropout functions</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#例子-1"><span class="nav-number">6.4.1.</span> <span class="nav-text">例子</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#linear-functions"><span class="nav-number">6.5.</span> <span class="nav-text">linear functions</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#loss-functions"><span class="nav-number">6.6.</span> <span class="nav-text">loss functions</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#torch-optim"><span class="nav-number">7.</span> <span class="nav-text">torch.optim</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#torchvision"><span class="nav-number">8.</span> <span class="nav-text">torchvision</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#torchvision-datasets"><span class="nav-number">8.1.</span> <span class="nav-text">torchvision.datasets</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CIFAR10"><span class="nav-number">8.2.</span> <span class="nav-text">CIFAR10</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#原型"><span class="nav-number">8.2.1.</span> <span class="nav-text">原型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#参数-1"><span class="nav-number">8.2.2.</span> <span class="nav-text">参数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#例子-2"><span class="nav-number">8.2.3.</span> <span class="nav-text">例子</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#torchvision-models"><span class="nav-number">8.3.</span> <span class="nav-text">torchvision.models</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#torchvision-transforms"><span class="nav-number">8.4.</span> <span class="nav-text">torchvision.transforms</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#torchvision-utils"><span class="nav-number">8.5.</span> <span class="nav-text">torchvision.utils</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#torch-utils-data"><span class="nav-number">9.</span> <span class="nav-text">torch.utils.data</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Dataloader"><span class="nav-number">9.1.</span> <span class="nav-text">Dataloader</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#原型-1"><span class="nav-number">9.1.1.</span> <span class="nav-text">原型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#参数-2"><span class="nav-number">9.1.2.</span> <span class="nav-text">参数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#例子-3"><span class="nav-number">9.1.3.</span> <span class="nav-text">例子</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#如何访问DataLoader返回值"><span class="nav-number">9.1.4.</span> <span class="nav-text">如何访问DataLoader返回值</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#torch-distributed"><span class="nav-number">10.</span> <span class="nav-text">torch.distributed</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#参考文献"><span class="nav-number">11.</span> <span class="nav-text">参考文献</span></a></li></ol></div>
            

          </div>
        </div>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">马晓鑫爱马荟荟</span>

  

  
</div>


  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v3.8.0</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> v6.6.0</div>




        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  
    <script src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  


  


  <script src="/js/src/utils.js?v=6.6.0"></script>

  <script src="/js/src/motion.js?v=6.6.0"></script>



  
  


  <script src="/js/src/affix.js?v=6.6.0"></script>

  <script src="/js/src/schemes/pisces.js?v=6.6.0"></script>



  
  <script src="/js/src/scrollspy.js?v=6.6.0"></script>
<script src="/js/src/post-details.js?v=6.6.0"></script>



  


  <script src="/js/src/bootstrap.js?v=6.6.0"></script>



  



  






<!-- LOCAL: You can save these files to your site and update links -->
    
        
        <link rel="stylesheet" href="https://aimingoo.github.io/gitmint/style/default.css">
        <script src="https://aimingoo.github.io/gitmint/dist/gitmint.browser.js"></script>
    
<!-- END LOCAL -->

    

    
      <script>
      function renderGitment(){
        var gitment = new Gitmint({
            id: window.location.pathname,
            owner: 'mxxhcm',
            repo: 'mxxhcm.github.io',
            
            lang: "" || navigator.language || navigator.systemLanguage || navigator.userLanguage,
            
            oauth: {
            
            
                client_secret: '9a33d210d29f526a771d47bff6940b5798b2631f',
            
                client_id: '61a64228cba52787dea0'
            }});
        gitment.render('gitment-container');
      }

      
      renderGitment();
      
      </script>
    







  





  

  

  

  

  
  

  
  

  


  
  

  

  

  

  

  

  

</body>
</html>
